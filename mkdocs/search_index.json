{
    "docs": [
        {
            "location": "/",
            "text": "DynaML is a Scala environment for conducting research and education in Machine Learning. DynaML comes packaged with a powerful library of classes for various predictive models and a Scala REPL where one can not only build custom models but also play around with data work-flows.\n\n\n\n\nHello World\n\u00b6\n\n\nRefer to the \ninstallation\n guide for getting up and running. The \ndata/\n directory contains data sets, which are used by the programs in the \ndynaml-examples/\n module. Lets run a Gaussian Process (GP) regression model on the synthetic 'delve' data set.\n\n\n1\nTestGPDelve\n(\n\"RBF\"\n,\n \n2.0\n,\n \n1.0\n,\n \n500\n,\n \n1000\n)\n\n\n\n\n\n\n\n\n\nIn this example \nTestGPDelve\n we train a GP model based on the RBF Kernel with its bandwidth/length scale set to \n2.0\n and the noise level set to \n1.0\n, we use 500 input output patterns to train and test on an independent sample of 1000 data points. Apart from printing a bunch of evaluation metrics in the console DynaML also generates Javascript plots using Wisp in the browser.",
            "title": "DynaML"
        },
        {
            "location": "/#hello-world",
            "text": "Refer to the  installation  guide for getting up and running. The  data/  directory contains data sets, which are used by the programs in the  dynaml-examples/  module. Lets run a Gaussian Process (GP) regression model on the synthetic 'delve' data set.  1 TestGPDelve ( \"RBF\" ,   2.0 ,   1.0 ,   500 ,   1000 )     In this example  TestGPDelve  we train a GP model based on the RBF Kernel with its bandwidth/length scale set to  2.0  and the noise level set to  1.0 , we use 500 input output patterns to train and test on an independent sample of 1000 data points. Apart from printing a bunch of evaluation metrics in the console DynaML also generates Javascript plots using Wisp in the browser.",
            "title": "Hello World"
        },
        {
            "location": "/installation/installation/",
            "text": "Platform Compatibility\n\u00b6\n\n\nCurrently DynaML installs and runs on *nix platforms, though it is possible to build the project on windows, running the generated .bat file might not work and one would need to resort to using the \njava -jar\n command.\n\n\nPre-requisites\n\u00b6\n\n\n\n\nsbt\n\n\nA modern HTML5 enabled browser (to view plots generated by Wisp)\n\n\nBLAS, LAPACK and ARPACK binaries for your platform. In case they are not installed, it is possible to disable this feature by commenting out (\n//\n) the section of the build.sbt file given below.\n\n\n\n\n1\n  \n\"org.scalanlp\"\n \n%\n \n\"breeze-natives_2.11\"\n \n%\n \n\"0.11.2\"\n \n%\n \n\"compile\"\n,\n\n\n\n\n\n\n\nSteps\n\u00b6\n\n\n\n\nClone this repository\n\n\n\n\nRun the following.\n\n\n1\nsbt\n\n\n\n\n\n\nThe sbt shell will open\n\n\n1\n2\n3\n[\ninfo\n]\n Loading project definition from ~/DynaML/project\n\n[\ninfo\n]\n Set current project to DynaML\n>\n\n\n\n\n\n\n\n\n\n\nBuild the source\n\n\n1\nstage\n\n\n\n\n\n\n\n\n\n\nAfter the project builds, exit from the sbt console and execute the DynaML start script from the bash shell. Make sure you have execute permissions on the DynaML start script.\n\n\n1\n./target/universal/stage/bin/dynaml\n\n\n\n\n\n\nYou will get the following prompt.\n\n\n1\n2\n3\n4\nWelcome\n \nto\n \nDynaML\n \nv1\n.\n4.1\n-\nbeta\n.\n3\n\n\nInteractive\n \nScala\n \nshell\n \nfor\n \nMachine\n \nLearning\n \nResearch\n\n\n(\nScala\n \n2.11\n.\n8\n \nJava\n \n1.8\n.\n0\n_101\n)\n\n\nDynaML\n>",
            "title": "Installation"
        },
        {
            "location": "/installation/installation/#platform-compatibility",
            "text": "Currently DynaML installs and runs on *nix platforms, though it is possible to build the project on windows, running the generated .bat file might not work and one would need to resort to using the  java -jar  command.",
            "title": "Platform Compatibility"
        },
        {
            "location": "/installation/installation/#pre-requisites",
            "text": "sbt  A modern HTML5 enabled browser (to view plots generated by Wisp)  BLAS, LAPACK and ARPACK binaries for your platform. In case they are not installed, it is possible to disable this feature by commenting out ( // ) the section of the build.sbt file given below.   1    \"org.scalanlp\"   %   \"breeze-natives_2.11\"   %   \"0.11.2\"   %   \"compile\" ,",
            "title": "Pre-requisites"
        },
        {
            "location": "/installation/installation/#steps",
            "text": "Clone this repository   Run the following.  1 sbt   The sbt shell will open  1\n2\n3 [ info ]  Loading project definition from ~/DynaML/project [ info ]  Set current project to DynaML\n>     Build the source  1 stage     After the project builds, exit from the sbt console and execute the DynaML start script from the bash shell. Make sure you have execute permissions on the DynaML start script.  1 ./target/universal/stage/bin/dynaml   You will get the following prompt.  1\n2\n3\n4 Welcome   to   DynaML   v1 . 4.1 - beta . 3  Interactive   Scala   shell   for   Machine   Learning   Research  ( Scala   2.11 . 8   Java   1.8 . 0 _101 )  DynaML >",
            "title": "Steps"
        },
        {
            "location": "/installation/include/",
            "text": "Maven\n\u00b6\n\n\nTo include DynaML in your maven JVM project edit your \npom.xml\n file as follows\n\n\n1\n2\n3\n4\n5\n6\n<repositories>\n\n   \n<repository>\n\n       \n<id>\njitpack.io\n</id>\n\n       \n<url>\nhttps://jitpack.io\n</url>\n\n     \n</repository>\n\n\n</repositories>\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n<dependency>\n\n    \n<groupId>\ncom.github.transcendent-ai-labs\n</groupId>\n\n    \n<artifactId>\nDynaML\n</artifactId>\n\n    \n<version>\nv1.4\n</version>\n\n\n</dependency>\n\n\n\n\n\n\n\nSBT\n\u00b6\n\n\nFor sbt projects edit your \nbuild.sbt\n (see \nJitPack\n for more details)\n\n\n1\n2\n    \nresolvers\n \n+=\n \n\"jitpack\"\n \nat\n \n\"https://jitpack.io\"\n\n    \nlibraryDependencies\n \n+=\n \n\"com.github.transcendent-ai-labs\"\n \n%\n \n\"DynaML\"\n \n%\n \nversion\n\n\n\n\n\n\n\nGradle\n\u00b6\n\n\nIn your gradle project, add the following to the root \nbuild.gradle\n as follows\n\n\n1\n2\n3\n4\n5\n6\nallprojects\n \n{\n\n  \nrepositories\n \n{\n\n    \n...\n\n    \nmaven\n \n{\n \nurl\n \n\"https://jitpack.io\"\n \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\n\nand then add the dependency like\n\n\n1\n2\n3\ndependencies\n \n{\n\n    \ncompile\n \n'com.github.User:Repo:Tag'\n\n\n}\n\n\n\n\n\n\n\nLeinengen\n\u00b6\n\n\nIn \nproject.clj\n\n\n1\n:repositories\n \n[[\n\"jitpack\"\n \n\"https://jitpack.io\"\n]]\n\n\n\n\n\n\n\n1\n:dependencies\n \n[[\ncom.github.User/Repo\n \n\"Tag\"\n]]",
            "title": "Import"
        },
        {
            "location": "/installation/include/#maven",
            "text": "To include DynaML in your maven JVM project edit your  pom.xml  file as follows  1\n2\n3\n4\n5\n6 <repositories> \n    <repository> \n        <id> jitpack.io </id> \n        <url> https://jitpack.io </url> \n      </repository>  </repositories>    1\n2\n3\n4\n5 <dependency> \n     <groupId> com.github.transcendent-ai-labs </groupId> \n     <artifactId> DynaML </artifactId> \n     <version> v1.4 </version>  </dependency>",
            "title": "Maven"
        },
        {
            "location": "/installation/include/#sbt",
            "text": "For sbt projects edit your  build.sbt  (see  JitPack  for more details)  1\n2      resolvers   +=   \"jitpack\"   at   \"https://jitpack.io\" \n     libraryDependencies   +=   \"com.github.transcendent-ai-labs\"   %   \"DynaML\"   %   version",
            "title": "SBT"
        },
        {
            "location": "/installation/include/#gradle",
            "text": "In your gradle project, add the following to the root  build.gradle  as follows  1\n2\n3\n4\n5\n6 allprojects   { \n   repositories   { \n     ... \n     maven   {   url   \"https://jitpack.io\"   } \n   }  }    and then add the dependency like  1\n2\n3 dependencies   { \n     compile   'com.github.User:Repo:Tag'  }",
            "title": "Gradle"
        },
        {
            "location": "/installation/include/#leinengen",
            "text": "In  project.clj  1 :repositories   [[ \"jitpack\"   \"https://jitpack.io\" ]]    1 :dependencies   [[ com.github.User/Repo   \"Tag\" ]]",
            "title": "Leinengen"
        },
        {
            "location": "/supported_features/",
            "text": "Summary\n\n\n\"If you're not sure whether DynaML fits your requirements, this list provides a semi-comprehensive overview of available features.\"\n\n\n\n\nModels\n\u00b6\n\n\n\n\n\n\n\n\nModel Family\n\n\nSupported\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Models\n\n\nYes\n\n\nSupports regularized least squares based models for regression as well as logistic and probit models for classification.\n\n\n\n\n\n\nGeneralized Least Squares Models\n\n\nYes\n\n\n-\n\n\n\n\n\n\nLeast Squares Support Vector Machines\n\n\nYes\n\n\nContains implementation of dual LS-SVM applied to classification and regression.\n\n\n\n\n\n\nGaussian Processes\n\n\nYes\n\n\nSupports gaussian process inference models for regression and binary classification; the binary classification GP implementation uses the Laplace approximation for posterior mode computation. For regression problems, there are also multi-output and multi-task GP implementations.\n\n\n\n\n\n\nStudent T Processes\n\n\nYes\n\n\nSupports student T process inference models for regression, there are also multi-output and multi-task STP implementations.\n\n\n\n\n\n\nSkew Gaussian Processes\n\n\nYes\n\n\nSupports extended skew gaussian process inference models for regression.\n\n\n\n\n\n\nFeed forward Neural Networks\n\n\nYes\n\n\nCan build and learn feedforward neural nets of various sizes.\n\n\n\n\n\n\nCommittee/Meta Models\n\n\nYes\n\n\nSupports creation of gating networks or committee models.\n\n\n\n\n\n\n\n\nOptimizers & Solvers\n\u00b6\n\n\nParametric Solvers\n\u00b6\n\n\n\n\n\n\n\n\nSolver\n\n\nSupported\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nRegularized Least Squares\n\n\nYes\n\n\nSolves the \nTikhonov Regularization\n problem exactly (not suitable for large data sets)\n\n\n\n\n\n\nGradient Descent\n\n\nYes\n\n\nStochastic and batch gradient descent is implemented.\n\n\n\n\n\n\nQuasi-Newton BFGS\n\n\nYes\n\n\nSecond order convex optimization (using Hessian).\n\n\n\n\n\n\nConjugate Gradient\n\n\nYes\n\n\nSupports solving of linear systems of type \n\\(A.x = b\\)\n where \n\\(A\\)\n is a symmetric positive definite matrix.\n\n\n\n\n\n\nCommittee Model Solver\n\n\nYes\n\n\nSolves any committee based model to calculate member model coefficients or confidences.\n\n\n\n\n\n\nBack-propagation\n\n\nYes\n\n\nLeast squares based back-propagation with momentum and regularization.\n\n\n\n\n\n\n\n\nGlobal Optimization Solvers\n\u00b6\n\n\n\n\n\n\n\n\nSolver\n\n\nSupported\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nGrid Search\n\n\nYes\n\n\nSimple search over a grid of configurations.\n\n\n\n\n\n\nCoupled Simulated Annealing\n\n\nYes\n\n\nSupports vanilla (simulated annealing) along with variants of CSA such as CSA with variance (temperature) control.\n\n\n\n\n\n\nML-II\n\n\nYes\n\n\nGradient based optimization of log marginal likelihood in Gaussian Process regression models.",
            "title": "Supported Features"
        },
        {
            "location": "/supported_features/#models",
            "text": "Model Family  Supported  Notes      Generalized Linear Models  Yes  Supports regularized least squares based models for regression as well as logistic and probit models for classification.    Generalized Least Squares Models  Yes  -    Least Squares Support Vector Machines  Yes  Contains implementation of dual LS-SVM applied to classification and regression.    Gaussian Processes  Yes  Supports gaussian process inference models for regression and binary classification; the binary classification GP implementation uses the Laplace approximation for posterior mode computation. For regression problems, there are also multi-output and multi-task GP implementations.    Student T Processes  Yes  Supports student T process inference models for regression, there are also multi-output and multi-task STP implementations.    Skew Gaussian Processes  Yes  Supports extended skew gaussian process inference models for regression.    Feed forward Neural Networks  Yes  Can build and learn feedforward neural nets of various sizes.    Committee/Meta Models  Yes  Supports creation of gating networks or committee models.",
            "title": "Models"
        },
        {
            "location": "/supported_features/#optimizers-solvers",
            "text": "",
            "title": "Optimizers &amp; Solvers"
        },
        {
            "location": "/supported_features/#parametric-solvers",
            "text": "Solver  Supported  Notes      Regularized Least Squares  Yes  Solves the  Tikhonov Regularization  problem exactly (not suitable for large data sets)    Gradient Descent  Yes  Stochastic and batch gradient descent is implemented.    Quasi-Newton BFGS  Yes  Second order convex optimization (using Hessian).    Conjugate Gradient  Yes  Supports solving of linear systems of type  \\(A.x = b\\)  where  \\(A\\)  is a symmetric positive definite matrix.    Committee Model Solver  Yes  Solves any committee based model to calculate member model coefficients or confidences.    Back-propagation  Yes  Least squares based back-propagation with momentum and regularization.",
            "title": "Parametric Solvers"
        },
        {
            "location": "/supported_features/#global-optimization-solvers",
            "text": "Solver  Supported  Notes      Grid Search  Yes  Simple search over a grid of configurations.    Coupled Simulated Annealing  Yes  Supports vanilla (simulated annealing) along with variants of CSA such as CSA with variance (temperature) control.    ML-II  Yes  Gradient based optimization of log marginal likelihood in Gaussian Process regression models.",
            "title": "Global Optimization Solvers"
        },
        {
            "location": "/structure/",
            "text": "Motivation\n\u00b6\n\n\nDynaML was born out of the need to have a performant, extensible and easy to use Machine Learning research environment. Scala was a natural choice for these requirements due to its sprawling data science ecosystem (i.e. \nApache Spark\n), its functional object-oriented duality and its interoperability with the Java Virtual Machine.\n\n\nThe DynaML distribution is divided into four principal modules.\n\n\nModules\n\u00b6\n\n\nCore\n\u00b6\n\n\nThe heart of the DynaML distribution is the \ndynaml-core\n module.\n\n\nThe \ncore\n api consists of :\n\n\n\n\nModel implementations\n\n\nParametric Models\n\n\nStochastic Process Models\n\n\n\n\n\n\nOptimization solvers\n\n\nProbability distributions/random variables\n\n\nKernel functions for Non parametric models\n\n\n\n\nData workflows & Pipes\n\u00b6\n\n\nThe \ndynaml-pipes\n module provides an API for creating modular data processing workflows.\n\n\nThe \npipes\n module aims to separate model pre-processing tasks such as cleaning data files, replacing missing or corrupt records, applying transformations on data etc.\n\n\n\n\nAbility to create arbitrary workflows from scala functions and join them\n\n\nFeature transformations such as wavelet transform, gaussian scaling, auto-encoders etc\n\n\n\n\nDynaML REPL\n\u00b6\n\n\nThe \nread evaluate print loop\n (REPL) gives the user the ability to experiment with the data pre-processing and model building process in a mix and match fashion. The DynaML shell is based on the \nAmmonite\n project which is an augmented Scala REPL, all the features of the Ammonite REPL are a part of the DynaML REPL.\n\n\nDynaML Examples\n\u00b6\n\n\nThe module \ndynaml-examples\n contains programs which build regression and classification models on various data sets. These examples serve as case studies as well as instructional material to show the capabilities of DynaML in a hands on manner. Click \nhere\n to get started with the examples.\n\n\nLibraries Used\n\u00b6\n\n\nDynaML leverages a number of open source projects and builds on their useful features.\n\n\n\n\nBreeze\n for linear algebra operations with vectors, matrices etc.\n\n\nGremlin\n for building graphs in Neural network based models.\n\n\nSpire\n for creating algebraic entities like Fields, Groups etc.\n\n\nAmmonite\n for the shell environment.\n\n\nDynaML uses the newly minted \nWisp\n plotting library to generate aesthetic charts of common model validation metrics. There is also support for the  \nJZY3D\n scientific plotting library.",
            "title": "Structure"
        },
        {
            "location": "/structure/#motivation",
            "text": "DynaML was born out of the need to have a performant, extensible and easy to use Machine Learning research environment. Scala was a natural choice for these requirements due to its sprawling data science ecosystem (i.e.  Apache Spark ), its functional object-oriented duality and its interoperability with the Java Virtual Machine.  The DynaML distribution is divided into four principal modules.",
            "title": "Motivation"
        },
        {
            "location": "/structure/#modules",
            "text": "",
            "title": "Modules"
        },
        {
            "location": "/structure/#core",
            "text": "The heart of the DynaML distribution is the  dynaml-core  module.  The  core  api consists of :   Model implementations  Parametric Models  Stochastic Process Models    Optimization solvers  Probability distributions/random variables  Kernel functions for Non parametric models",
            "title": "Core"
        },
        {
            "location": "/structure/#data-workflows-pipes",
            "text": "The  dynaml-pipes  module provides an API for creating modular data processing workflows.  The  pipes  module aims to separate model pre-processing tasks such as cleaning data files, replacing missing or corrupt records, applying transformations on data etc.   Ability to create arbitrary workflows from scala functions and join them  Feature transformations such as wavelet transform, gaussian scaling, auto-encoders etc",
            "title": "Data workflows &amp; Pipes"
        },
        {
            "location": "/structure/#dynaml-repl",
            "text": "The  read evaluate print loop  (REPL) gives the user the ability to experiment with the data pre-processing and model building process in a mix and match fashion. The DynaML shell is based on the  Ammonite  project which is an augmented Scala REPL, all the features of the Ammonite REPL are a part of the DynaML REPL.",
            "title": "DynaML REPL"
        },
        {
            "location": "/structure/#dynaml-examples",
            "text": "The module  dynaml-examples  contains programs which build regression and classification models on various data sets. These examples serve as case studies as well as instructional material to show the capabilities of DynaML in a hands on manner. Click  here  to get started with the examples.",
            "title": "DynaML Examples"
        },
        {
            "location": "/structure/#libraries-used",
            "text": "DynaML leverages a number of open source projects and builds on their useful features.   Breeze  for linear algebra operations with vectors, matrices etc.  Gremlin  for building graphs in Neural network based models.  Spire  for creating algebraic entities like Fields, Groups etc.  Ammonite  for the shell environment.  DynaML uses the newly minted  Wisp  plotting library to generate aesthetic charts of common model validation metrics. There is also support for the   JZY3D  scientific plotting library.",
            "title": "Libraries Used"
        },
        {
            "location": "/releases/mydoc_release_notes_142/",
            "text": "Version 1.4.2 of DynaML, released May 7, 2017. Updates, improvements and new features.\n\n\n\n\nCore API\n\u00b6\n\n\nAdditions\n\u00b6\n\n\nPackage\n \ndynaml\n.\nmodels\n.\nneuralnets\n\n\n\n\nAdded \nGenericAutoEncoder\n[\nLayerP\n, \nI\n]\n, the class \nAutoEncoder\n is now \ndeprecated\n\n\nAdded \nGenericNeuralStack\n[\nP\n, \nI\n, \nT\n]\n as a base class for Neural Stack API\n\n\nAdded \nLazyNeuralStack\n[\nP\n, \nI\n]\n where the layers are lazily spawned.\n\n\n\n\nPackage\n \ndynaml\n.\nkernels\n\n\n\n\nAdded \nScaledKernel\n[\nI\n]\n representing kernels of scaled Gaussian Processes.\n\n\n\n\nPackage\n \ndynaml\n.\nmodels\n.\nbayes\n\n\n\n\nAdded \n*\n method to \nGaussianProcessPrior\n[\nI\n, \nM\n]\n which creates a scaled Gaussian Process prior using the newly minted \nScaledKernel\n[\nI\n]\n class\n\n\nAdded Kronecker product GP priors with the \nCoRegGPPrior\n[\nI\n, \nJ\n, \nM\n]\n class\n\n\n\n\nPackage\n \ndynaml\n.\nmodels\n.\nstp\n\n\n\n\nAdded multi-output Students' T Regression model of \nConti & O' Hagan\n in class \nMVStudentsTModel\n\n\n\n\nPackage\n \ndynaml\n.\nprobability\n.\ndistributions\n\n\n\n\nAdded \nHasErrorBars\n[\nT\n]\n generic trait representing distributions which can generate confidence intervals around their mean value.\n\n\n\n\nImprovements\n\u00b6\n\n\nPackage\n \ndynaml\n.\nprobability\n\n\n\n\nFixed issue with creation of \nMeasurableFunction\n instances from \nRandomVariable\n instances\n\n\n\n\nPackage\n \ndynaml\n.\nprobability\n.\ndistributions\n\n\n\n\nChanged error bar calculations and sampling of Students T distributions (vector and matrix) and Matrix Normal distribution.\n\n\n\n\nPackage\n \ndynaml\n.\nmodels\n.\ngp\n\n\n\n\nAdded \nKronecker\n structure speed up to \nenergy\n (marginal likelihood) calculation of multi-output GP models\n\n\n\n\nPackage\n \ndynaml\n.\nkernels\n\n  - Improved implicit paramterization of Matern Covariance classes\n\n\nGeneral\n\n\n\n\nUpdated breeze version to latest.\n\n\nUpdated Ammonite version to latest",
            "title": "v1.4.2"
        },
        {
            "location": "/releases/mydoc_release_notes_142/#core-api",
            "text": "",
            "title": "Core API"
        },
        {
            "location": "/releases/mydoc_release_notes_142/#additions",
            "text": "Package   dynaml . models . neuralnets   Added  GenericAutoEncoder [ LayerP ,  I ] , the class  AutoEncoder  is now  deprecated  Added  GenericNeuralStack [ P ,  I ,  T ]  as a base class for Neural Stack API  Added  LazyNeuralStack [ P ,  I ]  where the layers are lazily spawned.   Package   dynaml . kernels   Added  ScaledKernel [ I ]  representing kernels of scaled Gaussian Processes.   Package   dynaml . models . bayes   Added  *  method to  GaussianProcessPrior [ I ,  M ]  which creates a scaled Gaussian Process prior using the newly minted  ScaledKernel [ I ]  class  Added Kronecker product GP priors with the  CoRegGPPrior [ I ,  J ,  M ]  class   Package   dynaml . models . stp   Added multi-output Students' T Regression model of  Conti & O' Hagan  in class  MVStudentsTModel   Package   dynaml . probability . distributions   Added  HasErrorBars [ T ]  generic trait representing distributions which can generate confidence intervals around their mean value.",
            "title": "Additions"
        },
        {
            "location": "/releases/mydoc_release_notes_142/#improvements",
            "text": "Package   dynaml . probability   Fixed issue with creation of  MeasurableFunction  instances from  RandomVariable  instances   Package   dynaml . probability . distributions   Changed error bar calculations and sampling of Students T distributions (vector and matrix) and Matrix Normal distribution.   Package   dynaml . models . gp   Added  Kronecker  structure speed up to  energy  (marginal likelihood) calculation of multi-output GP models   Package   dynaml . kernels \n  - Improved implicit paramterization of Matern Covariance classes  General   Updated breeze version to latest.  Updated Ammonite version to latest",
            "title": "Improvements"
        },
        {
            "location": "/releases/mydoc_release_notes_141/",
            "text": "Version 1.4.1 of DynaML, released March 26, 2017, implements a number of new models (Extended Skew GP, student T process, generalized least squares, etc) and features.\n\n\n\n\nPipes API\n\u00b6\n\n\nAdditions\n\u00b6\n\n\nThe pipes API has been vastly extended by creating pipes which encapsulate functions of multiple arguments leading to the following end points.\n\n\n\n\nDataPipe2\n[\nA\n, \nB\n, \nC\n]\n: Pipe which takes 2 arguments\n\n\nDataPipe3\n[\nA\n, \nB\n, \nC\n, \nD\n]\n : Pipe which takes 3 arguments\n\n\nDataPipe4\n[\nA\n, \nB\n, \nC\n, \nD\n, \nE\n]\n: Pipe which takes 4 arguments\n\n\n\n\nFurthermore there is now the ability to create pipes which return pipes, something akin to curried functions in functional programming.\n\n\n\n\nMetaPipe\n: Takes an argument returns a \nDataPipe\n\n\nMetaPipe21\n: Takes 2 arguments returns a \nDataPipe\n\n\nMetaPipe12\n: Takes an argument returns a \nDataPipe2\n\n\n\n\nA new kind of Stream data pipe, \nStreamFlatMapPipe\n is added to represent data pipelines which can perform flat map like operations on streams.\n\n\n1\n2\nval\n \nmapFunc\n:\n \n(\nI\n)\n \n=>\n \nStream\n[\nJ\n]\n \n=\n \n...\n\n\nval\n \nstreamFMPipe\n \n=\n \nStreamFlatMapPipe\n(\nmapFunc\n)\n\n\n\n\n\n\n\n\n\nAdded Data Pipes API for Apache Spark RDDs.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nval\n \nnum\n \n=\n \n20\n\n\nval\n \nnumbers\n \n=\n \nsc\n.\nparallelize\n(\n1\n \nto\n \nnum\n)\n\n\nval\n \nconvPipe\n \n=\n \nRDDPipe\n((\nn\n:\n \nInt\n)\n \n=>\n \nn\n.\ntoDouble\n)\n\n\n\nval\n \nsqPipe\n \n=\n \nRDDPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n*\nx\n)\n\n\n\nval\n \nsqrtPipe\n \n=\n \nRDDPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsqrt\n(\nx\n))\n\n\n\nval\n \nresultPipe\n \n=\n \nRDDPipe\n((\nr\n:\n \nRDD\n[\nDouble\n])\n \n=>\n \nr\n.\nreduce\n(\n_\n+\n_\n).\ntoInt\n)\n\n\n\nval\n \nnetPipeline\n \n=\n \nconvPipe\n \n>\n \nsqPipe\n \n>\n \nsqrtPipe\n \n>\n \nresultPipe\n\n\nnetPipeline\n(\nnumbers\n)\n\n\n\n\n\n\n\n\n\nAdded \nUnivariateGaussianScaler\n class for gaussian scaling of univariate data.\n\n\n\n\nCore API\n\u00b6\n\n\nAdditions\n\u00b6\n\n\nPackage\n \ndynaml\n.\nmodels\n.\nbayes\n\n\nThis new package will house stochastic prior models, currently there is support for GP and Skew GP priors, to see a starting example see \nstochasticPriors.sc\n in the \nscripts\n directory of the DynaML source.\n\n\n\n\nPackage\n \ndynaml\n.\nkernels\n\n\n\n\nAdded \nevaluateAt\n(\nh\n)(\nx\n,\ny\n)\n and \ngradientAt\n(\nh\n)(\nx\n,\ny\n)\n; expressing \nevaluate\n(\nx\n,\ny\n)\n and \ngradient\n(\nx\n,\ny\n)\n in terms of them\n\n\nAdded \nasPipe\n method for Covariance Functions\n\n\nFor backwards compatibility users are advised to extend\n   \nLocalSVMKernel\n in their custom Kernel implementations incase they do\n   not want to implement the \nevaluateAt\n API endpoints.\n\n\nAdded \nFeatureMapKernel\n, representing kernels which can be explicitly decomposed into feature mappings.\n\n\nAdded Matern half integer kernel \nGenericMaternKernel\n[\nI\n]\n\n\nAdded \nblock\n(\nS\n:\n \nString*\n)\n method to block any hyper-parameters of kernels.\n\n\nAdded \nNeuralNetworkKernel\n and \nGaussianSpectralKernel\n.\n\n\nAdded \nDecomposableCovariance\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nimport\n \nio.github.mandar2812.dynaml.DynaMLPipe._\n\n\nimport\n \nio.github.mandar2812.dynaml.kernels._\n\n\n\n\nimplicit\n \nval\n \nev\n \n=\n \nVectorField\n(\n6\n)\n\n\nimplicit\n \nval\n \nsp\n \n=\n \nbreezeDVSplitEncoder\n(\n2\n)\n\n\nimplicit\n \nval\n \nsumR\n \n=\n \nsumReducer\n\n\n\nval\n \nkernel\n \n=\n \nnew\n \nLaplacianKernel\n(\n1.5\n)\n\n\nval\n \nother_kernel\n \n=\n \nnew\n \nPolynomialKernel\n(\n1\n,\n \n0.05\n)\n\n\n\nval\n \ndecompKernel\n \n=\n \nnew\n \nDecomposableCovariance\n(\nkernel\n,\n \nother_kernel\n)(\nsp\n,\n \nsumReducer\n)\n\n\n\nval\n \nother_kernel1\n \n=\n \nnew\n \nFBMKernel\n(\n1.0\n)\n\n\n\nval\n \ndecompKernel1\n \n=\n \nnew\n \nDecomposableCovariance\n(\ndecompKernel\n,\n \nother_kernel1\n)(\nsp\n,\n \nsumReducer\n)\n\n\n\nval\n \nveca\n \n=\n \nDenseVector\n.\ntabulate\n[\nDouble\n](\n8\n)(\nmath\n.\nsin\n(\n_\n))\n\n\nval\n \nvecb\n \n=\n \nDenseVector\n.\ntabulate\n[\nDouble\n](\n8\n)(\nmath\n.\ncos\n(\n_\n))\n\n\n\ndecompKernel1\n.\nevaluate\n(\nveca\n,\n \nvecb\n)\n\n\n\n\n\n\n\n\n\nPackage\n \ndynaml\n.\nalgebra\n\n\nPartitioned Matrices/Vectors and the following operations\n\n\n\n\nAddition, Subtraction\n\n\nMatrix, vector multiplication\n\n\nLU, Cholesky\n\n\nA\n\\\ny\n, \nA\n\\\nY\n\n\n\n\nAdded calculation of quadratic forms, namely:\n\n\n\n\nquadraticForm\n which calculates \n\\(\\mathbf{x}^\\intercal A^{-1} \\mathbf{x}\\)\n\n\ncrossQuadraticForm\n which calculates \n\\(\\mathbf{y}^\\intercal A^{-1} \\mathbf{x}\\)\n\n\n\n\nWhere A is assumed to be a symmetric positive semi-definite matrix\n\n\nUsage:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nimport\n \nio.github.mandar2812.dynaml.algebra._\n\n\n\nval\n \nx\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \ny\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \na\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\n\nquadraticForm\n(\na\n,\nx\n)\n\n\ncrossQuadraticForm\n(\ny\n,\n \na\n,\n \nx\n)\n\n\n\n\n\n\n\n\n\nPackage\n \ndynaml\n.\nmodelpipe\n\n\nNew package created, moved all inheriting classes of \nModelPipe\n to this package.\n\n\nAdded the following:\n\n\n\n\nGLMPipe2\n A pipe taking two arguments and returning a \nGeneralizedLinearModel\n instance\n\n\nGeneralizedLeastSquaresPipe2\n:\n\n\nGeneralizedLeastSquaresPipe3\n:\n\n\n\n\n\n\nPackage\n \ndynaml\n.\nmodels\n\n\n\n\nAdded a new Neural Networks API: \nNeuralNet\n and \nGenericFFNeuralNet\n, for an example refer to \nTestNNDelve\n in \ndynaml\n-\nexamples\n.\n\n\nGeneralizedLeastSquaresModel\n: The  \nGLS\n model.\n\n\nESGPModel\n: The implementation of a skew gaussian process regression model\n\n\nWarped Gaussian Process\n models \nWIP\n\n\nAdded mean function capability to Gaussian Process and Student T process models.\n\n\nAdded Apache Spark implementation of Generalized Linear Models; see \nSparkGLM\n, \nSparkLogisticModel\n, \nSparkProbitGLM\n\n\n\n\n\n\nPackage\n \ndynaml.probability\n\n\n\n\nMultivariateSkewNormal\n as specified in \nAzzalani et. al\n\n\nExtendedMultivariateSkewNormal\n\n\nUESN\n and \nMESN\n representing an alternative formulation of the skew gaussian family from Adcock and Shutes.\n\n\nTruncatedGaussian\n: Truncated version of the Gaussian distribution.\n\n\nMatrix Normal Distribution\n\n\nAdded \nExpectation\n operator for \nRandomVariable\n implementations in the \nio\n.\ngithub\n.\nmandar2812\n.\ndynaml\n.\nprobability\n package object. Usage example given below.\n\n\nSkewGaussian\n, \nExtendedSkewGaussian\n: An breeze implementation of the SkewGaussian and extended Skew-Gaussian distributions respectively\n\n\nPushforwardMap\n, \nDifferentiableMap\n added: \nPushforwardMap\n enables creating new random variables with defined density from base random variables.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\nimport\n \nio.github.mandar2812.dynaml.analysis._\n\n\nimport\n \nio.github.mandar2812.dynaml.probability._\n\n\nimport\n \nio.github.mandar2812.dynaml.probability.distributions._\n\n\n\nval\n \ng\n \n=\n \nGaussianRV\n(\n0.0\n,\n \n0.25\n)\n\n\nval\n \nsg\n \n=\n \nRandomVariable\n(\nSkewGaussian\n(\n1.0\n,\n \n0.0\n,\n \n0.25\n))\n\n\n\n//Define a determinant implementation for the Jacobian type (Double in this case)\n\n\nimplicit\n \nval\n \ndetImpl\n \n=\n \nidentityPipe\n[\nDouble\n]\n\n\n\n//Defines a homeomorphism y = exp(x) x = log(y)\n\n\nval\n \nh\n:\n \nPushforwardMap\n[\nDouble\n, \nDouble\n, \nDouble\n]\n \n=\n \nPushforwardMap\n(\n\n  \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nexp\n(\nx\n)),\n\n  \nDifferentiableMap\n(\n\n    \n(\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nlog\n(\nx\n),\n\n    \n(\nx\n:\n \nDouble\n)\n \n=>\n \n1.0\n/\nx\n)\n\n\n)\n\n\n\n//Creates a log-normal random variable\n\n\nval\n \np\n \n=\n \nh\n->\ng\n\n\n\n//Creates a log-skew-gaussian random variable\n\n\nval\n \nq\n \n=\n \nh\n->\nsg\n\n\n\n//Calculate expectation of q\n\n\nprintln\n(\n\"E[Q] = \"\n+\nE\n(\nq\n))\n\n\n\n\n\n - Added \nMarkov Chain Monte Carlo\n (MCMC) based inference schemes \nContinuousMCMC\n and the underlying sampling implementation in \nGeneralMetropolisHastings\n.\n - Added implementation of \nApproximate Bayesian Computation\n (ABC) in the \nApproxBayesComputation\n class.\n\n\n1\n2\n3\n4\n5\n6\n7\n//The mean\n\n\nval\n \ncenter\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\n//Covariance (positive semi-def) matrix among rows\n\n\nval\n \nsigmaRows\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\n//Covariance (positive semi-def) matrix among columns\n\n\nval\n \nsigmaCols\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \nmatD\n \n=\n \nMatrixNormal\n(\ncenter\n,\n \nsigmaRows\n,\n \nsigmaCols\n)\n\n\n\n\n\n - \nMatrix T Distribution\n (\nExperimental\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n//The degrees of freedom (must be > 2.0 for existence of finite moments)\n\n\nval\n \nmu\n:\n \nDouble\n \n=\n \n...\n\n\n//The mean\n\n\nval\n \ncenter\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\n//Covariance (positive semi-def) matrix among rows\n\n\nval\n \nsigmaRows\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\n//Covariance (positive semi-def) matrix among columns\n\n\nval\n \nsigmaCols\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \nmatD\n \n=\n \nMatrixT\n(\nmu\n,\n \ncenter\n,\n \nsigmaCols\n,\n \nsigmaRows\n)\n\n\n\n\n\n\n\n\n\nPackage\n \ndynaml\n.\noptimization\n\n\n\n\nAdded \nProbGPCommMachine\n which performs grid search or CSA and then instead of selecting a single hyper-parameter configuration calculates a weighted Gaussian Process committee where the weights correspond to probabilities or confidence on each model instance (hyper-parameter configuration).\n\n\n\n\nPackage\n \ndynaml\n.\nutils\n\n\n\n\nAdded \nmultivariate gamma function\n\n\n\n\n1\n2\n//Returns logarithm of multivariate gamma function\n\n\nval\n \ng\n \n=\n \nmvlgamma\n(\n5\n,\n \n1.5\n)\n\n\n\n\n\n\n\n\n\nPackage\n \ndynaml\n.\ndataformat\n\n\n\n\nAdded support for reading \nMATLAB\n \n.mat\n files in the \nMAT\n object.\n\n\n\n\nImprovements/Bug Fixes\n\u00b6\n\n\nPackage\n \ndynaml\n.\nprobability\n\n\n\n\nRemoved \nProbabilityModel\n and replaced with \nJointProbabilityScheme\n and \nBayesJointProbabilityScheme\n, major refactoring to \nRandomVariable\n API.\n\n\n\n\n\n\nPackage\n \ndynaml\n.\noptimization\n\n\n\n\nImproved logging of \nCoupledSimulatedAnnealing\n\n\nRefactored \nGPMLOptimizer\n to \nGradBasedGlobalOptimizer\n\n\n\n\n\n\nPackage\n \ndynaml\n.\nutils\n\n - Correction to \nutils\n.\ngetStats\n method used for calculating mean and variance of data sets consisting of \nDenseVector\n[\nDouble\n]\n.\n - \nminMaxScalingTrainTest\n \nminMaxScaling\n of \nDynaMLPipe\n using \nGaussianScaler\n instead of \nMinMaxScaler\n for processing of features.\n\n\n\n\nPackage\n \ndynaml\n.\nkernels\n\n\n\n\nFix to \nCoRegCauchyKernel\n: corrected mismatch of hyper-parameter string\n\n\nFix to \nSVMKernel\n objects matrix gradient computation in the case when kernel dimensions are not multiples of block size.\n\n\nCorrection to gradient calculation in RBF kernel family.\n\n\nSpeed up of kernel gradient computation, kernel and kernel gradient matrices with respect to the model hyper-parameters now calculated in a single pass through the data.",
            "title": "v1.4.1"
        },
        {
            "location": "/releases/mydoc_release_notes_141/#pipes-api",
            "text": "",
            "title": "Pipes API"
        },
        {
            "location": "/releases/mydoc_release_notes_141/#additions",
            "text": "The pipes API has been vastly extended by creating pipes which encapsulate functions of multiple arguments leading to the following end points.   DataPipe2 [ A ,  B ,  C ] : Pipe which takes 2 arguments  DataPipe3 [ A ,  B ,  C ,  D ]  : Pipe which takes 3 arguments  DataPipe4 [ A ,  B ,  C ,  D ,  E ] : Pipe which takes 4 arguments   Furthermore there is now the ability to create pipes which return pipes, something akin to curried functions in functional programming.   MetaPipe : Takes an argument returns a  DataPipe  MetaPipe21 : Takes 2 arguments returns a  DataPipe  MetaPipe12 : Takes an argument returns a  DataPipe2   A new kind of Stream data pipe,  StreamFlatMapPipe  is added to represent data pipelines which can perform flat map like operations on streams.  1\n2 val   mapFunc :   ( I )   =>   Stream [ J ]   =   ...  val   streamFMPipe   =   StreamFlatMapPipe ( mapFunc )     Added Data Pipes API for Apache Spark RDDs.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 val   num   =   20  val   numbers   =   sc . parallelize ( 1   to   num )  val   convPipe   =   RDDPipe (( n :   Int )   =>   n . toDouble )  val   sqPipe   =   RDDPipe (( x :   Double )   =>   x * x )  val   sqrtPipe   =   RDDPipe (( x :   Double )   =>   math . sqrt ( x ))  val   resultPipe   =   RDDPipe (( r :   RDD [ Double ])   =>   r . reduce ( _ + _ ). toInt )  val   netPipeline   =   convPipe   >   sqPipe   >   sqrtPipe   >   resultPipe  netPipeline ( numbers )     Added  UnivariateGaussianScaler  class for gaussian scaling of univariate data.",
            "title": "Additions"
        },
        {
            "location": "/releases/mydoc_release_notes_141/#core-api",
            "text": "",
            "title": "Core API"
        },
        {
            "location": "/releases/mydoc_release_notes_141/#additions_1",
            "text": "Package   dynaml . models . bayes  This new package will house stochastic prior models, currently there is support for GP and Skew GP priors, to see a starting example see  stochasticPriors.sc  in the  scripts  directory of the DynaML source.   Package   dynaml . kernels   Added  evaluateAt ( h )( x , y )  and  gradientAt ( h )( x , y ) ; expressing  evaluate ( x , y )  and  gradient ( x , y )  in terms of them  Added  asPipe  method for Covariance Functions  For backwards compatibility users are advised to extend\n    LocalSVMKernel  in their custom Kernel implementations incase they do\n   not want to implement the  evaluateAt  API endpoints.  Added  FeatureMapKernel , representing kernels which can be explicitly decomposed into feature mappings.  Added Matern half integer kernel  GenericMaternKernel [ I ]  Added  block ( S :   String* )  method to block any hyper-parameters of kernels.  Added  NeuralNetworkKernel  and  GaussianSpectralKernel .  Added  DecomposableCovariance    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 import   io.github.mandar2812.dynaml.DynaMLPipe._  import   io.github.mandar2812.dynaml.kernels._  implicit   val   ev   =   VectorField ( 6 )  implicit   val   sp   =   breezeDVSplitEncoder ( 2 )  implicit   val   sumR   =   sumReducer  val   kernel   =   new   LaplacianKernel ( 1.5 )  val   other_kernel   =   new   PolynomialKernel ( 1 ,   0.05 )  val   decompKernel   =   new   DecomposableCovariance ( kernel ,   other_kernel )( sp ,   sumReducer )  val   other_kernel1   =   new   FBMKernel ( 1.0 )  val   decompKernel1   =   new   DecomposableCovariance ( decompKernel ,   other_kernel1 )( sp ,   sumReducer )  val   veca   =   DenseVector . tabulate [ Double ]( 8 )( math . sin ( _ ))  val   vecb   =   DenseVector . tabulate [ Double ]( 8 )( math . cos ( _ ))  decompKernel1 . evaluate ( veca ,   vecb )     Package   dynaml . algebra  Partitioned Matrices/Vectors and the following operations   Addition, Subtraction  Matrix, vector multiplication  LU, Cholesky  A \\ y ,  A \\ Y   Added calculation of quadratic forms, namely:   quadraticForm  which calculates  \\(\\mathbf{x}^\\intercal A^{-1} \\mathbf{x}\\)  crossQuadraticForm  which calculates  \\(\\mathbf{y}^\\intercal A^{-1} \\mathbf{x}\\)   Where A is assumed to be a symmetric positive semi-definite matrix  Usage:  1\n2\n3\n4\n5\n6\n7\n8 import   io.github.mandar2812.dynaml.algebra._  val   x :   DenseVector [ Double ]   =   ...  val   y :   DenseVector [ Double ]   =   ...  val   a :   DenseMatrix [ Double ]   =   ...  quadraticForm ( a , x )  crossQuadraticForm ( y ,   a ,   x )     Package   dynaml . modelpipe  New package created, moved all inheriting classes of  ModelPipe  to this package.  Added the following:   GLMPipe2  A pipe taking two arguments and returning a  GeneralizedLinearModel  instance  GeneralizedLeastSquaresPipe2 :  GeneralizedLeastSquaresPipe3 :    Package   dynaml . models   Added a new Neural Networks API:  NeuralNet  and  GenericFFNeuralNet , for an example refer to  TestNNDelve  in  dynaml - examples .  GeneralizedLeastSquaresModel : The   GLS  model.  ESGPModel : The implementation of a skew gaussian process regression model  Warped Gaussian Process  models  WIP  Added mean function capability to Gaussian Process and Student T process models.  Added Apache Spark implementation of Generalized Linear Models; see  SparkGLM ,  SparkLogisticModel ,  SparkProbitGLM    Package   dynaml.probability   MultivariateSkewNormal  as specified in  Azzalani et. al  ExtendedMultivariateSkewNormal  UESN  and  MESN  representing an alternative formulation of the skew gaussian family from Adcock and Shutes.  TruncatedGaussian : Truncated version of the Gaussian distribution.  Matrix Normal Distribution  Added  Expectation  operator for  RandomVariable  implementations in the  io . github . mandar2812 . dynaml . probability  package object. Usage example given below.  SkewGaussian ,  ExtendedSkewGaussian : An breeze implementation of the SkewGaussian and extended Skew-Gaussian distributions respectively  PushforwardMap ,  DifferentiableMap  added:  PushforwardMap  enables creating new random variables with defined density from base random variables.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 import   io.github.mandar2812.dynaml.analysis._  import   io.github.mandar2812.dynaml.probability._  import   io.github.mandar2812.dynaml.probability.distributions._  val   g   =   GaussianRV ( 0.0 ,   0.25 )  val   sg   =   RandomVariable ( SkewGaussian ( 1.0 ,   0.0 ,   0.25 ))  //Define a determinant implementation for the Jacobian type (Double in this case)  implicit   val   detImpl   =   identityPipe [ Double ]  //Defines a homeomorphism y = exp(x) x = log(y)  val   h :   PushforwardMap [ Double ,  Double ,  Double ]   =   PushforwardMap ( \n   DataPipe (( x :   Double )   =>   math . exp ( x )), \n   DifferentiableMap ( \n     ( x :   Double )   =>   math . log ( x ), \n     ( x :   Double )   =>   1.0 / x )  )  //Creates a log-normal random variable  val   p   =   h -> g  //Creates a log-skew-gaussian random variable  val   q   =   h -> sg  //Calculate expectation of q  println ( \"E[Q] = \" + E ( q ))   \n - Added  Markov Chain Monte Carlo  (MCMC) based inference schemes  ContinuousMCMC  and the underlying sampling implementation in  GeneralMetropolisHastings .\n - Added implementation of  Approximate Bayesian Computation  (ABC) in the  ApproxBayesComputation  class.  1\n2\n3\n4\n5\n6\n7 //The mean  val   center :   DenseMatrix [ Double ]   =   ...  //Covariance (positive semi-def) matrix among rows  val   sigmaRows :   DenseMatrix [ Double ]   =   ...  //Covariance (positive semi-def) matrix among columns  val   sigmaCols :   DenseMatrix [ Double ]   =   ...  val   matD   =   MatrixNormal ( center ,   sigmaRows ,   sigmaCols )   \n -  Matrix T Distribution  ( Experimental )  1\n2\n3\n4\n5\n6\n7\n8\n9 //The degrees of freedom (must be > 2.0 for existence of finite moments)  val   mu :   Double   =   ...  //The mean  val   center :   DenseMatrix [ Double ]   =   ...  //Covariance (positive semi-def) matrix among rows  val   sigmaRows :   DenseMatrix [ Double ]   =   ...  //Covariance (positive semi-def) matrix among columns  val   sigmaCols :   DenseMatrix [ Double ]   =   ...  val   matD   =   MatrixT ( mu ,   center ,   sigmaCols ,   sigmaRows )     Package   dynaml . optimization   Added  ProbGPCommMachine  which performs grid search or CSA and then instead of selecting a single hyper-parameter configuration calculates a weighted Gaussian Process committee where the weights correspond to probabilities or confidence on each model instance (hyper-parameter configuration).   Package   dynaml . utils   Added  multivariate gamma function   1\n2 //Returns logarithm of multivariate gamma function  val   g   =   mvlgamma ( 5 ,   1.5 )     Package   dynaml . dataformat   Added support for reading  MATLAB   .mat  files in the  MAT  object.",
            "title": "Additions"
        },
        {
            "location": "/releases/mydoc_release_notes_141/#improvementsbug-fixes",
            "text": "Package   dynaml . probability   Removed  ProbabilityModel  and replaced with  JointProbabilityScheme  and  BayesJointProbabilityScheme , major refactoring to  RandomVariable  API.    Package   dynaml . optimization   Improved logging of  CoupledSimulatedAnnealing  Refactored  GPMLOptimizer  to  GradBasedGlobalOptimizer    Package   dynaml . utils \n - Correction to  utils . getStats  method used for calculating mean and variance of data sets consisting of  DenseVector [ Double ] .\n -  minMaxScalingTrainTest   minMaxScaling  of  DynaMLPipe  using  GaussianScaler  instead of  MinMaxScaler  for processing of features.   Package   dynaml . kernels   Fix to  CoRegCauchyKernel : corrected mismatch of hyper-parameter string  Fix to  SVMKernel  objects matrix gradient computation in the case when kernel dimensions are not multiples of block size.  Correction to gradient calculation in RBF kernel family.  Speed up of kernel gradient computation, kernel and kernel gradient matrices with respect to the model hyper-parameters now calculated in a single pass through the data.",
            "title": "Improvements/Bug Fixes"
        },
        {
            "location": "/releases/mydoc_release_notes_14/",
            "text": "Version 1.4 of DynaML, released Sept 23, 2016, implements a number of new models (multi-output GP, student T process, random variables, etc) and features (Variance control for CSA, etc).\n\n\n\n\nModels\n\u00b6\n\n\nThe following inference models have been added.\n\n\nMeta Models & Ensembles\n\u00b6\n\n\n\n\nLSSVM committees.\n\n\n\n\nStochastic Processes\n\u00b6\n\n\n\n\nMulti-output, multi-task \nGaussian Process\n models as reviewed in \nLawrence et. al\n.\n\n\nStudent T Processes\n: single and multi output inspired from \nShah, Ghahramani et. al\n\n\nPerformance improvement to computation of \nmarginal likelihood\n and \nposterior predictive distribution\n in Gaussian Process models.\n\n\nPosterior predictive distribution outputted by the \nAbstractGPRegression\n base class is now changed to \nMultGaussianRV\n which is added to the \ndynaml\n.\nprobability\n package.\n\n\n\n\nKernels\n\u00b6\n\n\n\n\n\n\nAdded \nStationaryKernel\n and \nLocallyStationaryKernel\n classes in the kernel APIs, converted \nRBFKernel\n, \nCauchyKernel\n, \nRationalQuadraticKernel\n & \nLaplacianKernel\n to subclasses of \nStationaryKernel\n\n\n\n\n\n\nAdded \nMLPKernel\n which implements the \nmaximum likelihood perceptron\n kernel as shown \nhere\n.\n\n\n\n\n\n\nAdded \nco-regionalization kernels\n which are used in \nLawrence et. al\n to formulate kernels for vector valued functions. In this category the following co-regionalization kernels were implemented.\n\n\n\n\nCoRegRBFKernel\n\n\nCoRegCauchyKernel\n\n\nCoRegLaplaceKernel\n\n\n\n\nCoRegDiracKernel\n\n\n\n\n\n\nImproved performance when calculating kernel matrices for composite kernels.\n\n\n\n\n\n\nAdded \n:*\n operator to kernels so that one can create separable kernels used in \nco-regionalization models\n.\n\n\n\n\n\n\nOptimization\n\u00b6\n\n\n\n\nImproved performance of \nCoupledSimulatedAnnealing\n, enabled use of 4 variants of \nCoupled Simulated Annealing\n, adding the ability to set annealing schedule using so called \nvariance control\n scheme as outlined in \nde-Souza, Suykens et. al\n.\n\n\n\n\nPipes\n\u00b6\n\n\n\n\n\n\nAdded \nScaler\n and \nReversibleScaler\n traits to represent transformations which input and output into the same domain set, these traits are extensions of \nDataPipe\n.\n\n\n\n\n\n\nAdded \nDiscrete Wavelet Transform\n based on the \nHaar\n wavelet.",
            "title": "v1.4"
        },
        {
            "location": "/releases/mydoc_release_notes_14/#models",
            "text": "The following inference models have been added.",
            "title": "Models"
        },
        {
            "location": "/releases/mydoc_release_notes_14/#meta-models-ensembles",
            "text": "LSSVM committees.",
            "title": "Meta Models &amp; Ensembles"
        },
        {
            "location": "/releases/mydoc_release_notes_14/#stochastic-processes",
            "text": "Multi-output, multi-task  Gaussian Process  models as reviewed in  Lawrence et. al .  Student T Processes : single and multi output inspired from  Shah, Ghahramani et. al  Performance improvement to computation of  marginal likelihood  and  posterior predictive distribution  in Gaussian Process models.  Posterior predictive distribution outputted by the  AbstractGPRegression  base class is now changed to  MultGaussianRV  which is added to the  dynaml . probability  package.",
            "title": "Stochastic Processes"
        },
        {
            "location": "/releases/mydoc_release_notes_14/#kernels",
            "text": "Added  StationaryKernel  and  LocallyStationaryKernel  classes in the kernel APIs, converted  RBFKernel ,  CauchyKernel ,  RationalQuadraticKernel  &  LaplacianKernel  to subclasses of  StationaryKernel    Added  MLPKernel  which implements the  maximum likelihood perceptron  kernel as shown  here .    Added  co-regionalization kernels  which are used in  Lawrence et. al  to formulate kernels for vector valued functions. In this category the following co-regionalization kernels were implemented.   CoRegRBFKernel  CoRegCauchyKernel  CoRegLaplaceKernel   CoRegDiracKernel    Improved performance when calculating kernel matrices for composite kernels.    Added  :*  operator to kernels so that one can create separable kernels used in  co-regionalization models .",
            "title": "Kernels"
        },
        {
            "location": "/releases/mydoc_release_notes_14/#optimization",
            "text": "Improved performance of  CoupledSimulatedAnnealing , enabled use of 4 variants of  Coupled Simulated Annealing , adding the ability to set annealing schedule using so called  variance control  scheme as outlined in  de-Souza, Suykens et. al .",
            "title": "Optimization"
        },
        {
            "location": "/releases/mydoc_release_notes_14/#pipes",
            "text": "Added  Scaler  and  ReversibleScaler  traits to represent transformations which input and output into the same domain set, these traits are extensions of  DataPipe .    Added  Discrete Wavelet Transform  based on the  Haar  wavelet.",
            "title": "Pipes"
        },
        {
            "location": "/scaladoc/v1.4.2/core/",
            "text": "",
            "title": "dynaml-core"
        },
        {
            "location": "/scaladoc/v1.4.2/pipes/",
            "text": "",
            "title": "dynaml-pipes"
        },
        {
            "location": "/scaladoc/v1.4.2/examples/",
            "text": "",
            "title": "dynaml-examples"
        },
        {
            "location": "/scaladoc/v1.4.1/core/",
            "text": "",
            "title": "dynaml-core"
        },
        {
            "location": "/scaladoc/v1.4.1/pipes/",
            "text": "",
            "title": "dynaml-pipes"
        },
        {
            "location": "/scaladoc/v1.4.1/examples/",
            "text": "",
            "title": "dynaml-examples"
        },
        {
            "location": "/scaladoc/v1.4/core/",
            "text": "",
            "title": "dynaml-core"
        },
        {
            "location": "/scaladoc/v1.4/pipes/",
            "text": "",
            "title": "dynaml-pipes"
        },
        {
            "location": "/scaladoc/v1.4/examples/",
            "text": "",
            "title": "dynaml-examples"
        },
        {
            "location": "/core/core_model_hierarchy/",
            "text": "Model Classes\n\u00b6\n\n\nIn DynaML all model implementations fit into a well defined class hierarchy. In fact every DynaML machine learning model is an extension of the \nModel\n[\nT\n,\nQ\n,\nR\n]\n trait.\n\n\n\n\nModel\n[\nT\n,\nQ\n,\nR\n]\n\n\nThe \nModel\n trait is quite bare bones: machine learning models are viewed as objects containing two parts or components.\n\n\n\n\n\n\nA training data set (of type \nT\n).  \n\n\n\n\n\n\nA method \npredict\n(\npoint\n:\n \nQ\n)\n:\nR\n to generate a prediction of type \nR\n given a data point of type \nQ\n.\n\n\n\n\n\n\n\n\nParameterized Models\n\u00b6\n\n\nMany predictive models calculate predictions by formulating an expression which includes a set of parameters which are used along with the data points to generate predictions, the \nParameterizedLearner[G, T, Q, R, S]\n class represents a skeleton for all parametric machine learning models such as \nGeneralized Linear Models\n, \nNeural Networks\n, etc.\n\n\n\n\nTip\n\n\nThe defining characteristic of classes which extend \nParameterizedLearner\n is that they must contain a member variable \noptimizer: RegularizedOptimizer[T, Q, R, S]\n which represents a \nregularization enabled optimizer\n implementation along with a \nlearn\n()\n method which uses the optimizer member to calculate approximate values of the model parameters given the training data.\n\n\n\n\nLinear Models\n\u00b6\n\n\nLinear models; represented by the \nLinearModel\n[\nT\n, \nP\n, \nQ\n , \nR\n, \nS\n]\n trait are extensions of \nParameterizedLearner\n, this top level trait is extended to yield many useful linear prediction models.\n\n\nGeneralized Linear Models\n which are linear in parameters expression for the predictions \n\\(\n\\(y\\)\n\\)\n given a vector of processed features \n\\(\\phi(x)\\)\n or basis functions.\n\n\n\\[\n    \\begin{equation}\n        y = w^T\\varphi(x) + \\epsilon\n    \\end{equation}\n\\]\nStochastic Processes\n\u00b6\n\n\nStochastic processes (or random functions) are general probabilistic models which can be used to construct finite dimensional distributions over a set of sampled domain points. More specifically a stochastic process is a probabilistic function \n\\(f(.)\\)\n defined on any \ndomain\n or \nindex set\n \n\\(\\mathcal{X}\\)\n such that for any finite collection \n\\(x_i \\in \\mathcal{X}, i = 1 \\cdots N\\)\n, the finite dimensional distribution \n\\(P(f(x_1), \\cdots, f(x_N))\\)\n is coherently defined.\n\n\n\n\nTip\n\n\nThe \nStochasticProcessModel\n[\nT\n, \nI\n, \nY\n, \nW\n]\n trait extends \nModel\n[\nT\n, \nI\n, \nY\n]\n and is the top level trait for the implementation of general stochastic processes. In order to extend it, one must implement among others a function to output the posterior predictive distribution \npredictiveDistribution\n()\n.\n\n\n\n\nContinuous Processes\n\u00b6\n\n\nBy continuous processes, we mean processes whose values lie on a continuous domain (such as \n\\(\\mathbb{R}^d\\)\n). The \nContinuousProcessModel\n[\nT\n, \nI\n, \nY\n, \nW\n]\n abstract class provides a template which can be extended to implement continuous random process models.\n\n\n\n\nTip\n\n\nThe \nContinuousProcessModel\n class contains the method \npredictionWithErrorBars()\n which takes inputs test data and number of standard deviations, and generates predictions with upper and lower error bars around them. In order to create a sub-class of \nContinuousProcessModel\n, you must implement the method \npredictionWithErrorBars()\n.\n\n\n\n\nSecond Order Processes\n\u00b6\n\n\nSecond order stochastic processes can be described by specifying the \nmean\n (first order statistic) and \nvariance\n (second order statistic) of their finite dimensional distribution. The \nSecondOrderProcessModel\n[\nT\n, \nI\n, \nY\n, \nK\n, \nM\n, \nW\n]\n trait is an abstract skeleton which describes what elements a second order process model must have i.e. the mean and covariance functions.\n\n\nMeta Models/Model Ensembles\n\u00b6\n\n\nMeta models use predictions from several candidate models and derive a prediction that is a meaningful combination of the individual predictions. This may be achieved in several ways some of which are.\n\n\n\n\nAverage of predictions/voting\n\n\nWeighted predictions: Problem is now transferred to calculating appropriate weights.\n\n\nLearning some non-trivial functional transformation of the individual prediction, also known as \ngating networks\n.  \n\n\n\n\nCurrently the DynaML API has the following classes providing capabilities of meta models.\n\n\nAbstract Classes\n\n\n\n\nMetaModel\n[\nD\n, \nD1\n, \nBaseModel\n]\n\n\nCommitteeModel\n[\nD\n, \nD1\n, \nBaseModel\n]\n\n\n\n\nImplementations\n\n\n\n\nLS-SVM Committee\n\n\nNeural Committee",
            "title": "Model Hierarchy"
        },
        {
            "location": "/core/core_model_hierarchy/#model-classes",
            "text": "In DynaML all model implementations fit into a well defined class hierarchy. In fact every DynaML machine learning model is an extension of the  Model [ T , Q , R ]  trait.   Model [ T , Q , R ]  The  Model  trait is quite bare bones: machine learning models are viewed as objects containing two parts or components.    A training data set (of type  T ).      A method  predict ( point :   Q ) : R  to generate a prediction of type  R  given a data point of type  Q .",
            "title": "Model Classes"
        },
        {
            "location": "/core/core_model_hierarchy/#parameterized-models",
            "text": "Many predictive models calculate predictions by formulating an expression which includes a set of parameters which are used along with the data points to generate predictions, the  ParameterizedLearner[G, T, Q, R, S]  class represents a skeleton for all parametric machine learning models such as  Generalized Linear Models ,  Neural Networks , etc.   Tip  The defining characteristic of classes which extend  ParameterizedLearner  is that they must contain a member variable  optimizer: RegularizedOptimizer[T, Q, R, S]  which represents a  regularization enabled optimizer  implementation along with a  learn ()  method which uses the optimizer member to calculate approximate values of the model parameters given the training data.",
            "title": "Parameterized Models"
        },
        {
            "location": "/core/core_model_hierarchy/#linear-models",
            "text": "Linear models; represented by the  LinearModel [ T ,  P ,  Q  ,  R ,  S ]  trait are extensions of  ParameterizedLearner , this top level trait is extended to yield many useful linear prediction models.  Generalized Linear Models  which are linear in parameters expression for the predictions  \\( \\(y\\) \\)  given a vector of processed features  \\(\\phi(x)\\)  or basis functions.  \\[\n    \\begin{equation}\n        y = w^T\\varphi(x) + \\epsilon\n    \\end{equation}\n\\]",
            "title": "Linear Models"
        },
        {
            "location": "/core/core_model_hierarchy/#stochastic-processes",
            "text": "Stochastic processes (or random functions) are general probabilistic models which can be used to construct finite dimensional distributions over a set of sampled domain points. More specifically a stochastic process is a probabilistic function  \\(f(.)\\)  defined on any  domain  or  index set   \\(\\mathcal{X}\\)  such that for any finite collection  \\(x_i \\in \\mathcal{X}, i = 1 \\cdots N\\) , the finite dimensional distribution  \\(P(f(x_1), \\cdots, f(x_N))\\)  is coherently defined.   Tip  The  StochasticProcessModel [ T ,  I ,  Y ,  W ]  trait extends  Model [ T ,  I ,  Y ]  and is the top level trait for the implementation of general stochastic processes. In order to extend it, one must implement among others a function to output the posterior predictive distribution  predictiveDistribution () .",
            "title": "Stochastic Processes"
        },
        {
            "location": "/core/core_model_hierarchy/#continuous-processes",
            "text": "By continuous processes, we mean processes whose values lie on a continuous domain (such as  \\(\\mathbb{R}^d\\) ). The  ContinuousProcessModel [ T ,  I ,  Y ,  W ]  abstract class provides a template which can be extended to implement continuous random process models.   Tip  The  ContinuousProcessModel  class contains the method  predictionWithErrorBars()  which takes inputs test data and number of standard deviations, and generates predictions with upper and lower error bars around them. In order to create a sub-class of  ContinuousProcessModel , you must implement the method  predictionWithErrorBars() .",
            "title": "Continuous Processes"
        },
        {
            "location": "/core/core_model_hierarchy/#second-order-processes",
            "text": "Second order stochastic processes can be described by specifying the  mean  (first order statistic) and  variance  (second order statistic) of their finite dimensional distribution. The  SecondOrderProcessModel [ T ,  I ,  Y ,  K ,  M ,  W ]  trait is an abstract skeleton which describes what elements a second order process model must have i.e. the mean and covariance functions.",
            "title": "Second Order Processes"
        },
        {
            "location": "/core/core_model_hierarchy/#meta-modelsmodel-ensembles",
            "text": "Meta models use predictions from several candidate models and derive a prediction that is a meaningful combination of the individual predictions. This may be achieved in several ways some of which are.   Average of predictions/voting  Weighted predictions: Problem is now transferred to calculating appropriate weights.  Learning some non-trivial functional transformation of the individual prediction, also known as  gating networks .     Currently the DynaML API has the following classes providing capabilities of meta models.  Abstract Classes   MetaModel [ D ,  D1 ,  BaseModel ]  CommitteeModel [ D ,  D1 ,  BaseModel ]   Implementations   LS-SVM Committee  Neural Committee",
            "title": "Meta Models/Model Ensembles"
        },
        {
            "location": "/core/core_glm/",
            "text": "Summary\nGeneralized Linear Models\n are a class of models which belong to the \nordinary least squares\n framework. They generally consist of a set of parameters \n\\(\\mathbf{w}\\)\n, a feature mapping \n\\(\\varphi()\\)\n and a \nlink function\n which dictates how the probability distribution of the output quantity is described.\n\n\n\n\n\n\nGeneralized Linear Models\n (GLM) are available in the context of regression and binary classification, more specifically in DynaML the following members of the GLM family are implemented. The \nGeneralizedLinearModel\n[\nT\n]\n class is the base of the GLM hierarchy in DynaML, all linear models are extensions of it. It's companion object is used for the creation of GLM instances as follows.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\n\n//The task variable is a string which is set to \"regression\" or \"classification\"\n\n\nval\n \ntask\n \n=\n \n...\n\n\n\n//The map variable defines a possibly higher dimensional function of the input\n\n\n//which is akin to a basis function representation of the original features\n\n\nval\n \nmap\n:\n \nDenseVector\n[\nDouble\n]\n \n=>\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\n\n//modeltype is set to \"logit\" or \"probit\"\n\n\n//if one wishes to create a binary classification model,\n\n\n//depending on the classification model involved\n\n\nval\n \nmodeltype\n \n=\n \n\"logit\"\n\n\n\nval\n \nglm\n \n=\n \nGeneralizedLinearModel\n(\ndata\n,\n \ntask\n,\n \nmap\n,\n \nmodeltype\n)\n\n\n\n\n\n\n\nNormal GLM\n\u00b6\n\n\nThe most common regression model, also known as \nleast squares linear regression\n, implemented as the class \nRegularizedGLM\n which represents a regression model with the following prediction:\n\n\n\\[\n    \\begin{equation}\n        y \\ | \\ \\mathbf{x} \\sim \\mathcal{N}(w^T \\varphi(\\mathbf{x}), \\sigma^{2})\n    \\end{equation}\n\\]\nHere \n\\(\\varphi(.)\\)\n is an appropriately chosen set of \nbasis functions\n. The inference problem is formulated as\n\n\n\\[\n    \\begin{equation}\n        \\min_{w} \\ \\mathcal{J}_P(w) = \\frac{1}{2} \\gamma \\  w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2\n    \\end{equation}\n\\]\nLogit GLM\n\u00b6\n\n\nIn binary classification the most common GLM used is the \nlogistic regression\n model which is given by\n$$\n    \\begin{equation}\n        P(y  = 1  |  \\mathbf{x}) = \\sigma(w^T \\varphi(\\mathbf{x}) + b)\n    \\end{equation}\n$$\n\n\nWhere \n\\(\\sigma(z) = \\frac{1}{1 + exp(-z)}\\)\n is the logistic function which maps the output of the linear function \n\\(w^T \\varphi(\\mathbf{x}) + b\\)\n to a probability value.\n\n\nProbit GLM\n\u00b6\n\n\nThe \nprobit regression\n model is an alternative to the \nlogit\n model it is represented as:\n$$\n    \\begin{equation}\n        P(y  = 1  |  \\mathbf{x}) = \\Phi(w^T \\varphi(\\mathbf{x}) + b)\n    \\end{equation}\n$$\nWhere \n\\(\\Phi(z)\\)\n is the cumulative distribution function of the standard normal distribution.\n\n\n\n\nGLS\nThe \nGeneralized Least Squares\n model which is a more broad formulation of the \nOrdinary Least Squares\n (OLS) regression model.",
            "title": "Generalized Linear Models"
        },
        {
            "location": "/core/core_glm/#normal-glm",
            "text": "The most common regression model, also known as  least squares linear regression , implemented as the class  RegularizedGLM  which represents a regression model with the following prediction:  \\[\n    \\begin{equation}\n        y \\ | \\ \\mathbf{x} \\sim \\mathcal{N}(w^T \\varphi(\\mathbf{x}), \\sigma^{2})\n    \\end{equation}\n\\] Here  \\(\\varphi(.)\\)  is an appropriately chosen set of  basis functions . The inference problem is formulated as  \\[\n    \\begin{equation}\n        \\min_{w} \\ \\mathcal{J}_P(w) = \\frac{1}{2} \\gamma \\  w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2\n    \\end{equation}\n\\]",
            "title": "Normal GLM"
        },
        {
            "location": "/core/core_glm/#logit-glm",
            "text": "In binary classification the most common GLM used is the  logistic regression  model which is given by\n$$\n    \\begin{equation}\n        P(y  = 1  |  \\mathbf{x}) = \\sigma(w^T \\varphi(\\mathbf{x}) + b)\n    \\end{equation}\n$$  Where  \\(\\sigma(z) = \\frac{1}{1 + exp(-z)}\\)  is the logistic function which maps the output of the linear function  \\(w^T \\varphi(\\mathbf{x}) + b\\)  to a probability value.",
            "title": "Logit GLM"
        },
        {
            "location": "/core/core_glm/#probit-glm",
            "text": "The  probit regression  model is an alternative to the  logit  model it is represented as:\n$$\n    \\begin{equation}\n        P(y  = 1  |  \\mathbf{x}) = \\Phi(w^T \\varphi(\\mathbf{x}) + b)\n    \\end{equation}\n$$\nWhere  \\(\\Phi(z)\\)  is the cumulative distribution function of the standard normal distribution.   GLS The  Generalized Least Squares  model which is a more broad formulation of the  Ordinary Least Squares  (OLS) regression model.",
            "title": "Probit GLM"
        },
        {
            "location": "/core/core_gls/",
            "text": "Summary\n\n\nThe \nGeneralized Least Squares\n model is a regression formulation which does not assume that the model errors/residuals are independent of each other. Rather it borrows from the \nGaussian Process\n paradigm and assigns a covariance structure to the model residuals.\n\n\n\n\n\n\nWarning\n\n\nThe nomenclature \nGeneralized Least Squares\n (GLS) and \nGeneralized Linear Models\n (GLM) can cause much confusion. It is important to remember the context of both. GLS refers to relaxing of the independence of residuals assumption while GLM refers to \nOrdinary Least Squares\n OLS based models which are extended to model regression, counts, or classification tasks.\n\n\n\n\nFormulation.\n\u00b6\n\n\nLet \n\\(\\mathbf{X} \\in \\mathbb{R}^{n\\times m}\\)\n be a matrix containing data attributes. The GLS model builds a linear predictor of the target quantity of the following form.\n\n\n\\[\n\\begin{equation}\n\\mathbf {y} = \\varphi(\\mathbf {X}) \\mathbf {\\beta } +\\mathbf {\\varepsilon }\n\\end{equation}\n\\]\nWhere \n\\(\\varphi(.): \\mathbb{R}^m \\rightarrow \\mathbb{R}^d\\)\n is a feature mapping, \n\\(\\mathbf{y} \\in \\mathbb{R}^n\\)\n is the vector of output values found in the training data set and \n\\(\\mathbf{\\beta} \\in \\mathbb{R}^d\\)\n is a set of regression parameters.\n\n\nIn the GLS framework, it is assumed that the model errors \n\\(\\varepsilon \\in \\mathbb{R}^n\\)\n follow a multivariate gaussian distribution given by \n\\(\\mathbb {E} [\\varepsilon |\\mathbf {X} ] = 0\\)\n and \n\\(\\operatorname{Var} [\\varepsilon |\\mathbf {X} ] = \\mathbf {\\Omega }\\)\n, where \n\\(\\mathbf{\\Omega}\\)\n is a symmetric positive semi-definite covariance matrix.\n\n\nIn order to calculate the model parameters \n\\(\\mathbf{\\beta}\\)\n, the log-likelihood of the training data outputs must be maximized with respect to the parameters \n\\(\\mathbf{\\beta}\\)\n, which leads to.\n\n\n\\[\n\\begin{equation}\n\\min_{\\mathbf{\\beta}} \\ \\mathcal{J}_P(\\mathbf{\\beta}) = (\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )^{\\mathtt {T}}\\,\\mathbf {\\Omega } ^{-1}(\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )\n\\end{equation}\n\\]\nFor the GLS problem the analytical solution of the above optimization problem can be calculated.\n\n\n\\[\n{\\displaystyle \\mathbf {\\hat {\\beta }} =\\left(\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\varphi(\\mathbf {X})\\right)^{-1}\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\mathbf {y} .}\n\\]\nGLS Models\n\u00b6\n\n\nYou can create a GLS model using the \nGeneralizedLeastSquaresModel\n class.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n//Get the training data\n\n\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n_\n\n\n//Define a feature mapping\n\n\n//If it is not defined the GLS model\n\n\n//will assume a identity feature map.\n\n\nval\n \nfeature_map\n:\n \nDenseVector\n[\nDouble\n]\n \n=>\n \nDenseVector\n[\nDouble\n]\n \n=\n \n_\n\n\n\n//Initialize a kernel function.\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n//Construct the covariance matrix for model errors.\n\n\nval\n \ncovmat\n \n=\n \nkernel\n.\nbuildBlockedKernelMatrix\n(\ndata\n,\n \ndata\n.\nlength\n)\n\n\n\nval\n \ngls_model\n \n=\n \nnew\n \nGeneralizedLeastSquaresModel\n(\n\n  \ndata\n,\n \ncovmat\n,\n\n  \nfeature_map\n)\n\n\n\n//Train the model\n\n\ngls_model\n.\nlearn\n()",
            "title": "Generalized Least Squares"
        },
        {
            "location": "/core/core_gls/#formulation",
            "text": "Let  \\(\\mathbf{X} \\in \\mathbb{R}^{n\\times m}\\)  be a matrix containing data attributes. The GLS model builds a linear predictor of the target quantity of the following form.  \\[\n\\begin{equation}\n\\mathbf {y} = \\varphi(\\mathbf {X}) \\mathbf {\\beta } +\\mathbf {\\varepsilon }\n\\end{equation}\n\\] Where  \\(\\varphi(.): \\mathbb{R}^m \\rightarrow \\mathbb{R}^d\\)  is a feature mapping,  \\(\\mathbf{y} \\in \\mathbb{R}^n\\)  is the vector of output values found in the training data set and  \\(\\mathbf{\\beta} \\in \\mathbb{R}^d\\)  is a set of regression parameters.  In the GLS framework, it is assumed that the model errors  \\(\\varepsilon \\in \\mathbb{R}^n\\)  follow a multivariate gaussian distribution given by  \\(\\mathbb {E} [\\varepsilon |\\mathbf {X} ] = 0\\)  and  \\(\\operatorname{Var} [\\varepsilon |\\mathbf {X} ] = \\mathbf {\\Omega }\\) , where  \\(\\mathbf{\\Omega}\\)  is a symmetric positive semi-definite covariance matrix.  In order to calculate the model parameters  \\(\\mathbf{\\beta}\\) , the log-likelihood of the training data outputs must be maximized with respect to the parameters  \\(\\mathbf{\\beta}\\) , which leads to.  \\[\n\\begin{equation}\n\\min_{\\mathbf{\\beta}} \\ \\mathcal{J}_P(\\mathbf{\\beta}) = (\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )^{\\mathtt {T}}\\,\\mathbf {\\Omega } ^{-1}(\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )\n\\end{equation}\n\\] For the GLS problem the analytical solution of the above optimization problem can be calculated.  \\[\n{\\displaystyle \\mathbf {\\hat {\\beta }} =\\left(\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\varphi(\\mathbf {X})\\right)^{-1}\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\mathbf {y} .}\n\\]",
            "title": "Formulation."
        },
        {
            "location": "/core/core_gls/#gls-models",
            "text": "You can create a GLS model using the  GeneralizedLeastSquaresModel  class.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 //Get the training data  val   data :   Stream [( DenseVector [ Double ] ,  Double )]   =   _  //Define a feature mapping  //If it is not defined the GLS model  //will assume a identity feature map.  val   feature_map :   DenseVector [ Double ]   =>   DenseVector [ Double ]   =   _  //Initialize a kernel function.  val   kernel :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  //Construct the covariance matrix for model errors.  val   covmat   =   kernel . buildBlockedKernelMatrix ( data ,   data . length )  val   gls_model   =   new   GeneralizedLeastSquaresModel ( \n   data ,   covmat , \n   feature_map )  //Train the model  gls_model . learn ()",
            "title": "GLS Models"
        },
        {
            "location": "/core/core_gp/",
            "text": "Gaussian Processes\n are stochastic processes whose finite dimensional distributions are multivariate gaussians.\n\n\nGaussian Processes\n are powerful non-parametric predictive models, which represent probability measures over spaces of functions. \nRamussen and Williams\n is the definitive guide on understanding their applications in machine learning and a gateway to their deeper theoretical foundations.\n\n\n\n\n\n\nGaussian Process\n models are well supported in DynaML, the \nAbstractGPRegressionModel\n[\nT\n, \nI\n]\n and \nAbstractGPClassification\n[\nT\n, \nI\n]\n classes which extend the \nStochasticProcessModel\n[\nT\n, \nI\n, \nY\n, \nW\n]\n base trait are the starting point for all GP implementations.\n\n\nGaussian Process Regression\n\u00b6\n\n\nThe GP regression framework aims to infer an unknown function \n\\(f(x)\\)\n given \n\\(y_i\\)\n which are noise corrupted observations of this unknown function. This is done by adopting an explicit probabilistic formulation to the multi-variate distribution of the noise corrupted observations \n\\(y_i\\)\n conditioned on the input features (or design matrix) \n\\(X\\)\n\n\n\\[\n\\begin{align}\n        & y = f(x) + \\epsilon \\\\\n        & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\\n        & \\left(\\mathbf{y} \\ \\ \\mathbf{f_*} \\right)^T | X \\sim \\mathcal{N}\\left(\\mathbf{0}, \\left[ \\begin{matrix} K(X, X) + \\sigma^{2} \\it{I} & K(X, X_*) \\\\ K(X_*, X) & K(X_*, X_*) \\end{matrix} \\right ] \\right)\n\\end{align}\n\\]\nIn the presence of training data\n\n\n\\[\nX = (x_1, x_2, \\cdot , x_n) \\ \\mathbf{y} = (y_1, y_2, \\cdot , y_n)\n\\]\nInference is carried out by calculating the posterior predictive distribution over the unknown targets \n\\(\\mathbf{f_*}|X,\\mathbf{y},X_*\\)\n assuming \n\\(X_*\\)\n, the test inputs are known.\n\n\n\\[\n\\begin{align}\n        & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim \\mathcal{N}(\\mathbf{\\bar{f_*}}, cov(\\mathbf{f_*}))  \\label{eq:posterior}\\\\\n        & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\\n        & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*)\n\\end{align}\n\\]\nGP models for a single output\n\u00b6\n\n\nFor univariate GP models (single output), use the \nGPRegression\n class (an extension of \nAbstractGPRegressionModel\n). To construct a GP regression model you would need:\n\n\n\n\nTraining data\n\n\nKernel/covariance instance to model correlation between values of the latent function at each pair of input features.\n\n\nKernel instance to model the correlation of the additive noise, generally the \nDiracKernel\n (white noise) is used.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \ntrainingdata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nnum_features\n \n=\n \ntrainingdata\n.\nhead\n.\n_1\n.\nlength\n\n\n\n// Create an implicit vector field for the creation of the stationary\n\n\n// radial basis function kernel\n\n\nimplicit\n \nval\n \nfield\n \n=\n \nVectorField\n(\nnum_features\n)\n\n\n\nval\n \nkernel\n \n=\n \nnew\n \nRBFKernel\n(\n2.5\n)\n\n\nval\n \nnoiseKernel\n \n=\n \nnew\n \nDiracKernel\n(\n1.5\n)\n\n\nval\n \nmodel\n \n=\n \nnew\n \nGPRegression\n(\nkernel\n,\n \nnoiseKernel\n,\n \ntrainingData\n)\n\n\n\n\n\n\n\nGP models for multiple outputs\n\u00b6\n\n\nAs reviewed in \nLawrence et.al\n, Gaussian Processes for multiple outputs can be interpreted as single output GP models with an expanded index set. Recall that GPs are stochastic processes and thus are defined on some \nindex set\n, for example in the equations above it is noted that \n\\(x \\in \\mathbb{R}^p\\)\n making \n\\(\\mathbb{R}^p\\)\n the \nindex set\n of the process.\n\n\nIn case of multiple outputs the index set is expressed as a cartesian product \n\\(x \\in \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\}\\)\n, where \n\\(d\\)\n is the number of outputs to be modeled.\n\n\nIt needs to be noted that now we will also have to define the kernel function on the same index set i.e. \n\\(\\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\}\\)\n.\n\n\nIn multi-output GP literature a common way to construct kernels on such index sets is to multiply base kernels on each of the parts \n\\(\\mathbb{R}^p\\)\n and \n\\(\\{1,2,\\cdots,d\\}\\)\n, such kernels are known as \nseparable kernels\n.\n\n\n\\[\n\\begin{equation}\nK((\\mathbf{x}, d), (\\mathbf{x}', d')) = K_{x}(\\mathbf{x}, \\mathbf{x}') . K_{d}(d, d')\n\\end{equation}\n\\]\nTaking this idea further \nsum of separable kernels\n (SoS) are often employed in multi-output GP models. These models are also known as \nLinear Models of Co-Regionalization\n (LMC) and the kernels which encode correlation between the outputs \n\\(K_d(.,.)\\)\n are known as \nco-regionalization kernels\n.\n\n\n\\[\n\\begin{equation}\nK((\\mathbf{x}, d), (\\mathbf{x}', d')) = \\sum_{i = 1}^{D} K^{i}_{x}(\\mathbf{x}, \\mathbf{x}') . K^{i}_{d}(d, d')\n\\end{equation}\n\\]\n\n\nCreating separable kernels\nCreating SoS kernels in DynaML is quite straightforward, use the \n:*\n operator to multiply a kernel defined on \nDenseVector\n[\nDouble\n]\n with a kernel defined on \nInt\n.\n\n\n```scala\n\n\nval linearK = new PolynomialKernel(2, 1.0)\nval tKernel = new TStudentKernel(0.2)\nval d = new DiracKernel(0.037)\n\n\nval mixedEffects = new MixedEffectRegularizer(0.5)\nval coRegCauchyMatrix = new CoRegCauchyKernel(10.0)\nval coRegDiracMatrix = new CoRegDiracKernel\n\n\nval sos_kernel: CompositeCovariance[(DenseVector[Double], Int)] =\n    (linearK :* mixedEffects)  + (tKernel :* coRegCauchyMatrix)\n\n\n1\n2\n3\nval sos_noise: CompositeCovariance[(DenseVector[Double], Int)] = d :* coRegDiracMatrix\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\nTip\nYou can use the \nMOGPRegressionModel\n[\nI\n]\n class to create multi-output GP models.\n\n\n1\n2\n3\n4\n5\nval\n \ntrainingdata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]\n \n=\n \n...\n\n\n\nval\n \nmodel\n \n=\n \nnew\n \nMOGPRegressionModel\n[\nDenseVector\n[\nDouble\n]](\n\n    \nsos_kernel\n,\n \nsos_noise\n,\n \ntrainingdata\n,\n\n\ntrainingdata\n.\nlength\n,\n \ntrainingdata\n.\nhead\n.\n_2\n.\nlength\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Process Binary Classification\n\u00b6\n\n\nGaussian process models for classification are formulated using two components.\n\n\n\n\nA latent (nuisance) function \n\\(f(x)\\)\n\n\nA transfer function \n\\(\\sigma(.)\\)\n which transforms the value \n\\(f(x)\\)\n to a class probability\n\n\n\n\n\\[\n    \\begin{align}\n        & \\pi(x) \\overset{\\triangle}{=} p(y = +1| x) = \\sigma(f(x)) \\\\\n        & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\\n    \\end{align}\n\\]\nInference is divided into two steps.\n\n\n\n\nComputing the distribution of the latent function corresponding to a test case\n\n\n\n\n\\[\n\\begin{align}\n    & p(f_*|X, \\mathbf{y}, x_*) = \\int p(f_*|X, \\mathbf{y}, x_*, \\mathbf{f}) p(\\mathbf{f}|X, \\mathbf{y}) d\\mathbf{f} \\\\\n    & p(\\mathbf{f}|X, \\mathbf{y}) = p(\\mathbf{y}| \\mathbf{f}) p(\\mathbf{f}|X)/ p(\\mathbf{y}|X)\n\\end{align}\n\\]\n\n\nGenerating probabilistic prediction for a test case.\n\n\n\n\n\\[\n\\bar{\\pi_*} \\overset{\\triangle}{=} p(y_* = +1| X, \\mathbf{y}, x_*) = \\int \\sigma(f_*) p(f_*|X, \\mathbf{y}, x_*) df_*\n\\]\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \ntrainingdata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nnum_features\n \n=\n \ntrainingdata\n.\nhead\n.\n_1\n.\nlength\n\n\n\n// Create an implicit vector field for the creation of the stationary\n\n\n// radial basis function kernel\n\n\nimplicit\n \nval\n \nfield\n \n=\n \nVectorField\n(\nnum_features\n)\n\n\n\nval\n \nkernel\n \n=\n \nnew\n \nRBFKernel\n(\n2.5\n)\n\n\nval\n \nlikelihood\n \n=\n \nnew\n \nVectorIIDSigmoid\n()\n\n\nval\n \nmodel\n \n=\n \nnew\n \nLaplaceBinaryGPC\n(\ntrainingData\n,\n \nkernel\n,\n \nlikelihood\n)\n\n\n\n\n\n\n\n\n\nImplementing your own GP class\nTo learn more about extending the Gaussian Process base classes/traits refer to the \nwiki\n.",
            "title": "Gaussian Processes"
        },
        {
            "location": "/core/core_gp/#gaussian-process-regression",
            "text": "The GP regression framework aims to infer an unknown function  \\(f(x)\\)  given  \\(y_i\\)  which are noise corrupted observations of this unknown function. This is done by adopting an explicit probabilistic formulation to the multi-variate distribution of the noise corrupted observations  \\(y_i\\)  conditioned on the input features (or design matrix)  \\(X\\)  \\[\n\\begin{align}\n        & y = f(x) + \\epsilon \\\\\n        & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\\n        & \\left(\\mathbf{y} \\ \\ \\mathbf{f_*} \\right)^T | X \\sim \\mathcal{N}\\left(\\mathbf{0}, \\left[ \\begin{matrix} K(X, X) + \\sigma^{2} \\it{I} & K(X, X_*) \\\\ K(X_*, X) & K(X_*, X_*) \\end{matrix} \\right ] \\right)\n\\end{align}\n\\] In the presence of training data  \\[\nX = (x_1, x_2, \\cdot , x_n) \\ \\mathbf{y} = (y_1, y_2, \\cdot , y_n)\n\\] Inference is carried out by calculating the posterior predictive distribution over the unknown targets  \\(\\mathbf{f_*}|X,\\mathbf{y},X_*\\)  assuming  \\(X_*\\) , the test inputs are known.  \\[\n\\begin{align}\n        & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim \\mathcal{N}(\\mathbf{\\bar{f_*}}, cov(\\mathbf{f_*}))  \\label{eq:posterior}\\\\\n        & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\\n        & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*)\n\\end{align}\n\\]",
            "title": "Gaussian Process Regression"
        },
        {
            "location": "/core/core_gp/#gp-models-for-a-single-output",
            "text": "For univariate GP models (single output), use the  GPRegression  class (an extension of  AbstractGPRegressionModel ). To construct a GP regression model you would need:   Training data  Kernel/covariance instance to model correlation between values of the latent function at each pair of input features.  Kernel instance to model the correlation of the additive noise, generally the  DiracKernel  (white noise) is used.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   trainingdata :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   num_features   =   trainingdata . head . _1 . length  // Create an implicit vector field for the creation of the stationary  // radial basis function kernel  implicit   val   field   =   VectorField ( num_features )  val   kernel   =   new   RBFKernel ( 2.5 )  val   noiseKernel   =   new   DiracKernel ( 1.5 )  val   model   =   new   GPRegression ( kernel ,   noiseKernel ,   trainingData )",
            "title": "GP models for a single output"
        },
        {
            "location": "/core/core_gp/#gp-models-for-multiple-outputs",
            "text": "As reviewed in  Lawrence et.al , Gaussian Processes for multiple outputs can be interpreted as single output GP models with an expanded index set. Recall that GPs are stochastic processes and thus are defined on some  index set , for example in the equations above it is noted that  \\(x \\in \\mathbb{R}^p\\)  making  \\(\\mathbb{R}^p\\)  the  index set  of the process.  In case of multiple outputs the index set is expressed as a cartesian product  \\(x \\in \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\}\\) , where  \\(d\\)  is the number of outputs to be modeled.  It needs to be noted that now we will also have to define the kernel function on the same index set i.e.  \\(\\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\}\\) .  In multi-output GP literature a common way to construct kernels on such index sets is to multiply base kernels on each of the parts  \\(\\mathbb{R}^p\\)  and  \\(\\{1,2,\\cdots,d\\}\\) , such kernels are known as  separable kernels .  \\[\n\\begin{equation}\nK((\\mathbf{x}, d), (\\mathbf{x}', d')) = K_{x}(\\mathbf{x}, \\mathbf{x}') . K_{d}(d, d')\n\\end{equation}\n\\] Taking this idea further  sum of separable kernels  (SoS) are often employed in multi-output GP models. These models are also known as  Linear Models of Co-Regionalization  (LMC) and the kernels which encode correlation between the outputs  \\(K_d(.,.)\\)  are known as  co-regionalization kernels .  \\[\n\\begin{equation}\nK((\\mathbf{x}, d), (\\mathbf{x}', d')) = \\sum_{i = 1}^{D} K^{i}_{x}(\\mathbf{x}, \\mathbf{x}') . K^{i}_{d}(d, d')\n\\end{equation}\n\\]  Creating separable kernels Creating SoS kernels in DynaML is quite straightforward, use the  :*  operator to multiply a kernel defined on  DenseVector [ Double ]  with a kernel defined on  Int .  ```scala  val linearK = new PolynomialKernel(2, 1.0)\nval tKernel = new TStudentKernel(0.2)\nval d = new DiracKernel(0.037)  val mixedEffects = new MixedEffectRegularizer(0.5)\nval coRegCauchyMatrix = new CoRegCauchyKernel(10.0)\nval coRegDiracMatrix = new CoRegDiracKernel  val sos_kernel: CompositeCovariance[(DenseVector[Double], Int)] =\n    (linearK :* mixedEffects)  + (tKernel :* coRegCauchyMatrix)  1\n2\n3 val sos_noise: CompositeCovariance[(DenseVector[Double], Int)] = d :* coRegDiracMatrix\n\n```      Tip You can use the  MOGPRegressionModel [ I ]  class to create multi-output GP models.  1\n2\n3\n4\n5 val   trainingdata :   Stream [( DenseVector [ Double ] ,  DenseVector [ Double ])]   =   ...  val   model   =   new   MOGPRegressionModel [ DenseVector [ Double ]]( \n     sos_kernel ,   sos_noise ,   trainingdata ,  trainingdata . length ,   trainingdata . head . _2 . length )",
            "title": "GP models for multiple outputs"
        },
        {
            "location": "/core/core_gp/#gaussian-process-binary-classification",
            "text": "Gaussian process models for classification are formulated using two components.   A latent (nuisance) function  \\(f(x)\\)  A transfer function  \\(\\sigma(.)\\)  which transforms the value  \\(f(x)\\)  to a class probability   \\[\n    \\begin{align}\n        & \\pi(x) \\overset{\\triangle}{=} p(y = +1| x) = \\sigma(f(x)) \\\\\n        & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\\n    \\end{align}\n\\] Inference is divided into two steps.   Computing the distribution of the latent function corresponding to a test case   \\[\n\\begin{align}\n    & p(f_*|X, \\mathbf{y}, x_*) = \\int p(f_*|X, \\mathbf{y}, x_*, \\mathbf{f}) p(\\mathbf{f}|X, \\mathbf{y}) d\\mathbf{f} \\\\\n    & p(\\mathbf{f}|X, \\mathbf{y}) = p(\\mathbf{y}| \\mathbf{f}) p(\\mathbf{f}|X)/ p(\\mathbf{y}|X)\n\\end{align}\n\\]  Generating probabilistic prediction for a test case.   \\[\n\\bar{\\pi_*} \\overset{\\triangle}{=} p(y_* = +1| X, \\mathbf{y}, x_*) = \\int \\sigma(f_*) p(f_*|X, \\mathbf{y}, x_*) df_*\n\\]  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   trainingdata :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   num_features   =   trainingdata . head . _1 . length  // Create an implicit vector field for the creation of the stationary  // radial basis function kernel  implicit   val   field   =   VectorField ( num_features )  val   kernel   =   new   RBFKernel ( 2.5 )  val   likelihood   =   new   VectorIIDSigmoid ()  val   model   =   new   LaplaceBinaryGPC ( trainingData ,   kernel ,   likelihood )     Implementing your own GP class To learn more about extending the Gaussian Process base classes/traits refer to the  wiki .",
            "title": "Gaussian Process Binary Classification"
        },
        {
            "location": "/core/core_esgp/",
            "text": "Summary\n\n\nThe \nExtended Skew Gaussian Process\n (ESGP) uses the \nMESN\n distribution to define its finite dimensional probability distribution. It can be viewed as an generalization of the \nGaussian Process\n because when its skewness parameter approaches zero, the calculated probabilities are very close to gaussian probabilities.\n\n\n\n\nThe ESGP model uses the conditioning property of the MESN distribution, just like the multivariate normal distribution, the MESN retains its form when conditioned on a subset of its dimensions.\n\n\nCreating an ESGP model is very similar to creating a GP model in DynaML. The class \nESGPModel\n[\nT\n, \nI\n]\n can be instantiated much like the \nAbstractGPRegressionModel\n[\nT\n, \nI\n]\n, using the \napply\n method.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n//Obtain the data, some generic type\n\n\nval\n \ntrainingdata\n:\n \nDataType\n \n=\n \n...\n\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nI\n]\n \n=\n \n_\n\n\nval\n \nnoiseKernel\n:\n \nLocalScalarKernel\n[\nI\n]\n \n=\n \n_\n\n\nval\n \nmeanFunc\n:\n \nDataPipe\n[\nI\n, \nDouble\n]\n \n=\n \n_\n\n\n\nval\n \nlambda\n \n=\n \n1.5\n\n\nval\n \ntau\n \n=\n \n0.5\n\n\n\n//Define how the data is converted to a compatible type\n\n\nimplicit\n \nval\n \ntransform\n:\n \nDataPipe\n[\nDataType\n, \nSeq\n[(\nI\n, \nDouble\n)]]\n \n=\n \n_\n\n\n\nval\n \nmodel\n \n=\n \nESGPModel\n(\n\n  \nkernel\n,\n \nnoiseKernel\n,\n\n  \nmeanFunc\n,\n \nlambda\n,\n \ntau\n,\n\n  \ntrainingData\n)",
            "title": "Extended Skew Gaussian Processes"
        },
        {
            "location": "/core/core_stp/",
            "text": "Student T Processes\n (STP) can be viewed as a generalization of Gaussian Processes, in GP models we use the multivariate normal distribution to model noisy observations of an unknown function. Likewise for STP models, we employ the multivariate student t distribution. Formally a student t process is a stochastic process where the finite dimensional distribution is multivariate t.\n\n\n\\[\n\\begin{align}\n\\mathbf{y} & \\in \\mathbb{R}^n \\\\\n\\mathbf{y} & \\sim MVT_{n}(\\nu, \\phi, K) \\\\\np(\\mathbf{y}) & = \\frac{\\Gamma(\\frac{\\nu + n}{2})}{((\\nu - 2)\\pi)^{n/2} \\Gamma(\\nu/2)} |K|^{-1/2} \\\\\n& \\times (1 + (\\mathbf{y} - \\phi)^T K^{-1} (\\mathbf{y} - \\phi))^{-\\frac{\\nu +n}{2}}\n\\end{align}\n\\]\nIt is known that as \n\\(\\nu \\rightarrow \\infty\\)\n, the \n\\(MVT_{n}(\\nu, \\phi, K)\\)\n tends towards the multivariate normal distribution \n\\(\\mathcal{N}_{n}(\\phi, K)\\)\n.\n\n\nRegression with Student T Processes\n\u00b6\n\n\nThe regression formulation for STP models is identical to the GP regression framework, to summarize the posterior predictive distribution takes the following form.\n\n\nSuppose \n\\(\\mathbf{t} \\sim MVT_{n_{tr} + n_t}(\\nu, \\mathbf{0}, K)\\)\n is the process producing the data.\nLet \n\\([\\mathbf{f_*}]_{n_{t} \\times 1}\\)\n represent the values of the function on the test inputs and \n\\([\\mathbf{y}]_{n_{tr} \\times 1}\\)\n represent noisy observations made on the training data points.\n\n\n\\[\n\\begin{align}\n        & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim MVT_{\\nu + n_{tr}}(\\mathbf{\\bar{f_*}}, \\frac{\\nu + \\beta - 2}{\\nu + n_{tr} - 2} \\times cov(\\mathbf{f_*}))  \\label{eq:posterior}\\\\\n    & \\beta = \\mathbf{y}^T K^{-1} \\mathbf{y} \\\\\n        & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\\n        & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*)\n\\end{align}\n\\]\nSTP models for a single output\n\u00b6\n\n\nFor univariate GP models (single output), use the \nStudentTRegressionModel\n class (an extension of \nAbstractSTPRegressionModel\n). To construct a STP regression model you would need:\n\n\n\n\nThe degrees of freedom \n\\(\\nu\\)\n\n\nKernel/covariance instance to model correlation between values of the latent function at each pair of input features.\n\n\nKernel instance to model the correlation of the additive noise, generally the \nDiracKernel\n (white noise) is used.\n\n\nTraining data\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \ntrainingdata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nnum_features\n \n=\n \ntrainingdata\n.\nhead\n.\n_1\n.\nlength\n\n\n\n// Create an implicit vector field for the creation of the stationary\n\n\n// radial basis function kernel\n\n\nimplicit\n \nval\n \nfield\n \n=\n \nVectorField\n(\nnum_features\n)\n\n\n\nval\n \nkernel\n \n=\n \nnew\n \nRBFKernel\n(\n2.5\n)\n\n\nval\n \nnoiseKernel\n \n=\n \nnew\n \nDiracKernel\n(\n1.5\n)\n\n\nval\n \nmodel\n \n=\n \nnew\n \nStudentTRegression\n(\n1.5\n,\n \nkernel\n,\n \nnoiseKernel\n,\n \ntrainingData\n)\n\n\n\n\n\n\n\nSTP models for Multiple Outputs\n\u00b6\n\n\nYou can use the \nMOStudentTRegression\n[\nI\n]\n class to create multi-output GP models.\n\n\n1\n2\n3\n4\n5\nval\n \ntrainingdata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]\n \n=\n \n...\n\n\n\nval\n \nmodel\n \n=\n \nnew\n \nMOStudentTRegression\n[\nDenseVector\n[\nDouble\n]](\n\n    \nsos_kernel\n,\n \nsos_noise\n,\n \ntrainingdata\n,\n\n    \ntrainingdata\n.\nlength\n,\n \ntrainingdata\n.\nhead\n.\n_2\n.\nlength\n)\n\n\n\n\n\n\n\n\n\nTip\nWorking with multi-output Student T models is similar to \nmulti-output GP\n models. We need to create a kernel function over the combined index set \n(\nDenseVector\n[\nDouble\n],\n \nInt\n)\n. This can be done using the \nsum of separable\n kernel idea.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nval\n \nlinearK\n \n=\n \nnew\n \nPolynomialKernel\n(\n2\n,\n \n1.0\n)\n\n\nval\n \ntKernel\n \n=\n \nnew\n \nTStudentKernel\n(\n0.2\n)\n\n\nval\n \nd\n \n=\n \nnew\n \nDiracKernel\n(\n0.037\n)\n\n\n\nval\n \nmixedEffects\n \n=\n \nnew\n \nMixedEffectRegularizer\n(\n0.5\n)\n\n\nval\n \ncoRegCauchyMatrix\n \n=\n \nnew\n \nCoRegCauchyKernel\n(\n10.0\n)\n\n\nval\n \ncoRegDiracMatrix\n \n=\n \nnew\n \nCoRegDiracKernel\n\n\n\nval\n \nsos_kernel\n:\n \nCompositeCovariance\n[(\nDenseVector\n[\nDouble\n]\n, \nInt\n)]\n \n=\n\n    \n(\nlinearK\n \n:*\n \nmixedEffects\n)\n  \n+\n \n(\ntKernel\n \n:*\n \ncoRegCauchyMatrix\n)\n\n\n\nval\n \nsos_noise\n:\n \nCompositeCovariance\n[(\nDenseVector\n[\nDouble\n]\n, \nInt\n)]\n \n=\n\n    \nd\n \n:*\n \ncoRegDiracMatrix",
            "title": "Students T Processes"
        },
        {
            "location": "/core/core_stp/#regression-with-student-t-processes",
            "text": "The regression formulation for STP models is identical to the GP regression framework, to summarize the posterior predictive distribution takes the following form.  Suppose  \\(\\mathbf{t} \\sim MVT_{n_{tr} + n_t}(\\nu, \\mathbf{0}, K)\\)  is the process producing the data.\nLet  \\([\\mathbf{f_*}]_{n_{t} \\times 1}\\)  represent the values of the function on the test inputs and  \\([\\mathbf{y}]_{n_{tr} \\times 1}\\)  represent noisy observations made on the training data points.  \\[\n\\begin{align}\n        & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim MVT_{\\nu + n_{tr}}(\\mathbf{\\bar{f_*}}, \\frac{\\nu + \\beta - 2}{\\nu + n_{tr} - 2} \\times cov(\\mathbf{f_*}))  \\label{eq:posterior}\\\\\n    & \\beta = \\mathbf{y}^T K^{-1} \\mathbf{y} \\\\\n        & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\\n        & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*)\n\\end{align}\n\\]",
            "title": "Regression with Student T Processes"
        },
        {
            "location": "/core/core_stp/#stp-models-for-a-single-output",
            "text": "For univariate GP models (single output), use the  StudentTRegressionModel  class (an extension of  AbstractSTPRegressionModel ). To construct a STP regression model you would need:   The degrees of freedom  \\(\\nu\\)  Kernel/covariance instance to model correlation between values of the latent function at each pair of input features.  Kernel instance to model the correlation of the additive noise, generally the  DiracKernel  (white noise) is used.  Training data    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   trainingdata :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   num_features   =   trainingdata . head . _1 . length  // Create an implicit vector field for the creation of the stationary  // radial basis function kernel  implicit   val   field   =   VectorField ( num_features )  val   kernel   =   new   RBFKernel ( 2.5 )  val   noiseKernel   =   new   DiracKernel ( 1.5 )  val   model   =   new   StudentTRegression ( 1.5 ,   kernel ,   noiseKernel ,   trainingData )",
            "title": "STP models for a single output"
        },
        {
            "location": "/core/core_stp/#stp-models-for-multiple-outputs",
            "text": "You can use the  MOStudentTRegression [ I ]  class to create multi-output GP models.  1\n2\n3\n4\n5 val   trainingdata :   Stream [( DenseVector [ Double ] ,  DenseVector [ Double ])]   =   ...  val   model   =   new   MOStudentTRegression [ DenseVector [ Double ]]( \n     sos_kernel ,   sos_noise ,   trainingdata , \n     trainingdata . length ,   trainingdata . head . _2 . length )     Tip Working with multi-output Student T models is similar to  multi-output GP  models. We need to create a kernel function over the combined index set  ( DenseVector [ Double ],   Int ) . This can be done using the  sum of separable  kernel idea.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 val   linearK   =   new   PolynomialKernel ( 2 ,   1.0 )  val   tKernel   =   new   TStudentKernel ( 0.2 )  val   d   =   new   DiracKernel ( 0.037 )  val   mixedEffects   =   new   MixedEffectRegularizer ( 0.5 )  val   coRegCauchyMatrix   =   new   CoRegCauchyKernel ( 10.0 )  val   coRegDiracMatrix   =   new   CoRegDiracKernel  val   sos_kernel :   CompositeCovariance [( DenseVector [ Double ] ,  Int )]   = \n     ( linearK   :*   mixedEffects )    +   ( tKernel   :*   coRegCauchyMatrix )  val   sos_noise :   CompositeCovariance [( DenseVector [ Double ] ,  Int )]   = \n     d   :*   coRegDiracMatrix",
            "title": "STP models for Multiple Outputs"
        },
        {
            "location": "/core/core_ffn_new/",
            "text": "Feed forward neural networks\n\n\n\n\n\n\n\n\nFeed forward neural networks are the most common network architectures in predictive modeling, DynaML has an implementation of feed forward architectures that is trained using \nBackpropogation\n with momentum.\n\n\nIn a feed forward neural network with a single hidden layer the predicted target \n\\(y\\)\n is expressed using the edge weights and node values in the following manner (this expression is easily extended for multi-layer nets).\n\n\n\\[\n\\begin{equation}\ny = W_2 \\sigma(W_1 \\mathbf{x} + b_1) + b_2\n\\end{equation}\n\\]\nWhere \n\\(W_1 , \\ W_2\\)\n  are matrices representing edge weights for the hidden layer and output layer respectively and \n\\(\\sigma(.)\\)\n represents a monotonic \nactivation\n function, the usual choices are \nsigmoid\n, \ntanh\n, \nlinear\n or \nrectified linear\n functions.\n\n\n\n\nThe new neural network API extends the same top level traits as the old API, i.e.\n\nNeuralNet\n[\nData\n, \nBaseGraph\n, \nInput\n, \nOutput\n,\nGraph\n \n<:\n \nNeuralGraph\n[\nBaseGraph\n, \nInput\n, \nOutput\n]]\n which itself extends the \nParameterizedLearner\n[\nData\n, \nGraph\n, \nInput\n, \nOutput\n, \nStream\n[(\nInput\n, \nOutput\n)]]\n trait.\n\n\n\n\nTip\n\n\nTo learn more about \nParameterizedLearner\n and other major model classes, refer to the \nmodel hierarchy\n specification.\n\n\n\n\nIn the case of \nNeuralNet\n, the \nparameters\n are a generic (unknown) type \nGraph\n which has to be an extension of \nNeuralGraph\n[\nBaseGraph\n, \nInput\n, \nOutput\n]\n]\n trait.\n\n\nCreating and training feed forward networks can be done by creating a back propagation instance and preparing the training data.\n\n\n\n\nTip\n\n\nFor a more in-depth picture of how the neural network API works refer to the \nneural stack\n page.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n//Data is of some generic type\n\n\nval\n \ndata\n:\n \nDataType\n \n=\n \n_\n\n\n\n//specify how this data can be\n\n\n//converted to a sequence of input and output vectors.\n\n\nval\n \ntransform\n\n\n:\n \nDataPipe\n[\nDataType\n, \nSeq\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]]\n \n=\n \n_\n\n\n\n//Create the stack factory\n\n\n//and back propagation instance\n\n\n//Input, Hidden, Output\n\n\nval\n \nbreezeStackFactory\n \n=\n \nNeuralStackFactory\n(\n\n  \nSeq\n(\n5\n,\n \n8\n,\n \n3\n))(\n\n  \nSeq\n(\nVectorSigmoid\n,\n \nVectorTansig\n)\n\n\n)\n\n\n\n//Random variable which samples layer weights\n\n\nval\n \nstackInitializer\n \n=\n \nGenericFFNeuralNet\n.\ngetWeightInitializer\n(\n\n  \nSeq\n(\n5\n,\n \n8\n,\n \n3\n)\n\n\n)\n\n\n\nval\n \nopt_backprop\n \n=\n\n  \nnew\n \nFFBackProp\n(\nbreezeStackFactory\n)\n\n    \n.\nsetNumIterations\n(\n2000\n)\n\n    \n.\nsetRegParam\n(\n0.001\n)\n\n    \n.\nsetStepSize\n(\n0.05\n)\n\n    \n.\nsetMiniBatchFraction\n(\n0.8\n)\n\n    \n.\nmomentum_\n(\n0.3\n)\n\n\n\nval\n \nff_neural_model\n \n=\n \nGenericFFNeuralNet\n(\n\n  \nopt_backprop\n,\n \ndata\n,\n\n  \ntransform\n,\n \nstackInitializer\n\n\n)\n\n\n\n//train the model\n\n\nff_neural_model\n.\nlearn\n()",
            "title": "Feed Forward Networks"
        },
        {
            "location": "/core/core_ann_new/",
            "text": "Summary\n\n\nIn v1.4.2, the neural stack API was introduced. It defines some primitives for modular\nconstruction of neural networks. The main idioms will be computational layers & stacks.  \n\n\n\n\nThe neural stack API extends these abstract skeletons by defining two kinds of primitives.\n\n\n\n\nComputational layers: Defining how inputs are propagated forward; \nNeuralLayer\n\n\nActivation functions: \nActivation\n[\nI\n]\n\n\nComputational stacks: composed of a number of layers; \nGenericNeuralStack\n\n\n\n\n\n\nNote\n\n\nThe classes \nNeuralLayer\n and \nGenericNeuralStack\n define layers and stacks in an abstract manner, meaning that the parameters could be in principle of any type.\n\n\nThe key point to understand is that once a layer or stack is defined, it is immutable i.e. the parameters defining its forward computation can't be changed.\n\n\nThe API rather provides \nfactory\n objects which can spawn a particular layer or stack with any parameter assignments.\n\n\n\n\nActivation Functions\n\u00b6\n\n\nActivation functions are implemented using the \nActivation\n[\nI\n]\n object, its \napply\n method requires two\narguments.\n\n\n\n\nImplementation of the activation\n\n\nImplementation of the derivative of the activation.\n\n\n\n\n1\n2\n3\n4\n5\n6\n//Define forward mapping\n\n\nval\n \nactFunc\n:\n \n(\nI\n)\n \n=>\n \nI\n \n=\n \n_\n\n\n//Define derivative of forward mapping\n\n\nval\n \ngradAct\n:\n \n(\nI\n)\n \n=>\n \nI\n \n=\n \n_\n\n\n\nval\n \nact\n \n=\n \nActivation\n(\nactFunc\n,\n \ngradAct\n)\n\n\n\n\n\n\n\nThe \ndynaml.models.neuralnets\n package also contains implementation of the following activations.\n\n\n\n\n\n\nSigmoid \n\\(g(x) = \\frac{1}{1 + exp(-x)}\\)\n\n\nval\n \nact\n \n=\n \nVectorSigmoid\n\n\n\n\n\n\nTanh \n\\(g(x) = tanh(x)\\)\n\n\nval\n \nact\n \n=\n \nVectorTansig\n\n\n\n\n\n\nLinear \n\\(g(x) = x\\)\n\n\nval\n \nact\n \n=\n \nVectorLinear\n\n\n\n\n\n\nRectified Linear \n\\(g(x) = \\begin{cases} x & x \\geq 0\\\\0 & else\\end{cases}\\)\n\n\nval\n \nact\n \n=\n \nVectorRecLin\n\n\n\n\n\n\nComputational Layers\n\u00b6\n\n\nComputational layers are the most basic unit of neural networks. They define transformations of their inputs and with that define the forward data flow.\n\n\nEvery computational layer generally has a set of parameters describing how this transformation is going to be calculated given the inputs.\n\n\nIn DynaML, the central component of the \nNeuralLayer\n[\nParams\n, \nInput\n, \nOutput\n]\n trait is a \nMetaPipe\n[\nParams\n, \nInput\n, \nOutput\n]\n (\nhigher order pipe\n) instance.\n\n\nCreating Layers.\n\u00b6\n\n\nCreating an immutable computational layer can be done using the \nNeuralLayer\n object.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nimport\n \nscala.math._\n\n\n\nval\n \ncompute\n \n=\n \nMetaPipe\n(\n\n  \n(\nparams\n:\n \nDouble\n)\n \n=>\n \n(\nx\n:\n \nDouble\n)\n \n=>\n \n2\nd\n*\nPi\n*\nparams\n*\nx\n\n\n)\n\n\n\nval\n \nact\n \n=\n \nActivation\n(\n\n  \n(\nx\n:\n \nDouble\n)\n \n=>\n \ntanh\n(\nx\n),\n\n  \n(\nx\n:\n \nDouble\n)\n \n=>\n \ntanh\n(\nx\n)/(\nsinh\n(\nx\n)*\ncosh\n(\nx\n)))\n\n\n\nval\n \nlayer\n \n=\n \nNeuralLayer\n(\ncompute\n,\n \nact\n)(\n0.5\n)\n\n\n\n\n\n\n\n\n\nVector feed forward layers\n\n\nA common layer is the feed forward vector to vector layer which is given by.\n$$\n\\mathbf{h} = \\sigma(\\mathbf{W} \\mathbf{x} + \\mathbf{b})\n$$\n\n\n\n\nLayer Factories.\n\u00b6\n\n\nSince the computation and activation are the only two relevant inputs required to spawn any computational layer, the \nNeuralLayerFactory\n[\nParams\n, \nInput\n, \nOutput\n]\n class is the \nfactory\n for creating layers on the fly. Layer factories are data pipes which take the layer parameters as input and create computational layers on demand.\n\n\nA layer factory can be created as follows.\n\n\n1\n2\n3\nval\n \nfact\n \n=\n \nNeuralLayerFactory\n(\ncompute\n,\n \nact\n)\n\n\n\nval\n \nlayer1\n \n=\n \nfact\n(\n0.25\n)\n\n\n\n\n\n\n\n\n\nVector layer factory\n\n\nVector layers can be created using the \nVec2VecLayerFactory\n\n\n1\n2\nval\n \nlayerFactory\n \n=\n\n  \nnew\n \nVec2VecLayerFactory\n(\nVectorTansig\n)(\ninDim\n \n=\n \n4\n,\n \noutDim\n \n=\n \n5\n)\n\n\n\n\n\n\n\n\n\nNeural Stacks\n\u00b6\n\n\nA neural stack is a sequence of computational layers. Every layer represents some computation, so the neural stack is nothing but a sequence of computations or forward data flow. The top level class for neural stacks is \nGenericNeuralStack\n. Extending the base class there are two stack implementations.\n\n\n\n\n\n\nEagerly evaluated stack: Layers are spawned as soon as the stack is created.\n\n\n1\n2\n3\n4\n5\n6\nval\n \nlayers\n:\n \nSeq\n[\nNeuralLayer\n[\nP\n, \nI\n, \nI\n]]\n \n=\n \n_\n\n\n\n//Variable argument apply function\n\n\n//so the elements of the sequence\n\n\n//must be enumerated.\n\n\nval\n \nstack\n \n=\n \nNeuralStack\n(\nlayers\n:_\n*\n)\n\n\n\n\n\n\n\n\n\n\n\nLazy stack: Layers are spawned only as needed, but once created they are \nmemoized\n.\n\n\n1\n2\n3\nval\n \nlayers_func\n:\n \n(\nInt\n)\n \n=>\n \nNeuralLayer\n[\nP\n, \nI\n, \nI\n]\n \n=\n \n_\n\n\n\nval\n \nstack\n \n=\n \nLazyNeuralStack\n(\nlayers_func\n,\n \nnum_layers\n \n=\n \n4\n)\n\n\n\n\n\n\n\n\n\n\n\nStack Factories\n\u00b6\n\n\nStack factories like layer factories are pipe lines, which take as input a sequence of layer parameters and return a neural stack of the spawned layers.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nval\n \nlayerFactories\n:\n \nSeq\n[\nNeuralLayerFactory\n[\nP\n, \nI\n, \nI\n]]\n \n=\n \n_\n\n\n//Create a stack factory from a sequence of layer factories\n\n\nval\n \nstackFactory\n \n=\n \nNeuralStackFactory\n(\nlayerFactories\n:_\n*\n)\n\n\n\n//Create a stack factory that creates\n\n\n//feed forward neural stacks that take as inputs\n\n\n//breeze vectors.\n\n\n\n//Input, Hidden, Output\n\n\nval\n \nnum_units_by_layer\n \n=\n \nSeq\n(\n5\n,\n \n8\n,\n \n3\n)\n\n\nval\n \nacts\n \n=\n \nSeq\n(\nVectorSigmoid\n,\n \nVectorTansig\n)\n\n\nval\n \nbreezeStackFactory\n \n=\n \nNeuralStackFactory\n(\nnum_units_by_layer\n)(\nacts\n)",
            "title": "Neural Stack API"
        },
        {
            "location": "/core/core_ann_new/#activation-functions",
            "text": "Activation functions are implemented using the  Activation [ I ]  object, its  apply  method requires two\narguments.   Implementation of the activation  Implementation of the derivative of the activation.   1\n2\n3\n4\n5\n6 //Define forward mapping  val   actFunc :   ( I )   =>   I   =   _  //Define derivative of forward mapping  val   gradAct :   ( I )   =>   I   =   _  val   act   =   Activation ( actFunc ,   gradAct )    The  dynaml.models.neuralnets  package also contains implementation of the following activations.    Sigmoid  \\(g(x) = \\frac{1}{1 + exp(-x)}\\)  val   act   =   VectorSigmoid    Tanh  \\(g(x) = tanh(x)\\)  val   act   =   VectorTansig    Linear  \\(g(x) = x\\)  val   act   =   VectorLinear    Rectified Linear  \\(g(x) = \\begin{cases} x & x \\geq 0\\\\0 & else\\end{cases}\\)  val   act   =   VectorRecLin",
            "title": "Activation Functions"
        },
        {
            "location": "/core/core_ann_new/#computational-layers",
            "text": "Computational layers are the most basic unit of neural networks. They define transformations of their inputs and with that define the forward data flow.  Every computational layer generally has a set of parameters describing how this transformation is going to be calculated given the inputs.  In DynaML, the central component of the  NeuralLayer [ Params ,  Input ,  Output ]  trait is a  MetaPipe [ Params ,  Input ,  Output ]  ( higher order pipe ) instance.",
            "title": "Computational Layers"
        },
        {
            "location": "/core/core_ann_new/#creating-layers",
            "text": "Creating an immutable computational layer can be done using the  NeuralLayer  object.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 import   scala.math._  val   compute   =   MetaPipe ( \n   ( params :   Double )   =>   ( x :   Double )   =>   2 d * Pi * params * x  )  val   act   =   Activation ( \n   ( x :   Double )   =>   tanh ( x ), \n   ( x :   Double )   =>   tanh ( x )/( sinh ( x )* cosh ( x )))  val   layer   =   NeuralLayer ( compute ,   act )( 0.5 )     Vector feed forward layers  A common layer is the feed forward vector to vector layer which is given by.\n$$\n\\mathbf{h} = \\sigma(\\mathbf{W} \\mathbf{x} + \\mathbf{b})\n$$",
            "title": "Creating Layers."
        },
        {
            "location": "/core/core_ann_new/#layer-factories",
            "text": "Since the computation and activation are the only two relevant inputs required to spawn any computational layer, the  NeuralLayerFactory [ Params ,  Input ,  Output ]  class is the  factory  for creating layers on the fly. Layer factories are data pipes which take the layer parameters as input and create computational layers on demand.  A layer factory can be created as follows.  1\n2\n3 val   fact   =   NeuralLayerFactory ( compute ,   act )  val   layer1   =   fact ( 0.25 )     Vector layer factory  Vector layers can be created using the  Vec2VecLayerFactory  1\n2 val   layerFactory   = \n   new   Vec2VecLayerFactory ( VectorTansig )( inDim   =   4 ,   outDim   =   5 )",
            "title": "Layer Factories."
        },
        {
            "location": "/core/core_ann_new/#neural-stacks",
            "text": "A neural stack is a sequence of computational layers. Every layer represents some computation, so the neural stack is nothing but a sequence of computations or forward data flow. The top level class for neural stacks is  GenericNeuralStack . Extending the base class there are two stack implementations.    Eagerly evaluated stack: Layers are spawned as soon as the stack is created.  1\n2\n3\n4\n5\n6 val   layers :   Seq [ NeuralLayer [ P ,  I ,  I ]]   =   _  //Variable argument apply function  //so the elements of the sequence  //must be enumerated.  val   stack   =   NeuralStack ( layers :_ * )      Lazy stack: Layers are spawned only as needed, but once created they are  memoized .  1\n2\n3 val   layers_func :   ( Int )   =>   NeuralLayer [ P ,  I ,  I ]   =   _  val   stack   =   LazyNeuralStack ( layers_func ,   num_layers   =   4 )",
            "title": "Neural Stacks"
        },
        {
            "location": "/core/core_ann_new/#stack-factories",
            "text": "Stack factories like layer factories are pipe lines, which take as input a sequence of layer parameters and return a neural stack of the spawned layers.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 val   layerFactories :   Seq [ NeuralLayerFactory [ P ,  I ,  I ]]   =   _  //Create a stack factory from a sequence of layer factories  val   stackFactory   =   NeuralStackFactory ( layerFactories :_ * )  //Create a stack factory that creates  //feed forward neural stacks that take as inputs  //breeze vectors.  //Input, Hidden, Output  val   num_units_by_layer   =   Seq ( 5 ,   8 ,   3 )  val   acts   =   Seq ( VectorSigmoid ,   VectorTansig )  val   breezeStackFactory   =   NeuralStackFactory ( num_units_by_layer )( acts )",
            "title": "Stack Factories"
        },
        {
            "location": "/core/core_ann/",
            "text": "Warning\nThis API is deprecated since v1.4.2, users are advised to use the new \nneural stack API\n.\n\n\n\n\n\n\nFeed-forward Network\n\u00b6\n\n\nTo create a feedforward network we need three entities.\n\n\n\n\nThe training data (type parameter \nD\n)\n\n\nA data pipe which transforms the original data into a data structure that understood by the \nFeedForwardNetwork\n\n\nThe network architecture (i.e. the network as a graph object)\n\n\n\n\nNetwork graph\n\u00b6\n\n\nA standard feedforward network can be created by first initializing the network architecture/graph.\n\n\n1\n2\nval\n \ngr\n \n=\n \nFFNeuralGraph\n(\nnum_inputs\n \n=\n \n3\n,\n \nnum_outputs\n \n=\n \n1\n,\n\n\nhidden_layers\n \n=\n \n1\n,\n \nList\n(\n\"logsig\"\n,\n \n\"linear\"\n),\n \nList\n(\n5\n))\n\n\n\n\n\n\n\nThis creates a neural network graph with one hidden layer, 3 input nodes, 1 output node and assigns sigmoid activation in the hidden layer. It also creates 5 neurons in the hidden layer.\n\n\nNext we create a data transform pipe which converts instances of the data input-output patterns to \n(DenseVector[Double], DenseVector[Double])\n, this is required in many data processing applications where the data structure storing the training data is not a \nbreeze\n vector.\n\n\nLets say we have data in the form \ntrainingdata: Stream[(DenseVector[Double], Double)]\n, i.e. we have input features as breeze vectors and scalar output values which help the network learn an unknown function. We can write the transform as.\n\n\n1\n2\n3\n4\nval\n \ntransform\n \n=\n \nDataPipe\n(\n\n    \n(\nd\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)])\n \n=>\n\n        \nd\n.\nmap\n(\nel\n \n=>\n \n(\nel\n.\n_1\n,\n \nDenseVector\n(\nel\n.\n_2\n)))\n\n\n)\n\n\n\n\n\n\n\nModel Building\n\u00b6\n\n\nWe are now in a position to initialize a feed forward neural network model.\n\n\n1\n2\n3\nval\n \nmodel\n \n=\n \nnew\n \nFeedForwardNetwork\n[\n\n    \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n\n\n](\ntrainingdata\n,\n \ngr\n,\n \ntransform\n)\n\n\n\n\n\n\n\nHere the variable \ntrainingdata\n represents the training input output pairs, which must conform to the type argument given in square brackets (i.e. \nStream[(DenseVector[Double], Double)]\n).\n\n\nTraining the model using back propagation can be done as follows, you can set custom values for the backpropagation parameters like the learning rate, momentum factor, mini batch fraction, regularization and number of learning iterations.\n\n\n1\n2\n3\n4\n5\n6\nmodel\n.\nsetLearningRate\n(\n0.09\n)\n\n   \n.\nsetMaxIterations\n(\n100\n)\n\n   \n.\nsetBatchFraction\n(\n0.85\n)\n\n   \n.\nsetMomentum\n(\n0.45\n)\n\n   \n.\nsetRegParam\n(\n0.0001\n)\n\n   \n.\nlearn\n()\n\n\n\n\n\n\n\nThe trained model can now be used for prediction, by using either the \npredict()\n method or the \nfeedForward()\n value member both of which are members of \nFeedForwardNetwork\n (refer to the \napi\n docs for more details).\n\n\n1\n2\nval\n \npattern\n \n=\n \nDenseVector\n(\n2.0\n,\n \n3.5\n,\n \n2.5\n)\n\n\nval\n \nprediction\n \n=\n \nmodel\n.\npredict\n(\npattern\n)\n\n\n\n\n\n\n\nSparse Autoencoder\n\u00b6\n\n\nSparse autoencoders\n are a feedforward architecture that are useful for unsupervised feature learning. They learn a compressed (or expanded) vector representation of the original data features. This process is known by various terms like \nfeature learning\n, \nfeature engineering\n, \nrepresentation learning\n etc. Autoencoders are amongst several models used for feature learning. Other notable examples include \nconvolutional neural networks\n (CNN), \nprincipal component analysis\n (PCA), \nSingular Value Decomposition\n (PCA) (a variant of  PCA), \nDiscrete Wavelet Transform\n (DWT), etc.\n\n\nCreation\n\u00b6\n\n\nAutoencoders can be created using the \nAutoEncoder\n class. Its constructor has the following arguments.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nimport\n \nio.github.mandar2812.dynaml.models.neuralnets._\n\n\nimport\n \nio.github.mandar2812.dynaml.models.neuralnets.TransferFunctions._\n\n\nimport\n \nio.github.mandar2812.dynaml.optimization.BackPropagation\n\n\n\n//Cast the training data as a stream of (x,x),\n\n\n//where x are the DenseVector of features\n\n\nval\n \ntrainingData\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]\n \n=\n \n...\n\n\n\nval\n \ntestData\n \n=\n \n...\n\n\n\nval\n \nenc\n \n=\n \nnew\n \nAutoEncoder\n(\n\n    \ninDim\n \n=\n \ntrainingData\n.\nhead\n.\n_1\n.\nlength\n,\n\n    \noutDim\n \n=\n \n4\n,\n \nacts\n \n=\n \nList\n(\nSIGMOID\n,\n \nLIN\n))\n\n\n\n\n\n\n\nTraining\n\u00b6\n\n\nThe training algorithm used is a modified version of standard back-propagation. The objective function can be seen as an addition of three terms.\n\n\n\\[\n\\begin{align}\n\\mathcal{J}(\\mathbf{W}, \\mathbf{X}; \\lambda, \\rho) &= \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) + \\lambda \\mathcal{R}(\\mathbf{W}) + KL(\\hat{\\rho}\\ ||\\ \\rho) \\\\\nKL(\\hat{\\rho}\\ ||\\ \\rho) &= \\sum_{i = 1}^{n_h} \\rho log(\\frac{\\rho}{\\hat{\\rho}_i}) + (1 - \\rho) log(\\frac{1-\\rho}{1-\\hat{\\rho}_i}) \\\\\n\\hat{\\rho}_i &= \\frac{1}{m} \\sum_{j = 1}^{N} a_{i}(x_j)\n\\end{align}\n\\]\n\n\n\n\n\\(\\mathcal{L}(\\mathbf{W}, \\mathbf{X})\\)\n is the least squares loss.\n\n\n\n\n\n\n\\(\\mathcal{R}(\\mathbf{W})\\)\n is the regularization penalty, with parameter \n\\(\\lambda\\)\n.\n\n\n\n\n\n\n\\(KL(\\hat{\\rho} \\| \\rho)\\)\n is the \nKullback Leibler\n divergence, between the average activation (over all data instances \n\\(x \\in \\mathbf{X}\\)\n) of each hidden node and a specified value \n\\(\\rho \\in [0,1]\\)\n which is also known as the \nsparsity weight\n.\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n//Set sparsity parameter for back propagation\n\n\nBackPropagation\n.\nrho\n \n=\n \n0.5\n\n\n\nenc\n.\noptimizer\n\n  \n.\nsetRegParam\n(\n0.0\n)\n\n  \n.\nsetStepSize\n(\n1.5\n)\n\n  \n.\nsetNumIterations\n(\n200\n)\n\n  \n.\nsetMomentum\n(\n0.4\n)\n\n  \n.\nsetSparsityWeight\n(\n0.9\n)\n\n\n\nenc\n.\nlearn\n(\ntrainingData\n.\ntoStream\n)\n\n\n\nval\n \nmetrics\n \n=\n \nnew\n \nMultiRegressionMetrics\n(\n\n    \ntestData\n.\nmap\n(\nc\n \n=>\n \n(\nenc\n.\ni\n(\nenc\n(\nc\n.\n_1\n)),\n \nc\n.\n_2\n)).\ntoList\n,\n\n    \ntestData\n.\nlength\n)",
            "title": "Old Neural Net API"
        },
        {
            "location": "/core/core_ann/#feed-forward-network",
            "text": "To create a feedforward network we need three entities.   The training data (type parameter  D )  A data pipe which transforms the original data into a data structure that understood by the  FeedForwardNetwork  The network architecture (i.e. the network as a graph object)",
            "title": "Feed-forward Network"
        },
        {
            "location": "/core/core_ann/#network-graph",
            "text": "A standard feedforward network can be created by first initializing the network architecture/graph.  1\n2 val   gr   =   FFNeuralGraph ( num_inputs   =   3 ,   num_outputs   =   1 ,  hidden_layers   =   1 ,   List ( \"logsig\" ,   \"linear\" ),   List ( 5 ))    This creates a neural network graph with one hidden layer, 3 input nodes, 1 output node and assigns sigmoid activation in the hidden layer. It also creates 5 neurons in the hidden layer.  Next we create a data transform pipe which converts instances of the data input-output patterns to  (DenseVector[Double], DenseVector[Double]) , this is required in many data processing applications where the data structure storing the training data is not a  breeze  vector.  Lets say we have data in the form  trainingdata: Stream[(DenseVector[Double], Double)] , i.e. we have input features as breeze vectors and scalar output values which help the network learn an unknown function. We can write the transform as.  1\n2\n3\n4 val   transform   =   DataPipe ( \n     ( d :   Stream [( DenseVector [ Double ] ,  Double )])   => \n         d . map ( el   =>   ( el . _1 ,   DenseVector ( el . _2 )))  )",
            "title": "Network graph"
        },
        {
            "location": "/core/core_ann/#model-building",
            "text": "We are now in a position to initialize a feed forward neural network model.  1\n2\n3 val   model   =   new   FeedForwardNetwork [ \n     Stream [( DenseVector [ Double ] ,  Double )]  ]( trainingdata ,   gr ,   transform )    Here the variable  trainingdata  represents the training input output pairs, which must conform to the type argument given in square brackets (i.e.  Stream[(DenseVector[Double], Double)] ).  Training the model using back propagation can be done as follows, you can set custom values for the backpropagation parameters like the learning rate, momentum factor, mini batch fraction, regularization and number of learning iterations.  1\n2\n3\n4\n5\n6 model . setLearningRate ( 0.09 ) \n    . setMaxIterations ( 100 ) \n    . setBatchFraction ( 0.85 ) \n    . setMomentum ( 0.45 ) \n    . setRegParam ( 0.0001 ) \n    . learn ()    The trained model can now be used for prediction, by using either the  predict()  method or the  feedForward()  value member both of which are members of  FeedForwardNetwork  (refer to the  api  docs for more details).  1\n2 val   pattern   =   DenseVector ( 2.0 ,   3.5 ,   2.5 )  val   prediction   =   model . predict ( pattern )",
            "title": "Model Building"
        },
        {
            "location": "/core/core_ann/#sparse-autoencoder",
            "text": "Sparse autoencoders  are a feedforward architecture that are useful for unsupervised feature learning. They learn a compressed (or expanded) vector representation of the original data features. This process is known by various terms like  feature learning ,  feature engineering ,  representation learning  etc. Autoencoders are amongst several models used for feature learning. Other notable examples include  convolutional neural networks  (CNN),  principal component analysis  (PCA),  Singular Value Decomposition  (PCA) (a variant of  PCA),  Discrete Wavelet Transform  (DWT), etc.",
            "title": "Sparse Autoencoder"
        },
        {
            "location": "/core/core_ann/#creation",
            "text": "Autoencoders can be created using the  AutoEncoder  class. Its constructor has the following arguments.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 import   io.github.mandar2812.dynaml.models.neuralnets._  import   io.github.mandar2812.dynaml.models.neuralnets.TransferFunctions._  import   io.github.mandar2812.dynaml.optimization.BackPropagation  //Cast the training data as a stream of (x,x),  //where x are the DenseVector of features  val   trainingData :   Stream [( DenseVector [ Double ] ,  DenseVector [ Double ])]   =   ...  val   testData   =   ...  val   enc   =   new   AutoEncoder ( \n     inDim   =   trainingData . head . _1 . length , \n     outDim   =   4 ,   acts   =   List ( SIGMOID ,   LIN ))",
            "title": "Creation"
        },
        {
            "location": "/core/core_ann/#training",
            "text": "The training algorithm used is a modified version of standard back-propagation. The objective function can be seen as an addition of three terms.  \\[\n\\begin{align}\n\\mathcal{J}(\\mathbf{W}, \\mathbf{X}; \\lambda, \\rho) &= \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) + \\lambda \\mathcal{R}(\\mathbf{W}) + KL(\\hat{\\rho}\\ ||\\ \\rho) \\\\\nKL(\\hat{\\rho}\\ ||\\ \\rho) &= \\sum_{i = 1}^{n_h} \\rho log(\\frac{\\rho}{\\hat{\\rho}_i}) + (1 - \\rho) log(\\frac{1-\\rho}{1-\\hat{\\rho}_i}) \\\\\n\\hat{\\rho}_i &= \\frac{1}{m} \\sum_{j = 1}^{N} a_{i}(x_j)\n\\end{align}\n\\]   \\(\\mathcal{L}(\\mathbf{W}, \\mathbf{X})\\)  is the least squares loss.    \\(\\mathcal{R}(\\mathbf{W})\\)  is the regularization penalty, with parameter  \\(\\lambda\\) .    \\(KL(\\hat{\\rho} \\| \\rho)\\)  is the  Kullback Leibler  divergence, between the average activation (over all data instances  \\(x \\in \\mathbf{X}\\) ) of each hidden node and a specified value  \\(\\rho \\in [0,1]\\)  which is also known as the  sparsity weight .     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 //Set sparsity parameter for back propagation  BackPropagation . rho   =   0.5  enc . optimizer \n   . setRegParam ( 0.0 ) \n   . setStepSize ( 1.5 ) \n   . setNumIterations ( 200 ) \n   . setMomentum ( 0.4 ) \n   . setSparsityWeight ( 0.9 )  enc . learn ( trainingData . toStream )  val   metrics   =   new   MultiRegressionMetrics ( \n     testData . map ( c   =>   ( enc . i ( enc ( c . _1 )),   c . _2 )). toList , \n     testData . length )",
            "title": "Training"
        },
        {
            "location": "/core/core_lssvm/",
            "text": "Least Squares Support Vector Machines are a modification of the classical Support Vector Machine, please see \nSuykens et. al\n for a complete background.\n\n\n\n\nLSSVM Regression\n\u00b6\n\n\nIn case of LSSVM regression one solves (by applying the \nKKT\n conditions) the following constrained optimization problem.\n\n\n\\[\n    \\begin{align}\n        & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\\n        & y_k = w^\\intercal \\varphi(x) + b + e_k, \\ k =1, \\cdots, N\n    \\end{align}\n\\]\nLeading to a predictive model of the form.\n\n\n\\[\n    \\begin{equation}\n        y(x) = \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b\n    \\end{equation}\n\\]\nWhere the values \n\\(\\alpha \\ \\& \\ b\\)\n are the solution of\n\n\n\\[\n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\end{equation}\n\\]\nHere \nK\n is the \n\\(N \\times N\\)\n kernel matrix whose entries are given by \n\\(K_{kl} = \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N\\)\n and \n\\(I\\)\n is the identity matrix of order \n\\(N\\)\n.\n\n\nLSSVM Classification\n\u00b6\n\n\nIn case of LSSVM for binary classification one solves (by applying the \nKKT\n conditions) the following constrained optimization problem.\n\n\n\\[\n    \\begin{align}\n        & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\\n        & y_k[w^\\intercal \\varphi(x) + b] = 1 - e_k, \\ k =1, \\cdots, N\n    \\end{align}\n\\]\nLeading to a classifier of the form.\n\n\n\\[\n    \\begin{equation}\n        y(x) = sign \\left[ \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\right]\n    \\end{equation}\n\\]\nWhere the values \n\\(\\alpha \\ \\& \\ b\\)\n are the solution of\n\n\n\\[\n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & y^\\intercal   \\\\ \\hline\n   y & \\Omega + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   1_v  \n\\end{array}\\right]\n\\end{equation}\n\\]\nHere \n\\(\\Omega\\)\n is the \n\\(N \\times N\\)\n matrix whose entries are given by\n\n\n\\[\n\\begin{align}\n \\Omega_{kl} & = y_{k} y_{l} \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N \\\\\n             & = y_{k} y_{l} K(x_k, x_l)\n\\end{align}\n\\]\nand \n\\(I\\)\n is the identity matrix of order \n\\(N\\)\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n// Create the training data set\n\n\n\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nnumPoints\n \n=\n \ndata\n.\nlength\n\n\nval\n \nnum_features\n \n=\n \ndata\n.\nhead\n.\n_1\n.\nlength\n\n\n\n// Create an implicit vector field for the creation of the stationary\n\n\n// radial basis function kernel\n\n\n\nimplicit\n \nval\n \nfield\n \n=\n \nVectorField\n(\nnum_features\n)\n\n\nval\n \nkern\n \n=\n \nnew\n \nRBFKernel\n(\n2.0\n)\n\n\n\n//Create the model\n\n\nval\n \nlssvmModel\n \n=\n \nnew\n \nDLSSVM\n(\ndata\n,\n \nnumPoints\n,\n \nkern\n,\n \nmodelTask\n \n=\n \n\"regression\"\n)\n\n\n\n//Set the regularization parameter and learn the model\n\n\nmodel\n.\nsetRegParam\n(\n1.5\n).\nlearn\n()",
            "title": "Least Squares SVM"
        },
        {
            "location": "/core/core_lssvm/#lssvm-regression",
            "text": "In case of LSSVM regression one solves (by applying the  KKT  conditions) the following constrained optimization problem.  \\[\n    \\begin{align}\n        & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\\n        & y_k = w^\\intercal \\varphi(x) + b + e_k, \\ k =1, \\cdots, N\n    \\end{align}\n\\] Leading to a predictive model of the form.  \\[\n    \\begin{equation}\n        y(x) = \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b\n    \\end{equation}\n\\] Where the values  \\(\\alpha \\ \\& \\ b\\)  are the solution of  \\[\n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\end{equation}\n\\] Here  K  is the  \\(N \\times N\\)  kernel matrix whose entries are given by  \\(K_{kl} = \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N\\)  and  \\(I\\)  is the identity matrix of order  \\(N\\) .",
            "title": "LSSVM Regression"
        },
        {
            "location": "/core/core_lssvm/#lssvm-classification",
            "text": "In case of LSSVM for binary classification one solves (by applying the  KKT  conditions) the following constrained optimization problem.  \\[\n    \\begin{align}\n        & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\\n        & y_k[w^\\intercal \\varphi(x) + b] = 1 - e_k, \\ k =1, \\cdots, N\n    \\end{align}\n\\] Leading to a classifier of the form.  \\[\n    \\begin{equation}\n        y(x) = sign \\left[ \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\right]\n    \\end{equation}\n\\] Where the values  \\(\\alpha \\ \\& \\ b\\)  are the solution of  \\[\n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & y^\\intercal   \\\\ \\hline\n   y & \\Omega + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   1_v  \n\\end{array}\\right]\n\\end{equation}\n\\] Here  \\(\\Omega\\)  is the  \\(N \\times N\\)  matrix whose entries are given by  \\[\n\\begin{align}\n \\Omega_{kl} & = y_{k} y_{l} \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N \\\\\n             & = y_{k} y_{l} K(x_k, x_l)\n\\end{align}\n\\] and  \\(I\\)  is the identity matrix of order  \\(N\\) .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 // Create the training data set  val   data :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   numPoints   =   data . length  val   num_features   =   data . head . _1 . length  // Create an implicit vector field for the creation of the stationary  // radial basis function kernel  implicit   val   field   =   VectorField ( num_features )  val   kern   =   new   RBFKernel ( 2.0 )  //Create the model  val   lssvmModel   =   new   DLSSVM ( data ,   numPoints ,   kern ,   modelTask   =   \"regression\" )  //Set the regularization parameter and learn the model  model . setRegParam ( 1.5 ). learn ()",
            "title": "LSSVM Classification"
        },
        {
            "location": "/core/core_kernels/",
            "text": "The \ndynaml.kernels\n package has a highly developed API for creating kernel functions for machine learning applications. Here\nwe give the user an in-depth introduction to its capabilities.\n\n\n\n\n\n\n\n\nPositive definite\n functions or \npositive type\n functions occupy an important place in various areas of mathematics, from the construction of covariances of random variables to quantifying distance measures in \nHilbert spaces\n. Symmetric positive type functions defined on the cartesian product of a set with itself \n\\(K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}\\)\n are also known as \nkernel\n functions in machine learning. They are applied extensively in problems such as.\n\n\n\n\nModel non-linear behavior in SVM models: \nSVM\n and \nLSSVM\n\n\nQuantify covariance between input patterns: \nGaussian Processes\n\n\nRepresent degree of 'closeness' or affinity in unsupervised learning: \nKernel Spectral Clustering\n\n\n\n\nFor an in depth review of the various applications of kernels in the machine learning domain, refer to \nScholkopf et. al\n\n\n\n\nNomenclature\n\n\nIn the machine learning community the words \nkernel\n and \ncovariance function\n are used interchangeably.\n\n\n\n\nKernel API\n\u00b6\n\n\nThe kernel class hierarchy all stems from a simple trait shown here.\n\n\n1\n2\n3\ntrait\n \nKernel\n[\nT\n, \nV\n]\n \n{\n\n  \ndef\n \nevaluate\n(\nx\n:\n \nT\n,\n \ny\n:\n \nT\n)\n:\n \nV\n\n\n}\n\n\n\n\n\n\n\nThis outlines only one key feature for kernel functions i.e. their evaluation functional which takes two inputs from \n\\(\\mathcal{X}\\)\n and yields a scalar value.\n\n\n\n\nKernel\n vs \nCovarianceFunction\n\n\n\nFor practical purposes, the \nKernel\n[\nT\n, \nV\n]\n trait does not have enough functionality for usage in varied models like \nGaussian Processes\n, \nStudent's T Processes\n, \nLS-SVM\n etc.\n\n\nFor this reason there is the \nCovarianceFunction\n[\nT\n, \nV\n, \nM\n]\n abstract class. It contains methods to construct kernel matrices, keep track of hyper-parameter assignments among other things.\n\n\n\n\nCreating arbitrary kernel functions\n\u00b6\n\n\nApart from off the shelf kernel functions, it is also possible to create custom kernels on the fly by using the \nCovarianceFunction\n object.\n\n\nConstructing kernels via feature maps\n\u00b6\n\n\nIt is known from \nMercer's theorem\n that any valid kernel function must be decomposable as a dot product between certain \nbasis function\n representation of the inputs. This translates mathematically into.\n\n\n\\[\n\\begin{align}\n    & K(\\mathbf{x}, \\mathbf{y}) = \\varphi^{T}(\\mathbf{x}) . \\varphi(\\mathbf{y}) \\\\\n    & \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^n\n\\end{align}\n\\]\nThe function \n\\(\\varphi(.)\\)\n is some higher (possibly infinite) dimensional representation of the input features of a data point. Note that the input space \n\\(\\mathcal{X}\\)\n could be any of the following (but not limited to).\n\n\n\n\n\n\nThe space of all connection graphs with specific number of nodes.\n\n\n\n\n\n\nA multi-dimensional vector.\n\n\n\n\n\n\nThe space of all character sequences (binary or otherwise) up to a certain length.\n\n\n\n\n\n\nThe set of all integer tuples e.g. \n\\((1,2), (6,10), \\cdots\\)\n\n\n\n\n\n\nWe can use any function from some domain \n\\(\\mathcal{X}\\)\n yielding a \nDenseVector\n[\nDouble\n]\n to define a particular inner product/kernel function.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n// First create a function mapping from some input space to\n\n\n// Breeze dense vectors.\n\n\n\nval\n \nmapFunc\n \n=\n \n(\nvec\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n \n{\n\n    \nval\n \nmat\n \n=\n \nvec\n \n*\n \nvec\n.\nt\n\n    \nmat\n.\ntoDenseVector\n\n\n}\n\n\n\nval\n \nkernel\n \n=\n \nCovarianceFunction\n(\nmapFunc\n)\n\n\n\n\n\n\n\n\n\nFeature map kernels\n\n\nCovariance functions constructed using feature mappings as shown above return a special object; an instance of the \nFeatureMapCovariance\n[\nT\n, \nDenseVector\n[\nDouble\n]]\n class. In the section on composite kernels we will see why this is important.\n\n\n\n\nConstructing kernels via direct evaluation\n\u00b6\n\n\nInstead of defining a feature representation like \n\\(\\varphi(.)\\)\n as in the section above, you can also directly define the evaluation expression of the kernel.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n// Create the expression for the required kernel.\n\n\nval\n \nmapFunc\n \n=\n\n\n(\nstate\n:\n \nMap\n[\nString\n, \nDouble\n])\n \n=>\n\n  \n(\nx\n:\n \nDenseVector\n[\nDouble\n],\n \ny\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n \n{\n\n       \nstate\n(\n\"alpha\"\n)*(\nx\n \ndot\n \ny\n)\n \n+\n \nstate\n(\n\"intercept\"\n)\n\n  \n}\n\n\n\n//Creates kernel with two hyper-parameters: alpha and intercept\n\n\nval\n \nkernel\n \n=\n \nCovarianceFunction\n(\nmapFunc\n)(\n\n  \nMap\n(\n\"alpha\"\n \n->\n \n1.5\n,\n \n\"intercept\"\n \n->\n \n0.01\n)\n\n\n)\n\n\n\n\n\n\n\n\n\nCreating Composite Kernels\n\u00b6\n\n\nAlgebraic Operations\n\u00b6\n\n\nIn machine learning it is well known that kernels can be combined to give other valid kernels. The symmetric positive semi-definite property of a kernel is preserved as long as it is added or multiplied to another valid kernel. In DynaML adding and multiplying kernels is elementary.\n\n\n1\n2\n3\n4\n5\nval\n \nk1\n \n=\n \nnew\n \nRBFKernel\n(\n2.5\n)\n\n\nval\n \nk2\n \n=\n \nnew\n \nRationalQuadraticKernel\n(\n2.0\n)\n\n\n\nval\n \nk\n \n=\n \nk1\n \n+\n \nk2\n\n\nval\n \nk3\n \n=\n \nk\n*\nk2\n\n\n\n\n\n\n\nComposition\n\u00b6\n\n\nFrom Mercer's theorem, every kernel can be expressed as a dot product of feature mappings evaluated at the respective data points. We can use this to construct more complex covariances i.e. by successively applying feature mappings.\n\n\n\\[\n\\begin{align}\nC_{a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{a}(\\mathbf{x})^\\intercal \\varphi_{a}(\\mathbf{y}) \\\\\nC_{b}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\mathbf{x})^\\intercal \\varphi_{b}(\\mathbf{y}) \\\\\nC_{b . a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\varphi_{a}(\\mathbf{x}))^\\intercal \\varphi_{b}(\\varphi_{a}(\\mathbf{y}))\n\\end{align}\n\\]\nIn DynaML, we can create a composite kernel if the kernel represented by the map \n\\(\\varphi_{a}\\)\n, is explicitly of type \nFeatureMapCovariance\n[\nT\n, \nDenseVector\n[\nDouble\n]]\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \nmapFunc\n \n=\n \n(\nvec\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n \n{\n\n    \nvec\n/\n2\nd\n\n\n}\n\n\n\nval\n \nk1\n \n=\n \nCovarianceFunction\n(\nmapFunc\n)\n\n\n\nval\n \nk2\n \n=\n \nnew\n \nRationalQuadraticKernel\n(\n2.0\n)\n\n\n\n//Composite kernel\n\n\nval\n \nk3\n \n=\n \nk2\n \n>\n \nk1\n\n\n\n\n\n\n\nScaling Covariances\n\u00b6\n\n\nIf \n\\(C(\\mathbf{x}, \\mathbf{y})\\)\n is a valid covariance function, then \n\\(g(\\mathbf{x}) C(\\mathbf{x}, \\mathbf{y}) g(\\mathbf{x})\\)\n is also a valid covariance function, where \n\\(g(.): \\mathcal{X} \\rightarrow \\mathbb{R}\\)\n is a non-negative function from the domain of the inputs \n\\(\\mathcal{X}\\)\n to the real number line. We call these covariances \nscaled covariance functions\n.\n\n\n1\n2\n3\n4\n5\n6\n7\n//Instantiate some kernel\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nI\n]\n \n=\n \n_\n\n\n\nval\n \nscalingFunction\n:\n \n(\nI\n)\n \n=>\n \nDouble\n \n=\n \n_\n\n\n\nval\n \nscKernel\n \n=\n \nScaledKernel\n(\n\n  \nkernel\n,\n \nDataPipe\n(\nscalingFunction\n))\n\n\n\n\n\n\n\nAdvanced Composite Kernels\n\u00b6\n\n\nSometimes we would like to express a kernel function as a product (or sum) of component kernels each of which act on\na sub-set of the dimensions (degree of freedom) of the input attributes.\n\n\nFor example; for 4 dimensional input vector, we may define two component kernels acting on the first two and\nlast two dimensions respectively and combine their evaluations via addition or multiplication. For this purpose the\n\ndynaml.kernels\n package has the \nDecomposableCovariance\n[\nS\n]\n class.\n\n\nIn order to create a decomposable kernel you need three components.\n\n\n\n\nThe component kernels (order matters)\n\n\nAn \nEncoder\n[\nS\n, \nArray\n[\nS\n]]\n instance which splits the input into an array of components\n\n\nA \nReducer\n which combines the individual kernel evaluations.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n//Not required in REPL, already imported\n\n\nimport\n \nio.github.mandar2812.dynaml.DynaMLPipe._\n\n\nimport\n \nio.github.mandar2812.dynaml.pipes._\n\n\n\nval\n \nkernel1\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\nval\n \nkernel2\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n\n//Default Reducer is addition\n\n\nval\n \ndecomp_kernel\n \n=\n\n  \nnew\n \nDecomposableCovariance\n[\nDenseVector\n[\nDouble\n]](\n\n    \nkernel1\n,\n \nkernel2\n)(\n\n    \nbreezeDVSplitEncoder\n(\n2\n))\n\n\n\nval\n \ndecomp_kernel_mult\n \n=\n\n  \nnew\n \nDecomposableCovariance\n[\nDenseVector\n[\nDouble\n]](\n\n    \nkernel1\n,\n \nkernel2\n)(\n\n    \nbreezeDVSplitEncoder\n(\n2\n),\n\n    \nReducer\n.:*:)\n\n\n\n\n\n\n\n\n\n\n\nImplementing Custom Kernels\n\n\nFor more details on implementing user defined kernels, refer to the \nwiki\n.",
            "title": "Kernel API"
        },
        {
            "location": "/core/core_kernels/#kernel-api",
            "text": "The kernel class hierarchy all stems from a simple trait shown here.  1\n2\n3 trait   Kernel [ T ,  V ]   { \n   def   evaluate ( x :   T ,   y :   T ) :   V  }    This outlines only one key feature for kernel functions i.e. their evaluation functional which takes two inputs from  \\(\\mathcal{X}\\)  and yields a scalar value.   Kernel  vs  CovarianceFunction  \nFor practical purposes, the  Kernel [ T ,  V ]  trait does not have enough functionality for usage in varied models like  Gaussian Processes ,  Student's T Processes ,  LS-SVM  etc.  For this reason there is the  CovarianceFunction [ T ,  V ,  M ]  abstract class. It contains methods to construct kernel matrices, keep track of hyper-parameter assignments among other things.",
            "title": "Kernel API"
        },
        {
            "location": "/core/core_kernels/#creating-arbitrary-kernel-functions",
            "text": "Apart from off the shelf kernel functions, it is also possible to create custom kernels on the fly by using the  CovarianceFunction  object.",
            "title": "Creating arbitrary kernel functions"
        },
        {
            "location": "/core/core_kernels/#constructing-kernels-via-feature-maps",
            "text": "It is known from  Mercer's theorem  that any valid kernel function must be decomposable as a dot product between certain  basis function  representation of the inputs. This translates mathematically into.  \\[\n\\begin{align}\n    & K(\\mathbf{x}, \\mathbf{y}) = \\varphi^{T}(\\mathbf{x}) . \\varphi(\\mathbf{y}) \\\\\n    & \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^n\n\\end{align}\n\\] The function  \\(\\varphi(.)\\)  is some higher (possibly infinite) dimensional representation of the input features of a data point. Note that the input space  \\(\\mathcal{X}\\)  could be any of the following (but not limited to).    The space of all connection graphs with specific number of nodes.    A multi-dimensional vector.    The space of all character sequences (binary or otherwise) up to a certain length.    The set of all integer tuples e.g.  \\((1,2), (6,10), \\cdots\\)    We can use any function from some domain  \\(\\mathcal{X}\\)  yielding a  DenseVector [ Double ]  to define a particular inner product/kernel function.  1\n2\n3\n4\n5\n6\n7\n8\n9 // First create a function mapping from some input space to  // Breeze dense vectors.  val   mapFunc   =   ( vec :   DenseVector [ Double ])   =>   { \n     val   mat   =   vec   *   vec . t \n     mat . toDenseVector  }  val   kernel   =   CovarianceFunction ( mapFunc )     Feature map kernels  Covariance functions constructed using feature mappings as shown above return a special object; an instance of the  FeatureMapCovariance [ T ,  DenseVector [ Double ]]  class. In the section on composite kernels we will see why this is important.",
            "title": "Constructing kernels via feature maps"
        },
        {
            "location": "/core/core_kernels/#constructing-kernels-via-direct-evaluation",
            "text": "Instead of defining a feature representation like  \\(\\varphi(.)\\)  as in the section above, you can also directly define the evaluation expression of the kernel.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 // Create the expression for the required kernel.  val   mapFunc   =  ( state :   Map [ String ,  Double ])   => \n   ( x :   DenseVector [ Double ],   y :   DenseVector [ Double ])   =>   { \n        state ( \"alpha\" )*( x   dot   y )   +   state ( \"intercept\" ) \n   }  //Creates kernel with two hyper-parameters: alpha and intercept  val   kernel   =   CovarianceFunction ( mapFunc )( \n   Map ( \"alpha\"   ->   1.5 ,   \"intercept\"   ->   0.01 )  )",
            "title": "Constructing kernels via direct evaluation"
        },
        {
            "location": "/core/core_kernels/#creating-composite-kernels",
            "text": "",
            "title": "Creating Composite Kernels"
        },
        {
            "location": "/core/core_kernels/#algebraic-operations",
            "text": "In machine learning it is well known that kernels can be combined to give other valid kernels. The symmetric positive semi-definite property of a kernel is preserved as long as it is added or multiplied to another valid kernel. In DynaML adding and multiplying kernels is elementary.  1\n2\n3\n4\n5 val   k1   =   new   RBFKernel ( 2.5 )  val   k2   =   new   RationalQuadraticKernel ( 2.0 )  val   k   =   k1   +   k2  val   k3   =   k * k2",
            "title": "Algebraic Operations"
        },
        {
            "location": "/core/core_kernels/#composition",
            "text": "From Mercer's theorem, every kernel can be expressed as a dot product of feature mappings evaluated at the respective data points. We can use this to construct more complex covariances i.e. by successively applying feature mappings.  \\[\n\\begin{align}\nC_{a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{a}(\\mathbf{x})^\\intercal \\varphi_{a}(\\mathbf{y}) \\\\\nC_{b}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\mathbf{x})^\\intercal \\varphi_{b}(\\mathbf{y}) \\\\\nC_{b . a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\varphi_{a}(\\mathbf{x}))^\\intercal \\varphi_{b}(\\varphi_{a}(\\mathbf{y}))\n\\end{align}\n\\] In DynaML, we can create a composite kernel if the kernel represented by the map  \\(\\varphi_{a}\\) , is explicitly of type  FeatureMapCovariance [ T ,  DenseVector [ Double ]]   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   mapFunc   =   ( vec :   DenseVector [ Double ])   =>   { \n     vec / 2 d  }  val   k1   =   CovarianceFunction ( mapFunc )  val   k2   =   new   RationalQuadraticKernel ( 2.0 )  //Composite kernel  val   k3   =   k2   >   k1",
            "title": "Composition"
        },
        {
            "location": "/core/core_kernels/#scaling-covariances",
            "text": "If  \\(C(\\mathbf{x}, \\mathbf{y})\\)  is a valid covariance function, then  \\(g(\\mathbf{x}) C(\\mathbf{x}, \\mathbf{y}) g(\\mathbf{x})\\)  is also a valid covariance function, where  \\(g(.): \\mathcal{X} \\rightarrow \\mathbb{R}\\)  is a non-negative function from the domain of the inputs  \\(\\mathcal{X}\\)  to the real number line. We call these covariances  scaled covariance functions .  1\n2\n3\n4\n5\n6\n7 //Instantiate some kernel  val   kernel :   LocalScalarKernel [ I ]   =   _  val   scalingFunction :   ( I )   =>   Double   =   _  val   scKernel   =   ScaledKernel ( \n   kernel ,   DataPipe ( scalingFunction ))",
            "title": "Scaling Covariances"
        },
        {
            "location": "/core/core_kernels/#advanced-composite-kernels",
            "text": "Sometimes we would like to express a kernel function as a product (or sum) of component kernels each of which act on\na sub-set of the dimensions (degree of freedom) of the input attributes.  For example; for 4 dimensional input vector, we may define two component kernels acting on the first two and\nlast two dimensions respectively and combine their evaluations via addition or multiplication. For this purpose the dynaml.kernels  package has the  DecomposableCovariance [ S ]  class.  In order to create a decomposable kernel you need three components.   The component kernels (order matters)  An  Encoder [ S ,  Array [ S ]]  instance which splits the input into an array of components  A  Reducer  which combines the individual kernel evaluations.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 //Not required in REPL, already imported  import   io.github.mandar2812.dynaml.DynaMLPipe._  import   io.github.mandar2812.dynaml.pipes._  val   kernel1 :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  val   kernel2 :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  //Default Reducer is addition  val   decomp_kernel   = \n   new   DecomposableCovariance [ DenseVector [ Double ]]( \n     kernel1 ,   kernel2 )( \n     breezeDVSplitEncoder ( 2 ))  val   decomp_kernel_mult   = \n   new   DecomposableCovariance [ DenseVector [ Double ]]( \n     kernel1 ,   kernel2 )( \n     breezeDVSplitEncoder ( 2 ), \n     Reducer .:*:)      Implementing Custom Kernels  For more details on implementing user defined kernels, refer to the  wiki .",
            "title": "Advanced Composite Kernels"
        },
        {
            "location": "/core/core_kernel_stat/",
            "text": "Stationary kernels can be expressed as a function of the difference between their inputs.\n\n\n\\[\n    C(\\mathbf{x}, \\mathbf{y}) = K(||\\mathbf{x} - \\mathbf{y}||_{p})\n\\]\nNote that any norm may be used to quantify the distance between the two vectors \n\\(\\mathbf{x} \\ \\& \\ \\mathbf{y}\\)\n. The values \n\\(p = 1\\)\n and \n\\(p = 2\\)\n represent the \nManhattan distance\n and \nEuclidean distance\n respectively.\n\n\n\n\nInstantiating Stationary Kernels\nStationary kernels are implemented as a subset of the \nStationaryKernel[T, V, M]\n class which requires a \nField[T]\n implicit object (an algebraic field which has definitions for addition, subtraction, multiplication and division of its elements much like the number system). You may also import \nspire.implicits._\n in order to load the default field implementations for basic data types like \nInt\n, \nDouble\n and so on. Before instantiating any child class of \nStationaryKernel\n one needs to enter the following code.\n\n\n1\n2\n3\n4\n5\n6\n    \nimport\n \nspire.algebra.Field\n\n    \nimport\n \nio.github.mandar2812.dynaml.analysis.VectorField\n\n    \n//Calculate the number of input features\n\n    \n//and create a vector field of that dimension\n\n    \nval\n \nnum_features\n:\n \nInt\n \n=\n \n...\n\n    \nimplicit\n \nval\n \nf\n \n=\n \nVectorField\n(\nnum_features\n)\n\n\n\n\n\n\n\n\n\n\n\nRadial Basis Function Kernel\n\u00b6\n\n\n\n\n\\[\nC(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2\\sigma^2}\\right)\n\\]\nThe RBF kernel is the most popular kernel function applied in machine learning, it represents an inner product space which is spanned by the \nHermite\n polynomials and as such is suitable to model smooth functions. The RBF kernel is also called a \nuniversal\n kernel for the reason that any smooth function can be represented with a high degree of accuracy assuming we can find a suitable value of the bandwidth.\n\n\n1\nval\n \nrbf\n \n=\n \nnew\n \nRBFKernel\n(\n4.0\n)\n\n\n\n\n\n\n\nSquared Exponential Kernel\n\u00b6\n\n\nA generalization of the RBF Kernel is the Squared Exponential Kernel\n\n\n\\[\n    C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2l^2}\\right)\n\\]\n1\nval\n \nrbf\n \n=\n \nnew\n \nSEKernel\n(\n4.0\n,\n \n2.0\n)\n\n\n\n\n\n\n\nMahalanobis Kernel\n\u00b6\n\n\nThis kernel is a further generalization of the SE kernel. It uses the \nMahalanobis distance\n instead of the Euclidean distance between the inputs.\n\n\n\\[\n    C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\\right)\n\\]\nThe Mahalanobis distance \n\\((\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\\)\n is characterized by a symmetric positive definite matrix \n\\(\\Sigma\\)\n. This distance metric reduces to the Euclidean distance if \n\\(\\Sigma\\)\n is the \nidentity matrix\n. Further, if \n\\(\\Sigma\\)\n is diagonal, the \nMahalanobis kernel\n becomes the \nAutomatic Relevance Determination\n version of the SE kernel (SE-ARD).\n\n\nIn DynaML the class \nMahalanobisKernel\n implements the SE-ARD kernel with diagonal \n\\(\\Sigma\\)\n.\n\n\n1\n2\n3\n4\nval\n \nbandwidths\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n_\n\n\nval\n \namp\n \n=\n \n1.5\n\n\n\nval\n \nmaha_kernel\n \n=\n \nnew\n \nMahalanobisKernel\n(\nbandwidths\n,\n \namp\n)\n\n\n\n\n\n\n\nStudent T Kernel\n\u00b6\n\n\n\\[\n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + ||\\mathbf{x}-\\mathbf{y}||^d}\n\\]\n1\nval\n \ntstud\n \n=\n \nnew\n \nTStudentKernel\n(\n2.0\n)\n\n\n\n\n\n\n\nRational Quadratic Kernel\n\u00b6\n\n\n\\[\n    C(\\mathbf{x},\\mathbf{y}) = \\left( 1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2 \\mu \\ell^2} \\right)^{-\\frac{1}{2}  (dim(\\mathbf{x})+\\mu)}\n\\]\n1\nval\n \nrat\n \n=\n \nnew\n \nRationalQuadraticKernel\n(\nshape\n \n=\n \n1.5\n,\n \nl\n \n=\n \n1.5\n)\n\n\n\n\n\n\n\nCauchy Kernel\n\u00b6\n\n\n\\[\n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{\\sigma^2}}\n\\]\n1\nval\n \ncau\n \n=\n \nnew\n \nCauchyKernel\n(\n2.5\n)\n\n\n\n\n\n\n\nGaussian Spectral Kernel\n\u00b6\n\n\n\\[\nC(\\mathbf{x},\\mathbf{y}) = cos(2\\pi \\mu ||\\mathbf{x}-\\mathbf{y}||) \\ exp(-2\\pi^{2} \\sigma^{2} ||\\mathbf{x}-\\mathbf{y}||^{2} )\n\\]\n1\n2\n3\n4\n5\n6\n//Define how the hyper-parameter Map gets transformed to the kernel parameters\n\n\nval\n \nencoder\n \n=\n \nEncoder\n(\n\n  \n(\nconf\n:\n \nMap\n[\nString\n, \nDouble\n])\n \n=>\n \n(\nconf\n(\n\"c\"\n),\n \nconf\n(\n\"s\"\n)),\n\n  \n(\ncs\n:\n \n(\nDouble\n,\n \nDouble\n))\n \n=>\n \nMap\n(\n\"c\"\n \n->\n \ncs\n.\n_1\n,\n \n\"s\"\n \n->\n \ncs\n.\n_2\n))\n\n\n\nval\n \ngsmKernel\n \n=\n \nGaussianSpectralKernel\n[\nDouble\n](\n3.5\n,\n \n2.0\n,\n \nencoder\n)\n\n\n\n\n\n\n\nMatern Half Integer\n\u00b6\n\n\nThe Matern kernel is an important family of covariance functions. Matern covariances are parameterized via two quantities i.e. order \n\\(\\nu\\)\n and \n\\(\\rho\\)\n the characteristic length scale. The general matern covariance is defined in terms of modified \nBessel\n functions.\n\n\n\\[\nC_{\\nu}(\\mathbf{x},\\mathbf{y}) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)^{\\nu} K_{\\nu}\\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)\n\\]\nWhere \n\\(d = ||\\mathbf{x} - \\mathbf{y}||\\)\n is the Euclidean (\n\\(L_2\\)\n) distance between points.\n\n\nFor the case \n\\(\\nu = p + \\frac{1}{2}, p \\in \\mathbb{N}\\)\n the expression becomes.\n\n\n\\[\nC_{\\nu}(\\mathbf{x},\\mathbf{y}) =  exp\\left(-\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) \\frac{\\Gamma(p+1)}{\\Gamma(2p+1)} \\sum_{i = 0}^{p}{\\frac{(p+1)!}{i!(p-i)!}\\left(\\sqrt{8\\nu}\\frac{d}{\\rho}\\right)^{p-i}}\n\\]\nCurrently there is only support for matern half integer kernels.\n\n\n1\n2\nimplicit\n \nev\n \n=\n \nVectorField\n(\n2\n)\n\n\nval\n \nmatKern\n \n=\n \nnew\n \nGenericMaternKernel\n(\n1.5\n,\n \np\n \n=\n \n1\n)\n\n\n\n\n\n\n\nWavelet Kernel\n\u00b6\n\n\nThe Wavelet kernel (\nZhang et al, 2004\n) comes from Wavelet theory and is given as\n\n\n\\[\n    C(\\mathbf{x},\\mathbf{y}) = \\prod_{i = 1}^{d} h\\left(\\frac{x_i-y_i}{a}\\right)\n\\]\nWhere the function \nh\n is known as the mother wavelet function, Zhang et. al suggest the following expression for the mother wavelet function.\n\n\n\\[\n    h(x) = cos(1.75x)exp(-\\frac{1}{2}x^2)\n\\]\n1\nval\n \nwv\n \n=\n \nnew\n \nWaveletKernel\n(\nx\n \n=>\n \nmath\n.\ncos\n(\n1.75\n*\nx\n)*\nmath\n.\nexp\n(-\n1.0\n*\nx\n*\nx\n/\n2.0\n))(\n1.5\n)\n\n\n\n\n\n\n\nPeriodic Kernel\n\u00b6\n\n\nThe periodic kernel has \nFourier\n series as its orthogonal eigenfunctions. It is used when constructing predictive models over quantities which are known to have some periodic behavior.\n\n\n\\[\nC(\\mathbf{x},\\mathbf{y}) = exp\\left(-2 \\ sin^{2}\\left(\\frac{\\pi \\omega ||\\mathbf{x}-\\mathbf{y}||}{l^2}\\right)\\right)\n\\]\n1\nval\n \nperiodic_kernel\n \n=\n \nnew\n \nPeriodicKernel\n(\nlengthscale\n \n=\n \n1.5\n,\n \nfreq\n \n=\n \n2.5\n)\n\n\n\n\n\n\n\nWave Kernel\n\u00b6\n\n\n\\[\nC(\\mathbf{x},\\mathbf{y}) = \\frac{\\theta}{||\\mathbf{x} - \\mathbf{y}||^2} \\times sin(\\frac{||\\mathbf{x} - \\mathbf{y}||^2}{\\theta})\n\\]\n1\nval\n \nwv_kernel\n \n=\n \nWaveKernel\n(\nth\n \n=\n \n1.0\n)\n\n\n\n\n\n\n\nLaplacian Kernel\n\u00b6\n\n\nThe Laplacian kernel is the covariance function of the well known \nOrnstein Ulhenbeck process\n, samples drawn from this process are continuous and only once differentiable.\n\n\n\\[\n\\begin{equation}\nC(\\mathbf{x},\\mathbf{y}) = exp \\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||_{1}}{2\\beta}\\right)\n\\end{equation}\n\\]\n1\nval\n \nlap\n \n=\n \nnew\n \nLaplacianKernel\n(\n4.0\n)",
            "title": "Stationary Kernels"
        },
        {
            "location": "/core/core_kernel_stat/#radial-basis-function-kernel",
            "text": "\\[\nC(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2\\sigma^2}\\right)\n\\] The RBF kernel is the most popular kernel function applied in machine learning, it represents an inner product space which is spanned by the  Hermite  polynomials and as such is suitable to model smooth functions. The RBF kernel is also called a  universal  kernel for the reason that any smooth function can be represented with a high degree of accuracy assuming we can find a suitable value of the bandwidth.  1 val   rbf   =   new   RBFKernel ( 4.0 )",
            "title": "Radial Basis Function Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#squared-exponential-kernel",
            "text": "A generalization of the RBF Kernel is the Squared Exponential Kernel  \\[\n    C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2l^2}\\right)\n\\] 1 val   rbf   =   new   SEKernel ( 4.0 ,   2.0 )",
            "title": "Squared Exponential Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#mahalanobis-kernel",
            "text": "This kernel is a further generalization of the SE kernel. It uses the  Mahalanobis distance  instead of the Euclidean distance between the inputs.  \\[\n    C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\\right)\n\\] The Mahalanobis distance  \\((\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\\)  is characterized by a symmetric positive definite matrix  \\(\\Sigma\\) . This distance metric reduces to the Euclidean distance if  \\(\\Sigma\\)  is the  identity matrix . Further, if  \\(\\Sigma\\)  is diagonal, the  Mahalanobis kernel  becomes the  Automatic Relevance Determination  version of the SE kernel (SE-ARD).  In DynaML the class  MahalanobisKernel  implements the SE-ARD kernel with diagonal  \\(\\Sigma\\) .  1\n2\n3\n4 val   bandwidths :   DenseVector [ Double ]   =   _  val   amp   =   1.5  val   maha_kernel   =   new   MahalanobisKernel ( bandwidths ,   amp )",
            "title": "Mahalanobis Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#student-t-kernel",
            "text": "\\[\n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + ||\\mathbf{x}-\\mathbf{y}||^d}\n\\] 1 val   tstud   =   new   TStudentKernel ( 2.0 )",
            "title": "Student T Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#rational-quadratic-kernel",
            "text": "\\[\n    C(\\mathbf{x},\\mathbf{y}) = \\left( 1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2 \\mu \\ell^2} \\right)^{-\\frac{1}{2}  (dim(\\mathbf{x})+\\mu)}\n\\] 1 val   rat   =   new   RationalQuadraticKernel ( shape   =   1.5 ,   l   =   1.5 )",
            "title": "Rational Quadratic Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#cauchy-kernel",
            "text": "\\[\n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{\\sigma^2}}\n\\] 1 val   cau   =   new   CauchyKernel ( 2.5 )",
            "title": "Cauchy Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#gaussian-spectral-kernel",
            "text": "\\[\nC(\\mathbf{x},\\mathbf{y}) = cos(2\\pi \\mu ||\\mathbf{x}-\\mathbf{y}||) \\ exp(-2\\pi^{2} \\sigma^{2} ||\\mathbf{x}-\\mathbf{y}||^{2} )\n\\] 1\n2\n3\n4\n5\n6 //Define how the hyper-parameter Map gets transformed to the kernel parameters  val   encoder   =   Encoder ( \n   ( conf :   Map [ String ,  Double ])   =>   ( conf ( \"c\" ),   conf ( \"s\" )), \n   ( cs :   ( Double ,   Double ))   =>   Map ( \"c\"   ->   cs . _1 ,   \"s\"   ->   cs . _2 ))  val   gsmKernel   =   GaussianSpectralKernel [ Double ]( 3.5 ,   2.0 ,   encoder )",
            "title": "Gaussian Spectral Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#matern-half-integer",
            "text": "The Matern kernel is an important family of covariance functions. Matern covariances are parameterized via two quantities i.e. order  \\(\\nu\\)  and  \\(\\rho\\)  the characteristic length scale. The general matern covariance is defined in terms of modified  Bessel  functions.  \\[\nC_{\\nu}(\\mathbf{x},\\mathbf{y}) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)^{\\nu} K_{\\nu}\\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)\n\\] Where  \\(d = ||\\mathbf{x} - \\mathbf{y}||\\)  is the Euclidean ( \\(L_2\\) ) distance between points.  For the case  \\(\\nu = p + \\frac{1}{2}, p \\in \\mathbb{N}\\)  the expression becomes.  \\[\nC_{\\nu}(\\mathbf{x},\\mathbf{y}) =  exp\\left(-\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) \\frac{\\Gamma(p+1)}{\\Gamma(2p+1)} \\sum_{i = 0}^{p}{\\frac{(p+1)!}{i!(p-i)!}\\left(\\sqrt{8\\nu}\\frac{d}{\\rho}\\right)^{p-i}}\n\\] Currently there is only support for matern half integer kernels.  1\n2 implicit   ev   =   VectorField ( 2 )  val   matKern   =   new   GenericMaternKernel ( 1.5 ,   p   =   1 )",
            "title": "Matern Half Integer"
        },
        {
            "location": "/core/core_kernel_stat/#wavelet-kernel",
            "text": "The Wavelet kernel ( Zhang et al, 2004 ) comes from Wavelet theory and is given as  \\[\n    C(\\mathbf{x},\\mathbf{y}) = \\prod_{i = 1}^{d} h\\left(\\frac{x_i-y_i}{a}\\right)\n\\] Where the function  h  is known as the mother wavelet function, Zhang et. al suggest the following expression for the mother wavelet function.  \\[\n    h(x) = cos(1.75x)exp(-\\frac{1}{2}x^2)\n\\] 1 val   wv   =   new   WaveletKernel ( x   =>   math . cos ( 1.75 * x )* math . exp (- 1.0 * x * x / 2.0 ))( 1.5 )",
            "title": "Wavelet Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#periodic-kernel",
            "text": "The periodic kernel has  Fourier  series as its orthogonal eigenfunctions. It is used when constructing predictive models over quantities which are known to have some periodic behavior.  \\[\nC(\\mathbf{x},\\mathbf{y}) = exp\\left(-2 \\ sin^{2}\\left(\\frac{\\pi \\omega ||\\mathbf{x}-\\mathbf{y}||}{l^2}\\right)\\right)\n\\] 1 val   periodic_kernel   =   new   PeriodicKernel ( lengthscale   =   1.5 ,   freq   =   2.5 )",
            "title": "Periodic Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#wave-kernel",
            "text": "\\[\nC(\\mathbf{x},\\mathbf{y}) = \\frac{\\theta}{||\\mathbf{x} - \\mathbf{y}||^2} \\times sin(\\frac{||\\mathbf{x} - \\mathbf{y}||^2}{\\theta})\n\\] 1 val   wv_kernel   =   WaveKernel ( th   =   1.0 )",
            "title": "Wave Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#laplacian-kernel",
            "text": "The Laplacian kernel is the covariance function of the well known  Ornstein Ulhenbeck process , samples drawn from this process are continuous and only once differentiable.  \\[\n\\begin{equation}\nC(\\mathbf{x},\\mathbf{y}) = exp \\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||_{1}}{2\\beta}\\right)\n\\end{equation}\n\\] 1 val   lap   =   new   LaplacianKernel ( 4.0 )",
            "title": "Laplacian Kernel"
        },
        {
            "location": "/core/core_kernel_nonstat/",
            "text": "Non-stationary covariance functions cannot be expressed as simply a function of the distance between their inputs \n\\(\\mathbf{x} - \\mathbf{y}\\)\n.\n\n\nLocally Stationary Kernels\n\u00b6\n\n\nA simple way to construct non-stationary covariances from stationary ones is by scaling the original stationary covariance; \n\\(K(\\mathbf{x} - \\mathbf{y})\\)\n, by a function of \n\\(\\mathbf{x} + \\mathbf{y}\\)\n.\n\n\n\\[\nC(\\mathbf{x}, \\mathbf{y}) = G(\\mathbf{x} + \\mathbf{y}) K(\\mathbf{x} - \\mathbf{y})\n\\]\nHere \n\\(G(.): \\mathcal{X} \\rightarrow \\mathbb{R}\\)\n is a non-negative function of its inputs. These kernels are called \nlocally stationary kernels\n. For an in-depth review of locally stationary kernels refer to \nGenton et. al\n.\n\n\n1\n2\n3\n4\n5\n6\n7\n//Instantiate the base kernel\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nI\n]\n \n=\n \n_\n\n\n\nval\n \nscalingFunction\n:\n \n(\nI\n)\n \n=>\n \nDouble\n \n=\n \n_\n\n\n\nval\n \nscKernel\n \n=\n \nnew\n \nLocallyStationaryKernel\n(\n\n    \nkernel\n,\n \nDataPipe\n(\nscalingFunction\n))\n\n\n\n\n\n\n\nPolynomial Kernel\n\u00b6\n\n\nA very popular non-stationary kernel used in machine learning, the polynomial represents the data features as polynomial expansions up to an index \n\\(d\\)\n.\n\n\n\\[\nC(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^\\intercal \\mathbf{y} + a)^{d}\n\\]\n1\nval\n \nfbm\n \n=\n \nnew\n \nPolynomialKernel\n(\n2\n,\n \n0.99\n)\n\n\n\n\n\n\n\nFractional Brownian Field (FBM) Kernel\n\u00b6\n\n\n\\[\n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{2}\\left(||\\mathbf{x}||_{2}^{2H} + ||\\mathbf{y}||_{2}^{2H} - ||\\mathbf{x}-\\mathbf{y}||_{2}^{2H}\\right)\n\\]\n1\nval\n \nfbm\n \n=\n \nnew\n \nFBMKernel\n(\n0.99\n)\n\n\n\n\n\n\n\nThe FBM kernel is the generalization of fractional Brownian motion to multi-variate index sets. Fractional Brownian motion is a stochastic process which is the generalization of Brownian motion, it was first studied by \nMandelbrot and Von Ness\n. It is a \nself similar\n stochastic process, with stationary increments. However the process itself is non-stationary (as can be seen from the expression for the kernel) and has long range non vanishing covariance.\n\n\nMaximum Likelihood Perceptron Kernel\n\u00b6\n\n\nThe \nmaximum likelihood perceptron\n (MLP) kernel, was first arrived at in Radford Neal's \nthesis\n, by considering the limiting case of a bayesian feed forward neural network with sigmoid activation.\n\n\n\\[\nC(\\mathbf{x},\\mathbf{y}) = sin^{-1} \\left (\\frac{w \\mathbf{x}^\\intercal \\mathbf{y} + b}{(w \\mathbf{x}^\\intercal \\mathbf{x} + b) (w \\mathbf{y}^\\intercal \\mathbf{y} + b)} \\right )\n\\]\nNeural Network Kernel\n\u00b6\n\n\nAlso a result of limiting case of bayesian neural networks, albeit with \n\\(erf(.)\\)\n as the transfer function.\n\n\n\\[\nC(\\mathbf{x},\\mathbf{y}) = \\frac{2}{\\pi} sin \\left (\\frac{2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{y}}{(2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{x} + 1) (2 \\mathbf{y}^\\intercal \\Sigma \\mathbf{y} + 1)} \\right )\n\\]",
            "title": "Non Stationary Kernels"
        },
        {
            "location": "/core/core_kernel_nonstat/#locally-stationary-kernels",
            "text": "A simple way to construct non-stationary covariances from stationary ones is by scaling the original stationary covariance;  \\(K(\\mathbf{x} - \\mathbf{y})\\) , by a function of  \\(\\mathbf{x} + \\mathbf{y}\\) .  \\[\nC(\\mathbf{x}, \\mathbf{y}) = G(\\mathbf{x} + \\mathbf{y}) K(\\mathbf{x} - \\mathbf{y})\n\\] Here  \\(G(.): \\mathcal{X} \\rightarrow \\mathbb{R}\\)  is a non-negative function of its inputs. These kernels are called  locally stationary kernels . For an in-depth review of locally stationary kernels refer to  Genton et. al .  1\n2\n3\n4\n5\n6\n7 //Instantiate the base kernel  val   kernel :   LocalScalarKernel [ I ]   =   _  val   scalingFunction :   ( I )   =>   Double   =   _  val   scKernel   =   new   LocallyStationaryKernel ( \n     kernel ,   DataPipe ( scalingFunction ))",
            "title": "Locally Stationary Kernels"
        },
        {
            "location": "/core/core_kernel_nonstat/#polynomial-kernel",
            "text": "A very popular non-stationary kernel used in machine learning, the polynomial represents the data features as polynomial expansions up to an index  \\(d\\) .  \\[\nC(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^\\intercal \\mathbf{y} + a)^{d}\n\\] 1 val   fbm   =   new   PolynomialKernel ( 2 ,   0.99 )",
            "title": "Polynomial Kernel"
        },
        {
            "location": "/core/core_kernel_nonstat/#fractional-brownian-field-fbm-kernel",
            "text": "\\[\n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{2}\\left(||\\mathbf{x}||_{2}^{2H} + ||\\mathbf{y}||_{2}^{2H} - ||\\mathbf{x}-\\mathbf{y}||_{2}^{2H}\\right)\n\\] 1 val   fbm   =   new   FBMKernel ( 0.99 )    The FBM kernel is the generalization of fractional Brownian motion to multi-variate index sets. Fractional Brownian motion is a stochastic process which is the generalization of Brownian motion, it was first studied by  Mandelbrot and Von Ness . It is a  self similar  stochastic process, with stationary increments. However the process itself is non-stationary (as can be seen from the expression for the kernel) and has long range non vanishing covariance.",
            "title": "Fractional Brownian Field (FBM) Kernel"
        },
        {
            "location": "/core/core_kernel_nonstat/#maximum-likelihood-perceptron-kernel",
            "text": "The  maximum likelihood perceptron  (MLP) kernel, was first arrived at in Radford Neal's  thesis , by considering the limiting case of a bayesian feed forward neural network with sigmoid activation.  \\[\nC(\\mathbf{x},\\mathbf{y}) = sin^{-1} \\left (\\frac{w \\mathbf{x}^\\intercal \\mathbf{y} + b}{(w \\mathbf{x}^\\intercal \\mathbf{x} + b) (w \\mathbf{y}^\\intercal \\mathbf{y} + b)} \\right )\n\\]",
            "title": "Maximum Likelihood Perceptron Kernel"
        },
        {
            "location": "/core/core_kernel_nonstat/#neural-network-kernel",
            "text": "Also a result of limiting case of bayesian neural networks, albeit with  \\(erf(.)\\)  as the transfer function.  \\[\nC(\\mathbf{x},\\mathbf{y}) = \\frac{2}{\\pi} sin \\left (\\frac{2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{y}}{(2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{x} + 1) (2 \\mathbf{y}^\\intercal \\Sigma \\mathbf{y} + 1)} \\right )\n\\]",
            "title": "Neural Network Kernel"
        },
        {
            "location": "/core/core_kernel_nystrom/",
            "text": "Summary\n\n\n\"\nAutomatic Feature Extraction\n (AFE) is a technique by which approximate eigen-functions can be extracted from gram matrices constructed via kernel functions. These eigen-functions are \nfeatures\n engineered by a particular kernel.\"\"\n\n\n\n\nSome Mathematical Background\n\u00b6\n\n\nDefinitions\n\u00b6\n\n\nLet \n\\(X_k \\ \\in \\ \\mathbb{R}^d \\ , \\ k = 1, \\cdots ,n\\)\n be a random sample drawn from a distribution \n\\(F(x)\\)\n. Let \n\\(C \\in \\mathbb{R}^d\\)\n be a compact set such that, \n\\(\\mathcal{H} = \\mathcal{L}^2(C)\\)\n be a Hilbert space of functions given by the inner product below.\n\n\n\\[\n\\begin{equation}\n<f,g>_{\\mathcal{H}} = \\int f(x)g(x) dF(x)\n\\end{equation}\n\\]\nFurther let \n\\(M(\\mathcal{H}, \\mathcal{H})\\)\n be a class of linear operators from \n\\(\\mathcal{H}\\)\n to \n\\(\\mathcal{H}\\)\n.  \n\n\nNystr\u00f6m method\n\u00b6\n\n\nAutomatic Feature Extraction (AFE) using the \nNystr\u00f6m method\n aims at finding a finite dimensional approximation to the kernel eigenfunction expansion of Mercer kernels, as shown below.\n\n\n\\[\n\\begin{equation}\nK(x,t) = \\sum_i{\\lambda_i \\phi(x)\\phi(t)}\n\\end{equation}\n\\]\nIt is well known that Mercer kernels form a \nReproducing Kernel Hilbert Space\n (\nRHKS\n) of functions. Every Mercer kernel defines a unique \nRHKS\n of functions as shown by the Moore-Aronszajn theorem. For a more involved treatment of \nRHKS\n and their applications the reader may refer to the book written by Bertinet et.al.\n\n\nMercer's theorem states that the spectral decomposition of integral operator of \n\\(K\\)\n, \n\\(\\mathcal{T} \\in M(\\mathcal{H},\\mathcal{H})\\)\n defined below yields the eigenfunctions which span the RHKS generated by \n\\(K\\)\n and having an inner product defined as above.\n\n\n\\[\n\\begin{equation}\n(\\mathcal{T}\\phi_i)(t) = \\int K(x,t) \\phi(x) dF(x)\n\\end{equation}\n\\]\nEquation above is more commonly also known as the \nFredholm integral equation\n of the first kind. Nystr\u00f6m's method method approximates this integral using the quadrature constructed by considering a finite kernel matrix constructed out of a prototype set \n\\(X_k \\ k = 1, \\cdots, m\\)\n and calculating its spectral decomposition consisting of eigenvalues \n\\(\\lambda_k\\)\n and eigen-vectors \n\\(u_k\\)\n. This yields an expression for the approximate non-linear feature map \n\\(\\hat{\\phi} : \\mathbb{R}^d \\longrightarrow \\mathbb{R}^m\\)\n.\n\n\n\\[\n\\begin{equation}\n\\hat{\\phi}_{i}(t) = \\frac{\\sqrt{m}}{\\lambda_i}\\sum_{k=1}^{m}K(X_k, t)u_{k,i}\n\\end{equation}\n\\]\nAFE in DynaML Kernels\n\u00b6\n\n\nThe \nSVMKernel[M]\n contains an implementation of AFE in the method\n\n\n1\n2\n3\n4\nfeatureMapping\n(\n\n  \ndecomposition\n:\n \n(\nDenseVector\n[\nDouble\n],\n \nDenseMatrix\n[\nDouble\n]))(\n\n    \nprototypes\n:\n \nList\n[\nDenseVector\n[\nDouble\n]])(\n\n      \ndata\n:\n \nDenseVector\n[\nDouble\n])\n:\n \nDenseVector\n[\nDouble\n]\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe \nSVMKernel\n class is extended by all the implemented library kernels in DynaML thereby enabling the use of AFE in potentially any model employing kernels.",
            "title": "Automatic Feature Extraction"
        },
        {
            "location": "/core/core_kernel_nystrom/#some-mathematical-background",
            "text": "",
            "title": "Some Mathematical Background"
        },
        {
            "location": "/core/core_kernel_nystrom/#definitions",
            "text": "Let  \\(X_k \\ \\in \\ \\mathbb{R}^d \\ , \\ k = 1, \\cdots ,n\\)  be a random sample drawn from a distribution  \\(F(x)\\) . Let  \\(C \\in \\mathbb{R}^d\\)  be a compact set such that,  \\(\\mathcal{H} = \\mathcal{L}^2(C)\\)  be a Hilbert space of functions given by the inner product below.  \\[\n\\begin{equation}\n<f,g>_{\\mathcal{H}} = \\int f(x)g(x) dF(x)\n\\end{equation}\n\\] Further let  \\(M(\\mathcal{H}, \\mathcal{H})\\)  be a class of linear operators from  \\(\\mathcal{H}\\)  to  \\(\\mathcal{H}\\) .",
            "title": "Definitions"
        },
        {
            "location": "/core/core_kernel_nystrom/#nystrom-method",
            "text": "Automatic Feature Extraction (AFE) using the  Nystr\u00f6m method  aims at finding a finite dimensional approximation to the kernel eigenfunction expansion of Mercer kernels, as shown below.  \\[\n\\begin{equation}\nK(x,t) = \\sum_i{\\lambda_i \\phi(x)\\phi(t)}\n\\end{equation}\n\\] It is well known that Mercer kernels form a  Reproducing Kernel Hilbert Space  ( RHKS ) of functions. Every Mercer kernel defines a unique  RHKS  of functions as shown by the Moore-Aronszajn theorem. For a more involved treatment of  RHKS  and their applications the reader may refer to the book written by Bertinet et.al.  Mercer's theorem states that the spectral decomposition of integral operator of  \\(K\\) ,  \\(\\mathcal{T} \\in M(\\mathcal{H},\\mathcal{H})\\)  defined below yields the eigenfunctions which span the RHKS generated by  \\(K\\)  and having an inner product defined as above.  \\[\n\\begin{equation}\n(\\mathcal{T}\\phi_i)(t) = \\int K(x,t) \\phi(x) dF(x)\n\\end{equation}\n\\] Equation above is more commonly also known as the  Fredholm integral equation  of the first kind. Nystr\u00f6m's method method approximates this integral using the quadrature constructed by considering a finite kernel matrix constructed out of a prototype set  \\(X_k \\ k = 1, \\cdots, m\\)  and calculating its spectral decomposition consisting of eigenvalues  \\(\\lambda_k\\)  and eigen-vectors  \\(u_k\\) . This yields an expression for the approximate non-linear feature map  \\(\\hat{\\phi} : \\mathbb{R}^d \\longrightarrow \\mathbb{R}^m\\) .  \\[\n\\begin{equation}\n\\hat{\\phi}_{i}(t) = \\frac{\\sqrt{m}}{\\lambda_i}\\sum_{k=1}^{m}K(X_k, t)u_{k,i}\n\\end{equation}\n\\]",
            "title": "Nystr\u00f6m method"
        },
        {
            "location": "/core/core_kernel_nystrom/#afe-in-dynaml-kernels",
            "text": "The  SVMKernel[M]  contains an implementation of AFE in the method  1\n2\n3\n4 featureMapping ( \n   decomposition :   ( DenseVector [ Double ],   DenseMatrix [ Double ]))( \n     prototypes :   List [ DenseVector [ Double ]])( \n       data :   DenseVector [ Double ]) :   DenseVector [ Double ]     Note  The  SVMKernel  class is extended by all the implemented library kernels in DynaML thereby enabling the use of AFE in potentially any model employing kernels.",
            "title": "AFE in DynaML Kernels"
        },
        {
            "location": "/core/partitioned_objects/",
            "text": "Partitioned Vectors & Dual Vectors\n\n\nThe \ndynaml\n.\nalgebra\n package contains a number of classes and utilities\nfor constructing blocked linear algebra objects.\n\n\n\n\nDynaML makes extensive use of the \nbreeze\n linear algebra library for matrix-vector\noperations. Breeze is an attractive option because it is easy to use and has the ability to use low level implementations\nsuch as \nLAPACK\n and \nBLAS\n for performance speed-up.\n\n\nIf we are working with linear algebra objects which are large in size as compared to the available JVM memory, it may be necessary\nto not construct the entire vector in an eager fashion.\n\n\nIn the Scala language, there are \nlazy\n data structures, which are not computed unless they are required in further\ncomputation. The \ndynaml.algebra\n package leverages lazy data structures to create blocked vectors and matrices.\nEach partition/block of a blocked object is a breeze vector or matrix.\n\n\nThe proceeding pages give the user a glimpse of how to use and manipulate objects of the \nalgebra\n package.",
            "title": "Blocked Algebraic Objects"
        },
        {
            "location": "/core/partitioned_vectors/",
            "text": "Summary\n\n\nHere we show how to use blocked vectors and blocked dual vectors.\n\n\n\n\nBlocked vectors and dual vectors in the \nalgebra\n package are wrappers around \nStream\n[(\nLong\n, \nDenseVector\n[\nDouble\n])]\n\neach partition consists of an ordered index and the partition content which is in the form of a breeze vector.\n\n\nThe relevant API endpoints are \nPartitionedVector\n and \nPartitionedDualVector\n. In order to access these\nobjects, you must do \nimport\n \nio.github.mandar2812.dynaml.algebra._\n (already loaded by default in the DynaML shell).\n\n\nCreation\n\u00b6\n\n\nBlock vectors can be created in a number of ways. \nPartitionedVector\n and \nPartitionedDualVector\n\nare column row vectors respectively and treated as transposes of each other.\n\n\nFrom Input Blocks\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n//Create the data blocks\n\n\nval\n \ndata_blocks\n:\n \nStream\n[(\nLong\n, \nDenseVector\n[\nDouble\n])]\n \n=\n\n  \n(\n0L\n \nuntil\n \n10L\n).\ntoStream\n.\nmap\n(\nindex\n \n=>\n \n(\nindex\n,\n \nDenseVector\n.\nones\n[\nDouble\n](\n500\n)))\n\n\n\n//Instantiate the partitioned vector\n\n\n\nval\n \npart_vector\n \n=\n \nPartitionedVector\n(\ndata_blocks\n)\n\n\n\n//Optionally you may also provide the total length\n\n\n//of the partitioned vector\n\n\n\nval\n \npart_vector\n \n=\n \nPartitionedVector\n(\ndata_blocks\n,\n \nnum_rows\n:\n \nLong\n \n=\n \n5000L\n)\n\n\n\n\n//Created Block Dual Vector\n\n\n\nval\n \npart_dvector\n \n=\n \nPartitionedDualVector\n(\ndata_blocks\n)\n\n\n\n//Optionally you may also provide the total length\n\n\n//of the partitioned dual vector\n\n\n\nval\n \npart_dvector\n \n=\n \nPartitionedDualVector\n(\ndata_blocks\n,\n \nnum_rows\n:\n \nLong\n \n=\n \n5000L\n)\n\n\n\n\n\n\n\nFrom Tabulating Functions\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nval\n \ntabFunc\n:\n \n(\nLong\n)\n \n=>\n \nDouble\n \n=\n\n  \n(\nindex\n:\n \nLong\n)\n \n=>\n \nmath\n.\nsin\n(\n2\nd\n*\nmath\n.\nPi\n*\nindex\n/\n5000\nd\n)\n\n\n\n//Instantiate the partitioned vector\n\n\nval\n \npart_vector\n \n=\n \nPartitionedVector\n(\n\n  \nlength\n \n=\n \n5000L\n,\n \nnumElementsPerBlock\n \n=\n \n500\n,\n\n  \ntabFunc\n)\n\n\n\n//Instantiate the partitioned dual vector\n\n\nval\n \npart_dvector\n \n=\n \nPartitionedDualVector\n(\n\n  \nlength\n \n=\n \n5000L\n,\n \nnumElementsPerBlock\n \n=\n \n500\n,\n  \ntabFunc\n)\n\n\n\n\n\n\n\nFrom a Stream\n\u00b6\n\n\n1\n2\n3\n4\n5\n//Create the data stream\n\n\nval\n \ndata\n:\n \nStream\n[\nDouble\n]\n \n=\n \nStream\n.\nfill\n[\nDouble\n](\n5000\n)(\n1.0\n)\n\n\n\n//Instantiate the partitioned vector\n\n\nval\n \npart_vector\n \n=\n \nPartitionedVector\n(\ndata\n,\n \nlength\n \n=\n \n5000L\n,\n \nnum_elements_per_block\n \n=\n \n500\n)\n\n\n\n\n\n\n\nFrom a Breeze Vector\n\u00b6\n\n\n1\n2\n3\n4\n5\n//Create the data blocks\n\n\nval\n \ndata_vector\n \n=\n \nDenseVector\n.\nones\n[\nDouble\n](\n5000\n)\n\n\n\n//Instantiate the partitioned vector\n\n\nval\n \npart_vector\n \n=\n \nPartitionedVector\n(\ndata_vector\n,\n \nnum_elements_per_block\n \n=\n \n500\n)\n\n\n\n\n\n\n\nApart from the above methods of creation there are a number of convenience functions available.\n\n\nVector with Filled Values\n\u00b6\n\n\nVector of zeros\n\u00b6\n\n\n1\nval\n \nones_vec\n \n=\n \nPartitionedVector\n.\nzeros\n(\n5000L\n,\n \n500\n)\n\n\n\n\n\n\n\nVector of Ones\n\u00b6\n\n\n1\nval\n \nones_vec\n \n=\n \nPartitionedVector\n.\nones\n(\n5000L\n,\n \n500\n)\n\n\n\n\n\n\n\nVector of Random Values\n\u00b6\n\n\n1\n2\n3\nval\n \nrandom_var\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \nrand_vec\n \n=\n \nPartitionedVector\n.\nrand\n(\n5000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\n\n\n\n\n\nVector Concatenation\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\nval\n \nrandom_var\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \nrand_vec1\n \n=\n \nPartitionedVector\n.\nrand\n(\n2000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\nval\n \nrand_vec2\n \n=\n \nPartitionedVector\n.\nrand\n(\n2000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\n\n//Vector of length 4000, having 8 blocks of 500 elements each\n\n\nval\n \nvec\n \n=\n \nPartitionedVector\n.\nvertcat\n(\nrand_vec1\n,\n \nrand_vec2\n)\n\n\n\n\n\n\n\n\n\nTip\n\n\nA \nPartitionedDualVector\n can be created via the transpose operation\non a \nPartitionedVector\n instance and vice versa.\n\n\n1\n2\n3\n4\n5\nval\n \nrandom_var\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \np_vec\n \n=\n \nPartitionedVector\n.\nrand\n(\n5000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\n\nval\n \np_dvec\n \n=\n \np_vec\n.\nt\n\n\n\n\n\n\n\n\n\nAlgebraic Operations\n\u00b6\n\n\nPartitioned vectors and dual vectors have a number of algebraic operations available in the API.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nval\n \nbeta_var\n \n=\n \nRandomVariable\n(\nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\nval\n \ngamma_var\n \n=\n \nRandomVariable\n(\nGamma\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \np_vec_beta\n \n=\n \nPartitionedVector\n.\nrand\n(\n5000L\n,\n \n500\n,\n \nbeta_var\n)\n\n\nval\n \np_vec_gamma\n \n=\n \nPartitionedVector\n.\nrand\n(\n5000L\n,\n \n500\n,\n \ngamma_var\n)\n\n\n\nval\n \ndvec_beta\n \n=\n \np_vec_beta\n.\nt\n\n\nval\n \ndvec_gamma\n \n=\n \np_vec_gamma\n.\nt\n\n\n\n//Addition\n\n\nval\n \nadd_vec\n \n=\n \np_vec_beta\n \n+\n \np_vec_gamma\n\n\nval\n \nadd_dvec\n \n=\n \ndvec_beta\n \n+\n \ndvec_gamma\n\n\n\n//Subtraction\n\n\nval\n \nsub_vec\n \n=\n \np_vec_beta\n \n-\n \np_vec_gamma\n\n\nval\n \nsub_dvec\n \n=\n \ndvec_beta\n \n-\n \ndvec_gamma\n\n\n\n//Element wise multiplication\n\n\nval\n \nmult_vec\n \n=\n \np_vec_beta\n \n:*\n \np_vec_gamma\n\n\n\n//Element wise division\n\n\nval\n \ndiv_vec\n \n=\n \np_vec_beta\n \n:/\n \np_vec_gamma\n\n\n\n//Inner Product\n\n\nval\n \nprod\n \n=\n \ndvec_gamma\n*\np_vec_beta\n\n\n\n//Scaler multiplication\n\n\nval\n \nsc_vec\n \n=\n \nadd_vec\n*\n1.5\n\n\nval\n \nsc_dvec\n \n=\n \nadd_dvec\n*\n2.5\n\n\n\n\n\n\n\nMisc. Operations\n\u00b6\n\n\nMap Partitions\n\u00b6\n\n\nMap each index, partition pair by a scala function.\n\n\n1\n2\n3\n4\n5\nval\n \nvec\n:\n \nPartitionedVector\n \n=\n \n_\n\n\n\nval\n \nother_vec\n \n=\n \nvec\n.\nmap\n(\n\n   \n(\npair\n:\n \n(\nLong\n,\n \nDenseVector\n[\nDouble\n]))\n \n=>\n \n(\npair\n.\n_1\n,\n \npair\n.\n_2\n*\n1.5\n)\n\n\n)\n\n\n\n\n\n\n\nSlice\n\u00b6\n\n\nObtain subset of elements, the new vector is repartitioned and re-indexed accordingly.\n\n\n1\n2\n3\nval\n \nvec\n:\n \nPartitionedVector\n \n=\n \nPartitionedVector\n.\nones\n(\n5000L\n,\n \n500\n)\n\n\n\nval\n \nother_vec\n \n=\n \nvec\n(\n999L\n \nuntil\n \n2000L\n)\n\n\n\n\n\n\n\nReverse\n\u00b6\n\n\nReverse a block vector\n\n\n1\n2\n3\nval\n \nvec\n:\n \nPartitionedVector\n \n=\n \nPartitionedVector\n.\nones\n(\n5000L\n,\n \n500\n)\n\n\n\nval\n \nreverse_vec\n \n=\n \nvec\n.\nreverse\n\n\n\n\n\n\n\nConvert to Breeze Vector\n\u00b6\n\n\n1\n2\n3\n4\n5\nval\n \nvec\n:\n \nPartitionedVector\n \n=\n \nPartitionedVector\n.\nones\n(\n5000L\n,\n \n500\n)\n\n\n\n//Do not use on large vectors as\n\n\n//it might lead to overflow of memory.\n\n\nval\n \nbreeze_vec\n \n=\n \nvec\n.\ntoBreezeVector",
            "title": "Block Vectors"
        },
        {
            "location": "/core/partitioned_vectors/#creation",
            "text": "Block vectors can be created in a number of ways.  PartitionedVector  and  PartitionedDualVector \nare column row vectors respectively and treated as transposes of each other.",
            "title": "Creation"
        },
        {
            "location": "/core/partitioned_vectors/#from-input-blocks",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 //Create the data blocks  val   data_blocks :   Stream [( Long ,  DenseVector [ Double ])]   = \n   ( 0L   until   10L ). toStream . map ( index   =>   ( index ,   DenseVector . ones [ Double ]( 500 )))  //Instantiate the partitioned vector  val   part_vector   =   PartitionedVector ( data_blocks )  //Optionally you may also provide the total length  //of the partitioned vector  val   part_vector   =   PartitionedVector ( data_blocks ,   num_rows :   Long   =   5000L )  //Created Block Dual Vector  val   part_dvector   =   PartitionedDualVector ( data_blocks )  //Optionally you may also provide the total length  //of the partitioned dual vector  val   part_dvector   =   PartitionedDualVector ( data_blocks ,   num_rows :   Long   =   5000L )",
            "title": "From Input Blocks"
        },
        {
            "location": "/core/partitioned_vectors/#from-tabulating-functions",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 val   tabFunc :   ( Long )   =>   Double   = \n   ( index :   Long )   =>   math . sin ( 2 d * math . Pi * index / 5000 d )  //Instantiate the partitioned vector  val   part_vector   =   PartitionedVector ( \n   length   =   5000L ,   numElementsPerBlock   =   500 , \n   tabFunc )  //Instantiate the partitioned dual vector  val   part_dvector   =   PartitionedDualVector ( \n   length   =   5000L ,   numElementsPerBlock   =   500 ,    tabFunc )",
            "title": "From Tabulating Functions"
        },
        {
            "location": "/core/partitioned_vectors/#from-a-stream",
            "text": "1\n2\n3\n4\n5 //Create the data stream  val   data :   Stream [ Double ]   =   Stream . fill [ Double ]( 5000 )( 1.0 )  //Instantiate the partitioned vector  val   part_vector   =   PartitionedVector ( data ,   length   =   5000L ,   num_elements_per_block   =   500 )",
            "title": "From a Stream"
        },
        {
            "location": "/core/partitioned_vectors/#from-a-breeze-vector",
            "text": "1\n2\n3\n4\n5 //Create the data blocks  val   data_vector   =   DenseVector . ones [ Double ]( 5000 )  //Instantiate the partitioned vector  val   part_vector   =   PartitionedVector ( data_vector ,   num_elements_per_block   =   500 )    Apart from the above methods of creation there are a number of convenience functions available.",
            "title": "From a Breeze Vector"
        },
        {
            "location": "/core/partitioned_vectors/#vector-with-filled-values",
            "text": "",
            "title": "Vector with Filled Values"
        },
        {
            "location": "/core/partitioned_vectors/#vector-of-zeros",
            "text": "1 val   ones_vec   =   PartitionedVector . zeros ( 5000L ,   500 )",
            "title": "Vector of zeros"
        },
        {
            "location": "/core/partitioned_vectors/#vector-of-ones",
            "text": "1 val   ones_vec   =   PartitionedVector . ones ( 5000L ,   500 )",
            "title": "Vector of Ones"
        },
        {
            "location": "/core/partitioned_vectors/#vector-of-random-values",
            "text": "1\n2\n3 val   random_var   =   RandomVariable ( new   Beta ( 1.5 ,   2.5 ))  val   rand_vec   =   PartitionedVector . rand ( 5000L ,   500 ,   random_var )",
            "title": "Vector of Random Values"
        },
        {
            "location": "/core/partitioned_vectors/#vector-concatenation",
            "text": "1\n2\n3\n4\n5\n6\n7 val   random_var   =   RandomVariable ( new   Beta ( 1.5 ,   2.5 ))  val   rand_vec1   =   PartitionedVector . rand ( 2000L ,   500 ,   random_var )  val   rand_vec2   =   PartitionedVector . rand ( 2000L ,   500 ,   random_var )  //Vector of length 4000, having 8 blocks of 500 elements each  val   vec   =   PartitionedVector . vertcat ( rand_vec1 ,   rand_vec2 )     Tip  A  PartitionedDualVector  can be created via the transpose operation\non a  PartitionedVector  instance and vice versa.  1\n2\n3\n4\n5 val   random_var   =   RandomVariable ( new   Beta ( 1.5 ,   2.5 ))  val   p_vec   =   PartitionedVector . rand ( 5000L ,   500 ,   random_var )  val   p_dvec   =   p_vec . t",
            "title": "Vector Concatenation"
        },
        {
            "location": "/core/partitioned_vectors/#algebraic-operations",
            "text": "Partitioned vectors and dual vectors have a number of algebraic operations available in the API.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29 val   beta_var   =   RandomVariable ( Beta ( 1.5 ,   2.5 ))  val   gamma_var   =   RandomVariable ( Gamma ( 1.5 ,   2.5 ))  val   p_vec_beta   =   PartitionedVector . rand ( 5000L ,   500 ,   beta_var )  val   p_vec_gamma   =   PartitionedVector . rand ( 5000L ,   500 ,   gamma_var )  val   dvec_beta   =   p_vec_beta . t  val   dvec_gamma   =   p_vec_gamma . t  //Addition  val   add_vec   =   p_vec_beta   +   p_vec_gamma  val   add_dvec   =   dvec_beta   +   dvec_gamma  //Subtraction  val   sub_vec   =   p_vec_beta   -   p_vec_gamma  val   sub_dvec   =   dvec_beta   -   dvec_gamma  //Element wise multiplication  val   mult_vec   =   p_vec_beta   :*   p_vec_gamma  //Element wise division  val   div_vec   =   p_vec_beta   :/   p_vec_gamma  //Inner Product  val   prod   =   dvec_gamma * p_vec_beta  //Scaler multiplication  val   sc_vec   =   add_vec * 1.5  val   sc_dvec   =   add_dvec * 2.5",
            "title": "Algebraic Operations"
        },
        {
            "location": "/core/partitioned_vectors/#misc-operations",
            "text": "",
            "title": "Misc. Operations"
        },
        {
            "location": "/core/partitioned_vectors/#map-partitions",
            "text": "Map each index, partition pair by a scala function.  1\n2\n3\n4\n5 val   vec :   PartitionedVector   =   _  val   other_vec   =   vec . map ( \n    ( pair :   ( Long ,   DenseVector [ Double ]))   =>   ( pair . _1 ,   pair . _2 * 1.5 )  )",
            "title": "Map Partitions"
        },
        {
            "location": "/core/partitioned_vectors/#slice",
            "text": "Obtain subset of elements, the new vector is repartitioned and re-indexed accordingly.  1\n2\n3 val   vec :   PartitionedVector   =   PartitionedVector . ones ( 5000L ,   500 )  val   other_vec   =   vec ( 999L   until   2000L )",
            "title": "Slice"
        },
        {
            "location": "/core/partitioned_vectors/#reverse",
            "text": "Reverse a block vector  1\n2\n3 val   vec :   PartitionedVector   =   PartitionedVector . ones ( 5000L ,   500 )  val   reverse_vec   =   vec . reverse",
            "title": "Reverse"
        },
        {
            "location": "/core/partitioned_vectors/#convert-to-breeze-vector",
            "text": "1\n2\n3\n4\n5 val   vec :   PartitionedVector   =   PartitionedVector . ones ( 5000L ,   500 )  //Do not use on large vectors as  //it might lead to overflow of memory.  val   breeze_vec   =   vec . toBreezeVector",
            "title": "Convert to Breeze Vector"
        },
        {
            "location": "/core/partitioned_matrices/",
            "text": "Summary\n\n\nHere we show how to use the block matrix API\n\n\n\n\nThe \nalgebra\n package\ncontains a number of block matrix implementations.\n\n\n\n\n\n\n\n\nClass\n\n\nRepresents\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nPartitionedMatrix\n\n\nA general block matrix\n\n\nUser facing i.e. can be instantiated directly\n\n\n\n\n\n\nLowerTriPartitionedMatrix\n\n\nLower triangular block matrix\n\n\nResult of \nalgebra\n API calls\n\n\n\n\n\n\nUpperTriPartitionedMatrix\n\n\nUpper triangular block matrix\n\n\nResult of \nalgebra\n API calls\n\n\n\n\n\n\nPartitionedPSDMatrix\n\n\nSymmetric positive semi-definite matrix\n\n\nResult of applying kernel function on data.\n\n\n\n\n\n\n\n\nCreation\n\u00b6\n\n\nBlock can be created in two major ways.\n\n\nFrom Input Blocks\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nval\n \nindex_set\n \n=\n \n(\n0L\n \nuntil\n \n10L\n).\ntoStream\n\n\n//Create the data blocks\n\n\nval\n \ndata_blocks\n:\n \nStream\n[((\nLong\n, \nLong\n)\n, \nDenseVector\n[\nDouble\n])]\n \n=\n\n  \nutils\n.\ncombine\n(\nindex_set\n).\nmap\n(\n\n    \nindices\n \n=>\n \n(\n\n      \n(\nindices\n.\nhead\n,\n \nindices\n.\nlast\n),\n \nDenseMatrix\n.\nones\n[\nDouble\n](\n500\n,\n \n500\n)\n\n    \n)\n\n  \n)\n\n\n\n//Instantiate the partitioned matrix\n\n\n//must provide dimensions\n\n\nval\n \npart_matrix\n \n=\n \nPartitionedMatrix\n(\n\n  \ndata_blocks\n,\n \nnumrows\n \n=\n \n5000L\n,\n \nnumcols\n \n=\n \n5000L\n)\n\n\n\n\n\n\n\nFrom Tabulating Functions\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nval\n \ntabFunc\n:\n \n(\nLong\n,\n \nLong\n)\n \n=>\n \nDouble\n \n=\n\n  \n(\nindexR\n:\n \nLong\n,\n \nindexC\n:\n \nLong\n)\n \n=>\n \n{\n\n    \nmath\n.\nsin\n(\n2\nd\n*\nmath\n.\nPi\n*\nindexR\n/\n5000\nd\n)*\nmath\n.\ncos\n(\n2\nd\n*\nmath\n.\nPi\n*\nindexC\n/\n5000\nd\n)\n\n  \n}\n\n\n\n//Instantiate the partitioned matrix\n\n\nval\n \npart_matrix\n \n=\n \nPartitionedMatrix\n(\n\n  \nnRows\n \n=\n \n5000L\n,\n \nnCols\n \n=\n \n5000L\n,\n\n  \nnumElementsPerRBlock\n \n=\n \n1000\n,\n\n  \nnumElementsPerCBlock\n \n=\n \n1000\n,\n\n  \ntabFunc\n)\n\n\n\n\n\n\n\nFrom Outer Product\n\u00b6\n\n\nA \nPartitionedMatrix\n can also be constructed from the product of a \nPartitionedDualVector\n and\n\nPartitionedVector\n.\n\n\n1\n2\n3\n4\n5\n6\nval\n \nrandom_var\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \nrand_vec1\n \n=\n \nPartitionedVector\n.\nrand\n(\n2000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\nval\n \nrand_vec2\n \n=\n \nPartitionedVector\n.\nrand\n(\n2000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\n\nval\n \np_mat\n \n=\n \nrand_vec1\n*\nrand_vec2\n.\nt\n\n\n\n\n\n\n\nMatrix Concatenation\n\u00b6\n\n\nYou can vertically join matrices, as long as the number of rows and row blocks match.\n\n\n1\n2\n3\n4\nval\n \nmat1\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\nval\n \nmat2\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\nval\n \nmat3\n \n=\n \nPartitionedMatrix\n.\nvertcat\n(\nmat1\n,\n \nmat2\n)\n\n\n\n\n\n\n\n\n\nPositive Semi-Definite Matrices\n\n\nThe class \nPartitionedPSDMatrix\n can be instantiated in two ways.\n\n\n\n\n\n\nFrom outer product.\n\n\n1\n2\n3\n4\nval\n \nrandom_var\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \nrand_vec\n \n=\n \nPartitionedVector\n.\nrand\n(\n2000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\nval\n \npsd_mat\n \n=\n \nPartitionedPSDMatrix\n.\nfromOuterProduct\n(\nrand_vec\n)\n\n\n\n\n\n\n\n\n\n\n\nFrom kernel evaluation\n\n\n1\n2\n3\n4\n5\n6\n//Obtain data\n\n\nval\n \ndata\n:\n \nSeq\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n//Create kernel instance\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n\nval\n \npsd_gram_mat\n \n=\n \nkernel\n.\nbuildBlockedKernelMatrix\n(\ndata\n,\n \ndata\n.\nlength\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgebraic Operations\n\u00b6\n\n\nPartitioned vectors and dual vectors have a number of algebraic operations available in the API.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nval\n \nbeta_var\n \n=\n \nRandomVariable\n(\nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\nval\n \ngamma_var\n \n=\n \nRandomVariable\n(\nGamma\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \np_vec_beta\n \n=\n \nPartitionedVector\n.\nrand\n(\n5000L\n,\n \n1000\n,\n \nbeta_var\n)\n\n\nval\n \np_vec_gamma\n \n=\n \nPartitionedVector\n.\nrand\n(\n5000L\n,\n \n1000\n,\n \ngamma_var\n)\n\n\n\nval\n \ndvec_beta\n \n=\n \np_vec_beta\n.\nt\n\n\nval\n \ndvec_gamma\n \n=\n \np_vec_gamma\n.\nt\n\n\n\nval\n \nmat1\n \n=\n \np_vec_gamma\n*\ndvec_gamma\n\n\nval\n \nmat2\n \n=\n \np_vec_beta\n*\ndvec_beta\n\n\n\n//Addition\n\n\nval\n \nadd_mat\n \n=\n \nmat1\n \n+\n \nmat2\n\n\n\n//Subtraction\n\n\nval\n \nsub_mat\n \n=\n \nmat2\n \n-\n \nmat1\n\n\n\n//Element wise multiplication\n\n\nval\n \nmult_mat\n \n=\n \nmat1\n \n:*\n \nmat2\n\n\n\n//Matrix matrix product\n\n\n\nval\n \nprod_mat\n \n=\n \nmat1\n*\nmat2\n\n\n\n//matrix vector Product\n\n\nval\n \nprod\n \n=\n \nmat1\n*\np_vec_beta\n\n\nval\n \nprod_dual\n \n=\n \ndvec_gamma\n*\nmat2\n\n\n\n//Scaler multiplication\n\n\nval\n \nsc_mat\n \n=\n \nmat1\n*\n1.5\n\n\n\n\n\n\n\nMisc. Operations\n\u00b6\n\n\nMap Partitions\n\u00b6\n\n\nMap each index, partition pair by a Scala function.\n\n\n1\n2\n3\n4\n5\nval\n \nvec\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\nval\n \nother_vec\n \n=\n \nvec\n.\nmap\n(\n\n   \n(\npair\n:\n \n((\nLong\n,\n \nLong\n),\n \nDenseMatrix\n[\nDouble\n]))\n \n=>\n \n(\npair\n.\n_1\n,\n \npair\n.\n_2\n*\n1.5\n)\n\n\n)\n\n\n\n\n\n\n\nSlice\n\u00b6\n\n\nObtain subset of elements, the new matrix is repartitioned and re-indexed accordingly.\n\n\n1\n2\n3\n4\n5\nval\n \nvec\n:\n \nPartitionedVector\n \n=\n \nPartitionedVector\n.\nones\n(\n5000L\n,\n \n1000\n)\n\n\n\nval\n \nmat\n \n=\n \nvec\n*\nvec\n.\nt\n\n\n\nval\n \nother_mat\n \n=\n \nvec\n(\n999L\n \nuntil\n \n2000L\n,\n \n0L\n \nuntil\n \n999L\n)\n\n\n\n\n\n\n\nUpper and Lower Triangular Sections\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\nval\n \nvec\n:\n \nPartitionedVector\n \n=\n \nPartitionedVector\n.\nones\n(\n5000L\n,\n \n1000\n)\n\n\n\nval\n \nmat\n \n=\n \nvec\n*\nvec\n.\nt\n\n\n\nval\n \nlower_tri\n:\n \nLowerTriPartitionedMatrix\n \n=\n \nmat\n.\nL\n\n\nval\n \nupper_tri\n:\n \nUpperTriPartitionedMatrix\n \n=\n \nmat\n.\nU\n\n\n\n\n\n\n\nConvert to Breeze Matrix\n\u00b6\n\n\n1\n2\n3\n4\n5\nval\n \nmat\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\n//Do not use on large vectors as\n\n\n//it might lead to overflow of memory.\n\n\nval\n \nbreeze_mat\n \n=\n \nmat\n.\ntoBreezeMatrix",
            "title": "Block Matrices"
        },
        {
            "location": "/core/partitioned_matrices/#creation",
            "text": "Block can be created in two major ways.",
            "title": "Creation"
        },
        {
            "location": "/core/partitioned_matrices/#from-input-blocks",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 val   index_set   =   ( 0L   until   10L ). toStream  //Create the data blocks  val   data_blocks :   Stream [(( Long ,  Long ) ,  DenseVector [ Double ])]   = \n   utils . combine ( index_set ). map ( \n     indices   =>   ( \n       ( indices . head ,   indices . last ),   DenseMatrix . ones [ Double ]( 500 ,   500 ) \n     ) \n   )  //Instantiate the partitioned matrix  //must provide dimensions  val   part_matrix   =   PartitionedMatrix ( \n   data_blocks ,   numrows   =   5000L ,   numcols   =   5000L )",
            "title": "From Input Blocks"
        },
        {
            "location": "/core/partitioned_matrices/#from-tabulating-functions",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 val   tabFunc :   ( Long ,   Long )   =>   Double   = \n   ( indexR :   Long ,   indexC :   Long )   =>   { \n     math . sin ( 2 d * math . Pi * indexR / 5000 d )* math . cos ( 2 d * math . Pi * indexC / 5000 d ) \n   }  //Instantiate the partitioned matrix  val   part_matrix   =   PartitionedMatrix ( \n   nRows   =   5000L ,   nCols   =   5000L , \n   numElementsPerRBlock   =   1000 , \n   numElementsPerCBlock   =   1000 , \n   tabFunc )",
            "title": "From Tabulating Functions"
        },
        {
            "location": "/core/partitioned_matrices/#from-outer-product",
            "text": "A  PartitionedMatrix  can also be constructed from the product of a  PartitionedDualVector  and PartitionedVector .  1\n2\n3\n4\n5\n6 val   random_var   =   RandomVariable ( new   Beta ( 1.5 ,   2.5 ))  val   rand_vec1   =   PartitionedVector . rand ( 2000L ,   500 ,   random_var )  val   rand_vec2   =   PartitionedVector . rand ( 2000L ,   500 ,   random_var )  val   p_mat   =   rand_vec1 * rand_vec2 . t",
            "title": "From Outer Product"
        },
        {
            "location": "/core/partitioned_matrices/#matrix-concatenation",
            "text": "You can vertically join matrices, as long as the number of rows and row blocks match.  1\n2\n3\n4 val   mat1 :   PartitionedMatrix   =   _  val   mat2 :   PartitionedMatrix   =   _  val   mat3   =   PartitionedMatrix . vertcat ( mat1 ,   mat2 )     Positive Semi-Definite Matrices  The class  PartitionedPSDMatrix  can be instantiated in two ways.    From outer product.  1\n2\n3\n4 val   random_var   =   RandomVariable ( new   Beta ( 1.5 ,   2.5 ))  val   rand_vec   =   PartitionedVector . rand ( 2000L ,   500 ,   random_var )  val   psd_mat   =   PartitionedPSDMatrix . fromOuterProduct ( rand_vec )      From kernel evaluation  1\n2\n3\n4\n5\n6 //Obtain data  val   data :   Seq [ DenseVector [ Double ]]   =   _  //Create kernel instance  val   kernel :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  val   psd_gram_mat   =   kernel . buildBlockedKernelMatrix ( data ,   data . length )",
            "title": "Matrix Concatenation"
        },
        {
            "location": "/core/partitioned_matrices/#algebraic-operations",
            "text": "Partitioned vectors and dual vectors have a number of algebraic operations available in the API.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31 val   beta_var   =   RandomVariable ( Beta ( 1.5 ,   2.5 ))  val   gamma_var   =   RandomVariable ( Gamma ( 1.5 ,   2.5 ))  val   p_vec_beta   =   PartitionedVector . rand ( 5000L ,   1000 ,   beta_var )  val   p_vec_gamma   =   PartitionedVector . rand ( 5000L ,   1000 ,   gamma_var )  val   dvec_beta   =   p_vec_beta . t  val   dvec_gamma   =   p_vec_gamma . t  val   mat1   =   p_vec_gamma * dvec_gamma  val   mat2   =   p_vec_beta * dvec_beta  //Addition  val   add_mat   =   mat1   +   mat2  //Subtraction  val   sub_mat   =   mat2   -   mat1  //Element wise multiplication  val   mult_mat   =   mat1   :*   mat2  //Matrix matrix product  val   prod_mat   =   mat1 * mat2  //matrix vector Product  val   prod   =   mat1 * p_vec_beta  val   prod_dual   =   dvec_gamma * mat2  //Scaler multiplication  val   sc_mat   =   mat1 * 1.5",
            "title": "Algebraic Operations"
        },
        {
            "location": "/core/partitioned_matrices/#misc-operations",
            "text": "",
            "title": "Misc. Operations"
        },
        {
            "location": "/core/partitioned_matrices/#map-partitions",
            "text": "Map each index, partition pair by a Scala function.  1\n2\n3\n4\n5 val   vec :   PartitionedMatrix   =   _  val   other_vec   =   vec . map ( \n    ( pair :   (( Long ,   Long ),   DenseMatrix [ Double ]))   =>   ( pair . _1 ,   pair . _2 * 1.5 )  )",
            "title": "Map Partitions"
        },
        {
            "location": "/core/partitioned_matrices/#slice",
            "text": "Obtain subset of elements, the new matrix is repartitioned and re-indexed accordingly.  1\n2\n3\n4\n5 val   vec :   PartitionedVector   =   PartitionedVector . ones ( 5000L ,   1000 )  val   mat   =   vec * vec . t  val   other_mat   =   vec ( 999L   until   2000L ,   0L   until   999L )",
            "title": "Slice"
        },
        {
            "location": "/core/partitioned_matrices/#upper-and-lower-triangular-sections",
            "text": "1\n2\n3\n4\n5\n6 val   vec :   PartitionedVector   =   PartitionedVector . ones ( 5000L ,   1000 )  val   mat   =   vec * vec . t  val   lower_tri :   LowerTriPartitionedMatrix   =   mat . L  val   upper_tri :   UpperTriPartitionedMatrix   =   mat . U",
            "title": "Upper and Lower Triangular Sections"
        },
        {
            "location": "/core/partitioned_matrices/#convert-to-breeze-matrix",
            "text": "1\n2\n3\n4\n5 val   mat :   PartitionedMatrix   =   _  //Do not use on large vectors as  //it might lead to overflow of memory.  val   breeze_mat   =   mat . toBreezeMatrix",
            "title": "Convert to Breeze Matrix"
        },
        {
            "location": "/core/partitioned_alg_utils/",
            "text": "Summary\n\n\nThe \nalgebra\n package has utility functions for commonly used operations on block matrices and vectors. We give\nthe user a glimpse here.\n\n\n\n\n\n\nWarning\n\n\nThe routines in this section assume that the block sizes of the input matrix are homogenous i.e. the number of row blocks\nis equal to number of column blocks.\n\n\n\n\nBlocked Operations\n\u00b6\n\n\nFollowing operations are the blocked implementations of standard algorithms used for matrices.\n\n\n\\(LU\\)\n Decomposition\n\u00b6\n\n\n\\(LU\\)\n decomposition\n consists of decomposing a square matrix into\nlower and upper triangular factors.\n\n\n1\n2\n3\n4\n//Initialize a square matrix\n\n\nval\n \nsq_mat\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\nval\n \n(\nlower\n,\n \nupper\n)\n \n=\n \nbLU\n(\nsq_mat\n)\n\n\n\n\n\n\n\nCholesky Decomposition\n\u00b6\n\n\nCholesky decomposition\n consists of decomposing a\nsymmetric positive semi-definite matrix uniquely into lower and upper triangular factors.\n\n\n1\n2\n3\n4\n//Initialize a psd matrix\n\n\nval\n \npsd_mat\n:\n \nPartitionedPSDMatrix\n \n=\n \n_\n\n\n\nval\n \n(\nlower\n,\n \nupper\n)\n \n=\n \nbcholesky\n(\npsd_mat\n)\n\n\n\n\n\n\n\nTrace\n\u00b6\n\n\nTrace of a square matrix is the sum of the diagonal elements.\n\n\n1\n2\n3\n4\n//Initialize a square matrix\n\n\nval\n \nsq_mat\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\nval\n \ntr\n \n=\n \nbtrace\n(\nsq_mat\n)\n\n\n\n\n\n\n\nDeterminant\n\u00b6\n\n\nThe \ndeterminant\n of a square matrix represents the scaling factor of the\ntransformation described by the matrix\n\n\n1\n2\n3\n4\n//Initialize a square matrix\n\n\nval\n \nsq_mat\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\nval\n \nde\n \n=\n \nbdet\n(\nsq_mat\n)\n\n\n\n\n\n\n\nDiagonal\n\u00b6\n\n\nObtain diagonal elements of a square block matrix in the form of a block vector.\n\n\n1\n2\n3\n4\n//Initialize a square matrix\n\n\nval\n \nsq_mat\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\nval\n \ndia\n:\n \nPartitionedVector\n \n=\n \nbdiagonal\n(\nsq_mat\n)\n\n\n\n\n\n\n\nQuadratic Forms\n\u00b6\n\n\nQuadratic forms are often encountered in algebra, they involve products on inverse positive semi-definite matrices with\nvectors. The two common quadratic forms are.\n\n\n\n\n\n\nSelf Quadratic Forms\n:  \n\\(\\mathbf{x}^\\intercal \\Omega^{-1} \\mathbf{x}\\)\n\n\n\n\n\n\nCross Quadratic Form\n:  \n\\(\\mathbf{y}^\\intercal \\Omega^{-1} \\mathbf{x}\\)\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nval\n \n(\nx\n,\ny\n)\n:\n \n(\nDenseVector\n[\nDouble\n],\n \nDenseVector\n[\nDouble\n])\n \n=\n \n(\n_\n,\n_\n)\n\n\n\nval\n \nomega\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n_\n\n\n\n//Use breeze function\n\n\nval\n \nlower\n \n=\n \ncholesky\n(\nomega\n)\n\n\n\nval\n \nx_omega_x\n \n=\n \nquadraticForm\n(\nlower\n,\n \nx\n)\n\n\n\nval\n \ny_omega_x\n \n=\n \ncrossQuadraticForm\n(\ny\n,\n \nlower\n,\n \nx\n)\n\n\n\n//Blocked Version of the same.\n\n\n\nval\n \n(\nxb\n,\nyb\n)\n:\n \n(\nPartitionedVector\n,\n \nPartitionedVector\n)\n \n=\n \n(\n_\n,\n_\n)\n\n\n\nval\n \nomegab\n:\n \nPartitionedPSDMatrix\n \n=\n \n_\n\n\n\n//Use DynaML algebra function\n\n\nval\n \nlowerb\n \n=\n \nbcholesky\n(\nomegab\n)\n\n\n\nval\n \nx_omega_x_b\n \n=\n \nblockedQuadraticForm\n(\nlowerb\n,\n \nxb\n)\n\n\n\nval\n \ny_omega_x_b\n \n=\n \nblockedCrossQuadraticForm\n(\nyb\n,\n \nlowerb\n,\n \nxb\n)",
            "title": "Block Algebra Utilities"
        },
        {
            "location": "/core/partitioned_alg_utils/#blocked-operations",
            "text": "Following operations are the blocked implementations of standard algorithms used for matrices.",
            "title": "Blocked Operations"
        },
        {
            "location": "/core/partitioned_alg_utils/#lu-decomposition",
            "text": "\\(LU\\)  decomposition  consists of decomposing a square matrix into\nlower and upper triangular factors.  1\n2\n3\n4 //Initialize a square matrix  val   sq_mat :   PartitionedMatrix   =   _  val   ( lower ,   upper )   =   bLU ( sq_mat )",
            "title": "\\(LU\\) Decomposition"
        },
        {
            "location": "/core/partitioned_alg_utils/#cholesky-decomposition",
            "text": "Cholesky decomposition  consists of decomposing a\nsymmetric positive semi-definite matrix uniquely into lower and upper triangular factors.  1\n2\n3\n4 //Initialize a psd matrix  val   psd_mat :   PartitionedPSDMatrix   =   _  val   ( lower ,   upper )   =   bcholesky ( psd_mat )",
            "title": "Cholesky Decomposition"
        },
        {
            "location": "/core/partitioned_alg_utils/#trace",
            "text": "Trace of a square matrix is the sum of the diagonal elements.  1\n2\n3\n4 //Initialize a square matrix  val   sq_mat :   PartitionedMatrix   =   _  val   tr   =   btrace ( sq_mat )",
            "title": "Trace"
        },
        {
            "location": "/core/partitioned_alg_utils/#determinant",
            "text": "The  determinant  of a square matrix represents the scaling factor of the\ntransformation described by the matrix  1\n2\n3\n4 //Initialize a square matrix  val   sq_mat :   PartitionedMatrix   =   _  val   de   =   bdet ( sq_mat )",
            "title": "Determinant"
        },
        {
            "location": "/core/partitioned_alg_utils/#diagonal",
            "text": "Obtain diagonal elements of a square block matrix in the form of a block vector.  1\n2\n3\n4 //Initialize a square matrix  val   sq_mat :   PartitionedMatrix   =   _  val   dia :   PartitionedVector   =   bdiagonal ( sq_mat )",
            "title": "Diagonal"
        },
        {
            "location": "/core/partitioned_alg_utils/#quadratic-forms",
            "text": "Quadratic forms are often encountered in algebra, they involve products on inverse positive semi-definite matrices with\nvectors. The two common quadratic forms are.    Self Quadratic Forms :   \\(\\mathbf{x}^\\intercal \\Omega^{-1} \\mathbf{x}\\)    Cross Quadratic Form :   \\(\\mathbf{y}^\\intercal \\Omega^{-1} \\mathbf{x}\\)     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 val   ( x , y ) :   ( DenseVector [ Double ],   DenseVector [ Double ])   =   ( _ , _ )  val   omega :   DenseMatrix [ Double ]   =   _  //Use breeze function  val   lower   =   cholesky ( omega )  val   x_omega_x   =   quadraticForm ( lower ,   x )  val   y_omega_x   =   crossQuadraticForm ( y ,   lower ,   x )  //Blocked Version of the same.  val   ( xb , yb ) :   ( PartitionedVector ,   PartitionedVector )   =   ( _ , _ )  val   omegab :   PartitionedPSDMatrix   =   _  //Use DynaML algebra function  val   lowerb   =   bcholesky ( omegab )  val   x_omega_x_b   =   blockedQuadraticForm ( lowerb ,   xb )  val   y_omega_x_b   =   blockedCrossQuadraticForm ( yb ,   lowerb ,   xb )",
            "title": "Quadratic Forms"
        },
        {
            "location": "/core/core_prob_randomvar/",
            "text": "What I cannot create, I do not understand - Richard Feynman\n\n\n\n\n\n\n\n\nSummary\n\n\nSince version 1.4 a new package called \nprobability\n has been added to the core api with an aim to aid in the modeling of random variables and measurable functions.\n\n\n\n\nRandom variables and probability distributions form the bedrock of modern statistical based approaches to inference. Furthermore, analytically tractable inference is only possible for a small number of models while a wealth of interesting model structures don't yield themselves to analytical inference and approximate sampling based approaches are often employed.\n\n\nRandom Variable API\n\u00b6\n\n\nAlthough both random variable with tractable and intractable distributions can be constructed, the emphasis is on the sampling capabilities of random variable objects.\n\n\nThe \nprobability\n package class hierarchy consists of classes and traits which represent continuous and discrete random variables along with ability to endow them with distributions.\n\n\nDynaML Random Variable\n\u00b6\n\n\nThe \nRandomVariable\n[\nDomain\n]\n forms the top of the class hierarchy in the \nprobability\n package. It is a light weight trait which takes a form like so.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nabstract\n \nclass\n \nRandomVariable\n[\nDomain\n]\n \n{\n\n\n  \nval\n \nsample\n:\n \nDataPipe\n[\nUnit\n, \nDomain\n]\n\n\n  \ndef\n \n:*[\nDomain1\n](\nother\n:\n \nRandomVariable\n[\nDomain1\n])\n:\n \nRandomVariable\n[(\nDomain\n, \nDomain1\n)]\n \n=\n \n{\n\n    \nval\n \nsam\n \n=\n \nthis\n.\nsample\n\n    \nRandomVariable\n(\nBifurcationPipe\n(\nsam\n,\nother\n.\nsample\n))\n\n  \n}\n\n\n}\n\n\n\n\n\n\n\nA \nRandomVariable\n instance is defined by its type parameter \nDomain\n, in Mathematics this is the underlying space (referred to as the \nsupport\n) over which the random variable is defined (\n\\(\\mathbb{R}^p\\)\n for continuos variables, \n\\(\\mathbb{N}\\)\n for discrete variables).\n\n\nThe two main functionalities are as follows.\n\n\n\n\n\n\nsample\n which is a data pipe having no input and outputs a sample from the random variables distribution whenever invoked.\n\n\n\n\n\n\n:*\n the 'composition' operator between random variables, evaluating an expression like \nrandomVar1\n \n:*\n \nrandomVar2\n creates a new random variable whose domain is a cartesian product of the domains of \nrandomVar1\n and \nrandomVar2\n.\n\n\n\n\n\n\nContinuous and discrete distribution random variables are implemented through the \nContinuousDistrRV\n[\nDomain\n]\n and \nDiscreteDistrRV\n[\nDomain\n]\n respectively.\n\n\nCreating Random Variables\n\u00b6\n\n\nCreating random variables can be created by a number of ways.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nimport\n \nbreeze.stats.distributions._\n\n\nimport\n \nspire.implicits._\n\n\n\n\n//Create a sampling function\n\n\nval\n \nsampF\n:\n \n()\n \n=>\n \nDouble\n \n=\n \n...\n\n\nval\n \nrv\n \n=\n \nRandomVariable\n(\nsampF\n)\n\n\n\n//Also works with a pipe\n\n\nval\n \nsampF\n:\n \nDataPipe\n[\nUnit\n, \nDouble\n]\n \n=\n \n...\n\n\nval\n \nrv\n \n=\n \nRandomVariable\n(\nsampF\n)\n\n\n\n\n\n\n\n\n\nSampling is the core functionality of the classes extending \nRandomVariable\n but in some cases representing random variables having an underlying (tractable and known) distribution is a requirement, for that purpose there exists the  \nRandomVarWithDistr\n[\nDomain\n, \nDist\n]\n trait which is a bare bones extension of \nRandomVariable\n; it contains only one other member, \nunderlyingDist\n which is of abstract type \nDist\n.\n\n\nThe type \nDist\n can be any breeze distribution, which is either contained in the package \nbreeze\n.\nstats\n.\ndistributions\n or a user written extension of a breeze probability distribution.\n\n\n\n\n\n\nCreating random variables from breeze distributions\n\n\nCreating a random variable backed by a breeze distribution is easy, simply pass the breeze distribution to the \nRandomVariable\n companion object.\n\n\n1\nval\n \np\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n7.5\n,\n \n7.5\n))\n\n\n\n\n\n\n\nThe \nRandomVariable\n object recognizes the breeze distribution passed to it and creates a continuous or discrete random variable accordingly.",
            "title": "Random Variables"
        },
        {
            "location": "/core/core_prob_randomvar/#random-variable-api",
            "text": "Although both random variable with tractable and intractable distributions can be constructed, the emphasis is on the sampling capabilities of random variable objects.  The  probability  package class hierarchy consists of classes and traits which represent continuous and discrete random variables along with ability to endow them with distributions.",
            "title": "Random Variable API"
        },
        {
            "location": "/core/core_prob_randomvar/#dynaml-random-variable",
            "text": "The  RandomVariable [ Domain ]  forms the top of the class hierarchy in the  probability  package. It is a light weight trait which takes a form like so.  1\n2\n3\n4\n5\n6\n7\n8\n9 abstract   class   RandomVariable [ Domain ]   { \n\n   val   sample :   DataPipe [ Unit ,  Domain ] \n\n   def   :*[ Domain1 ]( other :   RandomVariable [ Domain1 ]) :   RandomVariable [( Domain ,  Domain1 )]   =   { \n     val   sam   =   this . sample \n     RandomVariable ( BifurcationPipe ( sam , other . sample )) \n   }  }    A  RandomVariable  instance is defined by its type parameter  Domain , in Mathematics this is the underlying space (referred to as the  support ) over which the random variable is defined ( \\(\\mathbb{R}^p\\)  for continuos variables,  \\(\\mathbb{N}\\)  for discrete variables).  The two main functionalities are as follows.    sample  which is a data pipe having no input and outputs a sample from the random variables distribution whenever invoked.    :*  the 'composition' operator between random variables, evaluating an expression like  randomVar1   :*   randomVar2  creates a new random variable whose domain is a cartesian product of the domains of  randomVar1  and  randomVar2 .    Continuous and discrete distribution random variables are implemented through the  ContinuousDistrRV [ Domain ]  and  DiscreteDistrRV [ Domain ]  respectively.",
            "title": "DynaML Random Variable"
        },
        {
            "location": "/core/core_prob_randomvar/#creating-random-variables",
            "text": "Creating random variables can be created by a number of ways.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 import   breeze.stats.distributions._  import   spire.implicits._  //Create a sampling function  val   sampF :   ()   =>   Double   =   ...  val   rv   =   RandomVariable ( sampF )  //Also works with a pipe  val   sampF :   DataPipe [ Unit ,  Double ]   =   ...  val   rv   =   RandomVariable ( sampF )     Sampling is the core functionality of the classes extending  RandomVariable  but in some cases representing random variables having an underlying (tractable and known) distribution is a requirement, for that purpose there exists the   RandomVarWithDistr [ Domain ,  Dist ]  trait which is a bare bones extension of  RandomVariable ; it contains only one other member,  underlyingDist  which is of abstract type  Dist .  The type  Dist  can be any breeze distribution, which is either contained in the package  breeze . stats . distributions  or a user written extension of a breeze probability distribution.    Creating random variables from breeze distributions  Creating a random variable backed by a breeze distribution is easy, simply pass the breeze distribution to the  RandomVariable  companion object.  1 val   p   =   RandomVariable ( new   Beta ( 7.5 ,   7.5 ))    The  RandomVariable  object recognizes the breeze distribution passed to it and creates a continuous or discrete random variable accordingly.",
            "title": "Creating Random Variables"
        },
        {
            "location": "/core/core_prob_operations/",
            "text": "Apart from just creating wrapper code around sampling procedures which represent random variables, it is also important to do transformations on random variables to yield new more interesting random variables and distributions. In statistics one often formulates certain random variables as algebraic operations on other simpler random variables.\n\n\n\n\nAlgebraic Operations\n\u00b6\n\n\nIt is possible to do common algebraic operations on instances of continuous random variables.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nimport\n \nspire.implicits._\n\n\n\nval\n \nb\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n7.5\n,\n \n7.5\n))\n\n\nval\n \ng\n \n=\n \nRandomVariable\n(\nnew\n \nGamma\n(\n1.5\n,\n \n1.2\n))\n\n\nval\n \nn\n \n=\n \nGaussianRV\n(\n0.0\n,\n \n1.0\n)\n\n\n\nval\n \naddR\n \n=\n \nb\n \n+\n \nn\n \n-\n \ng\n\n\n\nval\n \nmultR\n \n=\n \nb\n \n*\n \n(\nn\n \n+\n \ng\n)\n\n\n\nhistogram\n((\n1\n \nto\n \n1000\n).\nmap\n(\n_\n \n=>\n \nmultR\n.\nsample\n()))\n\n\n\n\n\n\n\n\n\nMeasurable Functions\n\u00b6\n\n\nIn many cases random variables can be expressed as functions of one another, for example chi square random variables are obtained by squaring normally distributed samples.\n\n\n1\n2\n3\n4\nval\n \nchsq\n \n=\n \nMeasurableFunction\n(\nn\n,\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n*\nx\n))\n\n\n\n//Generate a chi square distribution with one degree of freedom\n\n\nhistogram\n((\n1\n \nto\n \n1000\n).\nmap\n(\n_\n \n=>\n \nchsq\n.\nsample\n()))\n\n\n\n\n\n\n\n\n\nPush-forward Maps\n\u00b6",
            "title": "Operations on Random Variables"
        },
        {
            "location": "/core/core_prob_operations/#algebraic-operations",
            "text": "It is possible to do common algebraic operations on instances of continuous random variables.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 import   spire.implicits._  val   b   =   RandomVariable ( new   Beta ( 7.5 ,   7.5 ))  val   g   =   RandomVariable ( new   Gamma ( 1.5 ,   1.2 ))  val   n   =   GaussianRV ( 0.0 ,   1.0 )  val   addR   =   b   +   n   -   g  val   multR   =   b   *   ( n   +   g )  histogram (( 1   to   1000 ). map ( _   =>   multR . sample ()))",
            "title": "Algebraic Operations"
        },
        {
            "location": "/core/core_prob_operations/#measurable-functions",
            "text": "In many cases random variables can be expressed as functions of one another, for example chi square random variables are obtained by squaring normally distributed samples.  1\n2\n3\n4 val   chsq   =   MeasurableFunction ( n ,   DataPipe (( x :   Double )   =>   x * x ))  //Generate a chi square distribution with one degree of freedom  histogram (( 1   to   1000 ). map ( _   =>   chsq . sample ()))",
            "title": "Measurable Functions"
        },
        {
            "location": "/core/core_prob_operations/#push-forward-maps",
            "text": "",
            "title": "Push-forward Maps"
        },
        {
            "location": "/core/core_prob_dist/",
            "text": "Summary\n\n\nThe DynaML \ndynaml.probability.distributions\n package leverages and extends the \nbreeze.stats.distributions\n package. Below is a list of distributions implemented.\n\n\n\n\nSpecifying Distributions\n\u00b6\n\n\nEvery probability density function \n\\(\\rho(x)\\)\n defined over some domain \n\\(x \\in \\mathcal{X}\\)\n can be represented as \n\\(\\rho(x) = \\frac{1}{Z} f(x)\\)\n, where \n\\(f(x)\\)\n is the un-normalized probability weight and \n\\(Z\\)\n is the normalization constant. The normalization constant ensures that the density function sums to \n\\(1\\)\n over the whole domain \n\\(\\mathcal{X}\\)\n.\n\n\nDescribing Skewness\n\u00b6\n\n\nAn important analytical way to create skewed distributions was described by \nAzzalani et. al\n. It consists of four components.\n\n\n\n\nA symmetric probability density \n\\(\\varphi(.)\\)\n\n\nAn odd function \n\\(w(.)\\)\n\n\nA cumulative distribution function \n\\(G(.)\\)\n of some symmetric density\n\n\nA cut-off parameter \n\\(\\tau\\)\n\n\n\n\n\\[\n\\rho(x) = \\frac{1}{G(\\tau)} \\times \\varphi(x)\\times G(w(x) + \\tau)\n\\]\nDistributions API\n\u00b6\n\n\nThe \nDensity\n[\nT\n]\n and \nRand\n[\nT\n]\n traits form the API entry points for implementing probability distributions in breeze. In the \ndynaml\n.\nprobability\n.\ndistributions\n package, these two traits are inherited by \nGenericDistribution\n[\nT\n]\n which is extended by \nAbstractContinuousDistr\n[\nT\n]\n and \nAbstractDiscreteDistr\n[\nT\n]\n classes.\n\n\n\n\nDistributions which can produce confidence intervals\n\n\nThe trait \nHasErrorBars\n[\nT\n]\n can be used as a mix in to provide the ability of producing error bars to distributions. To extend it, one has to implement the \nconfidenceInterval\n(\ns\n:\n \nDouble\n)\n:\n \n(\nT\n,\n \nT\n)\n method.\n\n\n\n\n\n\nSkewness\n\n\nThe \nSkewSymmDistribution\n[\nT\n]\n class is the generic base implementations for skew symmetric family of distributions in DynaML.\n\n\n\n\nDistributions Library\n\u00b6\n\n\nApart from the distributions defined in the \nbreeze\n.\nstats\n.\ndistributions\n, users have access to the following distributions implemented in the \ndynaml\n.\nprobability\n.\ndistributions\n.\n\n\nMultivariate Students T\n\u00b6\n\n\nDefines a \nStudents' T\n distribution over the domain of finite dimensional vectors.\n\n\n\\(\\mathcal{X} \\equiv  \\mathbb{R}^{n}\\)\n\n\n\\(f(x) = \\left[1+{\\frac {1}{\\nu }}({\\mathbf {x} }-{\\boldsymbol {\\mu }})^{\\rm {T}}{\\boldsymbol {\\Sigma }}^{-1}({\\mathbf {x} }-{\\boldsymbol {\\mu }})\\right]^{-(\\nu +p)/2}\\)\n  \n\n\n\\(Z = \\frac{\\Gamma \\left[(\\nu +p)/2\\right]}{\\Gamma (\\nu /2)\\nu ^{p/2}\\pi ^{p/2}\\left|{\\boldsymbol {\\Sigma }}\\right|^{1/2}}\\)\n\n\nUsage\n:\n\n1\n2\n3\n4\nval\n \nmu\n \n=\n \n2.5\n\n\nval\n \nmean\n \n=\n \nDenseVector\n(\n1.0\n,\n \n0.0\n)\n\n\nval\n \ncov\n \n=\n \nDenseMatrix\n((\n1.5\n,\n \n0.5\n),\n \n(\n0.5\n,\n \n2.5\n))\n\n\nval\n \nd\n \n=\n \nMultivariateStudentsT\n(\nmu\n,\n \nmean\n,\n \ncov\n)\n\n\n\n\n\n\nMatrix T\n\u00b6\n\n\nDefines a \nStudents' T\n distribution over the domain of matrices.\n\n\n\\(\\mathcal{X} \\equiv  \\mathbb{R}^{n \\times p}\\)\n\n\n\\(f(x) = \\left|{\\mathbf {I}}_{n}+{\\boldsymbol \\Sigma }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}}){\\boldsymbol \\Omega }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}})^{{{\\rm {T}}}}\\right|^{{-{\\frac {\\nu +n+p-1}{2}}}}\\)\n  \n\n\n\\(Z = {\\frac {\\Gamma_{p}\\left({\\frac {\\nu +n+p-1}{2}}\\right)}{(\\pi )^{{\\frac {np}{2}}}\\Gamma _{p}\\left({\\frac {\\nu +p-1}{2}}\\right)}}|{\\boldsymbol \\Omega }|^{{-{\\frac {n}{2}}}}|{\\boldsymbol \\Sigma }|^{{-{\\frac {p}{2}}}}\\)\n\n\nUsage\n:\n\n1\n2\n3\n4\n5\nval\n \nmu\n \n=\n \n2.5\n\n\nval\n \nmean\n \n=\n \nDenseMatrix\n((-\n1.5\n,\n \n-\n0.5\n),\n \n(\n3.5\n,\n \n-\n2.5\n))\n\n\nval\n \ncov_rows\n \n=\n \nDenseMatrix\n((\n1.5\n,\n \n0.5\n),\n \n(\n0.5\n,\n \n2.5\n))\n\n\nval\n \ncov_cols\n \n=\n \nDenseMatrix\n((\n0.5\n,\n \n0.1\n),\n \n(\n0.1\n,\n \n1.5\n))\n\n\nval\n \nd\n \n=\n \nMatrixT\n(\nmu\n,\n \nmean\n,\n \ncov_rows\n,\n \ncov_cols\n)\n\n\n\n\n\n\nMatrix Normal\n\u00b6\n\n\nDefines a \nGaussian\n distribution over the domain of matrices.\n\n\n\\(\\mathcal{X} \\equiv  \\mathbb{R}^{n \\times p}\\)\n\n\n\\(f(x) = \\exp\\left( -\\frac{1}{2} \\, \\mathrm{tr}\\left[ \\mathbf{V}^{-1} (\\mathbf{X} - \\mathbf{M})^{T} \\mathbf{U}^{-1} (\\mathbf{X} - \\mathbf{M}) \\right] \\right)\\)\n  \n\n\n\\(Z = (2\\pi)^{np/2} |\\mathbf{V}|^{n/2} |\\mathbf{U}|^{p/2}\\)\n\n\nUsage\n:\n\n1\n2\n3\n4\nval\n \nmean\n \n=\n \nDenseMatrix\n((-\n1.5\n,\n \n-\n0.5\n),\n \n(\n3.5\n,\n \n-\n2.5\n))\n\n\nval\n \ncov_rows\n \n=\n \nDenseMatrix\n((\n1.5\n,\n \n0.5\n),\n \n(\n0.5\n,\n \n2.5\n))\n\n\nval\n \ncov_cols\n \n=\n \nDenseMatrix\n((\n0.5\n,\n \n0.1\n),\n \n(\n0.1\n,\n \n1.5\n))\n\n\nval\n \nd\n \n=\n \nMatrixNormal\n(\nmean\n,\n \ncov_rows\n,\n \ncov_cols\n)\n\n\n\n\n\n\nTruncated Normal\n\u00b6\n\n\nDefines a univariate \nGaussian\n distribution that is defined in a finite domain.\n\n\n\\(\\mathcal{X} \\equiv  [a, b]\\)\n\n\n\\(f(x) = \\begin{cases} \\phi ({\\frac {x-\\mu }{\\sigma }}) & a \\leq x \\leq b\\\\0 & else\\end{cases}\\)\n  \n\n\n\\(Z = \\sigma \\left(\\Phi ({\\frac {b-\\mu }{\\sigma }})-\\Phi ({\\frac {a-\\mu }{\\sigma }})\\right)\\)\n\n\n\\(\\phi()\\)\n and \n\\(\\Phi()\\)\n being the gaussian density function and cumulative distribution function respectively\n\n\nUsage\n:\n\n1\n2\n3\n4\nval\n \nmean\n \n=\n \n1.5\n\n\nval\n \nsigma\n \n=\n \n1.5\n\n\nval\n \n(\na\n,\nb\n)\n \n=\n \n(-\n0.5\n,\n \n2.5\n)\n\n\nval\n \nd\n \n=\n \nTruncatedGaussian\n(\nmean\n,\n \nsigma\n,\n \na\n,\n \nb\n)\n\n\n\n\n\n\nSkew Gaussian\n\u00b6\n\n\nUnivariate\n\u00b6\n\n\n\\(\\mathcal{X} \\equiv  \\mathbb{R}\\)\n\n\n\\(f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}))\\)\n  \n\n\n\\(Z = \\frac{1}{2}\\)\n\n\n\\(\\phi()\\)\n and \n\\(\\Phi()\\)\n being the standard gaussian density function and cumulative distribution function respectively\n\n\nMultivariate\n\u00b6\n\n\n\\(\\mathcal{X} \\equiv  \\mathbb{R}^d\\)\n\n\n\\(f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}))\\)\n  \n\n\n\\(Z = \\frac{1}{2}\\)\n\n\n\\(\\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})\\)\n and \n\\(\\Phi()\\)\n are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively and \n\\(L\\)\n is the lower triangular Cholesky decomposition of \n\\(\\Sigma\\)\n.\n\n\n\n\nSkewness parameter \n\\(\\alpha\\)\n\n\nThe parameter \n\\(\\alpha\\)\n determines the skewness of the distribution and its sign tells us in which direction the distribution has a fatter tail. In the univariate case the parameter \n\\(\\alpha\\)\n is a scalar, while in the multivariate case \n\\(\\alpha \\in \\mathbb{R}^d\\)\n, so for the multivariate skew gaussian distribution, there is a skewness value for each dimension.\n\n\n\n\nUsage\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n//Univariate\n\n\nval\n \nmean\n \n=\n \n1.5\n\n\nval\n \nsigma\n \n=\n \n1.5\n\n\nval\n \na\n \n=\n \n-\n0.5\n\n\nval\n \nd\n \n=\n \nSkewGaussian\n(\na\n,\n \nmean\n,\n \nsigma\n)\n\n\n\n//Multivariate\n\n\nval\n \nmu\n \n=\n \nDenseVector\n.\nones\n[\nDouble\n](\n4\n)\n\n\nval\n \nalpha\n \n=\n \nDenseVector\n.\nfill\n[\nDouble\n](\n4\n)(\n1.2\n)\n\n\nval\n \ncov\n \n=\n \nDenseMatrix\n.\neye\n[\nDouble\n](\n4\n)*\n1.5\n\n\nval\n \nmd\n \n=\n \nMultivariateSkewNormal\n(\nalpha\n,\n \nmu\n,\n \ncov\n)\n\n\n\n\n\n\nExtended Skew Gaussian\n\u00b6\n\n\nUnivariate\n\u00b6\n\n\nThe generalization of the univariate skew \nGaussian\n distribution.\n\n\n\\(\\mathcal{X} \\equiv  \\mathbb{R}\\)\n\n\n\\(f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}) + \\tau\\sqrt{1 + \\alpha^{2}})\\)\n  \n\n\n\\(Z = \\Phi(\\tau)\\)\n\n\n\\(\\phi()\\)\n and \n\\(\\Phi()\\)\n being the standard gaussian density function and cumulative distribution function respectively\n\n\nMultivariate\n\u00b6\n\n\n\\(\\mathcal{X} \\equiv  \\mathbb{R}^d\\)\n\n\n\\(f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\mathbf{\\alpha}})\\)\n  \n\n\n\\(Z = \\Phi(\\tau)\\)\n\n\n\\(\\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})\\)\n and \n\\(\\Phi()\\)\n are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively and \n\\(L\\)\n is the lower triangular Cholesky decomposition of \n\\(\\Sigma\\)\n.\n\n\nUsage\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n//Univariate\n\n\nval\n \nmean\n \n=\n \n1.5\n\n\nval\n \nsigma\n \n=\n \n1.5\n\n\nval\n \na\n \n=\n \n-\n0.5\n\n\nval\n \nc\n \n=\n \n0.5\n\n\nval\n \nd\n \n=\n \nExtendedSkewGaussian\n(\nc\n,\n \na\n,\n \nmean\n,\n \nsigma\n)\n\n\n\n//Multivariate\n\n\nval\n \nmu\n \n=\n \nDenseVector\n.\nones\n[\nDouble\n](\n4\n)\n\n\nval\n \nalpha\n \n=\n \nDenseVector\n.\nfill\n[\nDouble\n](\n4\n)(\n1.2\n)\n\n\nval\n \ncov\n \n=\n \nDenseMatrix\n.\neye\n[\nDouble\n](\n4\n)*\n1.5\n\n\nval\n \ntau\n \n=\n \n0.2\n\n\nval\n \nmd\n \n=\n \nExtendedMultivariateSkewNormal\n(\ntau\n,\n \nalpha\n,\n \nmu\n,\n \ncov\n)\n\n\n\n\n\n\n\n\nConfusing Nomenclature\n\n\nThe following distribution has a very similar form and name to the \nextended skew gaussian\n distribution shown above. But despite its deceptively similar formula, it is a very different object.\n\n\nWe use the name MESN to denote the variant below instead of its expanded form.\n\n\n\n\nMESN\n\u00b6\n\n\nThe  \nMultivariate Extended Skew Normal\n or MESN distribution was formulated by \nAdcock and Schutes\n. It is given by\n\n\n\\(\\mathcal{X} \\equiv  \\mathbb{R}^d\\)\n\n\n\\(f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu} + \\mathbf{\\alpha}\\tau, {\\Sigma} + \\mathbf{\\alpha}\\mathbf{\\alpha}^\\intercal) \\Phi\\left(\\frac{\\mathbf{\\alpha}^{\\intercal} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau}{\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\Sigma^{-1}\\mathbf{\\alpha}}}\\right)\\)\n  \n\n\n\\(Z = \\Phi(\\tau)\\)\n\n\n\\(\\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})\\)\n and \n\\(\\Phi()\\)\n are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively.\n\n\nUsage\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n//Univariate\n\n\nval\n \nmean\n \n=\n \n1.5\n\n\nval\n \nsigma\n \n=\n \n1.5\n\n\nval\n \na\n \n=\n \n-\n0.5\n\n\nval\n \nc\n \n=\n \n0.5\n\n\nval\n \nd\n \n=\n \nUESN\n(\nc\n,\n \na\n,\n \nmean\n,\n \nsigma\n)\n\n\n\n//Multivariate\n\n\nval\n \nmu\n \n=\n \nDenseVector\n.\nones\n[\nDouble\n](\n4\n)\n\n\nval\n \nalpha\n \n=\n \nDenseVector\n.\nfill\n[\nDouble\n](\n4\n)(\n1.2\n)\n\n\nval\n \ncov\n \n=\n \nDenseMatrix\n.\neye\n[\nDouble\n](\n4\n)*\n1.5\n\n\nval\n \ntau\n \n=\n \n0.2\n\n\nval\n \nmd\n \n=\n \nMESN\n(\ntau\n,\n \nalpha\n,\n \nmu\n,\n \ncov\n)\n\n\n\n\n\n\n\n\nExtended Skew Gaussian Process\n ESGP\n\n\nThe MESN distribution is used to define the finite dimensional probabilities for the \nESGP\n process.",
            "title": "Probability Distributions"
        },
        {
            "location": "/core/core_prob_dist/#specifying-distributions",
            "text": "Every probability density function  \\(\\rho(x)\\)  defined over some domain  \\(x \\in \\mathcal{X}\\)  can be represented as  \\(\\rho(x) = \\frac{1}{Z} f(x)\\) , where  \\(f(x)\\)  is the un-normalized probability weight and  \\(Z\\)  is the normalization constant. The normalization constant ensures that the density function sums to  \\(1\\)  over the whole domain  \\(\\mathcal{X}\\) .",
            "title": "Specifying Distributions"
        },
        {
            "location": "/core/core_prob_dist/#describing-skewness",
            "text": "An important analytical way to create skewed distributions was described by  Azzalani et. al . It consists of four components.   A symmetric probability density  \\(\\varphi(.)\\)  An odd function  \\(w(.)\\)  A cumulative distribution function  \\(G(.)\\)  of some symmetric density  A cut-off parameter  \\(\\tau\\)   \\[\n\\rho(x) = \\frac{1}{G(\\tau)} \\times \\varphi(x)\\times G(w(x) + \\tau)\n\\]",
            "title": "Describing Skewness"
        },
        {
            "location": "/core/core_prob_dist/#distributions-api",
            "text": "The  Density [ T ]  and  Rand [ T ]  traits form the API entry points for implementing probability distributions in breeze. In the  dynaml . probability . distributions  package, these two traits are inherited by  GenericDistribution [ T ]  which is extended by  AbstractContinuousDistr [ T ]  and  AbstractDiscreteDistr [ T ]  classes.   Distributions which can produce confidence intervals  The trait  HasErrorBars [ T ]  can be used as a mix in to provide the ability of producing error bars to distributions. To extend it, one has to implement the  confidenceInterval ( s :   Double ) :   ( T ,   T )  method.    Skewness  The  SkewSymmDistribution [ T ]  class is the generic base implementations for skew symmetric family of distributions in DynaML.",
            "title": "Distributions API"
        },
        {
            "location": "/core/core_prob_dist/#distributions-library",
            "text": "Apart from the distributions defined in the  breeze . stats . distributions , users have access to the following distributions implemented in the  dynaml . probability . distributions .",
            "title": "Distributions Library"
        },
        {
            "location": "/core/core_prob_dist/#multivariate-students-t",
            "text": "Defines a  Students' T  distribution over the domain of finite dimensional vectors.  \\(\\mathcal{X} \\equiv  \\mathbb{R}^{n}\\)  \\(f(x) = \\left[1+{\\frac {1}{\\nu }}({\\mathbf {x} }-{\\boldsymbol {\\mu }})^{\\rm {T}}{\\boldsymbol {\\Sigma }}^{-1}({\\mathbf {x} }-{\\boldsymbol {\\mu }})\\right]^{-(\\nu +p)/2}\\)     \\(Z = \\frac{\\Gamma \\left[(\\nu +p)/2\\right]}{\\Gamma (\\nu /2)\\nu ^{p/2}\\pi ^{p/2}\\left|{\\boldsymbol {\\Sigma }}\\right|^{1/2}}\\)  Usage : 1\n2\n3\n4 val   mu   =   2.5  val   mean   =   DenseVector ( 1.0 ,   0.0 )  val   cov   =   DenseMatrix (( 1.5 ,   0.5 ),   ( 0.5 ,   2.5 ))  val   d   =   MultivariateStudentsT ( mu ,   mean ,   cov )",
            "title": "Multivariate Students T"
        },
        {
            "location": "/core/core_prob_dist/#matrix-t",
            "text": "Defines a  Students' T  distribution over the domain of matrices.  \\(\\mathcal{X} \\equiv  \\mathbb{R}^{n \\times p}\\)  \\(f(x) = \\left|{\\mathbf {I}}_{n}+{\\boldsymbol \\Sigma }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}}){\\boldsymbol \\Omega }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}})^{{{\\rm {T}}}}\\right|^{{-{\\frac {\\nu +n+p-1}{2}}}}\\)     \\(Z = {\\frac {\\Gamma_{p}\\left({\\frac {\\nu +n+p-1}{2}}\\right)}{(\\pi )^{{\\frac {np}{2}}}\\Gamma _{p}\\left({\\frac {\\nu +p-1}{2}}\\right)}}|{\\boldsymbol \\Omega }|^{{-{\\frac {n}{2}}}}|{\\boldsymbol \\Sigma }|^{{-{\\frac {p}{2}}}}\\)  Usage : 1\n2\n3\n4\n5 val   mu   =   2.5  val   mean   =   DenseMatrix ((- 1.5 ,   - 0.5 ),   ( 3.5 ,   - 2.5 ))  val   cov_rows   =   DenseMatrix (( 1.5 ,   0.5 ),   ( 0.5 ,   2.5 ))  val   cov_cols   =   DenseMatrix (( 0.5 ,   0.1 ),   ( 0.1 ,   1.5 ))  val   d   =   MatrixT ( mu ,   mean ,   cov_rows ,   cov_cols )",
            "title": "Matrix T"
        },
        {
            "location": "/core/core_prob_dist/#matrix-normal",
            "text": "Defines a  Gaussian  distribution over the domain of matrices.  \\(\\mathcal{X} \\equiv  \\mathbb{R}^{n \\times p}\\)  \\(f(x) = \\exp\\left( -\\frac{1}{2} \\, \\mathrm{tr}\\left[ \\mathbf{V}^{-1} (\\mathbf{X} - \\mathbf{M})^{T} \\mathbf{U}^{-1} (\\mathbf{X} - \\mathbf{M}) \\right] \\right)\\)     \\(Z = (2\\pi)^{np/2} |\\mathbf{V}|^{n/2} |\\mathbf{U}|^{p/2}\\)  Usage : 1\n2\n3\n4 val   mean   =   DenseMatrix ((- 1.5 ,   - 0.5 ),   ( 3.5 ,   - 2.5 ))  val   cov_rows   =   DenseMatrix (( 1.5 ,   0.5 ),   ( 0.5 ,   2.5 ))  val   cov_cols   =   DenseMatrix (( 0.5 ,   0.1 ),   ( 0.1 ,   1.5 ))  val   d   =   MatrixNormal ( mean ,   cov_rows ,   cov_cols )",
            "title": "Matrix Normal"
        },
        {
            "location": "/core/core_prob_dist/#truncated-normal",
            "text": "Defines a univariate  Gaussian  distribution that is defined in a finite domain.  \\(\\mathcal{X} \\equiv  [a, b]\\)  \\(f(x) = \\begin{cases} \\phi ({\\frac {x-\\mu }{\\sigma }}) & a \\leq x \\leq b\\\\0 & else\\end{cases}\\)     \\(Z = \\sigma \\left(\\Phi ({\\frac {b-\\mu }{\\sigma }})-\\Phi ({\\frac {a-\\mu }{\\sigma }})\\right)\\)  \\(\\phi()\\)  and  \\(\\Phi()\\)  being the gaussian density function and cumulative distribution function respectively  Usage : 1\n2\n3\n4 val   mean   =   1.5  val   sigma   =   1.5  val   ( a , b )   =   (- 0.5 ,   2.5 )  val   d   =   TruncatedGaussian ( mean ,   sigma ,   a ,   b )",
            "title": "Truncated Normal"
        },
        {
            "location": "/core/core_prob_dist/#skew-gaussian",
            "text": "",
            "title": "Skew Gaussian"
        },
        {
            "location": "/core/core_prob_dist/#univariate",
            "text": "\\(\\mathcal{X} \\equiv  \\mathbb{R}\\)  \\(f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}))\\)     \\(Z = \\frac{1}{2}\\)  \\(\\phi()\\)  and  \\(\\Phi()\\)  being the standard gaussian density function and cumulative distribution function respectively",
            "title": "Univariate"
        },
        {
            "location": "/core/core_prob_dist/#multivariate",
            "text": "\\(\\mathcal{X} \\equiv  \\mathbb{R}^d\\)  \\(f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}))\\)     \\(Z = \\frac{1}{2}\\)  \\(\\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})\\)  and  \\(\\Phi()\\)  are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively and  \\(L\\)  is the lower triangular Cholesky decomposition of  \\(\\Sigma\\) .   Skewness parameter  \\(\\alpha\\)  The parameter  \\(\\alpha\\)  determines the skewness of the distribution and its sign tells us in which direction the distribution has a fatter tail. In the univariate case the parameter  \\(\\alpha\\)  is a scalar, while in the multivariate case  \\(\\alpha \\in \\mathbb{R}^d\\) , so for the multivariate skew gaussian distribution, there is a skewness value for each dimension.   Usage :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 //Univariate  val   mean   =   1.5  val   sigma   =   1.5  val   a   =   - 0.5  val   d   =   SkewGaussian ( a ,   mean ,   sigma )  //Multivariate  val   mu   =   DenseVector . ones [ Double ]( 4 )  val   alpha   =   DenseVector . fill [ Double ]( 4 )( 1.2 )  val   cov   =   DenseMatrix . eye [ Double ]( 4 )* 1.5  val   md   =   MultivariateSkewNormal ( alpha ,   mu ,   cov )",
            "title": "Multivariate"
        },
        {
            "location": "/core/core_prob_dist/#extended-skew-gaussian",
            "text": "",
            "title": "Extended Skew Gaussian"
        },
        {
            "location": "/core/core_prob_dist/#univariate_1",
            "text": "The generalization of the univariate skew  Gaussian  distribution.  \\(\\mathcal{X} \\equiv  \\mathbb{R}\\)  \\(f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}) + \\tau\\sqrt{1 + \\alpha^{2}})\\)     \\(Z = \\Phi(\\tau)\\)  \\(\\phi()\\)  and  \\(\\Phi()\\)  being the standard gaussian density function and cumulative distribution function respectively",
            "title": "Univariate"
        },
        {
            "location": "/core/core_prob_dist/#multivariate_1",
            "text": "\\(\\mathcal{X} \\equiv  \\mathbb{R}^d\\)  \\(f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\mathbf{\\alpha}})\\)     \\(Z = \\Phi(\\tau)\\)  \\(\\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})\\)  and  \\(\\Phi()\\)  are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively and  \\(L\\)  is the lower triangular Cholesky decomposition of  \\(\\Sigma\\) .  Usage :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 //Univariate  val   mean   =   1.5  val   sigma   =   1.5  val   a   =   - 0.5  val   c   =   0.5  val   d   =   ExtendedSkewGaussian ( c ,   a ,   mean ,   sigma )  //Multivariate  val   mu   =   DenseVector . ones [ Double ]( 4 )  val   alpha   =   DenseVector . fill [ Double ]( 4 )( 1.2 )  val   cov   =   DenseMatrix . eye [ Double ]( 4 )* 1.5  val   tau   =   0.2  val   md   =   ExtendedMultivariateSkewNormal ( tau ,   alpha ,   mu ,   cov )     Confusing Nomenclature  The following distribution has a very similar form and name to the  extended skew gaussian  distribution shown above. But despite its deceptively similar formula, it is a very different object.  We use the name MESN to denote the variant below instead of its expanded form.",
            "title": "Multivariate"
        },
        {
            "location": "/core/core_prob_dist/#mesn",
            "text": "The   Multivariate Extended Skew Normal  or MESN distribution was formulated by  Adcock and Schutes . It is given by  \\(\\mathcal{X} \\equiv  \\mathbb{R}^d\\)  \\(f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu} + \\mathbf{\\alpha}\\tau, {\\Sigma} + \\mathbf{\\alpha}\\mathbf{\\alpha}^\\intercal) \\Phi\\left(\\frac{\\mathbf{\\alpha}^{\\intercal} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau}{\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\Sigma^{-1}\\mathbf{\\alpha}}}\\right)\\)     \\(Z = \\Phi(\\tau)\\)  \\(\\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})\\)  and  \\(\\Phi()\\)  are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively.  Usage :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 //Univariate  val   mean   =   1.5  val   sigma   =   1.5  val   a   =   - 0.5  val   c   =   0.5  val   d   =   UESN ( c ,   a ,   mean ,   sigma )  //Multivariate  val   mu   =   DenseVector . ones [ Double ]( 4 )  val   alpha   =   DenseVector . fill [ Double ]( 4 )( 1.2 )  val   cov   =   DenseMatrix . eye [ Double ]( 4 )* 1.5  val   tau   =   0.2  val   md   =   MESN ( tau ,   alpha ,   mu ,   cov )     Extended Skew Gaussian Process  ESGP  The MESN distribution is used to define the finite dimensional probabilities for the  ESGP  process.",
            "title": "MESN"
        },
        {
            "location": "/core/core_opt_convex/",
            "text": "Model Solvers\n\u00b6\n\n\nModel solvers are implementations which either solve for the parameters/coefficients which determine the prediction of a model. Below is a list of all model solvers currently implemented, they are all sub-classes/subtraits of the top level optimization API. Refer to the \nwiki page\n on optimizers for more details on extending the API and writing your own optimizers.\n\n\nGradient Descent\n\u00b6\n\n\nThe bread and butter of any machine learning framework, the \nGradientDescent\n class in the \nio.github.mandar2812.dynaml.optimization\n package provides gradient based optimization primitives for solving optimization problems of the form.\n\n\n\\[\n\\begin{equation}\n    f(w) :=\n    \\lambda\\, R(w) +\n    \\frac1n \\sum_{k=1}^n L(w;x_k,y_k)\n    \\label{eq:regPrimal}\n    \\ .\n\\end{equation}\n\\]\nGradients\n\u00b6\n\n\n\n\n\n\n\n\nName\n\n\nClass\n\n\nEquation\n\n\n\n\n\n\n\n\n\n\nLogistic Gradient\n\n\nLogisticGradient\n\n\n\\(L = \\frac1n \\sum_{k=1}^n \\log(1+\\exp( -y_k w^T x_k)), y_k \\in \\{-1, +1\\}\\)\n\n\n\n\n\n\nLeast Squares Gradient\n\n\nLeastSquaresGradient\n\n\n\\(L = \\frac1n \\sum_{k=1}^n \\|w^{T} \\cdot x_k - y_k\\|^2\\)\n\n\n\n\n\n\n\n\nUpdaters\n\u00b6\n\n\n\n\n\n\n\n\nName\n\n\nClass\n\n\nEquation\n\n\n\n\n\n\n\n\n\n\n\\(L_1\\)\n Updater\n\n\nL1Updater\n\n\n\\(R = \\|\\|w\\|\\|_1\\)\n\n\n\n\n\n\n\\(L_2\\)\n Updater\n\n\nSquaredL2Updater\n\n\n\\(R = \\frac{1}{2} \\|\\|w\\|\\|^2\\)\n\n\n\n\n\n\nBFGS Updater\n\n\nSimpleBFGSUpdater\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nnum_points\n \n=\n \ndata\n.\nlength\n\n\nval\n \ninitial_params\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \noptimizer\n \n=\n \nnew\n \nGradientDescent\n(\n\n    \nnew\n \nLogisticGradient\n,\n\n    \nnew\n \nSquaredL2Updater\n\n\n)\n\n\nval\n \nparams\n \n=\n \noptimizer\n.\nsetRegParam\n(\n0.002\n).\noptimize\n(\n\n  \nnum_points\n,\n \ndata\n,\n \ninitial_params\n)\n\n\n\n\n\n\n\nQuasi-Newton (BFGS)\n\u00b6\n\n\nThe \nBroydon-Fletcher-Goldfarb-Shanno\n (BFGS) is a Quasi-Newton based second order optimization method. To calculate an update to the parameters, it requires calculation of the inverse \nHessian\n \n\\(\\mathit{H}^{-1}\\)\n as well as the gradient at each iteration.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \noptimizer\n \n=\n \nQuasiNewtonOptimizer\n(\n\n  \nnew\n \nLeastSquaresGradient\n,\n\n  \nnew\n \nSimpleBFGSUpdater\n)\n\n\n\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nnum_points\n \n=\n \ndata\n.\nlength\n\n\nval\n \ninitial_params\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\n\nval\n \nparams\n \n=\n \noptimizer\n.\nsetRegParam\n(\n0.002\n).\noptimize\n(\n\n  \nnum_points\n,\n \ndata\n,\n \ninitial_params\n)\n\n\n\n\n\n\n\nRegularized Least Squares\n\u00b6\n\n\nThis subroutine solves the regularized least squares optimization problem as shown below.\n\n\n\\[\n\\begin{equation}\n    \\min_{w} \\ \\mathcal{J}_{P}(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2\n\\end{equation}\n\\]\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \nnum_dim\n \n=\n \n...\n\n\nval\n \ndesignMatrix\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \nresponse\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\n\nval\n \noptimizer\n \n=\n \nnew\n \nRegularizedLSSolver\n()\n\n\n\n\nval\n \nx\n \n=\n \noptimizer\n.\nsetRegParam\n(\n0.05\n).\noptimize\n(\n\n  \ndesignMatrix\n.\nnrow\n,\n \n(\ndesignMatrix\n,\n \nresponse\n),\n\n  \nDenseVector\n.\nones\n[\nDouble\n](\nnum_dim\n))\n\n\n\n\n\n\n\nBack propagation with Momentum\n\u00b6\n\n\nThis is the most common learning methods for supervised training of feed forward neural networks, the edge weights are adjusted using the \ngeneralized delta rule\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nval\n \ndata\n:\n \nSeq\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]\n \n=\n \n_\n\n\n\n//Input, Hidden, Output\n\n\nval\n \nnum_units_by_layer\n \n=\n \nSeq\n(\n5\n,\n \n8\n,\n \n3\n)\n\n\nval\n \nacts\n \n=\n \nSeq\n(\nVectorSigmoid\n,\n \nVectorTansig\n)\n\n\nval\n \nbreezeStackFactory\n \n=\n \nNeuralStackFactory\n(\nnum_units_by_layer\n)(\nacts\n)\n\n\n\n//Random variable which samples layer weights\n\n\nval\n \nstackInitializer\n \n=\n \nGenericFFNeuralNet\n.\ngetWeightInitializer\n(\n\n  \nnum_units_by_layer\n\n\n)\n\n\n\nval\n \nopt_backprop\n \n=\n \nnew\n \nFFBackProp\n(\nbreezeStackFactory\n)\n\n\n\nval\n \nlearned_stack\n \n=\n \nopt_backprop\n.\noptimize\n(\n\n  \ndata\n.\nlength\n,\n \ndata\n,\n\n  \nstackInitializer\n.\ndraw\n)\n\n\n\n\n\n\n\n\n\nDeprecated back propagation API\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]\n \n=\n \n...\n\n\n\nval\n \ninitParam\n \n=\n \nFFNeuralGraph\n(\nnum_inputs\n \n=\n \ndata\n.\nhead\n.\n_1\n.\nlength\n,\n\n     \nnum_outputs\n \n=\n \ndata\n.\nhead\n.\n_2\n.\nlength\n,\n\n     \nhidden_layers\n \n=\n \n1\n,\n \nList\n(\n\"logsig\"\n,\n \n\"linear\"\n),\n\n     \nList\n(\n5\n))\n\n\n\nval\n \noptimizer\n \n=\n \nnew\n \nBackPropogation\n()\n\n     \n.\nsetNumIterations\n(\n100\n)\n\n     \n.\nsetStepSize\n(\n0.01\n)\n\n\n\nval\n \nnewparams\n \n=\n \noptimizer\n.\noptimize\n(\ndata\n.\nlength\n,\n \ndata\n,\n \ninitParam\n)\n\n\n\n\n\n\n\n\n\nConjugate Gradient\n\u00b6\n\n\nThe conjugate gradient method is used to solve linear systems of the form \n\\(Ax = b\\)\n where \n\\(A\\)\n is a symmetric positive definite matrix.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nval\n \nnum_dim\n \n=\n \n...\n\n\nval\n \nA\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \nb\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\n\n///Solves A.x = b\n\n\nval\n \nx\n \n=\n \nConjugateGradient\n.\nrunCG\n(\nA\n,\n \nb\n,\n\n    \nDenseVector\n.\nones\n[\nDouble\n](\nnum_dim\n),\n\n    \nepsilon\n \n=\n \n0.005\n,\n \nMAX_ITERATIONS\n \n=\n \n50\n)\n\n\n\n\n\n\n\nDual LSSVM Solver\n\u00b6\n\n\nThe LSSVM solver solves the linear program that results from the application of the \nKarush, Kuhn Tucker\n conditions on the LSSVM optimization problem.\n\n\n\\[\n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\end{equation}\n\\]\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\n\nval\n \nkernelMatrix\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\n\nval\n \ninitParam\n \n=\n  \nDenseVector\n.\nones\n[\nDouble\n](\nnum_points\n+\n1\n)\n\n\n\nval\n \noptimizer\n \n=\n \nnew\n \nLSSVMLinearSolver\n()\n\n\n\nval\n \nalpha\n \n=\n \noptimizer\n.\noptimize\n(\nnum_points\n,\n\n    \n(\nkernelMatrix\n,\n \nDenseVector\n(\ndata\n.\nmap\n(\n_\n.\n_2\n).\ntoArray\n)),\n\n    \ninitParam\n)\n\n\n\n\n\n\n\nCommittee Model Solver\n\u00b6\n\n\nThe committee model solver aims to find the optimum values of weights applied to the predictions of a set of base models. The weights are calculated as follows.\n\n\n\\[\n\\alpha = \\frac{C^{-1} \\overrightarrow{1}}{\\overrightarrow{1}^T C^{-1} \\overrightarrow{1}}\n\\]\nWhere \n\\(C\\)\n is the sample correlation matrix of errors for all combinations of the base models calculated on the training data.\n\n\n1\n2\n3\n4\n5\n6\n7\nval\n \noptimizer\n=\n \nnew\n \nCommitteeModelSolver\n()\n\n\n//Data Structure containing for each training point the following couple\n\n\n//(predictions from base models as a vector, actual target)\n\n\nval\n \npredictionsTargets\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nparams\n \n=\n \noptimizer\n.\noptimize\n(\nnum_points\n,\n\n    \npredictionsTargets\n,\n\n    \nDenseVector\n.\nones\n[\nDouble\n](\nnum_of_models\n))",
            "title": "Convex"
        },
        {
            "location": "/core/core_opt_convex/#model-solvers",
            "text": "Model solvers are implementations which either solve for the parameters/coefficients which determine the prediction of a model. Below is a list of all model solvers currently implemented, they are all sub-classes/subtraits of the top level optimization API. Refer to the  wiki page  on optimizers for more details on extending the API and writing your own optimizers.",
            "title": "Model Solvers"
        },
        {
            "location": "/core/core_opt_convex/#gradient-descent",
            "text": "The bread and butter of any machine learning framework, the  GradientDescent  class in the  io.github.mandar2812.dynaml.optimization  package provides gradient based optimization primitives for solving optimization problems of the form.  \\[\n\\begin{equation}\n    f(w) :=\n    \\lambda\\, R(w) +\n    \\frac1n \\sum_{k=1}^n L(w;x_k,y_k)\n    \\label{eq:regPrimal}\n    \\ .\n\\end{equation}\n\\]",
            "title": "Gradient Descent"
        },
        {
            "location": "/core/core_opt_convex/#gradients",
            "text": "Name  Class  Equation      Logistic Gradient  LogisticGradient  \\(L = \\frac1n \\sum_{k=1}^n \\log(1+\\exp( -y_k w^T x_k)), y_k \\in \\{-1, +1\\}\\)    Least Squares Gradient  LeastSquaresGradient  \\(L = \\frac1n \\sum_{k=1}^n \\|w^{T} \\cdot x_k - y_k\\|^2\\)",
            "title": "Gradients"
        },
        {
            "location": "/core/core_opt_convex/#updaters",
            "text": "Name  Class  Equation      \\(L_1\\)  Updater  L1Updater  \\(R = \\|\\|w\\|\\|_1\\)    \\(L_2\\)  Updater  SquaredL2Updater  \\(R = \\frac{1}{2} \\|\\|w\\|\\|^2\\)    BFGS Updater  SimpleBFGSUpdater      1\n2\n3\n4\n5\n6\n7\n8\n9 val   data :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   num_points   =   data . length  val   initial_params :   DenseVector [ Double ]   =   ...  val   optimizer   =   new   GradientDescent ( \n     new   LogisticGradient , \n     new   SquaredL2Updater  )  val   params   =   optimizer . setRegParam ( 0.002 ). optimize ( \n   num_points ,   data ,   initial_params )",
            "title": "Updaters"
        },
        {
            "location": "/core/core_opt_convex/#quasi-newton-bfgs",
            "text": "The  Broydon-Fletcher-Goldfarb-Shanno  (BFGS) is a Quasi-Newton based second order optimization method. To calculate an update to the parameters, it requires calculation of the inverse  Hessian   \\(\\mathit{H}^{-1}\\)  as well as the gradient at each iteration.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   optimizer   =   QuasiNewtonOptimizer ( \n   new   LeastSquaresGradient , \n   new   SimpleBFGSUpdater )  val   data :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   num_points   =   data . length  val   initial_params :   DenseVector [ Double ]   =   ...  val   params   =   optimizer . setRegParam ( 0.002 ). optimize ( \n   num_points ,   data ,   initial_params )",
            "title": "Quasi-Newton (BFGS)"
        },
        {
            "location": "/core/core_opt_convex/#regularized-least-squares",
            "text": "This subroutine solves the regularized least squares optimization problem as shown below.  \\[\n\\begin{equation}\n    \\min_{w} \\ \\mathcal{J}_{P}(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2\n\\end{equation}\n\\]  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   num_dim   =   ...  val   designMatrix :   DenseMatrix [ Double ]   =   ...  val   response :   DenseVector [ Double ]   =   ...  val   optimizer   =   new   RegularizedLSSolver ()  val   x   =   optimizer . setRegParam ( 0.05 ). optimize ( \n   designMatrix . nrow ,   ( designMatrix ,   response ), \n   DenseVector . ones [ Double ]( num_dim ))",
            "title": "Regularized Least Squares"
        },
        {
            "location": "/core/core_opt_convex/#back-propagation-with-momentum",
            "text": "This is the most common learning methods for supervised training of feed forward neural networks, the edge weights are adjusted using the  generalized delta rule .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 val   data :   Seq [( DenseVector [ Double ] ,  DenseVector [ Double ])]   =   _  //Input, Hidden, Output  val   num_units_by_layer   =   Seq ( 5 ,   8 ,   3 )  val   acts   =   Seq ( VectorSigmoid ,   VectorTansig )  val   breezeStackFactory   =   NeuralStackFactory ( num_units_by_layer )( acts )  //Random variable which samples layer weights  val   stackInitializer   =   GenericFFNeuralNet . getWeightInitializer ( \n   num_units_by_layer  )  val   opt_backprop   =   new   FFBackProp ( breezeStackFactory )  val   learned_stack   =   opt_backprop . optimize ( \n   data . length ,   data , \n   stackInitializer . draw )     Deprecated back propagation API   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 val   data :   Stream [( DenseVector [ Double ] ,  DenseVector [ Double ])]   =   ...  val   initParam   =   FFNeuralGraph ( num_inputs   =   data . head . _1 . length , \n      num_outputs   =   data . head . _2 . length , \n      hidden_layers   =   1 ,   List ( \"logsig\" ,   \"linear\" ), \n      List ( 5 ))  val   optimizer   =   new   BackPropogation () \n      . setNumIterations ( 100 ) \n      . setStepSize ( 0.01 )  val   newparams   =   optimizer . optimize ( data . length ,   data ,   initParam )",
            "title": "Back propagation with Momentum"
        },
        {
            "location": "/core/core_opt_convex/#conjugate-gradient",
            "text": "The conjugate gradient method is used to solve linear systems of the form  \\(Ax = b\\)  where  \\(A\\)  is a symmetric positive definite matrix.  1\n2\n3\n4\n5\n6\n7\n8 val   num_dim   =   ...  val   A :   DenseMatrix [ Double ]   =   ...  val   b :   DenseVector [ Double ]   =   ...  ///Solves A.x = b  val   x   =   ConjugateGradient . runCG ( A ,   b , \n     DenseVector . ones [ Double ]( num_dim ), \n     epsilon   =   0.005 ,   MAX_ITERATIONS   =   50 )",
            "title": "Conjugate Gradient"
        },
        {
            "location": "/core/core_opt_convex/#dual-lssvm-solver",
            "text": "The LSSVM solver solves the linear program that results from the application of the  Karush, Kuhn Tucker  conditions on the LSSVM optimization problem.  \\[\n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\end{equation}\n\\]  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 val   data :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   kernelMatrix :   DenseMatrix [ Double ]   =   ...  val   initParam   =    DenseVector . ones [ Double ]( num_points + 1 )  val   optimizer   =   new   LSSVMLinearSolver ()  val   alpha   =   optimizer . optimize ( num_points , \n     ( kernelMatrix ,   DenseVector ( data . map ( _ . _2 ). toArray )), \n     initParam )",
            "title": "Dual LSSVM Solver"
        },
        {
            "location": "/core/core_opt_convex/#committee-model-solver",
            "text": "The committee model solver aims to find the optimum values of weights applied to the predictions of a set of base models. The weights are calculated as follows.  \\[\n\\alpha = \\frac{C^{-1} \\overrightarrow{1}}{\\overrightarrow{1}^T C^{-1} \\overrightarrow{1}}\n\\] Where  \\(C\\)  is the sample correlation matrix of errors for all combinations of the base models calculated on the training data.  1\n2\n3\n4\n5\n6\n7 val   optimizer =   new   CommitteeModelSolver ()  //Data Structure containing for each training point the following couple  //(predictions from base models as a vector, actual target)  val   predictionsTargets :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   params   =   optimizer . optimize ( num_points , \n     predictionsTargets , \n     DenseVector . ones [ Double ]( num_of_models ))",
            "title": "Committee Model Solver"
        },
        {
            "location": "/core/core_opt_global/",
            "text": "Model Selection Routines\n\u00b6\n\n\nThese routines are also known as \nglobal optimizers\n, paradigms/algorithms such as genetic algorithms, gibbs sampling, simulated annealing, evolutionary optimization fall under this category. They can be used in situations when the objective function in not \"smooth\".\n\n\nIn DynaML they are most prominently used in hyper-parameter optimization in kernel based learning methods. All \nglobal optimizers\n in DynaML extend the \nGlobalOptimizer\n trait, which implies that they provide an implementation for its \noptimize\n method.\n\n\nIn order to use a global optimization routine on an model, the model implementation in question must be extending the \nGloballyOptimizable\n trait in the \ndynaml.optimization\n package, this trait has only one method called \nenergy\n which is to be implemented by all sub-classes/traits.\n\n\nThe \nenergy\n method calculates the value of the global objective function for a particular configuration i.e. for particular values of model hyper-parameters. This objective function can be defined differently for each model class (marginal likelihood for Gaussian Processes, cross validation score for parametric models, etc).\n\n\nThe following model selection routines are available in DynaML so far.\n\n\nGrid Search\n\u00b6\n\n\nThe most elementary (naive) method of model selection is to evaluate its performance (value returned by \nenergy\n) on a fixed set of grid points which are initialized for the model hyper-parameters.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nval\n \nkernel\n \n=\n \n...\n\n\nval\n \nnoise\n \n=\n \n...\n\n\nval\n \ndata\n \n=\n \n...\n\n\nval\n \nmodel\n \n=\n \nnew\n \nGPRegression\n(\nkernel\n,\n \nnoise\n,\n \ndata\n)\n\n\n\nval\n \ngrid\n \n=\n \n5\n\n\nval\n \nstep\n \n=\n \n0.2\n\n\n\nval\n \ngs\n \n=\n \nnew\n \nGridSearch\n[\nmodel.\ntype\n](\nmodel\n)\n\n    \n.\nsetGridSize\n(\ngrid\n)\n\n    \n.\nsetStepSize\n(\nstep\n)\n\n    \n.\nsetLogScale\n(\nfalse\n)\n\n\n\nval\n \nstartConf\n \n=\n \nkernel\n.\nstate\n \n++\n \nnoise\n.\nstate\n\n\nval\n \n(\n_\n,\n \nconf\n)\n \n=\n \ngs\n.\noptimize\n(\nstartConf\n,\n \nopt\n)\n\n\n\nmodel\n.\nsetState\n(\nconf\n)\n\n\n\n\n\n\n\nCoupled Simulated Annealing\n\u00b6\n\n\nCoupled Simulated Annealing\n (CSA) is an iterative search procedure which evaluates model performance on a grid and in each iteration perturbs the grid points in a randomized manner. Each perturbed point is accepted using a certain acceptance probability which is a function of the performance on the whole grid.\n\n\nCoupled Simulated Annealing can be seen as an extension to the classical Simulated Annealing algorithm, since the acceptance probability and perturbation function are design choices, we can formulate a number of variants of CSA. Any CSA-like algorithm must have the following components.\n\n\n\n\nAn ensemble or grid of points \n\\(x_i \\in \\Theta\\)\n.\n\n\nA perturbation distribution or function $P: x_i \\rightarrow y_i $.\n\n\nA coupling term \n\\(\\gamma\\)\n for an ensemble.\n\n\nAn acceptance probability function \n\\(A_{\\Theta}(\\gamma, x_i \\rightarrow y_i)\\)\n.\n\n\nAn \nannealing schedule\n \n\\(T_{k}^{ac}, k = 0, 1, \\cdots\\)\n.\n\n\n\n\n\nThe \nCoupledSimulatedAnnealing\n class has a companion \nobject\n with the following available variants.\n\n\n\n\n\n\n\n\n\nVariant\n\n\nAcceptance Probability\n\n\nCoupling term \n\\(\\gamma\\)\n\n\n\n\n\n\n\n\n\n\nSA\n:  Classical \nSimulated Annealing\n\n\n\\(1/(1 + exp(\\frac{E(y) - E(x)}{T^{ac}_{k}}))\\)\n\n\n-\n\n\n\n\n\n\nMuSA\n: \nMulti-state Simulated Annealing\n: Direct generalization of \nSimulated Annealing\n\n\n\\(exp(-E(y_i))/(exp(-E(y_i)) + \\gamma)\\)\n\n\n\\(\\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})}\\)\n\n\n\n\n\n\nBA\n:  \nBlind Acceptance\n CSA\n\n\n\\(1 - exp(-E(x_i)/T_{k}^{ac})/\\gamma\\)\n\n\n\\(\\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})}\\)\n\n\n\n\n\n\nM\n:  Modified CSA\n\n\n\\(exp(E(x_i)/T_{k}^{ac})/\\gamma\\)\n\n\n\\(\\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})}\\)\n\n\n\n\n\n\nMwVC\n:  Modified CSA with Variance Control: Employs an \nannealing schedule\n that controls the variance of the acceptance probabilities of states\n\n\n\\(exp(E(x_i)/T_{k}^{ac})/\\gamma\\)\n\n\n\\(\\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})}\\)\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nval\n \nkernel\n \n=\n \n...\n\n\nval\n \nnoise\n \n=\n \n...\n\n\nval\n \ndata\n \n=\n \n...\n\n\nval\n \nmodel\n \n=\n \nnew\n \nGPRegression\n(\nkernel\n,\n \nnoise\n,\n \ndata\n)\n\n\n\n//The default variant of CSA is Mw-VC\n\n\nval\n \ngs\n \n=\n \nnew\n \nCoupledSimulatedAnnealing\n[\nmodel.\ntype\n](\nmodel\n)\n\n    \n.\nsetGridSize\n(\ngrid\n)\n\n    \n.\nsetStepSize\n(\nstep\n)\n\n    \n.\nsetLogScale\n(\nfalse\n)\n\n    \n.\nsetVariant\n(\nCoupledSimulatedAnnealing\n.\nMuSA\n)\n\n\n\nval\n \nstartConf\n \n=\n \nkernel\n.\nstate\n \n++\n \nnoise\n.\nstate\n\n\nval\n \n(\n_\n,\n \nconf\n)\n \n=\n \ngs\n.\noptimize\n(\nstartConf\n,\n \nopt\n)\n\n\n\nmodel\n.\nsetState\n(\nconf\n)\n\n\n\n\n\n\n\nGradient based Model Selection\n\u00b6\n\n\nGradient based model selection can be used if the model fitness function implemented in the \nenergy\n method has differentiability properties (e.g. using marginal likelihood in the case of stochastic process inference). The \nGloballyOptWithGrad\n trait is an extension of \nGlobalOptimizer\n and adds a method \ngradEnergy\n that should return the gradient of the fitness function in each hyper-parameter in the form of a \nMap[String, Double]\n.\n\n\nMaximum Likelihood ML-II\n\u00b6\n\n\nIn the \nMaximum Likelihood\n (ML-II) algorithm (refer to \nRamussen & Williams\n for more details), we aim to maximize the log marginal likelihood by calculating its gradient with respect to the hyper-parameters \n\\(\n\\(\\theta_j\\)\n\\)\n in each iteration and performing \nsteepest ascent\n. The calculations are summarized below.\n\n\n\\[\n\\begin{equation}\nlog \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = - \\frac{1}{2} \\mathbf{y}^T K^{-1} \\mathbf{y} - \\frac{1}{2} log |K| - \\frac{n}{2} log 2\\pi\n\\end{equation}\n\\]\n\\[\n\\begin{align}\n& \\frac{\\partial }{\\partial \\theta_j} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = \\frac{1}{2} tr ((\\mathbf{\\alpha} \\mathbf{\\alpha}^T - K^{-1}) \\frac{\\partial K}{\\partial \\theta_j}) \\\\\n& \\mathbf{\\alpha} = K^{-1} \\mathbf{y}\n\\end{align}\n\\]\nThe \nGPMLOptimizer[I, T, M]\n class implements ML-II, by using the \ngradEnergy\n method implemented by the \nsystem: M\n member value (which refers to a model extending  \nGloballyOptWithGrad\n).\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nval\n \nkernel\n \n=\n \n...\n\n\nval\n \nnoise\n \n=\n \n...\n\n\nval\n \ndata\n \n=\n \n...\n\n\nval\n \nmodel\n \n=\n \nnew\n \nGPRegression\n(\nkernel\n,\n \nnoise\n,\n \ndata\n)\n\n\n\nval\n \nml\n \n=\n \nnew\n \nGPMLOptimizer\n[\nDenseVector\n[\nDouble\n]\n,\n    \nSeq\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n,\n    \nGPRegression\n](\nmodel\n)\n\n\n\nval\n \nstartConf\n \n=\n \nkernel\n.\nstate\n \n++\n \nnoise\n.\nstate\n\n\nval\n \n(\n_\n,\n \nconf\n)\n \n=\n \nml\n.\noptimize\n(\nstartConf\n,\n \nopt\n)\n\n\n\nmodel\n.\nsetState\n(\nconf\n)",
            "title": "Global"
        },
        {
            "location": "/core/core_opt_global/#model-selection-routines",
            "text": "These routines are also known as  global optimizers , paradigms/algorithms such as genetic algorithms, gibbs sampling, simulated annealing, evolutionary optimization fall under this category. They can be used in situations when the objective function in not \"smooth\".  In DynaML they are most prominently used in hyper-parameter optimization in kernel based learning methods. All  global optimizers  in DynaML extend the  GlobalOptimizer  trait, which implies that they provide an implementation for its  optimize  method.  In order to use a global optimization routine on an model, the model implementation in question must be extending the  GloballyOptimizable  trait in the  dynaml.optimization  package, this trait has only one method called  energy  which is to be implemented by all sub-classes/traits.  The  energy  method calculates the value of the global objective function for a particular configuration i.e. for particular values of model hyper-parameters. This objective function can be defined differently for each model class (marginal likelihood for Gaussian Processes, cross validation score for parametric models, etc).  The following model selection routines are available in DynaML so far.",
            "title": "Model Selection Routines"
        },
        {
            "location": "/core/core_opt_global/#grid-search",
            "text": "The most elementary (naive) method of model selection is to evaluate its performance (value returned by  energy ) on a fixed set of grid points which are initialized for the model hyper-parameters.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 val   kernel   =   ...  val   noise   =   ...  val   data   =   ...  val   model   =   new   GPRegression ( kernel ,   noise ,   data )  val   grid   =   5  val   step   =   0.2  val   gs   =   new   GridSearch [ model. type ]( model ) \n     . setGridSize ( grid ) \n     . setStepSize ( step ) \n     . setLogScale ( false )  val   startConf   =   kernel . state   ++   noise . state  val   ( _ ,   conf )   =   gs . optimize ( startConf ,   opt )  model . setState ( conf )",
            "title": "Grid Search"
        },
        {
            "location": "/core/core_opt_global/#coupled-simulated-annealing",
            "text": "Coupled Simulated Annealing  (CSA) is an iterative search procedure which evaluates model performance on a grid and in each iteration perturbs the grid points in a randomized manner. Each perturbed point is accepted using a certain acceptance probability which is a function of the performance on the whole grid.  Coupled Simulated Annealing can be seen as an extension to the classical Simulated Annealing algorithm, since the acceptance probability and perturbation function are design choices, we can formulate a number of variants of CSA. Any CSA-like algorithm must have the following components.   An ensemble or grid of points  \\(x_i \\in \\Theta\\) .  A perturbation distribution or function $P: x_i \\rightarrow y_i $.  A coupling term  \\(\\gamma\\)  for an ensemble.  An acceptance probability function  \\(A_{\\Theta}(\\gamma, x_i \\rightarrow y_i)\\) .  An  annealing schedule   \\(T_{k}^{ac}, k = 0, 1, \\cdots\\) .   \nThe  CoupledSimulatedAnnealing  class has a companion  object  with the following available variants.     Variant  Acceptance Probability  Coupling term  \\(\\gamma\\)      SA :  Classical  Simulated Annealing  \\(1/(1 + exp(\\frac{E(y) - E(x)}{T^{ac}_{k}}))\\)  -    MuSA :  Multi-state Simulated Annealing : Direct generalization of  Simulated Annealing  \\(exp(-E(y_i))/(exp(-E(y_i)) + \\gamma)\\)  \\(\\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})}\\)    BA :   Blind Acceptance  CSA  \\(1 - exp(-E(x_i)/T_{k}^{ac})/\\gamma\\)  \\(\\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})}\\)    M :  Modified CSA  \\(exp(E(x_i)/T_{k}^{ac})/\\gamma\\)  \\(\\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})}\\)    MwVC :  Modified CSA with Variance Control: Employs an  annealing schedule  that controls the variance of the acceptance probabilities of states  \\(exp(E(x_i)/T_{k}^{ac})/\\gamma\\)  \\(\\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})}\\)      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 val   kernel   =   ...  val   noise   =   ...  val   data   =   ...  val   model   =   new   GPRegression ( kernel ,   noise ,   data )  //The default variant of CSA is Mw-VC  val   gs   =   new   CoupledSimulatedAnnealing [ model. type ]( model ) \n     . setGridSize ( grid ) \n     . setStepSize ( step ) \n     . setLogScale ( false ) \n     . setVariant ( CoupledSimulatedAnnealing . MuSA )  val   startConf   =   kernel . state   ++   noise . state  val   ( _ ,   conf )   =   gs . optimize ( startConf ,   opt )  model . setState ( conf )",
            "title": "Coupled Simulated Annealing"
        },
        {
            "location": "/core/core_opt_global/#gradient-based-model-selection",
            "text": "Gradient based model selection can be used if the model fitness function implemented in the  energy  method has differentiability properties (e.g. using marginal likelihood in the case of stochastic process inference). The  GloballyOptWithGrad  trait is an extension of  GlobalOptimizer  and adds a method  gradEnergy  that should return the gradient of the fitness function in each hyper-parameter in the form of a  Map[String, Double] .",
            "title": "Gradient based Model Selection"
        },
        {
            "location": "/core/core_opt_global/#maximum-likelihood-ml-ii",
            "text": "In the  Maximum Likelihood  (ML-II) algorithm (refer to  Ramussen & Williams  for more details), we aim to maximize the log marginal likelihood by calculating its gradient with respect to the hyper-parameters  \\( \\(\\theta_j\\) \\)  in each iteration and performing  steepest ascent . The calculations are summarized below.  \\[\n\\begin{equation}\nlog \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = - \\frac{1}{2} \\mathbf{y}^T K^{-1} \\mathbf{y} - \\frac{1}{2} log |K| - \\frac{n}{2} log 2\\pi\n\\end{equation}\n\\] \\[\n\\begin{align}\n& \\frac{\\partial }{\\partial \\theta_j} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = \\frac{1}{2} tr ((\\mathbf{\\alpha} \\mathbf{\\alpha}^T - K^{-1}) \\frac{\\partial K}{\\partial \\theta_j}) \\\\\n& \\mathbf{\\alpha} = K^{-1} \\mathbf{y}\n\\end{align}\n\\] The  GPMLOptimizer[I, T, M]  class implements ML-II, by using the  gradEnergy  method implemented by the  system: M  member value (which refers to a model extending   GloballyOptWithGrad ).   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 val   kernel   =   ...  val   noise   =   ...  val   data   =   ...  val   model   =   new   GPRegression ( kernel ,   noise ,   data )  val   ml   =   new   GPMLOptimizer [ DenseVector [ Double ] ,\n     Seq [( DenseVector [ Double ] ,  Double )] ,\n     GPRegression ]( model )  val   startConf   =   kernel . state   ++   noise . state  val   ( _ ,   conf )   =   ml . optimize ( startConf ,   opt )  model . setState ( conf )",
            "title": "Maximum Likelihood ML-II"
        },
        {
            "location": "/core/core_model_evaluation/",
            "text": "Model evaluation is the litmus test for knowing if your modeling effort is headed in the right direction and for comparing various alternative models (or hypothesis) attempting to explain a phenomenon. The \nevaluation\n package contains classes and traits to calculate performance metrics for DynaML models.\n\n\nClasses which implement model performance calculation can extend the \nMetrics\n[\nP\n]\n trait. The \nMetrics\n trait requires that its sub-classes implement three methods or behaviors.\n\n\n\n\nPrint out the performance metrics (whatever they may be) to the screen i.e. \nprint\n method.\n\n\nReturn the key performance indicators in the form of a breeze \nDenseVector\n[\nDouble\n]\n, i.e. the \nkpi\n()\n method.\n\n\n\n\nRegression Models\n\u00b6\n\n\nRegression models are generally evaluated on a few standard metrics such as \nmean square error\n, \nmean absolute error\n, \ncoefficient of determination\n (\n\\(R^2\\)\n), etc. DynaML has implementations for single output and multi-output regression models.\n\n\nSingle Output\n\u00b6\n\n\nSmall Test Set\n\n\nThe \nRegressionMetrics\n class takes as input a scala list containing the predictions and actual outputs and calculates the following metrics.\n\n\n\n\nMean Absolute Error\n (mae)\n\n\nRoot Mean Square Error\n (rmse)\n\n\nCorrelation Coefficient\n (\n\\(\\rho_{y \\hat{y}}\\)\n)\n\n\nCoefficient of Determination\n (\n\\(R^2\\)\n)\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n//Predictions computed by any model.\n\n\nval\n \npredictionAndOutputs\n:\n \nList\n[(\nDouble\n, \nDouble\n)]\n \n=\n \n...\n\n\n\nval\n \nmetrics\n \n=\n \nnew\n \nRegressionMetrics\n(\npredictionAndOutputs\n,\n \npredictionAndOutputs\n.\nlength\n)\n\n\n\n//Print results on screen\n\n\nmetrics\n.\nprint\n\n\n\n\n\n\n\nLarge Test Set\n\n\nThe \nRegressionMetricsSpark\n class takes as input an \nApache Spark\n RDD\n containing the predictions and actual outputs and calculates the same metrics as above.\n\n\n1\n2\n3\n4\n5\n6\n7\n//Predictions computed by any model.\n\n\nval\n \npredictionAndOutputs\n:\n \nRDD\n[(\nDouble\n, \nDouble\n)]\n \n=\n \n...\n\n\n\nval\n \nmetrics\n \n=\n \nnew\n \nRegressionMetricsSpark\n(\npredictionAndOutputs\n,\n \npredictionAndOutputs\n.\nlength\n)\n\n\n\n//Print results on screen\n\n\nmetrics\n.\nprint\n\n\n\n\n\n\n\nMultiple Outputs\n\u00b6\n\n\nThe \nMultiRegressionMetrics\n class calculates regression performance for multi-output models.\n\n\n1\n2\n3\n4\n5\n6\n7\n//Predictions computed by any model.\n\n\nval\n \npredictionAndOutputs\n:\n \nList\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]\n \n=\n \n...\n\n\n\nval\n \nmetrics\n \n=\n \nnew\n \nMultiRegressionMetrics\n(\npredictionAndOutputs\n,\n \npredictionAndOutputs\n.\nlength\n)\n\n\n\n//Print results on screen\n\n\nmetrics\n.\nprint\n\n\n\n\n\n\n\nClassification Models\n\u00b6\n\n\nCurrently (as of v1.4) there is only a binary classification implementation for calculating model performance.\n\n\nBinary Classification\n\u00b6\n\n\nSmall Test Sets\n\n\nThe \nBinaryClassificationMetrics\n class calculates the following performance indicators.\n\n\n\n\nClassification accuracy\n\n\nF-measure\n\n\nPrecision-Recall Curve (and area under it).\n\n\nReceiver Operating Characteristic (and area under it)\n\n\nMatthew's Correlation Coefficient\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nval\n \nscoresAndLabels\n:\n \nList\n[(\nDouble\n, \nDouble\n)]\n \n=\n \n...\n\n\n\n//Set logisticFlag = true in case outputs are produced via logistic regression\n\n\nval\n \nmetrics\n \n=\n \nnew\n \nBinaryClassificationMetrics\n(\n\n          \nscoresAndLabels\n,\n\n          \nscoresAndLabels\n.\nlength\n,\n\n          \nlogisticFlag\n \n=\n \ntrue\n)\n\n\n\nmetrics\n.\nprint\n\n\n\n\n\n\n\nLarge Test Sets\n\n\nThe \nBinaryClassificationMetricsSpark\n class takes as input an \nApache Spark\n RDD\n containing the predictions and actual labels and calculates the same metrics as above.",
            "title": "Performance Evaluation"
        },
        {
            "location": "/core/core_model_evaluation/#regression-models",
            "text": "Regression models are generally evaluated on a few standard metrics such as  mean square error ,  mean absolute error ,  coefficient of determination  ( \\(R^2\\) ), etc. DynaML has implementations for single output and multi-output regression models.",
            "title": "Regression Models"
        },
        {
            "location": "/core/core_model_evaluation/#single-output",
            "text": "Small Test Set  The  RegressionMetrics  class takes as input a scala list containing the predictions and actual outputs and calculates the following metrics.   Mean Absolute Error  (mae)  Root Mean Square Error  (rmse)  Correlation Coefficient  ( \\(\\rho_{y \\hat{y}}\\) )  Coefficient of Determination  ( \\(R^2\\) )   1\n2\n3\n4\n5\n6\n7 //Predictions computed by any model.  val   predictionAndOutputs :   List [( Double ,  Double )]   =   ...  val   metrics   =   new   RegressionMetrics ( predictionAndOutputs ,   predictionAndOutputs . length )  //Print results on screen  metrics . print    Large Test Set  The  RegressionMetricsSpark  class takes as input an  Apache Spark  RDD  containing the predictions and actual outputs and calculates the same metrics as above.  1\n2\n3\n4\n5\n6\n7 //Predictions computed by any model.  val   predictionAndOutputs :   RDD [( Double ,  Double )]   =   ...  val   metrics   =   new   RegressionMetricsSpark ( predictionAndOutputs ,   predictionAndOutputs . length )  //Print results on screen  metrics . print",
            "title": "Single Output"
        },
        {
            "location": "/core/core_model_evaluation/#multiple-outputs",
            "text": "The  MultiRegressionMetrics  class calculates regression performance for multi-output models.  1\n2\n3\n4\n5\n6\n7 //Predictions computed by any model.  val   predictionAndOutputs :   List [( DenseVector [ Double ] ,  DenseVector [ Double ])]   =   ...  val   metrics   =   new   MultiRegressionMetrics ( predictionAndOutputs ,   predictionAndOutputs . length )  //Print results on screen  metrics . print",
            "title": "Multiple Outputs"
        },
        {
            "location": "/core/core_model_evaluation/#classification-models",
            "text": "Currently (as of v1.4) there is only a binary classification implementation for calculating model performance.",
            "title": "Classification Models"
        },
        {
            "location": "/core/core_model_evaluation/#binary-classification",
            "text": "Small Test Sets  The  BinaryClassificationMetrics  class calculates the following performance indicators.   Classification accuracy  F-measure  Precision-Recall Curve (and area under it).  Receiver Operating Characteristic (and area under it)  Matthew's Correlation Coefficient   1\n2\n3\n4\n5\n6\n7\n8\n9 val   scoresAndLabels :   List [( Double ,  Double )]   =   ...  //Set logisticFlag = true in case outputs are produced via logistic regression  val   metrics   =   new   BinaryClassificationMetrics ( \n           scoresAndLabels , \n           scoresAndLabels . length , \n           logisticFlag   =   true )  metrics . print    Large Test Sets  The  BinaryClassificationMetricsSpark  class takes as input an  Apache Spark  RDD  containing the predictions and actual labels and calculates the same metrics as above.",
            "title": "Binary Classification"
        },
        {
            "location": "/pipes/pipes/",
            "text": "Summary\n\n\nIn this section we attempt to give a simple yet effective introduction to the data pipes module of DynaML.\n\n\n\n\nMotivation\n\u00b6\n\n\nMachine Learning involves operations that can be thought of as being part of different stages.\n\n\n\n\n\n\nData pre-processing\n\n\nData \nmunging\n or pre-processing is one of the most time consuming activities in the analysis   and modeling cycle, yet very few libraries do justice to this need.\n\n\n\n\n\n\nModeling: train, validation & test :\n\n\nTraining and testing models on data is a cyclical process and in the interest of keeping things manageable, it is important to separate operations in stage 1 from stage 2 and 3.\n\n\n\n\n\n\nPost processing: produce and summarize results via reports, visualizations etc.\n\n\n\n\n\n\nWhat are Data Pipes?\n\u00b6\n\n\nAt their heart data pipes in DynaML are (wrapped) Scala functions. Every machine learning workflow can be thought of as a chain of functional transformations on data. These functional transformations are applied one after another (in fancy language \ncomposed\n) to yield a result which is then suitable for modeling/training.\n\n\nCreating a Pipe\n\u00b6\n\n\nAs we mentioned earlier a DynaML pipe is nothing but a thin wrapper around a scala function. Creating a new data pipe is very easy, you just create a scala function and give it to the \n`#!scala DataPipe()\n object.\n\n\n1\n2\n3\nval\n \nfunc\n \n=\n \n(\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n)\n\n\n\nval\n \ntPipe\n \n=\n \nDataPipe\n(\nfunc\n)\n\n\n\n\n\n\n\nStacking/Composing Data Pipes\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nval\n \npre_processing\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n))\n\n\nval\n \npost_processing\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nif\n(\nx\n \n<=\n \n0.2\n)\n \n\"Y\"\n \nelse\n \n\"N\"\n)\n\n\n\n//Compose the two pipes\n\n\n//The result will be \"Y\"\n\n\nval\n \ntPipe\n \n=\n \npre_processing\n \n>\n \npost_processing\n\n\n\ntPipe\n(\n15.5\n)\n\n\n\n\n\n\n\n\n\nTip\n\n\nIt is possible to create a pipe from any scala type to another, inclucing \nUnit\n. For example the statement \nval\n \np\n \n=\n \nDataPipe\n(()\n \n=>\n \nscala\n.\nRandom\n.\nnextGaussian\n())\n creates a pipe which when executed samples from a univariate gaussian distribution \nval\n \nsample\n \n=\n \np\n.\nrun\n()\n\n\n\n\n\n\nYou can compose or stack any number of pipes using the \n>\n character to create a composite data workflow. There is only one constraint when joining two pipes, that the destination type of the first pipe must be the same as the source type of the second pipe, in other words:\n\n\n\n\ndont put square pegs into round holes",
            "title": "DynaML Pipes"
        },
        {
            "location": "/pipes/pipes/#motivation",
            "text": "Machine Learning involves operations that can be thought of as being part of different stages.    Data pre-processing  Data  munging  or pre-processing is one of the most time consuming activities in the analysis   and modeling cycle, yet very few libraries do justice to this need.    Modeling: train, validation & test :  Training and testing models on data is a cyclical process and in the interest of keeping things manageable, it is important to separate operations in stage 1 from stage 2 and 3.    Post processing: produce and summarize results via reports, visualizations etc.",
            "title": "Motivation"
        },
        {
            "location": "/pipes/pipes/#what-are-data-pipes",
            "text": "At their heart data pipes in DynaML are (wrapped) Scala functions. Every machine learning workflow can be thought of as a chain of functional transformations on data. These functional transformations are applied one after another (in fancy language  composed ) to yield a result which is then suitable for modeling/training.",
            "title": "What are Data Pipes?"
        },
        {
            "location": "/pipes/pipes/#creating-a-pipe",
            "text": "As we mentioned earlier a DynaML pipe is nothing but a thin wrapper around a scala function. Creating a new data pipe is very easy, you just create a scala function and give it to the  `#!scala DataPipe()  object.  1\n2\n3 val   func   =   ( x :   Double )   =>   math . sin ( 2.0 * x )* math . exp (- 2.0 * x )  val   tPipe   =   DataPipe ( func )",
            "title": "Creating a Pipe"
        },
        {
            "location": "/pipes/pipes/#stackingcomposing-data-pipes",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 val   pre_processing   =   DataPipe (( x :   Double )   =>   math . sin ( 2.0 * x )* math . exp (- 2.0 * x ))  val   post_processing   =   DataPipe (( x :   Double )   =>   if ( x   <=   0.2 )   \"Y\"   else   \"N\" )  //Compose the two pipes  //The result will be \"Y\"  val   tPipe   =   pre_processing   >   post_processing  tPipe ( 15.5 )     Tip  It is possible to create a pipe from any scala type to another, inclucing  Unit . For example the statement  val   p   =   DataPipe (()   =>   scala . Random . nextGaussian ())  creates a pipe which when executed samples from a univariate gaussian distribution  val   sample   =   p . run ()    You can compose or stack any number of pipes using the  >  character to create a composite data workflow. There is only one constraint when joining two pipes, that the destination type of the first pipe must be the same as the source type of the second pipe, in other words:   dont put square pegs into round holes",
            "title": "Stacking/Composing Data Pipes"
        },
        {
            "location": "/pipes/pipes_api/",
            "text": "Base\n\u00b6\n\n\nAt the top of the pipes hierarchy is the base trait \nDataPipe[Source, Destination]\n which is a thin wrapper for a Scala function having the type \n(Source) => Destination\n. Along with that the base trait also defines how pipes are composed with each other to yield more complex workflows.\n\n\nPipes in Parallel\n\u00b6\n\n\nThe \nParallelPipe[Source1, Result1, Source2, Result2]\n trait models pipes which are attached to each other, from an implementation point of view these can be seen as data pipes taking input from \n(Source1, Source2)\n and yielding values from \n(Result1, Result2)\n. They can be created in two ways:\n\n\nBy supplying two pipes to the \nDataPipe()\n object.\n\n\n1\n2\n3\n4\n5\n6\nval\n \npipe1\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n))\n\n\nval\n \npipe2\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nif\n(\nx\n \n<=\n \n0.2\n)\n \n\"Y\"\n \nelse\n \n\"N\"\n)\n\n\n\nval\n \npipe3\n \n=\n \nDataPipe\n(\npipe1\n,\n \npipe2\n)\n\n\n//Returns (-0.013, \"N\")\n\n\npipe3\n((\n2.0\n,\n \n15.0\n))\n\n\n\n\n\n\n\nBy duplicating a single pipe using \nDynaMLPipe.duplicate\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n//Already imported in DynaML repl\n\n\n//but should be imported when using DynaML API\n\n\n//outside of its provided repl environment.\n\n\nimport\n \nio.github.mandar2812.dynaml.DynaMLPipe._\n\n\n\nval\n \npipe1\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n))\n\n\n\nval\n \npipe3\n \n=\n \nduplicate\n(\npipe1\n)\n\n\n//Returns (-0.013, -9E-14)\n\n\npipe3\n((\n2.0\n,\n \n15.0\n))\n\n\n\n\n\n\n\nDiverging Pipes\n\u00b6\n\n\nThe \nBifurcationPipe[Source, Result1, Result2]\n trait represents pipes which start from the same source and yield two result types, from an implementation point of view these can be seen as data pipes taking input from \nSource1\n and yielding values from \n(Result1, Result2)\n. They can be created in two ways:\n\n\nBy supplying a function of type \n(Source) => (Result1, Result2)\n to the \nDataPipe()\n object.\n\n\n1\n2\n3\nval\n \npipe1\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \n(\n1.0\n*\nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n),\n \nmath\n.\nexp\n(-\n2.0\n*\nx\n)))\n\n\n\npipe1\n(\n2.0\n)\n\n\n\n\n\n\n\nBy using the \nBifurcationPipe()\n object\n\n\n1\n2\n3\n4\n5\nval\n \npipe1\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n))\n\n\nval\n \npipe2\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nif\n(\nx\n \n<=\n \n0.2\n)\n \n\"Y\"\n \nelse\n \n\"N\"\n)\n\n\n\nval\n \npipe3\n \n=\n \nBifurcationPipe\n(\npipe1\n,\n \npipe2\n)\n\n\npipe3\n(\n2.0\n)\n\n\n\n\n\n\n\nSide Effects\n\u00b6\n\n\nIn order to enable pipes which have side effects i.e. writing to disk, the \nSideEffectPipe[Source]\n trait is used. Conceptually it is a pipe taking as input a value from \nSource\n but has a return type of \nUnit\n.\n\n\nStream Processing\n\u00b6\n\n\nTo simplify writing pipes for scala streams, the \nStreamDataPipe[I, J, K]\n and its subclasses implement workflows on streams.  \n\n\nMap\n\u00b6\n\n\nMap every element of a stream.\n\n\n1\n2\n3\n4\nval\n \npipe1\n \n=\n \nStreamDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n))\n\n\n\nval\n \nstr\n:\n \nStream\n[\nDouble\n]\n \n=\n \n(\n1\n \nto\n \n5\n).\nmap\n(\n_\n.\ntoDouble\n).\ntoStream\n\n\npipe1\n(\nstr\n)\n\n\n\n\n\n\n\nFilter\n\u00b6\n\n\nFilter certain elements of a stream.\n\n\n1\n2\n3\n4\nval\n \npipe1\n \n=\n \nStreamDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n \n<=\n \n2.5\n)\n\n\n\nval\n \nstr\n:\n \nStream\n[\nDouble\n]\n \n=\n \n(\n1\n \nto\n \n5\n).\nmap\n(\n_\n.\ntoDouble\n).\ntoStream\n\n\npipe1\n(\nstr\n)\n\n\n\n\n\n\n\nBifurcate stream\n\u00b6\n\n\n1\n2\n3\n4\nval\n \npipe1\n \n=\n \nStreamPartitionPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n \n<=\n \n2.5\n)\n\n\n\nval\n \nstr\n:\n \nStream\n[\nDouble\n]\n \n=\n \n(\n1\n \nto\n \n5\n).\nmap\n(\n_\n.\ntoDouble\n).\ntoStream\n\n\npipe1\n(\nstr\n)\n\n\n\n\n\n\n\nSide effect\n\u00b6\n\n\n1\n2\n3\n4\nval\n \npipe1\n \n=\n \nStreamDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nprintln\n(\n\"Number is: \"\n+\nx\n))\n\n\n\nval\n \nstr\n:\n \nStream\n[\nDouble\n]\n \n=\n \n(\n1\n \nto\n \n5\n).\nmap\n(\n_\n.\ntoDouble\n).\ntoStream\n\n\npipe1\n(\nstr\n)\n\n\n\n\n\n\n\n\n\nThe following API members were added in \nv1.4.1\n\n\nFlat Map\n\u00b6\n\n\nStreamFlatMapPipe\n carries out the scala flat-map operation on a stream.\n\n\n1\n2\n3\n4\nval\n \nmapFunc\n \n=\n \n(\nn\n:\n \nInt\n)\n \n=>\n \n(\n1\n \nto\n \nn\n).\nsliding\n(\n2\n).\ntoStream\n\n\nval\n \nstreamFMPipe\n \n=\n \nStreamFlatMapPipe\n(\nmapFunc\n)\n\n\n\nstreamFMPipe\n((\n1\n \nto\n \n20\n).\ntoStream\n)\n\n\n\n\n\n\n\n\n\nPipes on Spark RDDs\n\n\nIt is also possible to create pipes acting on Spark RDDs.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nval\n \nnum\n \n=\n \n20\n\n\nval\n \nsc\n:\n \nSparkContext\n \n=\n \n_\n\n\nval\n \nnumbers\n \n=\n \nsc\n.\nparallelize\n(\n1\n \nto\n \nnum\n)\n\n\nval\n \nconvPipe\n \n=\n \nRDDPipe\n((\nn\n:\n \nInt\n)\n \n=>\n \nn\n.\ntoDouble\n)\n\n\n\nval\n \nsqPipe\n \n=\n \nRDDPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n*\nx\n)\n\n\n\nval\n \nsqrtPipe\n \n=\n \nRDDPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsqrt\n(\nx\n))\n\n\n\nval\n \nresultPipe\n \n=\n \nRDDPipe\n((\nr\n:\n \nRDD\n[\nDouble\n])\n \n=>\n \nr\n.\nreduce\n(\n_\n+\n_\n).\ntoInt\n)\n\n\n\nval\n \nnetPipeline\n \n=\n \nconvPipe\n \n>\n \nsqPipe\n \n>\n \nsqrtPipe\n \n>\n \nresultPipe\n\n\nnetPipeline\n(\nnumbers\n)\n\n\n\n\n\n\n\n\n\nAdvanced Pipes\n\u00b6\n\n\nApart from the basic capabilities offered by the \nDataPipe\n[\nSource\n, \nDestination\n]\n interface and its family, users can also work with\nmore complex workflow components some of which are shown below.\n\n\nThe advanced components of the pipes API enable two key extensions.\n\n\n\n\nData pipes which take more than one argument\n1\n.\n\n\nData pipes which take an argument and return a data pipe\n2\n\n\n\n\nData Pipe 2\n\u00b6\n\n\nDataPipe2\n[\nA\n, \nB\n, \nC\n]\n\n\narguments\n: 2 of type \nA\n and \nB\n respectively\n\n\nreturns\n: result of type \nC\n\n\n1\n2\nval\n \nf2\n:\n \n(\nA\n,\n \nB\n)\n \n=>\n \nC\n \n=\n \n_\n  \n\nval\n \npipe2\n \n=\n \nDataPipe2\n(\nf2\n)\n\n\n\n\n\n\n\nDataPipe 3\n\u00b6\n\n\nDataPipe3\n[\nA\n, \nB\n, \nC\n, \nD\n]\n\n\narguments\n: 3 of type \nA\n, \nB\n and \nC\n respectively\n\n\nreturns\n: result of type \nD\n\n\n1\n2\nval\n \nf3\n:\n \n(\nA\n,\n \nB\n,\n \nC\n)\n \n=>\n \nD\n \n=\n \n_\n  \n\nval\n \npipe3\n \n=\n \nDataPipe3\n(\nf3\n)\n\n\n\n\n\n\n\nDataPipe 4\n\u00b6\n\n\nDataPipe4\n[\nA\n, \nB\n, \nC\n, \nD\n, \nE\n]\n\n\narguments\n: 4 of type \nA\n, \nB\n, \nC\n and \nD\n respectively\n\n\nreturns\n: result of type \nE\n\n\n1\n2\nval\n \nf4\n:\n \n(\nA\n,\n \nB\n,\n \nC\n,\n \nD\n)\n \n=>\n \nE\n \n=\n \n_\n  \n\nval\n \npipe4\n \n=\n \nDataPipe4\n(\nf4\n)\n\n\n\n\n\n\n\nMeta Pipe\n\u00b6\n\n\nMetaPipe\n[\nA\n, \nB\n, \nC\n]\n\n\nTakes an argument returns a \nDataPipe\n\n\n1\n2\n3\n4\n5\n6\nval\n \nmpipe\n \n=\n \nMetaPipe\n(\n\n  \n(\nomega\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n\n  \n(\nx\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n \nmath\n.\nexp\n(-\nomega\n.\nt\n*\nx\n))\n\n\n\n//Returns a pipe which computes exp(-2pi/10 1Tx)\n\n\nval\n \nexpPipe\n \n=\n \nmpipe\n(\nDenseVector\n.\nfill\n[\nDouble\n](\n10\n)(\n2.0\n*\nmath\n.\nPi\n/\n10.0\n))\n\n\n\n\n\n\n\nMeta Pipe (2, 1)\n\u00b6\n\n\nMetaPipe21\n[\nA\n, \nB\n, \nC\n, \nD\n]\n\n\nTakes 2 arguments returns a \nDataPipe\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \npipe21\n \n=\n \nMetaPipe21\n(\n\n  \n(\nalpha\n:\n \nDouble\n,\n \nbeta\n:\n \nDouble\n)\n \n=>\n\n  \n(\nrv\n:\n \nContinuousRandomVariable\n[\nDouble\n])\n \n=>\n \n(\nrv\n*\nbeta\n)\n \n+\n \nalpha\n\n\n)\n\n\n\nval\n \nrandom_func\n \n=\n \npipe21\n(\n1.5\n,\n \n-\n0.5\n)\n\n\nval\n \nresult_rv\n \n=\n \nrandom_func\n(\nRandomVariable\n(\nGamma\n(\n1.5\n,\n \n2.5\n)))\n\n\n\n//Draw samples from resulting random variable\n\n\nresult_rv\n.\niid\n(\n500\n).\ndraw\n\n\n\n\n\n\n\nMeta Pipe (1, 2)\n\u00b6\n\n\nMetaPipe12\n[\nA\n, \nB\n, \nC\n, \nD\n]\n\n\nTakes an argument returns a \nDataPipe2\n\n\n\n\n>\n and \n>>\n operators on higher order pipes\n\n\nAlthough the \n>\n operator is defined on higher order pipes, it quickly becomes difficult\nto imagine what transformation it can be joined with. For example the \n>\n operator applied\nafter a \nMetaPipe\n[\nI\n, \nJ\n, \nK\n]\n instance would expect a \nDataPipe\n[\nDataPipe\n[\nJ\n,\nK\n]\n, \n_\n]\n\ninstance in order for the join to proceed.\n\n\nThe \n>>\n operator on the other hand has a different and easier purpose.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nval\n \nmpipe\n \n=\n \nMetaPipe\n(\n\n  \n(\nomega\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n\n  \n(\nx\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n \nmath\n.\nexp\n(-\nomega\n.\nt\n*\nx\n))\n\n\n\nval\n \nfurther_pipe\n \n=\n \nDataPipe\n((\ny\n:\n \nDouble\n)\n \n=>\n \ny\n*\ny\n \n+\n \n2.0\n)\n\n\n\n//Returns MetaPipe[DenseVector[Double], DenseVector[Double], Double]\n\n\n//Computes exp(2*omega.x) + 2.0\n\n\nval\n \nfinal_pipe\n \n=\n \nmpipe\n \n>>\n \nfurther_pipe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nso far DynaML has support till 4\u00a0\n\u21a9\n\n\n\n\n\n\nsimilar to curried functions in scala.\u00a0\n\u21a9",
            "title": "Pipes API"
        },
        {
            "location": "/pipes/pipes_api/#base",
            "text": "At the top of the pipes hierarchy is the base trait  DataPipe[Source, Destination]  which is a thin wrapper for a Scala function having the type  (Source) => Destination . Along with that the base trait also defines how pipes are composed with each other to yield more complex workflows.",
            "title": "Base"
        },
        {
            "location": "/pipes/pipes_api/#pipes-in-parallel",
            "text": "The  ParallelPipe[Source1, Result1, Source2, Result2]  trait models pipes which are attached to each other, from an implementation point of view these can be seen as data pipes taking input from  (Source1, Source2)  and yielding values from  (Result1, Result2) . They can be created in two ways:  By supplying two pipes to the  DataPipe()  object.  1\n2\n3\n4\n5\n6 val   pipe1   =   DataPipe (( x :   Double )   =>   math . sin ( 2.0 * x )* math . exp (- 2.0 * x ))  val   pipe2   =   DataPipe (( x :   Double )   =>   if ( x   <=   0.2 )   \"Y\"   else   \"N\" )  val   pipe3   =   DataPipe ( pipe1 ,   pipe2 )  //Returns (-0.013, \"N\")  pipe3 (( 2.0 ,   15.0 ))    By duplicating a single pipe using  DynaMLPipe.duplicate   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 //Already imported in DynaML repl  //but should be imported when using DynaML API  //outside of its provided repl environment.  import   io.github.mandar2812.dynaml.DynaMLPipe._  val   pipe1   =   DataPipe (( x :   Double )   =>   math . sin ( 2.0 * x )* math . exp (- 2.0 * x ))  val   pipe3   =   duplicate ( pipe1 )  //Returns (-0.013, -9E-14)  pipe3 (( 2.0 ,   15.0 ))",
            "title": "Pipes in Parallel"
        },
        {
            "location": "/pipes/pipes_api/#diverging-pipes",
            "text": "The  BifurcationPipe[Source, Result1, Result2]  trait represents pipes which start from the same source and yield two result types, from an implementation point of view these can be seen as data pipes taking input from  Source1  and yielding values from  (Result1, Result2) . They can be created in two ways:  By supplying a function of type  (Source) => (Result1, Result2)  to the  DataPipe()  object.  1\n2\n3 val   pipe1   =   DataPipe (( x :   Double )   =>   ( 1.0 * math . sin ( 2.0 * x )* math . exp (- 2.0 * x ),   math . exp (- 2.0 * x )))  pipe1 ( 2.0 )    By using the  BifurcationPipe()  object  1\n2\n3\n4\n5 val   pipe1   =   DataPipe (( x :   Double )   =>   math . sin ( 2.0 * x )* math . exp (- 2.0 * x ))  val   pipe2   =   DataPipe (( x :   Double )   =>   if ( x   <=   0.2 )   \"Y\"   else   \"N\" )  val   pipe3   =   BifurcationPipe ( pipe1 ,   pipe2 )  pipe3 ( 2.0 )",
            "title": "Diverging Pipes"
        },
        {
            "location": "/pipes/pipes_api/#side-effects",
            "text": "In order to enable pipes which have side effects i.e. writing to disk, the  SideEffectPipe[Source]  trait is used. Conceptually it is a pipe taking as input a value from  Source  but has a return type of  Unit .",
            "title": "Side Effects"
        },
        {
            "location": "/pipes/pipes_api/#stream-processing",
            "text": "To simplify writing pipes for scala streams, the  StreamDataPipe[I, J, K]  and its subclasses implement workflows on streams.",
            "title": "Stream Processing"
        },
        {
            "location": "/pipes/pipes_api/#map",
            "text": "Map every element of a stream.  1\n2\n3\n4 val   pipe1   =   StreamDataPipe (( x :   Double )   =>   math . sin ( 2.0 * x )* math . exp (- 2.0 * x ))  val   str :   Stream [ Double ]   =   ( 1   to   5 ). map ( _ . toDouble ). toStream  pipe1 ( str )",
            "title": "Map"
        },
        {
            "location": "/pipes/pipes_api/#filter",
            "text": "Filter certain elements of a stream.  1\n2\n3\n4 val   pipe1   =   StreamDataPipe (( x :   Double )   =>   x   <=   2.5 )  val   str :   Stream [ Double ]   =   ( 1   to   5 ). map ( _ . toDouble ). toStream  pipe1 ( str )",
            "title": "Filter"
        },
        {
            "location": "/pipes/pipes_api/#bifurcate-stream",
            "text": "1\n2\n3\n4 val   pipe1   =   StreamPartitionPipe (( x :   Double )   =>   x   <=   2.5 )  val   str :   Stream [ Double ]   =   ( 1   to   5 ). map ( _ . toDouble ). toStream  pipe1 ( str )",
            "title": "Bifurcate stream"
        },
        {
            "location": "/pipes/pipes_api/#side-effect",
            "text": "1\n2\n3\n4 val   pipe1   =   StreamDataPipe (( x :   Double )   =>   println ( \"Number is: \" + x ))  val   str :   Stream [ Double ]   =   ( 1   to   5 ). map ( _ . toDouble ). toStream  pipe1 ( str )     The following API members were added in  v1.4.1",
            "title": "Side effect"
        },
        {
            "location": "/pipes/pipes_api/#flat-map",
            "text": "StreamFlatMapPipe  carries out the scala flat-map operation on a stream.  1\n2\n3\n4 val   mapFunc   =   ( n :   Int )   =>   ( 1   to   n ). sliding ( 2 ). toStream  val   streamFMPipe   =   StreamFlatMapPipe ( mapFunc )  streamFMPipe (( 1   to   20 ). toStream )     Pipes on Spark RDDs  It is also possible to create pipes acting on Spark RDDs.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 val   num   =   20  val   sc :   SparkContext   =   _  val   numbers   =   sc . parallelize ( 1   to   num )  val   convPipe   =   RDDPipe (( n :   Int )   =>   n . toDouble )  val   sqPipe   =   RDDPipe (( x :   Double )   =>   x * x )  val   sqrtPipe   =   RDDPipe (( x :   Double )   =>   math . sqrt ( x ))  val   resultPipe   =   RDDPipe (( r :   RDD [ Double ])   =>   r . reduce ( _ + _ ). toInt )  val   netPipeline   =   convPipe   >   sqPipe   >   sqrtPipe   >   resultPipe  netPipeline ( numbers )",
            "title": "Flat Map"
        },
        {
            "location": "/pipes/pipes_api/#advanced-pipes",
            "text": "Apart from the basic capabilities offered by the  DataPipe [ Source ,  Destination ]  interface and its family, users can also work with\nmore complex workflow components some of which are shown below.  The advanced components of the pipes API enable two key extensions.   Data pipes which take more than one argument 1 .  Data pipes which take an argument and return a data pipe 2",
            "title": "Advanced Pipes"
        },
        {
            "location": "/pipes/pipes_api/#data-pipe-2",
            "text": "DataPipe2 [ A ,  B ,  C ]  arguments : 2 of type  A  and  B  respectively  returns : result of type  C  1\n2 val   f2 :   ( A ,   B )   =>   C   =   _    val   pipe2   =   DataPipe2 ( f2 )",
            "title": "Data Pipe 2"
        },
        {
            "location": "/pipes/pipes_api/#datapipe-3",
            "text": "DataPipe3 [ A ,  B ,  C ,  D ]  arguments : 3 of type  A ,  B  and  C  respectively  returns : result of type  D  1\n2 val   f3 :   ( A ,   B ,   C )   =>   D   =   _    val   pipe3   =   DataPipe3 ( f3 )",
            "title": "DataPipe 3"
        },
        {
            "location": "/pipes/pipes_api/#datapipe-4",
            "text": "DataPipe4 [ A ,  B ,  C ,  D ,  E ]  arguments : 4 of type  A ,  B ,  C  and  D  respectively  returns : result of type  E  1\n2 val   f4 :   ( A ,   B ,   C ,   D )   =>   E   =   _    val   pipe4   =   DataPipe4 ( f4 )",
            "title": "DataPipe 4"
        },
        {
            "location": "/pipes/pipes_api/#meta-pipe",
            "text": "MetaPipe [ A ,  B ,  C ]  Takes an argument returns a  DataPipe  1\n2\n3\n4\n5\n6 val   mpipe   =   MetaPipe ( \n   ( omega :   DenseVector [ Double ])   => \n   ( x :   DenseVector [ Double ])   =>   math . exp (- omega . t * x ))  //Returns a pipe which computes exp(-2pi/10 1Tx)  val   expPipe   =   mpipe ( DenseVector . fill [ Double ]( 10 )( 2.0 * math . Pi / 10.0 ))",
            "title": "Meta Pipe"
        },
        {
            "location": "/pipes/pipes_api/#meta-pipe-2-1",
            "text": "MetaPipe21 [ A ,  B ,  C ,  D ]  Takes 2 arguments returns a  DataPipe .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   pipe21   =   MetaPipe21 ( \n   ( alpha :   Double ,   beta :   Double )   => \n   ( rv :   ContinuousRandomVariable [ Double ])   =>   ( rv * beta )   +   alpha  )  val   random_func   =   pipe21 ( 1.5 ,   - 0.5 )  val   result_rv   =   random_func ( RandomVariable ( Gamma ( 1.5 ,   2.5 )))  //Draw samples from resulting random variable  result_rv . iid ( 500 ). draw",
            "title": "Meta Pipe (2, 1)"
        },
        {
            "location": "/pipes/pipes_api/#meta-pipe-1-2",
            "text": "MetaPipe12 [ A ,  B ,  C ,  D ]  Takes an argument returns a  DataPipe2   >  and  >>  operators on higher order pipes  Although the  >  operator is defined on higher order pipes, it quickly becomes difficult\nto imagine what transformation it can be joined with. For example the  >  operator applied\nafter a  MetaPipe [ I ,  J ,  K ]  instance would expect a  DataPipe [ DataPipe [ J , K ] ,  _ ] \ninstance in order for the join to proceed.  The  >>  operator on the other hand has a different and easier purpose.  1\n2\n3\n4\n5\n6\n7\n8\n9 val   mpipe   =   MetaPipe ( \n   ( omega :   DenseVector [ Double ])   => \n   ( x :   DenseVector [ Double ])   =>   math . exp (- omega . t * x ))  val   further_pipe   =   DataPipe (( y :   Double )   =>   y * y   +   2.0 )  //Returns MetaPipe[DenseVector[Double], DenseVector[Double], Double]  //Computes exp(2*omega.x) + 2.0  val   final_pipe   =   mpipe   >>   further_pipe         so far DynaML has support till 4\u00a0 \u21a9    similar to curried functions in scala.\u00a0 \u21a9",
            "title": "Meta Pipe (1, 2)"
        },
        {
            "location": "/pipes/pipes_scalers_encoders/",
            "text": "Summary\n\n\nThe pipes API provides a good foundation to construct data processing pipelines, in this section we show how it is extended for application to a specific application i.e. attribute scaling & transformation.  \n\n\n\n\nTransforming data attributes is an often repeated task, some examples include re-scaling values in a finite domain \n\\([min, max]\\)\n, gaussian centering, \nprincipal component analysis\n (PCA), \ndiscreet Haar wavelet\n (DWT) transform etc.\n\n\nThe pipes API contains traits for these tasks, they are abstract skeletons which can be extended by the user to create arbitrary feature re-scaling transformations.\n\n\nEncoders\n\u00b6\n\n\nEncoder\n[\nI\n, \nJ\n]\n are an extension of \nDataPipe\n[\nI\n, \nJ\n]\n class which has an extra value member \ni\n:\n \nDataPipe\n[\nJ\n, \nI\n]\n which represents the inverse transformation.\n\n\n\n\nNote\n\n\nEncoder\n[\nI\n, \nJ\n]\n implies a reversible, one to one transformation of the input. Mathematically this can be expressed as\n\n\n\\[\n\\begin{align}\ng: \\mathcal{X} &\\rightarrow \\mathcal{Y} \\\\\nh: \\mathcal{Y} &\\rightarrow \\mathcal{X} \\\\\nh(g(x)) &= x \\ \\ \\ \\forall x \\in \\mathcal{X} \\\\\nh &\\equiv g^{-1}\n\\end{align}\n\\]\n\n\nAn encoder can be created by calling the \napply\n method of the \nEncoder\n object.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n//Converts a point expressed in cartesian coordinates\n\n\n//into a point expressed in polar coordinates and vice versa.\n\n\nval\n \ncartesianToPolar\n \n=\n \nEncoder\n(\n\n  \n(\npointCart\n:\n \n(\nDouble\n,\n \nDouble\n))\n \n=>\n \n{\n\n    \nval\n \n(\nx\n,\ny\n)\n \n=\n \npointCart\n\n    \nval\n \nr\n \n=\n \nmath\n.\nsqrt\n(\nx\n*\nx\n \n+\n \ny\n*\ny\n)\n\n    \nif\n(\nr\n \n!=\n \n0.0\n)\n \n(\nr\n,\n \nmath\n.\narcsin\n(\ny\n/\nr\n))\n \nelse\n \n(\n0.0\n,\n \n0.0\n)\n\n  \n}),\n\n  \n(\npointPolar\n:\n \n(\nDouble\n,\n \nDouble\n))\n \n=>\n \n{\n\n    \nval\n \n(\nr\n,\n \ntheta\n)\n \n=\n \npointPolar\n\n    \n(\nr\n*\nmath\n.\ncos\n(\ntheta\n),\n \nr\n*\nmath\n.\nsin\n(\ntheta\n))\n\n  \n}\n\n\n)\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn the above example, we created a cartesian to polar coordinate converter by specifying the forward and reverse transformations as anonymous scala functions. But we could as well have passed the forward and reverse transforms as \nDataPipe\n instances.\n\n\n1\n2\n3\n4\nval\n \nforwardTransform\n:\n \nDataPipe\n[\nI\n, \nJ\n]\n \n=\n \n_\n\n\nval\n \nreverseTransform\n:\n \nDataPipe\n[\nJ\n, \nI\n]\n \n=\n \n_\n\n\n//Still works.\n\n\nval\n \nenc\n \n=\n \nEncoder\n(\nforwardTransform\n,\n \nreverseTransform\n)\n\n\n\n\n\n\n\n\n\nScalers\n\u00b6\n\n\nScaler\n[\nI\n]\n is an extension of the \nDataPipe\n[\nI\n, \nI\n]\n trait. Represents transformations of inputs which don't change their type.\n\n\n1\nval\n \nlinTr\n \n=\n \nScaler\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n*\n5.0\n \n+\n \n-\n1.5\n)\n\n\n\n\n\n\n\nReversible Scalers\n\u00b6\n\n\nReversibleScaler\n[\nI\n]\n extends \nScaler\n[\nI\n]\n along with \nEncoder\n[\nI\n, \nJ\n]\n, a reversible re-scaling of inputs.\n\n\n\n\nThe \n>\n and \n*\n for scalers and encoders\n\n\nSince \nEncoder\n[\nS\n, \nD\n]\n, \nScaler\n[\nS\n]\n and \nReversibleScaler\n[\nS\n, \nD\n]\n are inherit the \nDataPipe\n trait, they can be composed with any data pipeline as usual, but there are special cases.\n\n\nIf an \nEncoder\n[\nI\n, \nJ\n]\n instance is composed with \nEncoder\n[\nJ\n, \nK\n]\n, the result is of type \nEncoder\n[\nI\n, \nK\n]\n and accordingly for \nScaler\n[\nI\n]\n and \nReversibleScaler\n[\nI\n]\n.\n\n\nThe \n*\n can be used to create cartesian products of encoders and scalers.\n\n\n1\n2\n3\nval\n \nenc1\n:\n \nEncoder\n[\nI\n, \nJ\n]\n \n=\n \n_\n\n\nval\n \nenc2\n:\n \nEncoder\n[\nK\n, \nL\n]\n \n=\n \n_\n\n\nval\n \nenc3\n:\n \nEncoder\n[(\nI\n, \nK\n)\n, \n(\nJ\n, \nL\n)]\n \n=\n \nenc1\n*\nenc2\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nCommon attribute transformations like gaussian centering, min-max scaling, etc are included in the \ndynaml\n.\nutils\n package, click \nhere\n to see their syntax.",
            "title": "Scalers & Encoders"
        },
        {
            "location": "/pipes/pipes_scalers_encoders/#encoders",
            "text": "Encoder [ I ,  J ]  are an extension of  DataPipe [ I ,  J ]  class which has an extra value member  i :   DataPipe [ J ,  I ]  which represents the inverse transformation.   Note  Encoder [ I ,  J ]  implies a reversible, one to one transformation of the input. Mathematically this can be expressed as  \\[\n\\begin{align}\ng: \\mathcal{X} &\\rightarrow \\mathcal{Y} \\\\\nh: \\mathcal{Y} &\\rightarrow \\mathcal{X} \\\\\nh(g(x)) &= x \\ \\ \\ \\forall x \\in \\mathcal{X} \\\\\nh &\\equiv g^{-1}\n\\end{align}\n\\]  An encoder can be created by calling the  apply  method of the  Encoder  object.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 //Converts a point expressed in cartesian coordinates  //into a point expressed in polar coordinates and vice versa.  val   cartesianToPolar   =   Encoder ( \n   ( pointCart :   ( Double ,   Double ))   =>   { \n     val   ( x , y )   =   pointCart \n     val   r   =   math . sqrt ( x * x   +   y * y ) \n     if ( r   !=   0.0 )   ( r ,   math . arcsin ( y / r ))   else   ( 0.0 ,   0.0 ) \n   }), \n   ( pointPolar :   ( Double ,   Double ))   =>   { \n     val   ( r ,   theta )   =   pointPolar \n     ( r * math . cos ( theta ),   r * math . sin ( theta )) \n   }  )     Note  In the above example, we created a cartesian to polar coordinate converter by specifying the forward and reverse transformations as anonymous scala functions. But we could as well have passed the forward and reverse transforms as  DataPipe  instances.  1\n2\n3\n4 val   forwardTransform :   DataPipe [ I ,  J ]   =   _  val   reverseTransform :   DataPipe [ J ,  I ]   =   _  //Still works.  val   enc   =   Encoder ( forwardTransform ,   reverseTransform )",
            "title": "Encoders"
        },
        {
            "location": "/pipes/pipes_scalers_encoders/#scalers",
            "text": "Scaler [ I ]  is an extension of the  DataPipe [ I ,  I ]  trait. Represents transformations of inputs which don't change their type.  1 val   linTr   =   Scaler (( x :   Double )   =>   x * 5.0   +   - 1.5 )",
            "title": "Scalers"
        },
        {
            "location": "/pipes/pipes_scalers_encoders/#reversible-scalers",
            "text": "ReversibleScaler [ I ]  extends  Scaler [ I ]  along with  Encoder [ I ,  J ] , a reversible re-scaling of inputs.   The  >  and  *  for scalers and encoders  Since  Encoder [ S ,  D ] ,  Scaler [ S ]  and  ReversibleScaler [ S ,  D ]  are inherit the  DataPipe  trait, they can be composed with any data pipeline as usual, but there are special cases.  If an  Encoder [ I ,  J ]  instance is composed with  Encoder [ J ,  K ] , the result is of type  Encoder [ I ,  K ]  and accordingly for  Scaler [ I ]  and  ReversibleScaler [ I ] .  The  *  can be used to create cartesian products of encoders and scalers.  1\n2\n3 val   enc1 :   Encoder [ I ,  J ]   =   _  val   enc2 :   Encoder [ K ,  L ]   =   _  val   enc3 :   Encoder [( I ,  K ) ,  ( J ,  L )]   =   enc1 * enc2      Tip  Common attribute transformations like gaussian centering, min-max scaling, etc are included in the  dynaml . utils  package, click  here  to see their syntax.",
            "title": "Reversible Scalers"
        },
        {
            "location": "/pipes/model_pipes/",
            "text": "Summary\n\n\nModel pipes define pipelines which involve predictive models.\n\n\n\n\n\n\nNote\n\n\nThe classes described here exist in the \ndynaml.modelpipe\n package of the \ndynaml-core\n module. Although they are not strictly part of the pipes module, they are included here for clarity and continuity.  \n\n\n\n\nThe pipes module gives the user the ability to create workflows of arbitrary complexity. In order to enable end to end machine learning, we need pipelines which involve predictive models. These pipelines can be of two types.\n\n\n\n\n\n\nPipelines which take data as input and output a predictive model.\n\n\nIt is evident that the model creation itself is a common step in the data analysis workflow, therefore one needs library pipes which create machine learning models given the training data and other relevant inputs.\n\n\n\n\n\n\nPipelines which encapsulate predictive models and generate predictions for test data splits.\n\n\nOnce a model has been tuned/trained, it can be a part of a pipeline which generates predictions for previously unobserved data.\n\n\n\n\n\n\nModel Creation\n\u00b6\n\n\nAll pipelines which return predictive models as outputs extend the \nModelPipe\n trait.\n\n\nGeneralized Linear Model Pipe\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n//Pre-process data\n\n\nval\n \npre\n:\n \n(\nSource\n)\n \n=>\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n_\n\n\nval\n \nfeature_map\n:\n \n(\nDenseVector\n[\nDouble\n])\n \n=>\n \n(\nDenseVector\n[\nDouble\n])\n \n=\n \n_\n\n\n\nval\n \nglm_pipe\n \n=\n\n  \nGLMPipe\n[(\nDenseMatrix\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])\n, \nSource\n](\n\n    \npre\n,\n \nmap\n,\n \ntask\n \n=\n \n\"regression\"\n,\n\n    \nmodelType\n \n=\n \n\"\"\n)\n\n\n\nval\n \ndataSource\n:\n \nSource\n \n=\n \n_\n\n\n\nval\n \nglm_model\n \n=\n \nglm_pipe\n(\ndataSource\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nSource\n, \nGeneralizedLinearModel\n[\nT\n]]\n\n\nResult\n: Takes as input a data of type \nSource\n and outputs a \nGeneralized Linear Model\n.\n\n\n\n\nGeneralized Least Squares Model Pipe\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n\n\nval\n \ngls_pipe2\n \n=\n \nGeneralizedLeastSquaresPipe2\n(\nkernel\n)\n\n\n\nval\n \nfeaturemap\n:\n \n(\nDenseVector\n[\nDouble\n])\n \n=>\n \n(\nDenseVector\n[\nDouble\n])\n \n=\n \n_\n\n\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n_\n\n\n\nval\n \ngls_model\n \n=\n \ngls_pipe2\n(\ndata\n,\n \nfeaturemap\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe2\n[\nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n, \nDataPipe\n[\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n]]\n, \nGeneralizedLeastSquaresModel\n]\n]\n\n\nResult\n: Takes as inputs data and a feature mapping and outputs a \nGeneralized Least Squares Model\n.\n\n\n\n\nGaussian Process Regression Model Pipe\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n//Pre-process data\n\n\nval\n \npre\n:\n \n(\nSource\n)\n \n=>\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n_\n\n\n//Declare kernel and noise\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\nval\n \nnoise\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n\nGPRegressionPipe\n(\n\n  \npre\n,\n \nkernel\n,\n \nnoise\n,\n\n  \norder\n:\n \nInt\n \n=\n \n0\n,\n \nex\n:\n \nInt\n \n=\n \n0\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nSource\n, \nM\n]\n\n\nResult\n: Takes as input data of type \nSource\n and outputs a \nGaussian Process\n regression\n model as the output.\n\n\n\n\nDual LS-SVM Model Pipe\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n//Pre-process data\n\n\nval\n \npre\n:\n \n(\nSource\n)\n \n=>\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n_\n\n\n//Declare kernel\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n\nDLSSVMPipe\n(\npre\n,\n \nkernel\n,\n \ntask\n \n=\n \n\"regression\"\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nSource\n, \nDLSSVM\n]\n\n\nResult\n: Takes as input data of type \nSource\n and outputs a \nLS-SVM\n regression/classification model as the output.\n\n\n\n\nModel Prediction\n\u00b6\n\n\nPrediction pipelines encapsulate predictive models, the \nModelPredictionPipe\n class provides an expressive API for creating prediction pipelines.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n//Any model\n\n\nval\n \nmodel\n:\n \nModel\n[\nT\n, \nQ\n, \nR\n]\n \n=\n \n_\n\n\n\n//Data pre and post processing\n\n\nval\n \npreprocessing\n:\n \nDataPipe\n[\nP\n, \nQ\n]\n \n=\n \n_\n\n\nval\n \npostprocessing\n:\n \nDataPipe\n[\nR\n, \nS\n]\n \n=\n \n_\n\n\n\nval\n \nprediction_pipeline\n \n=\n \nModelPredictionPipe\n(\n\n  \npreprocessing\n,\n\n  \nmodel\n,\n\n  \npostprocessing\n)\n\n\n\n//In case no pre or post processing is done.\n\n\nval\n \nprediction_pipeline2\n \n=\n \nModelPredictionPipe\n(\nmodel\n)\n\n\n\n//Incase feature and target scaling is performed\n\n\n\nval\n \nfeatureScaling\n:\n \nReversibleScaler\n[\nQ\n]\n \n=\n \n_\n\n\nval\n \ntargetScaling\n:\n \nReversibleScaler\n[\nR\n]\n \n=\n \n_\n\n\n\nval\n \nprediction_pipeline3\n \n=\n \nModelPredictionPipe\n(\n\n  \nfeatureScaling\n,\n\n  \nmodel\n,\n\n  \ntargetScaling\n)",
            "title": "Model Pipes"
        },
        {
            "location": "/pipes/model_pipes/#model-creation",
            "text": "All pipelines which return predictive models as outputs extend the  ModelPipe  trait.",
            "title": "Model Creation"
        },
        {
            "location": "/pipes/model_pipes/#generalized-linear-model-pipe",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 //Pre-process data  val   pre :   ( Source )   =>   Stream [( DenseVector [ Double ] ,  Double )]   =   _  val   feature_map :   ( DenseVector [ Double ])   =>   ( DenseVector [ Double ])   =   _  val   glm_pipe   = \n   GLMPipe [( DenseMatrix [ Double ] ,  DenseVector [ Double ]) ,  Source ]( \n     pre ,   map ,   task   =   \"regression\" , \n     modelType   =   \"\" )  val   dataSource :   Source   =   _  val   glm_model   =   glm_pipe ( dataSource )     Type :  DataPipe [ Source ,  GeneralizedLinearModel [ T ]]  Result : Takes as input a data of type  Source  and outputs a  Generalized Linear Model .",
            "title": "Generalized Linear Model Pipe"
        },
        {
            "location": "/pipes/model_pipes/#generalized-least-squares-model-pipe",
            "text": "1\n2\n3\n4\n5\n6\n7 val   kernel :   LocalScalarKernel [ DenseVector [ Double ]]  val   gls_pipe2   =   GeneralizedLeastSquaresPipe2 ( kernel )  val   featuremap :   ( DenseVector [ Double ])   =>   ( DenseVector [ Double ])   =   _  val   data :   Stream [( DenseVector [ Double ] ,  Double )]   =   _  val   gls_model   =   gls_pipe2 ( data ,   featuremap )     Type :  DataPipe2 [ Stream [( DenseVector [ Double ] ,  Double )] ,  DataPipe [ DenseVector [ Double ] ,  DenseVector [ Double ]] ,  GeneralizedLeastSquaresModel ] ]  Result : Takes as inputs data and a feature mapping and outputs a  Generalized Least Squares Model .",
            "title": "Generalized Least Squares Model Pipe"
        },
        {
            "location": "/pipes/model_pipes/#gaussian-process-regression-model-pipe",
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 //Pre-process data  val   pre :   ( Source )   =>   Stream [( DenseVector [ Double ] ,  Double )]   =   _  //Declare kernel and noise  val   kernel :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  val   noise :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  GPRegressionPipe ( \n   pre ,   kernel ,   noise , \n   order :   Int   =   0 ,   ex :   Int   =   0 )     Type :  DataPipe [ Source ,  M ]  Result : Takes as input data of type  Source  and outputs a  Gaussian Process  regression  model as the output.",
            "title": "Gaussian Process Regression Model Pipe"
        },
        {
            "location": "/pipes/model_pipes/#dual-ls-svm-model-pipe",
            "text": "1\n2\n3\n4\n5\n6 //Pre-process data  val   pre :   ( Source )   =>   Stream [( DenseVector [ Double ] ,  Double )]   =   _  //Declare kernel  val   kernel :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  DLSSVMPipe ( pre ,   kernel ,   task   =   \"regression\" )     Type :  DataPipe [ Source ,  DLSSVM ]  Result : Takes as input data of type  Source  and outputs a  LS-SVM  regression/classification model as the output.",
            "title": "Dual LS-SVM Model Pipe"
        },
        {
            "location": "/pipes/model_pipes/#model-prediction",
            "text": "Prediction pipelines encapsulate predictive models, the  ModelPredictionPipe  class provides an expressive API for creating prediction pipelines.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 //Any model  val   model :   Model [ T ,  Q ,  R ]   =   _  //Data pre and post processing  val   preprocessing :   DataPipe [ P ,  Q ]   =   _  val   postprocessing :   DataPipe [ R ,  S ]   =   _  val   prediction_pipeline   =   ModelPredictionPipe ( \n   preprocessing , \n   model , \n   postprocessing )  //In case no pre or post processing is done.  val   prediction_pipeline2   =   ModelPredictionPipe ( model )  //Incase feature and target scaling is performed  val   featureScaling :   ReversibleScaler [ Q ]   =   _  val   targetScaling :   ReversibleScaler [ R ]   =   _  val   prediction_pipeline3   =   ModelPredictionPipe ( \n   featureScaling , \n   model , \n   targetScaling )",
            "title": "Model Prediction"
        },
        {
            "location": "/pipes/file_processing/",
            "text": "Summary\n\n\nA List of data pipes useful for processing contents of data files.\n\n\n\n\nData Pre-processing\n\u00b6\n\n\nFile to Stream of Lines\n\u00b6\n\n\n1\nfileToStream\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nString\n, \nStream\n[\nString\n]]\n\n\nResult\n: Converts a text file (inputted as a file path string) into \nStream\n[\nString\n]\n   \n\n\n\n\nWrite Stream of Lines to File\n\u00b6\n\n\n1\nstreamToFile\n(\nfileName\n:\n \nString\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nUnit\n]\n\n\nResult\n: Writes a stream of lines to the file specified by \nfilePath\n\n\n\n\nDrop first line in Stream\n\u00b6\n\n\n1\ndropHead\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Drop the first element of a \nStream\n of \nString\n\n\n\n\nReplace Occurrences in of a String\n\u00b6\n\n\n1\nreplace\n(\noriginal\n,\n \nnewString\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Replace all occurrences of a regular expression or string in a \nStream\n of \nString\n with with a specified replacement string.\n\n\n\n\nReplace White Spaces\n\u00b6\n\n\n1\nreplaceWhiteSpaces\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Replace all white space characters in a stream of lines.\n\n\n\n\nRemove Trailing White Spaces\n\u00b6\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Trim white spaces from both sides of every line.\n\n\n\n\nRemove White Spaces\n\u00b6\n\n\n1\nreplaceWhiteSpaces\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Replace all white space characters in a stream of lines.\n\n\n\n\nRemove Missing Records\n\u00b6\n\n\n1\nremoveMissingLines\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Remove all lines/records which contain missing values\n\n\n\n\nCreate Train/Test splits\n\u00b6\n\n\n1\nsplitTrainingTest\n(\nnum_training\n,\n \nnum_test\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[(\nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n, \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)])\n, \n(\nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n, \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)])]\n\n\nResult\n: Extract a subset of the data into a \nTuple2\n which can be used as a training, test combo for model learning and evaluation.",
            "title": "String & File Processing"
        },
        {
            "location": "/pipes/file_processing/#data-pre-processing",
            "text": "",
            "title": "Data Pre-processing"
        },
        {
            "location": "/pipes/file_processing/#file-to-stream-of-lines",
            "text": "1 fileToStream     Type :  DataPipe [ String ,  Stream [ String ]]  Result : Converts a text file (inputted as a file path string) into  Stream [ String ]",
            "title": "File to Stream of Lines"
        },
        {
            "location": "/pipes/file_processing/#write-stream-of-lines-to-file",
            "text": "1 streamToFile ( fileName :   String )     Type :  DataPipe [ Stream [ String ] ,  Unit ]  Result : Writes a stream of lines to the file specified by  filePath",
            "title": "Write Stream of Lines to File"
        },
        {
            "location": "/pipes/file_processing/#drop-first-line-in-stream",
            "text": "1 dropHead     Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Drop the first element of a  Stream  of  String",
            "title": "Drop first line in Stream"
        },
        {
            "location": "/pipes/file_processing/#replace-occurrences-in-of-a-string",
            "text": "1 replace ( original ,   newString )     Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Replace all occurrences of a regular expression or string in a  Stream  of  String  with with a specified replacement string.",
            "title": "Replace Occurrences in of a String"
        },
        {
            "location": "/pipes/file_processing/#replace-white-spaces",
            "text": "1 replaceWhiteSpaces     Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Replace all white space characters in a stream of lines.",
            "title": "Replace White Spaces"
        },
        {
            "location": "/pipes/file_processing/#remove-trailing-white-spaces",
            "text": "Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Trim white spaces from both sides of every line.",
            "title": "Remove Trailing White Spaces"
        },
        {
            "location": "/pipes/file_processing/#remove-white-spaces",
            "text": "1 replaceWhiteSpaces     Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Replace all white space characters in a stream of lines.",
            "title": "Remove White Spaces"
        },
        {
            "location": "/pipes/file_processing/#remove-missing-records",
            "text": "1 removeMissingLines     Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Remove all lines/records which contain missing values",
            "title": "Remove Missing Records"
        },
        {
            "location": "/pipes/file_processing/#create-traintest-splits",
            "text": "1 splitTrainingTest ( num_training ,   num_test )     Type :  DataPipe [( Stream [( DenseVector [ Double ] ,  Double )] ,  Stream [( DenseVector [ Double ] ,  Double )]) ,  ( Stream [( DenseVector [ Double ] ,  Double )] ,  Stream [( DenseVector [ Double ] ,  Double )])]  Result : Extract a subset of the data into a  Tuple2  which can be used as a training, test combo for model learning and evaluation.",
            "title": "Create Train/Test splits"
        },
        {
            "location": "/pipes/feature_processing/",
            "text": "Feature Processing\n\u00b6\n\n\nExtract features and targets\n\u00b6\n\n\n1\nsplitFeaturesAndTargets\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]]\n\n\nResult\n: Take each line which is a comma separated string and extract all but the last element into a feature vector and leave the last element as the \"target\" value.\n\n\n\n\nExtract Specific Columns\n\u00b6\n\n\n1\nextractTrainingFeatures\n(\ncolumns\n,\n \nmissingVals\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Extract a subset of columns from a stream of comma separated string also replace any missing value strings with the empty string.\n\n\nUsage\n: \nDynaMLPipe\n.\nextractTrainingFeatures\n(\nList\n(\n1\n,\n2\n,\n3\n),\n \nMap\n(\n1\n \n->\n \n\"N.A.\"\n,\n \n2\n \n->\n \n\"NA\"\n,\n \n3\n \n->\n \n\"na\"\n))\n\n\n\n\nGaussian Scaling of Data\n\u00b6\n\n\n1\ngaussianScaling\n\n\n\n\n\n\n\n\n\nResult\n:  Perform gaussian normalization of features & targets, on a data stream which is a of the form \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n.\n\n\n\n\nGaussian Scaling of Train/Test Splits\n\u00b6\n\n\n1\ngaussianScalingTrainTest\n\n\n\n\n\n\n\n\n\nResult\n:  Perform gaussian normalization of features & targets, on a data stream which is a \nTuple2\n of the form \n(\nStream\n(\ntraining\n \ndata\n),\n \nStream\n(\ntest\n \ndata\n))\n.\n\n\n\n\nMin-Max Scaling of Data\n\u00b6\n\n\n1\nminMaxScaling\n\n\n\n\n\n\n\n\n\nResult\n:  Perform 0-1 scaling of features & targets, on a data stream which is a of the form \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n.\n\n\n\n\nMin-Max Scaling of Train/Test Splits\n\u00b6\n\n\n1\nminMaxScalingTrainTest\n\n\n\n\n\n\n\n\n\nResult\n:  Perform 0-1 scaling of features & targets, on a data stream which is a \nTuple2\n of the form \n(\nStream\n(\ntraining_data\n),\n \nStream\n(\ntest_data\n))\n.",
            "title": "Feature Processing"
        },
        {
            "location": "/pipes/feature_processing/#feature-processing",
            "text": "",
            "title": "Feature Processing"
        },
        {
            "location": "/pipes/feature_processing/#extract-features-and-targets",
            "text": "1 splitFeaturesAndTargets     Type :  DataPipe [ Stream [ String ] ,  Stream [( DenseVector [ Double ] ,  Double )]]  Result : Take each line which is a comma separated string and extract all but the last element into a feature vector and leave the last element as the \"target\" value.",
            "title": "Extract features and targets"
        },
        {
            "location": "/pipes/feature_processing/#extract-specific-columns",
            "text": "1 extractTrainingFeatures ( columns ,   missingVals )     Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Extract a subset of columns from a stream of comma separated string also replace any missing value strings with the empty string.  Usage :  DynaMLPipe . extractTrainingFeatures ( List ( 1 , 2 , 3 ),   Map ( 1   ->   \"N.A.\" ,   2   ->   \"NA\" ,   3   ->   \"na\" ))",
            "title": "Extract Specific Columns"
        },
        {
            "location": "/pipes/feature_processing/#gaussian-scaling-of-data",
            "text": "1 gaussianScaling     Result :  Perform gaussian normalization of features & targets, on a data stream which is a of the form  Stream [( DenseVector [ Double ] ,  Double )] .",
            "title": "Gaussian Scaling of Data"
        },
        {
            "location": "/pipes/feature_processing/#gaussian-scaling-of-traintest-splits",
            "text": "1 gaussianScalingTrainTest     Result :  Perform gaussian normalization of features & targets, on a data stream which is a  Tuple2  of the form  ( Stream ( training   data ),   Stream ( test   data )) .",
            "title": "Gaussian Scaling of Train/Test Splits"
        },
        {
            "location": "/pipes/feature_processing/#min-max-scaling-of-data",
            "text": "1 minMaxScaling     Result :  Perform 0-1 scaling of features & targets, on a data stream which is a of the form  Stream [( DenseVector [ Double ] ,  Double )] .",
            "title": "Min-Max Scaling of Data"
        },
        {
            "location": "/pipes/feature_processing/#min-max-scaling-of-traintest-splits",
            "text": "1 minMaxScalingTrainTest     Result :  Perform 0-1 scaling of features & targets, on a data stream which is a  Tuple2  of the form  ( Stream ( training_data ),   Stream ( test_data )) .",
            "title": "Min-Max Scaling of Train/Test Splits"
        },
        {
            "location": "/pipes/pipes_time_series/",
            "text": "Extract Data as Univariate Time Series\n\u00b6\n\n\n1\nextractTimeSeries\n(\nTfunc\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[(\nDouble\n, \nDouble\n)]]\n\n\nResult\n: This pipe assumes its input to be of the form \nYYYY,Day,Hour,Value\n. It takes as input a function (TFunc) which converts a \n(\nDouble\n,\n \nDouble\n,\n \nDouble\n)\n into a single \ntimestamp\n like value. The pipe processes its data source line by line and outputs a \nTuple2\n in the following format \n(Timestamp,Value)\n.\n\n\n\n\nExtract data as Multivariate Time Series\n\u00b6\n\n\n1\nextractTimeSeriesVec\n(\nTfunc\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[(\nDouble\n, \nDenseVector\n[\nDouble\n])]]\n\n\nResult\n: This pipe is similar to \nextractTimeSeries\n but for application in multivariate time series analysis such as nonlinear autoregressive models with exogenous inputs. The pipe processes its data source line by line and outputs a \n(\nDouble\n,\n \nDenseVector\n[\nDouble\n])\n in the following format \n(Timestamp,Values)\n.\n\n\n\n\nConstruct Time differenced Data\n\u00b6\n\n\n1\ndeltaOperation\n(\ndeltaT\n,\n \ntimelag\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[(\nDouble\n, \nDouble\n)]\n, \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]]\n\n\nResult\n: In order to generate features for auto-regressive models, one needs to construct sliding windows in time. This function takes two parameters \ndeltaT\n: the auto-regressive order and \ntimelag\n: the time lag after which the windowing is conducted. E.g Let \ndeltaT\n \n=\n \n2\n and \ntimelag\n \n=\n \n1\n This pipe will take stream data of the form \n\\((t, y(t))\\)\n and output a stream which looks like \n\\((t, [y(t-2), y(t-3)])\\)\n\n\n\n\nConstruct multivariate Time differenced Data\n\u00b6\n\n\n1\ndeltaOperationVec\n(\ndeltaT\n:\n \nInt\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[(\nDouble\n, \nDouble\n)]\n, \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]]\n\n\nResult\n: A variant of \ndeltaOperation\n for NARX models.\n\n\n\n\nHaar Discrete Wavelet Transform\n\u00b6\n\n\n1\nhaarWaveletFilter\n(\norder\n:\n \nInt\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n]]\n\n\nResult\n: A Haar Discrete wavelet transform.",
            "title": "Time Series"
        },
        {
            "location": "/pipes/pipes_time_series/#extract-data-as-univariate-time-series",
            "text": "1 extractTimeSeries ( Tfunc )     Type :  DataPipe [ Stream [ String ] ,  Stream [( Double ,  Double )]]  Result : This pipe assumes its input to be of the form  YYYY,Day,Hour,Value . It takes as input a function (TFunc) which converts a  ( Double ,   Double ,   Double )  into a single  timestamp  like value. The pipe processes its data source line by line and outputs a  Tuple2  in the following format  (Timestamp,Value) .",
            "title": "Extract Data as Univariate Time Series"
        },
        {
            "location": "/pipes/pipes_time_series/#extract-data-as-multivariate-time-series",
            "text": "1 extractTimeSeriesVec ( Tfunc )     Type :  DataPipe [ Stream [ String ] ,  Stream [( Double ,  DenseVector [ Double ])]]  Result : This pipe is similar to  extractTimeSeries  but for application in multivariate time series analysis such as nonlinear autoregressive models with exogenous inputs. The pipe processes its data source line by line and outputs a  ( Double ,   DenseVector [ Double ])  in the following format  (Timestamp,Values) .",
            "title": "Extract data as Multivariate Time Series"
        },
        {
            "location": "/pipes/pipes_time_series/#construct-time-differenced-data",
            "text": "1 deltaOperation ( deltaT ,   timelag )     Type :  DataPipe [ Stream [( Double ,  Double )] ,  Stream [( DenseVector [ Double ] ,  Double )]]  Result : In order to generate features for auto-regressive models, one needs to construct sliding windows in time. This function takes two parameters  deltaT : the auto-regressive order and  timelag : the time lag after which the windowing is conducted. E.g Let  deltaT   =   2  and  timelag   =   1  This pipe will take stream data of the form  \\((t, y(t))\\)  and output a stream which looks like  \\((t, [y(t-2), y(t-3)])\\)",
            "title": "Construct Time differenced Data"
        },
        {
            "location": "/pipes/pipes_time_series/#construct-multivariate-time-differenced-data",
            "text": "1 deltaOperationVec ( deltaT :   Int )     Type :  DataPipe [ Stream [( Double ,  Double )] ,  Stream [( DenseVector [ Double ] ,  Double )]]  Result : A variant of  deltaOperation  for NARX models.",
            "title": "Construct multivariate Time differenced Data"
        },
        {
            "location": "/pipes/pipes_time_series/#haar-discrete-wavelet-transform",
            "text": "1 haarWaveletFilter ( order :   Int )     Type :  DataPipe [ DenseVector [ Double ] ,  DenseVector [ Double ]]  Result : A Haar Discrete wavelet transform.",
            "title": "Haar Discrete Wavelet Transform"
        },
        {
            "location": "/pipes/pipes_models/",
            "text": "Operations on Models\n\u00b6\n\n\nTrain a parametric model\n\u00b6\n\n\n1\n2\n3\n4\ntrainParametricModel\n[\n\n  \nG\n, \nT\n, \nQ\n, \nR\n,\n  \nS\n, \nM\n \n<:\n \nParameterizedLearner\n[\nG\n, \nT\n, \nQ\n, \nR\n, \nS\n]](\n\n  \nregParameter\n:\n \nDouble\n,\n \nstep\n:\n \nDouble\n,\n \nmaxIt\n:\n \nInt\n,\n \nmini\n:\n \nDouble\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe[M, M]\n\n\nResult\n: Takes as input a parametric model i.e. a subclass of \nParameterizedLearner[G, T, Q, R, S]\n, trains it and outputs the trained model.\n\n\n\n\nTune a model using global optimization\n\u00b6\n\n\n1\n2\n3\nmodelTuning\n[\nM\n \n<:\n \nGloballyOptWithGrad\n](\n\n  \nstartingState\n:\n \nMap\n[\nString\n, \nDouble\n],\n \nglobalOpt\n:\n \nString\n,\n\n  \ngrid\n:\n \nInt\n,\n \nstep\n:\n \nDouble\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe[(S, S), (D, D)]\n\n\nResult\n: Takes as input a parametric model i.e. a subclass of \nGloballyOptimizableWithGrad\n, tunes it using a global optimization procedure \nglobalOpt\n and outputs the tuned model.",
            "title": "Workflows on Models"
        },
        {
            "location": "/pipes/pipes_models/#operations-on-models",
            "text": "",
            "title": "Operations on Models"
        },
        {
            "location": "/pipes/pipes_models/#train-a-parametric-model",
            "text": "1\n2\n3\n4 trainParametricModel [ \n   G ,  T ,  Q ,  R ,\n   S ,  M   <:   ParameterizedLearner [ G ,  T ,  Q ,  R ,  S ]]( \n   regParameter :   Double ,   step :   Double ,   maxIt :   Int ,   mini :   Double )     Type :  DataPipe[M, M]  Result : Takes as input a parametric model i.e. a subclass of  ParameterizedLearner[G, T, Q, R, S] , trains it and outputs the trained model.",
            "title": "Train a parametric model"
        },
        {
            "location": "/pipes/pipes_models/#tune-a-model-using-global-optimization",
            "text": "1\n2\n3 modelTuning [ M   <:   GloballyOptWithGrad ]( \n   startingState :   Map [ String ,  Double ],   globalOpt :   String , \n   grid :   Int ,   step :   Double )     Type :  DataPipe[(S, S), (D, D)]  Result : Takes as input a parametric model i.e. a subclass of  GloballyOptimizableWithGrad , tunes it using a global optimization procedure  globalOpt  and outputs the tuned model.",
            "title": "Tune a model using global optimization"
        },
        {
            "location": "/pipes/pipes_misc/",
            "text": "General\n\u00b6\n\n\nDuplicate a pipe\n\u00b6\n\n\n1\nduplicate\n[\nS\n, \nD\n](\npipe\n:\n \nDataPipe\n[\nS\n, \nD\n])\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe[(S, S), (D, D)]\n\n\nResult\n: Takes a base pipe and creates a parallel pipe by duplicating it.",
            "title": "Miscellaneuos"
        },
        {
            "location": "/pipes/pipes_misc/#general",
            "text": "",
            "title": "General"
        },
        {
            "location": "/pipes/pipes_misc/#duplicate-a-pipe",
            "text": "1 duplicate [ S ,  D ]( pipe :   DataPipe [ S ,  D ])     Type :  DataPipe[(S, S), (D, D)]  Result : Takes a base pipe and creates a parallel pipe by duplicating it.",
            "title": "Duplicate a pipe"
        },
        {
            "location": "/pipes/pipes_library/",
            "text": "DynaML Library Pipes\n\u00b6\n\n\nDynaML comes bundled with a set of data pipes which enable certain standard data processing tasks, they are defined in the \nDynaMLPipe\n object in the \nio.github.mandar2812.dynaml.pipes\n package and they can be invoked as \nDynaMLPipe.<pipe name>\n.\n\n\n\n\nExample\n\u00b6\n\n\nAs a simple motivating example consider the following hypothetical csv data file called \nsample.csv\n.\n\n\n1\n2\n3\na  b  c  NA  e f\nr  s  q  t  l   m\nu v w x z d\n\n\n\n\n\n\nLets say one wants to extract only the first, fourth and last columns of this file for further processing, also one is only interested in records which do not have missing values in any of the columns we want to extract. One can think of a data pipe as follows.\n\n\n\n\nReplace the erratic white space separators with a consistent separator character\n\n\nExtract a subset of the columns\n\n\nRemove the records with missing values \nNA\n\n\nWrite output to another file \nprocessedsample.csv\n with the comma character as separator\n\n\n\n\nWe can do this by 'composing' data flow pipes which achieve each of the sub tasks.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n//Import the workflow library.\n\n\nimport\n \nio.github.mandar2812.dynaml.DynaMLPipe._\n\n\n\nval\n \ncolumns\n \n=\n \nList\n(\n0\n,\n3\n,\n5\n)\n\n\nval\n \ndataPipe\n \n=\n\n  \nfileToStream\n \n>\n\n  \nreplaceWhiteSpaces\n \n>\n\n  \nextractTrainingFeatures\n(\n\n    \ncolumns\n,\n \nMap\n(\n0\n \n->\n \n\"NA\"\n,\n \n3\n \n->\n \n\"NA\"\n,\n \n5\n \n->\n \n\"NA\"\n)\n\n  \n)\n \n>\n\n  \nremoveMissingLines\n \n>\n\n  \nstreamToFile\n(\n\"processed_sample.csv\"\n)\n\n\n\nval\n \nresult\n \n=\n \ndataPipe\n(\n\"sample.csv\"\n)\n\n\n\n\n\n\n\nLets go over the code snippet piece by piece.\n\n\n\n\nFirst convert the text file to a Stream using \nfileToStream\n\n\nReplace white spaces in each line by using \nreplaceWhiteSpaces\n\n\nExtract the required columns by \nextractTrainingFeatures\n, be sure to supply it the column numbers (indexed from 0) and the missing value strings for each column to be extracted.\n\n\nRemove missing records \nremoveMissingLines\n\n\nWrite the resulting data stream to a file \nstreamToFile(\"processed_sample.csv\")",
            "title": "Pipes Example"
        },
        {
            "location": "/pipes/pipes_library/#dynaml-library-pipes",
            "text": "DynaML comes bundled with a set of data pipes which enable certain standard data processing tasks, they are defined in the  DynaMLPipe  object in the  io.github.mandar2812.dynaml.pipes  package and they can be invoked as  DynaMLPipe.<pipe name> .",
            "title": "DynaML Library Pipes"
        },
        {
            "location": "/pipes/pipes_library/#example",
            "text": "As a simple motivating example consider the following hypothetical csv data file called  sample.csv .  1\n2\n3 a  b  c  NA  e f\nr  s  q  t  l   m\nu v w x z d   Lets say one wants to extract only the first, fourth and last columns of this file for further processing, also one is only interested in records which do not have missing values in any of the columns we want to extract. One can think of a data pipe as follows.   Replace the erratic white space separators with a consistent separator character  Extract a subset of the columns  Remove the records with missing values  NA  Write output to another file  processedsample.csv  with the comma character as separator   We can do this by 'composing' data flow pipes which achieve each of the sub tasks.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 //Import the workflow library.  import   io.github.mandar2812.dynaml.DynaMLPipe._  val   columns   =   List ( 0 , 3 , 5 )  val   dataPipe   = \n   fileToStream   > \n   replaceWhiteSpaces   > \n   extractTrainingFeatures ( \n     columns ,   Map ( 0   ->   \"NA\" ,   3   ->   \"NA\" ,   5   ->   \"NA\" ) \n   )   > \n   removeMissingLines   > \n   streamToFile ( \"processed_sample.csv\" )  val   result   =   dataPipe ( \"sample.csv\" )    Lets go over the code snippet piece by piece.   First convert the text file to a Stream using  fileToStream  Replace white spaces in each line by using  replaceWhiteSpaces  Extract the required columns by  extractTrainingFeatures , be sure to supply it the column numbers (indexed from 0) and the missing value strings for each column to be extracted.  Remove missing records  removeMissingLines  Write the resulting data stream to a file  streamToFile(\"processed_sample.csv\")",
            "title": "Example"
        },
        {
            "location": "/repl-examples/p2_examples/",
            "text": "A good way to learn using DynaML's many features is to practice on some data science case studies. The \ndynaml-examples\n module contains a number of model training and testing experiments which are intended to serve as instructive material for getting comfortable with the DynaML API.\n\n\nThe example programs cover a number of categories.\n\n\n\n\nRegression\n\n\nClassification\n\n\nSystem Identification",
            "title": "Examples Module"
        },
        {
            "location": "/repl-examples/p2_boston_housing/",
            "text": "The \nHousing\n data set is a popular regression benchmarking data set hosted on the \nUCI Machine Learning Repository\n. It contains 506 records consisting of multivariate data attributes for various real estate zones and their housing price indices. The task is then to learn a regression model that can predict the price index or range.\n\n\nAttribute Information:\n\u00b6\n\n\n\n\nCRIM\n: per capita crime rate by town\n\n\nZN\n: proportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nINDUS\n: proportion of non-retail business acres per town\n\n\nCHAS\n: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n\nNOX\n: nitric oxides concentration (parts per 10 million)\n\n\nRM\n: average number of rooms per dwelling\n\n\nAGE\n: proportion of owner-occupied units built prior to 1940\n\n\nDIS\n: weighted distances to five Boston employment centres\n\n\nRAD\n: index of accessibility to radial highways\n\n\nTAX\n: full-value property-tax rate per $10,000\n\n\nPTRATIO\n: pupil-teacher ratio by town\n\n\nB\n: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n\n\nLSTAT\n: % lower status of the population\n\n\nMEDV\n: Median value of owner-occupied homes in $1000's\n\n\n\n\nModel\n\u00b6\n\n\nBelow is a GP model for predicting the \nMEDV\n\n\n\\[\n    \\begin{align}\n        & MEDV(\\mathbf{u}) = f(\\mathbf{u}) + \\epsilon(\\mathbf{u}) \\\\\n        & f \\sim \\mathcal{GP}(m(\\mathbf{u}), K(\\mathbf{u},\\mathbf{v})) \\\\\n        & \\mathbb{E}[\\epsilon(\\mathbf{u}).\\epsilon(\\mathbf{v})] = K_{noise}(\\mathbf{u}, \\mathbf{v})\\\\\n    \\end{align}\n\\]\nSyntax\n\u00b6\n\n\nThe \nTestGPHousing()\n program can be run in the REPL, below is a description of each of its arguments.\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault value\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\n\nkernel\n\n\nCovarianceFunction\n\n\n-\n\n\nThe kernel function driving the GP model.\n\n\n\n\n\n\n\n\nnoise\n\n\nCovarianceFunction\n\n\n-\n\n\nThe additive noise that corrupts the values of the latent function.\n\n\n\n\n\n\n\n\ntrainFraction\n\n\nDouble\n\n\n0.75\n\n\nFraction of the data to be used for model training and hyper-parameter selection.\n\n\n\n\n\n\n\n\ncolumns\n\n\nList\n[\nInt\n]\n\n\n13, 0,.., 12\n\n\nThe columns to be selected for analysis (indexed from 0), first one is the target column.\n\n\n\n\n\n\n\n\ngrid\n\n\nInt\n\n\n5\n\n\nThe number of grid points for each hyper-parameter\n\n\n\n\n\n\n\n\nstep\n\n\nDouble\n\n\n0.2\n\n\nThe space between grid points.\n\n\n\n\n\n\n\n\nglobalOpt\n\n\nString\n\n\nML\n\n\nThe model selection procedure \n\"GS\"\n,\n \n\"CSA\"\n,\n or \n\"ML\"\n\n\n\n\n\n\n\n\nstepSize\n\n\nDouble\n\n\n0.01\n\n\nOnly relevant if \nglobalOpt\n \n=\n \n\"ML\"\n, determines step size of steepest ascent.\n\n\n\n\n\n\n\n\nmaxIt\n\n\nInt\n\n\n300\n\n\nMaximum iterations for ML model selection procedure.\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\nTestGPHousing\n(\n\n  \nkernel\n \n=\n \nnew\n \nFBMKernel\n(\n0.55\n)\n \n+\n \nnew\n \nLaplacianKernel\n(\n2.5\n),\n\n  \nnoise\n \n=\n \nnew\n \nRBFKernel\n(\n1.5\n),\n\n  \ngrid\n \n=\n \n5\n,\n \nstep\n \n=\n \n0.03\n,\n\n  \nglobalOpt\n \n=\n \n\"GS\"\n,\n \ntrainFraction\n \n=\n \n0.45\n)\n\n\n\n\n\n\n\n1\n2\n3\n16/03/03 20:45:41 INFO GridSearch: Optimum value of energy is: 278.1603309851301\nConfiguration: Map(hurst -> 0.4, beta -> 2.35, bandwidth -> 1.35)\n16/03/03 20:45:41 INFO SVMKernel$: Constructing kernel matrix.\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n16/03/03 20:45:42 INFO GPRegression: Generating error bars\n16/03/03 20:45:42 INFO RegressionMetrics: Regression Model Performance: MEDV\n16/03/03 20:45:42 INFO RegressionMetrics: ============================\n16/03/03 20:45:42 INFO RegressionMetrics: MAE: 5.800070254265218\n16/03/03 20:45:42 INFO RegressionMetrics: RMSE: 7.739266267762397\n16/03/03 20:45:42 INFO RegressionMetrics: RMSLE: 0.4150438478412412\n16/03/03 20:45:42 INFO RegressionMetrics: R^2: 0.3609909626630624\n16/03/03 20:45:42 INFO RegressionMetrics: Corr. Coefficient: 0.7633838930006132\n16/03/03 20:45:42 INFO RegressionMetrics: Model Yield: 0.7341944950376289\n16/03/03 20:45:42 INFO RegressionMetrics: Std Dev of Residuals: 6.287519509352036\n\n\n\n\n\n\nSource Code\n\u00b6\n\n\nBelow is the example program as a github gist, to view the original program in DynaML, click \nhere\n.",
            "title": "Boston Housing"
        },
        {
            "location": "/repl-examples/p2_boston_housing/#attribute-information",
            "text": "CRIM : per capita crime rate by town  ZN : proportion of residential land zoned for lots over 25,000 sq.ft.  INDUS : proportion of non-retail business acres per town  CHAS : Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)  NOX : nitric oxides concentration (parts per 10 million)  RM : average number of rooms per dwelling  AGE : proportion of owner-occupied units built prior to 1940  DIS : weighted distances to five Boston employment centres  RAD : index of accessibility to radial highways  TAX : full-value property-tax rate per $10,000  PTRATIO : pupil-teacher ratio by town  B : 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town  LSTAT : % lower status of the population  MEDV : Median value of owner-occupied homes in $1000's",
            "title": "Attribute Information:"
        },
        {
            "location": "/repl-examples/p2_boston_housing/#model",
            "text": "Below is a GP model for predicting the  MEDV  \\[\n    \\begin{align}\n        & MEDV(\\mathbf{u}) = f(\\mathbf{u}) + \\epsilon(\\mathbf{u}) \\\\\n        & f \\sim \\mathcal{GP}(m(\\mathbf{u}), K(\\mathbf{u},\\mathbf{v})) \\\\\n        & \\mathbb{E}[\\epsilon(\\mathbf{u}).\\epsilon(\\mathbf{v})] = K_{noise}(\\mathbf{u}, \\mathbf{v})\\\\\n    \\end{align}\n\\]",
            "title": "Model"
        },
        {
            "location": "/repl-examples/p2_boston_housing/#syntax",
            "text": "The  TestGPHousing()  program can be run in the REPL, below is a description of each of its arguments.     Parameter  Type  Default value  Notes       kernel  CovarianceFunction  -  The kernel function driving the GP model.     noise  CovarianceFunction  -  The additive noise that corrupts the values of the latent function.     trainFraction  Double  0.75  Fraction of the data to be used for model training and hyper-parameter selection.     columns  List [ Int ]  13, 0,.., 12  The columns to be selected for analysis (indexed from 0), first one is the target column.     grid  Int  5  The number of grid points for each hyper-parameter     step  Double  0.2  The space between grid points.     globalOpt  String  ML  The model selection procedure  \"GS\" ,   \"CSA\" ,  or  \"ML\"     stepSize  Double  0.01  Only relevant if  globalOpt   =   \"ML\" , determines step size of steepest ascent.     maxIt  Int  300  Maximum iterations for ML model selection procedure.      1\n2\n3\n4\n5 TestGPHousing ( \n   kernel   =   new   FBMKernel ( 0.55 )   +   new   LaplacianKernel ( 2.5 ), \n   noise   =   new   RBFKernel ( 1.5 ), \n   grid   =   5 ,   step   =   0.03 , \n   globalOpt   =   \"GS\" ,   trainFraction   =   0.45 )    1\n2\n3 16/03/03 20:45:41 INFO GridSearch: Optimum value of energy is: 278.1603309851301\nConfiguration: Map(hurst -> 0.4, beta -> 2.35, bandwidth -> 1.35)\n16/03/03 20:45:41 INFO SVMKernel$: Constructing kernel matrix.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 16/03/03 20:45:42 INFO GPRegression: Generating error bars\n16/03/03 20:45:42 INFO RegressionMetrics: Regression Model Performance: MEDV\n16/03/03 20:45:42 INFO RegressionMetrics: ============================\n16/03/03 20:45:42 INFO RegressionMetrics: MAE: 5.800070254265218\n16/03/03 20:45:42 INFO RegressionMetrics: RMSE: 7.739266267762397\n16/03/03 20:45:42 INFO RegressionMetrics: RMSLE: 0.4150438478412412\n16/03/03 20:45:42 INFO RegressionMetrics: R^2: 0.3609909626630624\n16/03/03 20:45:42 INFO RegressionMetrics: Corr. Coefficient: 0.7633838930006132\n16/03/03 20:45:42 INFO RegressionMetrics: Model Yield: 0.7341944950376289\n16/03/03 20:45:42 INFO RegressionMetrics: Std Dev of Residuals: 6.287519509352036",
            "title": "Syntax"
        },
        {
            "location": "/repl-examples/p2_boston_housing/#source-code",
            "text": "Below is the example program as a github gist, to view the original program in DynaML, click  here .",
            "title": "Source Code"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/",
            "text": "The \nwine quality\n data set is a common example used to benchmark classification models. Here we use the \nDynaML\n scala machine learning environment to train classifiers to detect 'good' wine from 'bad' wine. A short listing of the data attributes/columns is given below. The UCI archive has two files in the wine quality data set namely \nwinequality-red.csv\n and \nwinequality-white.csv\n. We train two separate classification models, one for red wine and one for white.\n\n\n\n\nAttribute Information:\n\u00b6\n\n\nInputs:\n\u00b6\n\n\n\n\nfixed acidity\n\n\nvolatile acidity\n\n\ncitric acid\n\n\nresidual sugar\n\n\nchlorides\n\n\nfree sulfur dioxide\n\n\ntotal sulfur dioxide\n\n\ndensity\n\n\npH\n\n\nsulphates\n\n\nalcohol\n\n\n\n\nOutput (based on sensory data):\n\u00b6\n\n\n\n\nquality (score between 0 and 10)\n\n\n\n\nData Output Preprocessing\n\u00b6\n\n\nThe wine quality target variable can take integer values from \n0\n to \n10\n, first we convert this into a binary class variable by setting the quality to be 'good'(encoded by the value \n1\n) if the numerical value is greater than \n6\n and 'bad' (encoded by value \n0\n) otherwise.\n\n\nModel\n\u00b6\n\n\nBelow is a classification model for predicting the quality label \n\\(y\\)\n.\n\n\nLogit\n\u00b6\n\n\n\\[\n\\begin{align}\n  P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\sigma(w^T \\varphi(\\mathbf{x}) + b) \\\\\n  \\sigma(z) &= \\frac{1}{1 + exp(-z)}\n\\end{align}\n\\]\nProbit\n\u00b6\n\n\nThe \nprobit regression\n model is an alternative to the \nlogit\n model it is represented as.\n\n\n\\[\n\\begin{align}\n  P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\Phi(w^T \\varphi(\\mathbf{x}) + b) \\\\\n  \\Phi(z) &= \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2 \\pi}} exp(-\\frac{z^{2}}{2}) dz  \n\\end{align}\n\\]\nSyntax\n\u00b6\n\n\nThe \nTestLogisticWineQuality\n program in the \nexamples\n package trains and tests logit and probit models on the wine quality data.\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault value\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\n\ntraining\n\n\nInt\n\n\n100\n\n\nNumber of training samples\n\n\n\n\n\n\n\n\ntest\n\n\nInt\n\n\n1000\n\n\nNumber of test samples\n\n\n\n\n\n\n\n\ncolumns\n\n\nList\n[\nInt\n]\n\n\n11, 0, ... , 10\n\n\nThe columns to be selected for analysis (indexed from 0), first one is the target column.\n\n\n\n\n\n\n\n\nstepSize\n\n\nDouble\n\n\n0.01\n\n\nStep size chosen for \nGradientDescent\n\n\n\n\n\n\n\n\nmaxIt\n\n\nInt\n\n\n30\n\n\nMaximum number of iterations for gradient descent update.\n\n\n\n\n\n\n\n\nmini\n\n\nDouble\n\n\n1.0\n\n\nFraction of training samples to sample for each batch update.\n\n\n\n\n\n\n\n\nregularization\n\n\nDouble\n\n\n0.5\n\n\nRegularization parameter.\n\n\n\n\n\n\n\n\nwineType\n\n\nString\n\n\nred\n\n\nThe type of wine: red or white\n\n\n\n\n\n\n\n\nmodelType\n\n\nString\n\n\nlogistic\n\n\nThe type of model: logistic or probit\n\n\n\n\n\n\n\n\n\n\nRed Wine\n\u00b6\n\n\n1\n2\n3\n4\nTestLogisticWineQuality\n(\nstepSize\n \n=\n \n0.2\n,\n \nmaxIt\n \n=\n \n120\n,\n\n\nmini\n \n=\n \n1.0\n,\n \ntraining\n \n=\n \n800\n,\n\n\ntest\n \n=\n \n800\n,\n \nregularization\n \n=\n \n0.2\n,\n\n\nwineType\n \n=\n \n\"red\"\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Classification Model Performance\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: ============================\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Accuracy: 0.8475\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Area under ROC: 0.7968417788802267\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Maximum F Measure: 0.7493563745371187\n\n\n\n\n\n\n\n\n\n\nWhite Wine\n\u00b6\n\n\n1\n2\n3\n4\nTestLogisticWineQuality\n(\nstepSize\n \n=\n \n0.26\n,\n \nmaxIt\n \n=\n \n300\n,\n\n\nmini\n \n=\n \n1.0\n,\n \ntraining\n \n=\n \n3800\n,\n\n\ntest\n \n=\n \n1000\n,\n \nregularization\n \n=\n \n0.0\n,\n\n\nwineType\n \n=\n \n\"white\"\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Classification Model Performance\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: ============================\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Accuracy: 0.829\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Area under ROC: 0.7184782682020251\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Maximum F Measure: 0.7182203962483446\n\n\n\n\n\n\n\n\n\n\nSource Code\n\u00b6",
            "title": "Wine Quality"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#attribute-information",
            "text": "",
            "title": "Attribute Information:"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#inputs",
            "text": "fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  density  pH  sulphates  alcohol",
            "title": "Inputs:"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#output-based-on-sensory-data",
            "text": "quality (score between 0 and 10)",
            "title": "Output (based on sensory data):"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#data-output-preprocessing",
            "text": "The wine quality target variable can take integer values from  0  to  10 , first we convert this into a binary class variable by setting the quality to be 'good'(encoded by the value  1 ) if the numerical value is greater than  6  and 'bad' (encoded by value  0 ) otherwise.",
            "title": "Data Output Preprocessing"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#model",
            "text": "Below is a classification model for predicting the quality label  \\(y\\) .",
            "title": "Model"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#logit",
            "text": "\\[\n\\begin{align}\n  P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\sigma(w^T \\varphi(\\mathbf{x}) + b) \\\\\n  \\sigma(z) &= \\frac{1}{1 + exp(-z)}\n\\end{align}\n\\]",
            "title": "Logit"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#probit",
            "text": "The  probit regression  model is an alternative to the  logit  model it is represented as.  \\[\n\\begin{align}\n  P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\Phi(w^T \\varphi(\\mathbf{x}) + b) \\\\\n  \\Phi(z) &= \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2 \\pi}} exp(-\\frac{z^{2}}{2}) dz  \n\\end{align}\n\\]",
            "title": "Probit"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#syntax",
            "text": "The  TestLogisticWineQuality  program in the  examples  package trains and tests logit and probit models on the wine quality data.     Parameter  Type  Default value  Notes       training  Int  100  Number of training samples     test  Int  1000  Number of test samples     columns  List [ Int ]  11, 0, ... , 10  The columns to be selected for analysis (indexed from 0), first one is the target column.     stepSize  Double  0.01  Step size chosen for  GradientDescent     maxIt  Int  30  Maximum number of iterations for gradient descent update.     mini  Double  1.0  Fraction of training samples to sample for each batch update.     regularization  Double  0.5  Regularization parameter.     wineType  String  red  The type of wine: red or white     modelType  String  logistic  The type of model: logistic or probit",
            "title": "Syntax"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#red-wine",
            "text": "1\n2\n3\n4 TestLogisticWineQuality ( stepSize   =   0.2 ,   maxIt   =   120 ,  mini   =   1.0 ,   training   =   800 ,  test   =   800 ,   regularization   =   0.2 ,  wineType   =   \"red\" )    1\n2\n3\n4\n5 16/04/01 15:21:57 INFO BinaryClassificationMetrics: Classification Model Performance\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: ============================\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Accuracy: 0.8475\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Area under ROC: 0.7968417788802267\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Maximum F Measure: 0.7493563745371187",
            "title": "Red Wine"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#white-wine",
            "text": "1\n2\n3\n4 TestLogisticWineQuality ( stepSize   =   0.26 ,   maxIt   =   300 ,  mini   =   1.0 ,   training   =   3800 ,  test   =   1000 ,   regularization   =   0.0 ,  wineType   =   \"white\" )    1\n2\n3\n4\n5 16/04/01 15:27:17 INFO BinaryClassificationMetrics: Classification Model Performance\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: ============================\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Accuracy: 0.829\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Area under ROC: 0.7184782682020251\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Maximum F Measure: 0.7182203962483446",
            "title": "White Wine"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#source-code",
            "text": "",
            "title": "Source Code"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/",
            "text": "System identification\n refers to the process of learning a predictive model for a given dynamic system i.e. a system whose dynamics evolve with time. The most important aspect of these models is their structure, specifically the following are the common dynamic system models for discretely sampled time dependent systems.\n\n\nDaISy: System Identification Database\n\u00b6\n\n\nDaISy\n is a database of (artificial and real world) dynamic systems maintained by the \nSTADIUS\n research group at KU Leuven. We will work with the power plant data set listed on the \nDaISy\n home page in this post. Using \nDynaML\n, which comes preloaded with the power plant data, we will train \nLSSVM\n models to predict the various output indicators of the power plant in question.\n\n\nSystem Identification Models\n\u00b6\n\n\nBelow is a quick and dirty description of \nnon-linear auto-regressive\n (NARX) models which are popular in the system identification research community and among practitioners.\n\n\nNonlinear AutoRegresive (NAR)\n\u00b6\n\n\nSignal \n\\(y(t)\\)\n modeled as a function of its previous \n\\(p\\)\n values\n\n\n\\[\n    \\begin{align}\n    y(t) = f(y(t-1), y(t-2), \\cdots, y(t-p)) + \\epsilon(t)\n    \\end{align}\n\\]\nNonlinear AutoRegressive with eXogenous inputs (NARX)\n\u00b6\n\n\nSignal \n\\(y(t)\\)\n modeled as a function of the previous \n\\(p\\)\n values of itself and the \n\\(m\\)\n exogenous inputs \n\\(u_{1}, \\cdots u_{m}\\)\n\n\n\\[\n    \\begin{align}\n    \\begin{split}\n        y(t) = & f(y(t-1), y(t-2), \\cdots, y(t-p), \\\\\n        & u_{1}(t-1), u_{1}(t-2), \\cdots, u_{1}(t-p),\\\\\n        & \\cdots, \\\\\n        & u_{m}(t-1), u_{m}(t-2), \\cdots, u_{m}(t-p)) \\\\\n        & + \\epsilon(t)\n    \\end{split}\n    \\end{align}\n\\]\n\n\nPont-sur-Sambre Power Plant Data\n\u00b6\n\n\n\n\nYou can obtain the metadata from this \nlink\n, it is also summarized below.\n\n\nData Attributes\n\u00b6\n\n\nInstances\n: 200\n\n\nInputs\n:\n\n\n\n\nGas flow\n\n\nTurbine valves opening\n\n\nSuper heater spray flow\n\n\nGas dampers\n\n\nAir flow\n\n\n\n\nOutputs\n:\n6. Steam pressure\n7. Main stem temperature\n8. Reheat steam temperature\n\n\nSystem Model\n\u00b6\n\n\nAn \nLS-SVM\n \nNARX\n of autoregressive order \n\\(p = 2\\)\n is chosen to model the plant output data. An LS-SVM model builds a predictor of the following form.\n\n\n\\[\n    \\begin{align*}\n    y(x) = \\sum_{k = 1}^{N}\\alpha_k K(\\mathbf{x}, \\mathbf{x_k}) + b\n    \\end{align*}\n\\]\nWhich is the result of solving the following linear system.\n\n\n\\[\n    \\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\]\nHere the matrix \n\\(K\\)\n is constructed from the training data using a kernel function \n\\(K(\\mathbf{x}, \\mathbf{y})\\)\n.\n\n\nChoice of Kernel Function\n\u00b6\n\n\nFor this problem we choose a polynomial kernel.\n\n\n\\[\n    \\begin{align*}\n        K(\\mathbf{x},\\mathbf{y}) = K_{poly}(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^{T}.\\mathbf{y} + \\alpha)^{d}\n    \\end{align*}\n\\]\nSyntax\n\u00b6\n\n\nThe \nDaisyPowerPlant\n program can be used to train and test LS-SVM models on the Pont Sur-Sambre power plant data.\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault value\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\n\nkernel\n\n\nCovarianceFunction\n\n\n-\n\n\nThe kernel function driving the LS-SVM model.\n\n\n\n\n\n\n\n\ndeltaT\n\n\nInt\n\n\n2\n\n\nOrder of auto-regressive model i.e. number of steps in the past to look for input features.\n\n\n\n\n\n\n\n\ntimelag\n\n\nInt\n\n\n0\n\n\nThe number of steps in the past to start using inputs.\n\n\n\n\n\n\n\n\nnum_training\n\n\nInt\n\n\n150\n\n\nNumber of training data instances.\n\n\n\n\n\n\n\n\ncolumn\n\n\nInt\n\n\n7\n\n\nThe column number of the output variable (indexed from 0).\n\n\n\n\n\n\n\n\nopt\n\n\nMap\n[\nString\n, \nDouble\n]\n\n\n-\n\n\nExtra options for model selection routine.\n\n\n\n\n\n\n\n\n\n\nSteam Pressure\n\u00b6\n\n\n1\n2\n3\n4\n5\nDynaML\n>\nDaisyPowerPlant\n(\nnew\n \nPolynomialKernel\n(\n2\n,\n \n0.5\n),\n\n\nopt\n \n=\n \nMap\n(\n\"regularization\"\n \n->\n \n\"2.5\"\n,\n \n\"globalOpt\"\n \n->\n \n\"GS\"\n,\n\n\n\"grid\"\n \n->\n \n\"4\"\n,\n \n\"step\"\n \n->\n \n\"0.1\"\n),\n\n\nnum_training\n \n=\n \n100\n,\n \ndeltaT\n \n=\n \n2\n,\n\n\ncolumn\n \n=\n \n6\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: Regression Model Performance: steam pressure\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: \n============================\n\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: MAE: \n82\n.12740530161123\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: RMSE: \n104\n.39251587470388\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: RMSLE: \n0\n.9660077848586197\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: R^2: \n0\n.8395534877128238\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: Corr. Coefficient: \n0\n.9311734118932473\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: Model Yield: \n0\n.6288000962818303\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: Std Dev of Residuals: \n87\n.82754320038951\n\n\n\n\n\n\n\n\n\n\nReheat Steam Temperature\n\u00b6\n\n\n1\n2\n3\n4\nDaisyPowerPlant\n(\nnew\n \nPolynomialKernel\n(\n2\n,\n \n1.5\n),\n\n\nopt\n \n=\n \nMap\n(\n\"regularization\"\n \n->\n \n\"2.5\"\n,\n \n\"globalOpt\"\n \n->\n \n\"GS\"\n,\n\n\n\"grid\"\n \n->\n \n\"4\"\n,\n \n\"step\"\n \n->\n \n\"0.1\"\n),\n \nnum_training\n \n=\n \n150\n,\n\n\ndeltaT\n \n=\n \n1\n,\n \ncolumn\n \n=\n \n8\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: Regression Model Performance: reheat steam temperature\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: \n============================\n\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: MAE: \n124\n.60921194767073\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: RMSE: \n137\n.33314302068544\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: RMSLE: \n0\n.5275727128626408\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: R^2: \n0\n.8247581957573777\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: Corr. Coefficient: \n0\n.9744133881055823\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: Model Yield: \n0\n.7871288689840381\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: Std Dev of Residuals: \n111\n.86852905896446\n\n\n\n\n\n\n\n\n\n\n\n\nSource Code\n\u00b6\n\n\nBelow is the example program as a github gist, to view the original program in DynaML, click \nhere\n.",
            "title": "Pont-sur-Sambre Power Plant"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#daisy-system-identification-database",
            "text": "DaISy  is a database of (artificial and real world) dynamic systems maintained by the  STADIUS  research group at KU Leuven. We will work with the power plant data set listed on the  DaISy  home page in this post. Using  DynaML , which comes preloaded with the power plant data, we will train  LSSVM  models to predict the various output indicators of the power plant in question.",
            "title": "DaISy: System Identification Database"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#system-identification-models",
            "text": "Below is a quick and dirty description of  non-linear auto-regressive  (NARX) models which are popular in the system identification research community and among practitioners.",
            "title": "System Identification Models"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#nonlinear-autoregresive-nar",
            "text": "Signal  \\(y(t)\\)  modeled as a function of its previous  \\(p\\)  values  \\[\n    \\begin{align}\n    y(t) = f(y(t-1), y(t-2), \\cdots, y(t-p)) + \\epsilon(t)\n    \\end{align}\n\\]",
            "title": "Nonlinear AutoRegresive (NAR)"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#nonlinear-autoregressive-with-exogenous-inputs-narx",
            "text": "Signal  \\(y(t)\\)  modeled as a function of the previous  \\(p\\)  values of itself and the  \\(m\\)  exogenous inputs  \\(u_{1}, \\cdots u_{m}\\)  \\[\n    \\begin{align}\n    \\begin{split}\n        y(t) = & f(y(t-1), y(t-2), \\cdots, y(t-p), \\\\\n        & u_{1}(t-1), u_{1}(t-2), \\cdots, u_{1}(t-p),\\\\\n        & \\cdots, \\\\\n        & u_{m}(t-1), u_{m}(t-2), \\cdots, u_{m}(t-p)) \\\\\n        & + \\epsilon(t)\n    \\end{split}\n    \\end{align}\n\\]",
            "title": "Nonlinear AutoRegressive with eXogenous inputs (NARX)"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#pont-sur-sambre-power-plant-data",
            "text": "You can obtain the metadata from this  link , it is also summarized below.",
            "title": "Pont-sur-Sambre Power Plant Data"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#data-attributes",
            "text": "Instances : 200  Inputs :   Gas flow  Turbine valves opening  Super heater spray flow  Gas dampers  Air flow   Outputs :\n6. Steam pressure\n7. Main stem temperature\n8. Reheat steam temperature",
            "title": "Data Attributes"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#system-model",
            "text": "An  LS-SVM   NARX  of autoregressive order  \\(p = 2\\)  is chosen to model the plant output data. An LS-SVM model builds a predictor of the following form.  \\[\n    \\begin{align*}\n    y(x) = \\sum_{k = 1}^{N}\\alpha_k K(\\mathbf{x}, \\mathbf{x_k}) + b\n    \\end{align*}\n\\] Which is the result of solving the following linear system.  \\[\n    \\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\] Here the matrix  \\(K\\)  is constructed from the training data using a kernel function  \\(K(\\mathbf{x}, \\mathbf{y})\\) .",
            "title": "System Model"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#choice-of-kernel-function",
            "text": "For this problem we choose a polynomial kernel.  \\[\n    \\begin{align*}\n        K(\\mathbf{x},\\mathbf{y}) = K_{poly}(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^{T}.\\mathbf{y} + \\alpha)^{d}\n    \\end{align*}\n\\]",
            "title": "Choice of Kernel Function"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#syntax",
            "text": "The  DaisyPowerPlant  program can be used to train and test LS-SVM models on the Pont Sur-Sambre power plant data.     Parameter  Type  Default value  Notes       kernel  CovarianceFunction  -  The kernel function driving the LS-SVM model.     deltaT  Int  2  Order of auto-regressive model i.e. number of steps in the past to look for input features.     timelag  Int  0  The number of steps in the past to start using inputs.     num_training  Int  150  Number of training data instances.     column  Int  7  The column number of the output variable (indexed from 0).     opt  Map [ String ,  Double ]  -  Extra options for model selection routine.",
            "title": "Syntax"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#steam-pressure",
            "text": "1\n2\n3\n4\n5 DynaML > DaisyPowerPlant ( new   PolynomialKernel ( 2 ,   0.5 ),  opt   =   Map ( \"regularization\"   ->   \"2.5\" ,   \"globalOpt\"   ->   \"GS\" ,  \"grid\"   ->   \"4\" ,   \"step\"   ->   \"0.1\" ),  num_training   =   100 ,   deltaT   =   2 ,  column   =   6 )    1\n2\n3\n4\n5\n6\n7\n8\n9 16 /03/04  17 :13:43 INFO RegressionMetrics: Regression Model Performance: steam pressure 16 /03/04  17 :13:43 INFO RegressionMetrics:  ============================  16 /03/04  17 :13:43 INFO RegressionMetrics: MAE:  82 .12740530161123 16 /03/04  17 :13:43 INFO RegressionMetrics: RMSE:  104 .39251587470388 16 /03/04  17 :13:43 INFO RegressionMetrics: RMSLE:  0 .9660077848586197 16 /03/04  17 :13:43 INFO RegressionMetrics: R^2:  0 .8395534877128238 16 /03/04  17 :13:43 INFO RegressionMetrics: Corr. Coefficient:  0 .9311734118932473 16 /03/04  17 :13:43 INFO RegressionMetrics: Model Yield:  0 .6288000962818303 16 /03/04  17 :13:43 INFO RegressionMetrics: Std Dev of Residuals:  87 .82754320038951",
            "title": "Steam Pressure"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#reheat-steam-temperature",
            "text": "1\n2\n3\n4 DaisyPowerPlant ( new   PolynomialKernel ( 2 ,   1.5 ),  opt   =   Map ( \"regularization\"   ->   \"2.5\" ,   \"globalOpt\"   ->   \"GS\" ,  \"grid\"   ->   \"4\" ,   \"step\"   ->   \"0.1\" ),   num_training   =   150 ,  deltaT   =   1 ,   column   =   8 )    1\n2\n3\n4\n5\n6\n7\n8\n9 16 /03/04  16 :50:42 INFO RegressionMetrics: Regression Model Performance: reheat steam temperature 16 /03/04  16 :50:42 INFO RegressionMetrics:  ============================  16 /03/04  16 :50:42 INFO RegressionMetrics: MAE:  124 .60921194767073 16 /03/04  16 :50:42 INFO RegressionMetrics: RMSE:  137 .33314302068544 16 /03/04  16 :50:42 INFO RegressionMetrics: RMSLE:  0 .5275727128626408 16 /03/04  16 :50:42 INFO RegressionMetrics: R^2:  0 .8247581957573777 16 /03/04  16 :50:42 INFO RegressionMetrics: Corr. Coefficient:  0 .9744133881055823 16 /03/04  16 :50:42 INFO RegressionMetrics: Model Yield:  0 .7871288689840381 16 /03/04  16 :50:42 INFO RegressionMetrics: Std Dev of Residuals:  111 .86852905896446",
            "title": "Reheat Steam Temperature"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#source-code",
            "text": "Below is the example program as a github gist, to view the original program in DynaML, click  here .",
            "title": "Source Code"
        },
        {
            "location": "/utils/utils_data_transforms/",
            "text": "Summary\n\n\nSome attribute transformations are included in the DynaML distribution, here we show how to use them. All of them inherit \nReversibleScaler\n[\nI\n]\n trait. They are contained in the \ndynml.utils\n package.\n\n\n\n\nGaussian Centering\n\u00b6\n\n\nGaussian scaling/centering involves calculating the sample mean and variance of data and applying a gaussian standardization operations using the calculated statistics.\n\n\nIt has different implementations in slightly varying contexts.\n\n\nUnivariate\n\u00b6\n\n\nUnivariate gaussian scaling involves\n\n\n\\[\n\\begin{align}\nx &\\in \\mathbb{R} \\\\\n\\mu &\\in \\mathbb{R} \\\\\n\\sigma &\\in \\mathbb{R} \\\\\n\\bar{x} &= \\frac{x-\\mu}{\\sigma}\n\\end{align}\n\\]\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \nmean\n \n=\n \n-\n1.5\n\n\nval\n \nsigma\n \n=\n \n2.5\n\n\n\nval\n \nugs\n \n=\n \nUnivariateGaussianScaler\n(\nmean\n,\n \nsigma\n)\n\n\n\nval\n \nx\n \n=\n \n3.0\n\n\n\nval\n \nxs\n \n=\n \nugs\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \nugs\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\nMultivariate\n\u00b6\n\n\nThe data attributes form components of a vector, in this case we can assume each component is independent and calculate the diagonal variance or compute all the component covariances in the form of a symmetric matrix.\n\n\n\\[\n\\begin{align}\nx &\\in \\mathbb{R}^n \\\\\n\\mu &\\in \\mathbb{R}^n \\\\\n\\Sigma &\\in \\mathbb{R}^{n \\times n}\\\\\nL L^\\intercal &= \\Sigma \\\\\n\\bar{x} &= L^{-1} (x - \\mu)\n\\end{align}\n\\]\nDiagonal\n\u00b6\n\n\nIn this case the sample covariance matrix calculated from the data is diagonal and neglecting the correlations between the attributes.\n\n\n\\[\n\\Sigma = \\begin{pmatrix}\n\\sigma^{2}_1 & \\cdots & 0\\\\\n \\vdots & \\ddots  & \\vdots\\\\\n 0 & \\cdots & \\sigma^{2}_n  \n\\end{pmatrix}\n\\]\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \nmean\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(-\n1.5\n,\n \n1.5\n,\n \n0.25\n)\n\n\nval\n \nsigma\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.5\n,\n \n2.5\n,\n \n1.0\n)\n\n\n\nval\n \ngs\n \n=\n \nGaussianScaler\n(\nmean\n,\n \nsigma\n)\n\n\n\nval\n \nx\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.2\n,\n \n-\n3.5\n,\n \n-\n1.5\n)\n\n\n\nval\n \nxs\n \n=\n \ngs\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \ngs\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\nFull Matrix\n\u00b6\n\n\nWhen the sample covariance matrix is calculated taking into account correlations between data attributes.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nval\n \nmean\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(-\n1.5\n,\n \n1.5\n,\n \n0.25\n)\n\n\nval\n \nsigma\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \nDenseMatrix\n(\n\n  \n(\n2.5\n,\n \n0.5\n,\n \n0.25\n),\n\n  \n(\n0.5\n,\n \n3.5\n,\n \n1.2\n),\n\n  \n(\n0.25\n,\n \n1.2\n,\n \n2.25\n)\n\n\n)\n\n\n\nval\n \nmv_gs\n \n=\n \nMVGaussianScaler\n(\nmean\n,\n \nsigma\n)\n\n\n\nval\n \nx\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.2\n,\n \n-\n3.5\n,\n \n-\n1.5\n)\n\n\n\nval\n \nxs\n \n=\n \nmv_gs\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \nmv_gs\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\nMean Centering\n\u00b6\n\n\nUnivariate\n\u00b6\n\n\n\\[\n\\begin{align}\nx &\\in \\mathbb{R} \\\\\n\\mu &\\in \\mathbb{R} \\\\\n\\bar{x} &= x-\\mu\n\\end{align}\n\\]\n1\n2\n3\n4\n5\n6\n7\n8\n9\nval\n \nc\n \n=\n \n-\n1.5\n\n\n\nval\n \nums\n \n=\n \nUnivariateMeanScaler\n(\nc\n)\n\n\n\nval\n \nx\n \n=\n \n3.0\n\n\n\nval\n \nxs\n \n=\n \nums\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \nums\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\nMultivariate\n\u00b6\n\n\n\\[\n\\begin{align}\nx &\\in \\mathbb{R}^n \\\\\n\\mu &\\in \\mathbb{R}^n \\\\\n\\bar{x} &= x - \\mu\n\\end{align}\n\\]\n1\n2\n3\n4\n5\n6\n7\n8\n9\nval\n \nmean\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(-\n1.5\n,\n \n1.5\n,\n \n0.25\n)\n\n\n\nval\n \nmms\n \n=\n \nMeanScaler\n(\nmean\n)\n\n\n\nval\n \nx\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.2\n,\n \n-\n3.5\n,\n \n-\n1.5\n)\n\n\n\nval\n \nxs\n \n=\n \nmms\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \nmms\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\nMin-Max Scaling\n\u00b6\n\n\nMin-max scaling is also known as \n\\(0,1\\)\n scaling because attributes are scaled down to the domain \n\\([0, 1]\\)\n. This is done by calculating the minimum and maximum of attribute values.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \nmin\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(-\n1.5\n,\n \n1.5\n,\n \n0.25\n)\n\n\nval\n \nmax\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.5\n,\n \n2.5\n,\n \n1.0\n)\n\n\n\nval\n \nmin_max_scaler\n \n=\n \nMinMaxScaler\n(\nmin\n,\n \nmax\n)\n\n\n\nval\n \nx\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.2\n,\n \n-\n3.5\n,\n \n-\n1.5\n)\n\n\n\nval\n \nxs\n \n=\n \nmin_max_scaler\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \nmin_max_scaler\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\nPrincipal Component Analysis\n\u00b6\n\n\nPrincipal component analysis\n consists of projecting data onto the eigenvectors of its sample covariance matrix.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nval\n \nmean\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(-\n1.5\n,\n \n1.5\n,\n \n0.25\n)\n\n\nval\n \nsigma\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \nDenseMatrix\n(\n\n  \n(\n2.5\n,\n \n0.5\n,\n \n0.25\n),\n\n  \n(\n0.5\n,\n \n3.5\n,\n \n1.2\n),\n\n  \n(\n0.25\n,\n \n1.2\n,\n \n2.25\n)\n\n\n)\n\n\n\nval\n \npca\n \n=\n \nPCAScaler\n(\nmean\n,\n \nsigma\n)\n\n\n\nval\n \nx\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.2\n,\n \n-\n3.5\n,\n \n-\n1.5\n)\n\n\n\nval\n \nxs\n \n=\n \npca\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \npca\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\n\n\nSlicing scalers\n\n\nIt is possible to \nslice\n the scalers shown above if they act on vectors. For example.\n\n1\n2\n3\n4\n//Slice on subset of columns\n\n\nval\n \ngs_sub\n:\n \nGaussianScaler\n \n=\n \ngs\n(\n0\n \nto\n \n1\n)\n\n\n//Slice on a single column\n\n\nval\n \ngs_last\n:\n \nUnivariateGaussianScaler\n \n=\n \ngs\n(\n2\n)",
            "title": "Data Transforms"
        },
        {
            "location": "/utils/utils_data_transforms/#gaussian-centering",
            "text": "Gaussian scaling/centering involves calculating the sample mean and variance of data and applying a gaussian standardization operations using the calculated statistics.  It has different implementations in slightly varying contexts.",
            "title": "Gaussian Centering"
        },
        {
            "location": "/utils/utils_data_transforms/#univariate",
            "text": "Univariate gaussian scaling involves  \\[\n\\begin{align}\nx &\\in \\mathbb{R} \\\\\n\\mu &\\in \\mathbb{R} \\\\\n\\sigma &\\in \\mathbb{R} \\\\\n\\bar{x} &= \\frac{x-\\mu}{\\sigma}\n\\end{align}\n\\]  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   mean   =   - 1.5  val   sigma   =   2.5  val   ugs   =   UnivariateGaussianScaler ( mean ,   sigma )  val   x   =   3.0  val   xs   =   ugs ( x )  val   xhat   =   ugs . i ( xs )",
            "title": "Univariate"
        },
        {
            "location": "/utils/utils_data_transforms/#multivariate",
            "text": "The data attributes form components of a vector, in this case we can assume each component is independent and calculate the diagonal variance or compute all the component covariances in the form of a symmetric matrix.  \\[\n\\begin{align}\nx &\\in \\mathbb{R}^n \\\\\n\\mu &\\in \\mathbb{R}^n \\\\\n\\Sigma &\\in \\mathbb{R}^{n \\times n}\\\\\nL L^\\intercal &= \\Sigma \\\\\n\\bar{x} &= L^{-1} (x - \\mu)\n\\end{align}\n\\]",
            "title": "Multivariate"
        },
        {
            "location": "/utils/utils_data_transforms/#diagonal",
            "text": "In this case the sample covariance matrix calculated from the data is diagonal and neglecting the correlations between the attributes.  \\[\n\\Sigma = \\begin{pmatrix}\n\\sigma^{2}_1 & \\cdots & 0\\\\\n \\vdots & \\ddots  & \\vdots\\\\\n 0 & \\cdots & \\sigma^{2}_n  \n\\end{pmatrix}\n\\]  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   mean :   DenseVector [ Double ]   =   DenseVector (- 1.5 ,   1.5 ,   0.25 )  val   sigma :   DenseVector [ Double ]   =   DenseVector ( 0.5 ,   2.5 ,   1.0 )  val   gs   =   GaussianScaler ( mean ,   sigma )  val   x :   DenseVector [ Double ]   =   DenseVector ( 0.2 ,   - 3.5 ,   - 1.5 )  val   xs   =   gs ( x )  val   xhat   =   gs . i ( xs )",
            "title": "Diagonal"
        },
        {
            "location": "/utils/utils_data_transforms/#full-matrix",
            "text": "When the sample covariance matrix is calculated taking into account correlations between data attributes.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 val   mean :   DenseVector [ Double ]   =   DenseVector (- 1.5 ,   1.5 ,   0.25 )  val   sigma :   DenseMatrix [ Double ]   =   DenseMatrix ( \n   ( 2.5 ,   0.5 ,   0.25 ), \n   ( 0.5 ,   3.5 ,   1.2 ), \n   ( 0.25 ,   1.2 ,   2.25 )  )  val   mv_gs   =   MVGaussianScaler ( mean ,   sigma )  val   x :   DenseVector [ Double ]   =   DenseVector ( 0.2 ,   - 3.5 ,   - 1.5 )  val   xs   =   mv_gs ( x )  val   xhat   =   mv_gs . i ( xs )",
            "title": "Full Matrix"
        },
        {
            "location": "/utils/utils_data_transforms/#mean-centering",
            "text": "",
            "title": "Mean Centering"
        },
        {
            "location": "/utils/utils_data_transforms/#univariate_1",
            "text": "\\[\n\\begin{align}\nx &\\in \\mathbb{R} \\\\\n\\mu &\\in \\mathbb{R} \\\\\n\\bar{x} &= x-\\mu\n\\end{align}\n\\] 1\n2\n3\n4\n5\n6\n7\n8\n9 val   c   =   - 1.5  val   ums   =   UnivariateMeanScaler ( c )  val   x   =   3.0  val   xs   =   ums ( x )  val   xhat   =   ums . i ( xs )",
            "title": "Univariate"
        },
        {
            "location": "/utils/utils_data_transforms/#multivariate_1",
            "text": "\\[\n\\begin{align}\nx &\\in \\mathbb{R}^n \\\\\n\\mu &\\in \\mathbb{R}^n \\\\\n\\bar{x} &= x - \\mu\n\\end{align}\n\\] 1\n2\n3\n4\n5\n6\n7\n8\n9 val   mean :   DenseVector [ Double ]   =   DenseVector (- 1.5 ,   1.5 ,   0.25 )  val   mms   =   MeanScaler ( mean )  val   x :   DenseVector [ Double ]   =   DenseVector ( 0.2 ,   - 3.5 ,   - 1.5 )  val   xs   =   mms ( x )  val   xhat   =   mms . i ( xs )",
            "title": "Multivariate"
        },
        {
            "location": "/utils/utils_data_transforms/#min-max-scaling",
            "text": "Min-max scaling is also known as  \\(0,1\\)  scaling because attributes are scaled down to the domain  \\([0, 1]\\) . This is done by calculating the minimum and maximum of attribute values.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   min :   DenseVector [ Double ]   =   DenseVector (- 1.5 ,   1.5 ,   0.25 )  val   max :   DenseVector [ Double ]   =   DenseVector ( 0.5 ,   2.5 ,   1.0 )  val   min_max_scaler   =   MinMaxScaler ( min ,   max )  val   x :   DenseVector [ Double ]   =   DenseVector ( 0.2 ,   - 3.5 ,   - 1.5 )  val   xs   =   min_max_scaler ( x )  val   xhat   =   min_max_scaler . i ( xs )",
            "title": "Min-Max Scaling"
        },
        {
            "location": "/utils/utils_data_transforms/#principal-component-analysis",
            "text": "Principal component analysis  consists of projecting data onto the eigenvectors of its sample covariance matrix.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 val   mean :   DenseVector [ Double ]   =   DenseVector (- 1.5 ,   1.5 ,   0.25 )  val   sigma :   DenseMatrix [ Double ]   =   DenseMatrix ( \n   ( 2.5 ,   0.5 ,   0.25 ), \n   ( 0.5 ,   3.5 ,   1.2 ), \n   ( 0.25 ,   1.2 ,   2.25 )  )  val   pca   =   PCAScaler ( mean ,   sigma )  val   x :   DenseVector [ Double ]   =   DenseVector ( 0.2 ,   - 3.5 ,   - 1.5 )  val   xs   =   pca ( x )  val   xhat   =   pca . i ( xs )     Slicing scalers  It is possible to  slice  the scalers shown above if they act on vectors. For example. 1\n2\n3\n4 //Slice on subset of columns  val   gs_sub :   GaussianScaler   =   gs ( 0   to   1 )  //Slice on a single column  val   gs_last :   UnivariateGaussianScaler   =   gs ( 2 )",
            "title": "Principal Component Analysis"
        },
        {
            "location": "/utils/package/",
            "text": "Summary\n\n\nThe \nutils\n object contains some useful helper functions which are used by a number of API components of DynaML.\n\n\n\n\nString/File Processing\n\u00b6\n\n\nLoad File into a Stream\n\u00b6\n\n\n1\nval\n \ncontent\n \n=\n \nutils\n.\ntextFileToStream\n(\n\"data.csv\"\n)\n\n\n\n\n\n\n\nString Replace\n\u00b6\n\n\nReplace all occurrences of a string (or regular expression) in a target string\n\n\n1\nval\n \nnew_str\n \n=\n \nutils\n.\nreplace\n(\nfind\n \n=\n \n\",\"\n)(\nreplace\n \n=\n \n\"|\"\n)(\ninput\n \n=\n \n\"1,2,3,4\"\n)\n\n\n\n\n\n\n\nURL download\n\u00b6\n\n\nDownload the content of a url to a specified location on disk.\n\n\n1\nutils\n.\ndownloadURL\n(\n\"www.google.com\"\n,\n \n\"google_home_page.html\"\n)\n\n\n\n\n\n\n\nWrite to File\n\u00b6\n\n\n1\n2\nval\n \ncontent\n:\n \nStream\n[\nString\n]\n \n=\n \n_\n\n\nutils\n.\nwriteToFile\n(\n\"foo.csv\"\n)(\ncontent\n)\n\n\n\n\n\n\n\nNumerics\n\u00b6\n\n\nlog1p\n\u00b6\n\n\nCalculates \n\\(log_{e}(1+x)\\)\n.\n\n\n1\nval\n \nl\n \n=\n \nutils\n.\nlog1pExp\n(\n0.02\n)\n\n\n\n\n\n\n\nHaar DWT Matrix\n\u00b6\n\n\nConstructs the Haar \ndiscrete wavelet transform\n matrix for orders which are powers of two.\n\n\n1\nval\n \ndwt_mat\n \n=\n \nutils\n.\nhaarMatrix\n(\nmath\n.\npow\n(\n2\n,\n \n3\n).\ntoInt\n)\n\n\n\n\n\n\n\nHermite Polynomials\n\u00b6\n\n\nThe \nHermite polynomials\n are an important class of orthogonal polynomials used in numerical analysis. There are two definitions of the \nHermite\n polynomials i.e. the probabilist and physicist definitions, which are equivalent up-to a scale factor. The the \nutils\n object, the probabilist polynomials are calculated.\n\n\n1\n2\n3\n4\n5\n//Calculate the 3rd order Hermite polynomial\n\n\n\nval\n \nh3\n \n=\n \n(\nx\n:\n \nDouble\n)\n \n=>\n \nutils\n.\nhermite\n(\n3\n,\n \nx\n)\n\n\n\nh3\n(\n2.5\n)\n\n\n\n\n\n\n\nChebyshev Polynomials\n\u00b6\n\n\nChebyshev polynomials\n are another important class of orthogonal polynomials used in numerical analysis. There are two types, the \nfirst kind\n and \nsecond kind\n.\n\n\n1\n2\n3\n4\n5\n//Calculate the Chebyshev polynomial of second kind order 3\n\n\n\nval\n \nc23\n \n=\n \n(\nx\n:\n \nDouble\n)\n \n=>\n \nutils\n.\nchebyshev\n(\n3\n,\n \nx\n,\n \nkind\n \n=\n \n2\n)\n\n\n\nc23\n(\n2.5\n)\n\n\n\n\n\n\n\nQuick Select\n\u00b6\n\n\nThe quick select aims to find the \n\\(k^{th}\\)\n smallest element of a list of numbers.\n\n\n1\nval\n \nsecond\n \n=\n \nutils\n.\nquickselect\n(\nList\n(\n3\n,\n2\n,\n4\n,\n5\n,\n1\n,\n6\n),\n \n2\n)\n\n\n\n\n\n\n\nMedian\n\u00b6\n\n\n1\nval\n \nsecond\n \n=\n \nutils\n.\nmedian\n(\nList\n(\n3\n,\n2\n,\n4\n,\n5\n,\n1\n,\n6\n))\n\n\n\n\n\n\n\nSample Statistics\n\u00b6\n\n\nCalculate the mean and variance (or covariance), minimum, maximum of a list of \nDenseVector\n[\nDouble\n]\n instances.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nval\n \ndata\n:\n \nList\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n\nval\n \n(\nmu\n,\n \nvard\n)\n:\n \n(\nDenseVector\n[\nDouble\n],\n \nDenseVector\n[\nDouble\n])\n \n=\n\n  \nutils\n.\ngetStats\n(\ndata\n)\n\n\n\nval\n \n(\nmean\n,\n \ncov\n)\n:\n \n(\nDenseVector\n[\nDouble\n],\n \nDenseMatrix\n[\nDouble\n])\n \n=\n\n  \nutils\n.\ngetStatsMult\n(\ndata\n)\n\n\n\nval\n \n(\nmin\n,\n \nmax\n)\n \n=\n \nutils\n.\ngetMinMax\n(\ndata\n)",
            "title": "`utils` object"
        },
        {
            "location": "/utils/package/#stringfile-processing",
            "text": "",
            "title": "String/File Processing"
        },
        {
            "location": "/utils/package/#load-file-into-a-stream",
            "text": "1 val   content   =   utils . textFileToStream ( \"data.csv\" )",
            "title": "Load File into a Stream"
        },
        {
            "location": "/utils/package/#string-replace",
            "text": "Replace all occurrences of a string (or regular expression) in a target string  1 val   new_str   =   utils . replace ( find   =   \",\" )( replace   =   \"|\" )( input   =   \"1,2,3,4\" )",
            "title": "String Replace"
        },
        {
            "location": "/utils/package/#url-download",
            "text": "Download the content of a url to a specified location on disk.  1 utils . downloadURL ( \"www.google.com\" ,   \"google_home_page.html\" )",
            "title": "URL download"
        },
        {
            "location": "/utils/package/#write-to-file",
            "text": "1\n2 val   content :   Stream [ String ]   =   _  utils . writeToFile ( \"foo.csv\" )( content )",
            "title": "Write to File"
        },
        {
            "location": "/utils/package/#numerics",
            "text": "",
            "title": "Numerics"
        },
        {
            "location": "/utils/package/#log1p",
            "text": "Calculates  \\(log_{e}(1+x)\\) .  1 val   l   =   utils . log1pExp ( 0.02 )",
            "title": "log1p"
        },
        {
            "location": "/utils/package/#haar-dwt-matrix",
            "text": "Constructs the Haar  discrete wavelet transform  matrix for orders which are powers of two.  1 val   dwt_mat   =   utils . haarMatrix ( math . pow ( 2 ,   3 ). toInt )",
            "title": "Haar DWT Matrix"
        },
        {
            "location": "/utils/package/#hermite-polynomials",
            "text": "The  Hermite polynomials  are an important class of orthogonal polynomials used in numerical analysis. There are two definitions of the  Hermite  polynomials i.e. the probabilist and physicist definitions, which are equivalent up-to a scale factor. The the  utils  object, the probabilist polynomials are calculated.  1\n2\n3\n4\n5 //Calculate the 3rd order Hermite polynomial  val   h3   =   ( x :   Double )   =>   utils . hermite ( 3 ,   x )  h3 ( 2.5 )",
            "title": "Hermite Polynomials"
        },
        {
            "location": "/utils/package/#chebyshev-polynomials",
            "text": "Chebyshev polynomials  are another important class of orthogonal polynomials used in numerical analysis. There are two types, the  first kind  and  second kind .  1\n2\n3\n4\n5 //Calculate the Chebyshev polynomial of second kind order 3  val   c23   =   ( x :   Double )   =>   utils . chebyshev ( 3 ,   x ,   kind   =   2 )  c23 ( 2.5 )",
            "title": "Chebyshev Polynomials"
        },
        {
            "location": "/utils/package/#quick-select",
            "text": "The quick select aims to find the  \\(k^{th}\\)  smallest element of a list of numbers.  1 val   second   =   utils . quickselect ( List ( 3 , 2 , 4 , 5 , 1 , 6 ),   2 )",
            "title": "Quick Select"
        },
        {
            "location": "/utils/package/#median",
            "text": "1 val   second   =   utils . median ( List ( 3 , 2 , 4 , 5 , 1 , 6 ))",
            "title": "Median"
        },
        {
            "location": "/utils/package/#sample-statistics",
            "text": "Calculate the mean and variance (or covariance), minimum, maximum of a list of  DenseVector [ Double ]  instances.  1\n2\n3\n4\n5\n6\n7\n8\n9 val   data :   List [ DenseVector [ Double ]]   =   _  val   ( mu ,   vard ) :   ( DenseVector [ Double ],   DenseVector [ Double ])   = \n   utils . getStats ( data )  val   ( mean ,   cov ) :   ( DenseVector [ Double ],   DenseMatrix [ Double ])   = \n   utils . getStatsMult ( data )  val   ( min ,   max )   =   utils . getMinMax ( data )",
            "title": "Sample Statistics"
        },
        {
            "location": "/license/",
            "text": "Copyright 2015 Mandar Chandorkar (\nmandar2812@gmail.com\n)\n\n\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n\nApache License\n\n\n\n\n\n\nWarning\n\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
            "title": "License"
        },
        {
            "location": "/about/",
            "text": "Affliations\n\u00b6\n\n\nDynaML is developed and maintained by \nTranscendent AI Labs\n\n\nDynaML is proud to be a part of the \nMozilla Science\n Collaborate \nplatform\n\n\nContributors\n\u00b6\n\n\n\n\n\n\nMandar Chandorkar\n\n\n\n\n\n\nAmit Kumar Jaiswal\n\n\n\n\n\n\nHave questions or suggestions? Feel free to \nopen an issue on GitHub\n or \nask me on Twitter\n.",
            "title": "About Us"
        },
        {
            "location": "/about/#affliations",
            "text": "DynaML is developed and maintained by  Transcendent AI Labs  DynaML is proud to be a part of the  Mozilla Science  Collaborate  platform",
            "title": "Affliations"
        },
        {
            "location": "/about/#contributors",
            "text": "Mandar Chandorkar    Amit Kumar Jaiswal    Have questions or suggestions? Feel free to  open an issue on GitHub  or  ask me on Twitter .",
            "title": "Contributors"
        },
        {
            "location": "/blog/2016-12-11-dynaml-new-features/",
            "text": "Summarizes some of the pet projects being tackled in DynaML\n\n\n\n\nThe past year has seen DynaML grow by leaps and bounds, this post hopes to give you an update about what has been achieved\nand a taste for what is to come.\n\n\nCompleted Features\n\u00b6\n\n\nA short tour of the enhancements which were completed.\n\n\nJanuary to June\n\u00b6\n\n\n\n\nReleased \nv1.3.x\n series with the following new additions\n\n\n\n\nModels\n\n\n\n\nRegularized Least Squares\n\n\nLogistic and Probit Regression\n\n\nFeed Forward Neural Nets\n\n\nGaussian Process (GP) classification and NARX based models\n\n\nLeast Squares Support Vector Machines (LSSVM) for classification and regression\n\n\nMeta model API, committee models\n\n\n\n\nOptimization Primitives\n\n\n\n\nRegularized Least Squares Solvers\n\n\nGradient Descent\n\n\nCommittee model solvers\n\n\nLinear Solvers for LSSVM\n\n\nLaplace approximation for GPs\n\n\n\n\nMiscellaneous\n\n\n\n\nData Pipes API\n\n\n\n\nMigration to scala version 2.11.8\n\n\n\n\n\n\nStarted work on release \n1.4.x\n series with initial progress\n\n\n\n\n\n\nImprovements\n\n\n\n\nMigrated from Maven to Sbt.\n\n\nSet \nAmmonite\n as default REPL.\n\n\n\n\nJune to December\n\u00b6\n\n\n\n\nReleased \nv1.4\n with the following features.\n\n\n\n\nModels\n\n\nThe following inference models have been added.\n\n\n\n\nLSSVM committees.\n\n\nMulti-output, multi-task \nGaussian Process\n models as reviewed in \nLawrence et. al\n.\n\n\nStudent T Processes\n: single and multi output inspired from \nShah, Ghahramani et. al\n\n\nPerformance improvement to computation of \nmarginal likelihood\n and \nposterior predictive distribution\n in Gaussian Process models.\n\n\nPosterior predictive distribution outputted by the \nAbstractGPRegression\n base class is now changed to \nMultGaussianRV\n which is added to the \ndynaml.probability\n package.\n\n\n\n\nKernels\n\n\n\n\n\n\nAdded \nStationaryKernel\n and \nLocallyStationaryKernel\n classes in the kernel APIs, converted \nRBFKernel\n, \nCauchyKernel\n, \nRationalQuadraticKernel\n & \nLaplacianKernel\n to subclasses of \nStationaryKernel\n\n\n\n\n\n\nAdded \nMLPKernel\n which implements the \nmaximum likelihood perceptron\n kernel as shown \nhere\n.\n\n\n\n\n\n\nAdded \nco-regionalization kernels\n which are used in \nLawrence et. al\n to formulate kernels for vector valued functions. In this category the following co-regionalization kernels were implemented.\n\n\n\n\nCoRegRBFKernel\n\n\nCoRegCauchyKernel\n\n\nCoRegLaplaceKernel\n\n\n\n\nCoRegDiracKernel\n\n\n\n\n\n\nImproved performance when calculating kernel matrices for composite kernels.\n\n\n\n\n\n\nAdded \n:*\n operator to kernels so that one can create separable kernels used in \nco-regionalization models\n.\n\n\n\n\n\n\nOptimization\n\n\n\n\nImproved performance of \nCoupledSimulatedAnnealing\n, enabled use of 4 variants of \nCoupled Simulated Annealing\n, adding the ability to set annealing schedule using so called \nvariance control\n scheme as outlined in \nde-Souza, Suykens et. al\n.\n\n\n\n\nPipes\n\n\n\n\n\n\nAdded \nScaler\n and \nReversibleScaler\n traits to represent transformations which input and output into the same domain set, these traits are extensions of \nDataPipe\n.\n\n\n\n\n\n\nAdded \nDiscrete Wavelet Transform\n based on the \nHaar\n wavelet.\n\n\n\n\n\n\n\n\n\n\nStarted work on \nv1.4.1\n with the following progress\n\n\n\n\n\n\nLinear Algebra API\n\n\n\n\n\n\nPartitioned Matrices/Vectors and the following operations\n\n\n\n\nAddition, Subtraction\n\n\nMatrix, vector multiplication\n\n\nLU, Cholesky\n\n\nA\\y, A\\Y\n\n\n\n\n\n\n\n\nProbability API\n\n\n\n\nAdded API end points for representing Measurable Functions of random variables.\n\n\n\n\nModel Evaluation\n\n\n\n\nAdded Matthews Correlation Coefficient calculation to \nBinaryClassificationMetrics\n via the \nmatthewsCCByThreshold\n method  \n\n\n\n\nData Pipes API\n\n\n\n\nAdded \nEncoder[S,D]\n traits which are reversible data pipes representing an encoding between types \nS\n and \nD\n.\n\n\n\n\nMiscellaneous\n\n\n\n\nUpdated \nammonite\n version to \n0.8.1\n\n\nAdded support for compiling basic R code with \nrenjin\n. Run R code in the following manner:\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nval\n \ntoRDF\n \n=\n \ncsvToRDF\n(\n\"dfWine\"\n,\n \n';'\n)\n\n\nval\n \nwine_quality_red\n \n=\n \ntoRDF\n(\n\"data/winequality-red.csv\"\n)\n\n\n//Descriptive statistics\n\n\nval\n \ncommands\n:\n \nString\n \n=\n \n\"\"\"\n\n\nprint(summary(dfWine))\n\n\nprint(\"\\n\")\n\n\nprint(str(dfWine))\n\n\n\"\"\"\n\n\nr\n(\ncommands\n)\n\n\n//Build Linear Model\n\n\nval\n \nmodelGLM\n \n=\n \nrdfToGLM\n(\n\"model\"\n,\n \n\"quality\"\n,\n \nArray\n(\n\"fixed.acidity\"\n,\n \n\"citric.acid\"\n,\n \n\"chlorides\"\n))\n\n\nmodelGLM\n(\n\"dfWine\"\n)\n\n\n//Print goodness of fit\n\n\nr\n(\n\"print(summary(model))\"\n)\n\n\n\n\n\n\n\nOngoing Work\n\u00b6\n\n\nSome projects being worked on right now are.\n\n\n\n\nBayesian optimization using Gaussian Process models.\n\n\nImplementation of Neural Networks using the \nakka\n actor API.\n\n\nImplementation of kernels which can be decomposed on data dimensions \n\\(k((x_1, x_2), (y_1, y_2)) = k_1(x_1, y_1) + k_2(x_2, y_2)\\)",
            "title": "State of DynaML 2016"
        },
        {
            "location": "/blog/2016-12-11-dynaml-new-features/#completed-features",
            "text": "A short tour of the enhancements which were completed.",
            "title": "Completed Features"
        },
        {
            "location": "/blog/2016-12-11-dynaml-new-features/#january-to-june",
            "text": "Released  v1.3.x  series with the following new additions   Models   Regularized Least Squares  Logistic and Probit Regression  Feed Forward Neural Nets  Gaussian Process (GP) classification and NARX based models  Least Squares Support Vector Machines (LSSVM) for classification and regression  Meta model API, committee models   Optimization Primitives   Regularized Least Squares Solvers  Gradient Descent  Committee model solvers  Linear Solvers for LSSVM  Laplace approximation for GPs   Miscellaneous   Data Pipes API   Migration to scala version 2.11.8    Started work on release  1.4.x  series with initial progress    Improvements   Migrated from Maven to Sbt.  Set  Ammonite  as default REPL.",
            "title": "January to June"
        },
        {
            "location": "/blog/2016-12-11-dynaml-new-features/#june-to-december",
            "text": "Released  v1.4  with the following features.   Models  The following inference models have been added.   LSSVM committees.  Multi-output, multi-task  Gaussian Process  models as reviewed in  Lawrence et. al .  Student T Processes : single and multi output inspired from  Shah, Ghahramani et. al  Performance improvement to computation of  marginal likelihood  and  posterior predictive distribution  in Gaussian Process models.  Posterior predictive distribution outputted by the  AbstractGPRegression  base class is now changed to  MultGaussianRV  which is added to the  dynaml.probability  package.   Kernels    Added  StationaryKernel  and  LocallyStationaryKernel  classes in the kernel APIs, converted  RBFKernel ,  CauchyKernel ,  RationalQuadraticKernel  &  LaplacianKernel  to subclasses of  StationaryKernel    Added  MLPKernel  which implements the  maximum likelihood perceptron  kernel as shown  here .    Added  co-regionalization kernels  which are used in  Lawrence et. al  to formulate kernels for vector valued functions. In this category the following co-regionalization kernels were implemented.   CoRegRBFKernel  CoRegCauchyKernel  CoRegLaplaceKernel   CoRegDiracKernel    Improved performance when calculating kernel matrices for composite kernels.    Added  :*  operator to kernels so that one can create separable kernels used in  co-regionalization models .    Optimization   Improved performance of  CoupledSimulatedAnnealing , enabled use of 4 variants of  Coupled Simulated Annealing , adding the ability to set annealing schedule using so called  variance control  scheme as outlined in  de-Souza, Suykens et. al .   Pipes    Added  Scaler  and  ReversibleScaler  traits to represent transformations which input and output into the same domain set, these traits are extensions of  DataPipe .    Added  Discrete Wavelet Transform  based on the  Haar  wavelet.      Started work on  v1.4.1  with the following progress    Linear Algebra API    Partitioned Matrices/Vectors and the following operations   Addition, Subtraction  Matrix, vector multiplication  LU, Cholesky  A\\y, A\\Y     Probability API   Added API end points for representing Measurable Functions of random variables.   Model Evaluation   Added Matthews Correlation Coefficient calculation to  BinaryClassificationMetrics  via the  matthewsCCByThreshold  method     Data Pipes API   Added  Encoder[S,D]  traits which are reversible data pipes representing an encoding between types  S  and  D .   Miscellaneous   Updated  ammonite  version to  0.8.1  Added support for compiling basic R code with  renjin . Run R code in the following manner:    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 val   toRDF   =   csvToRDF ( \"dfWine\" ,   ';' )  val   wine_quality_red   =   toRDF ( \"data/winequality-red.csv\" )  //Descriptive statistics  val   commands :   String   =   \"\"\"  print(summary(dfWine))  print(\"\\n\")  print(str(dfWine))  \"\"\"  r ( commands )  //Build Linear Model  val   modelGLM   =   rdfToGLM ( \"model\" ,   \"quality\" ,   Array ( \"fixed.acidity\" ,   \"citric.acid\" ,   \"chlorides\" ))  modelGLM ( \"dfWine\" )  //Print goodness of fit  r ( \"print(summary(model))\" )",
            "title": "June to December"
        },
        {
            "location": "/blog/2016-12-11-dynaml-new-features/#ongoing-work",
            "text": "Some projects being worked on right now are.   Bayesian optimization using Gaussian Process models.  Implementation of Neural Networks using the  akka  actor API.  Implementation of kernels which can be decomposed on data dimensions  \\(k((x_1, x_2), (y_1, y_2)) = k_1(x_1, y_1) + k_2(x_2, y_2)\\)",
            "title": "Ongoing Work"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/",
            "text": "Summary\n\n\nSome resources for people interested in contributing code or starting ML research/applications\n\n\n\n\nDynaML aims to make a versatile and powerful data analysis and machine learning toolkit available as a shell and runtime environment. As is the case with any tool, relevant background and knowledge is crucial in order to yield its power. This post is intended to be a starting point for online resources which are relevant to DynaML.\n\n\nMachine Learning\n\u00b6\n\n\nMachine Learning\n refers to the ability to make predictions and decisions from data. It is a field that has evolved from the study of \npattern recognition\n and \ncomputational learning theory\n in artificial intelligence.\n\n\nMachine Learning theory and applications lie at the intersection of a number of concepts in the domains of Mathematics, Physics and Computer Science. It is no surprise that machine learning is a rich and deep domain with much intellectual and practical rewards to offer to the persistent and observant student.\n\n\nThe following is a non-exhaustive list of educational resources for learning ML.\n\n\nOnline Courses\n\u00b6\n\n\n\n\nAndrew Ng's famous \ncourse\n on \nCoursera\n.\n\n\nIntro to Machine Learning\n at \nUdacity\n\n\nMachine Learning\n: MIT Open Course Ware\n\n\n\n\nVideos/Youtube\n\u00b6\n\n\n\n\nMachine Learning Playlist\n by \nmathematicalmonk\n\n\nMachine Learning Course\n by \ncaltech\n\n\n\n\nBooks\n\u00b6\n\n\n\n\nBayesian Reasoning and Machine Learning\n by David Barber\n\n\nMachine Learning: A Probabilistic Perspective\n by Kevin P. Murphy\n\n\nPattern Recognition and Machine Learning\n by Christopher Bishop\n\n\nUnderstanding Machine Learning: From Theory to Algorithms\n by Shai Shalev-Shwartz and Shai Ben-David\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n by Trevor Hastie, Robert Tibshirani and Jerome Friedman\n\n\n\n\nForums\n\u00b6\n\n\n\n\nHow do I learn machine learning\n on \nQuora\n\n\n\n\nBlogs\n\u00b6\n\n\n\n\nDeepmind Blog\n\n\nCortana Blog\n\n\nShakir Mohammad's Blog\n\n\nDarren Wilkinson's research blog\n\n\nJohn's Langford's Blog\n.\n\n\nKyle Kastner's Blog\n\n\nSander Dieleman's Blog\n\n\n\n\nProgramming Environment: Scala\n\u00b6\n\n\nScala\n is the implementation language for DynaML, it is a hybrid language which gives the user the ability to leverage functional and object oriented programming styles. Scala code compiles to Java \nbyte code\n giving Scala complete interoperability with Java, i.e. you can use Java libraries and classes in Scala code.\n\n\nThe \nJava Virtual Machine\n which executes \nbyte code\n is the run time for the complete Java ecosystem. This enables Scala, Java, Groovy and Clojure programs to run on a common platform, which is a boon for Machine Learning applications as we can leverage all the libraries in the Java ecosystem.\n\n\nLearning Scala can be a significant investment as the language has a large number of features which require varying levels of skill and practice to master. Some resources for learning Scala are given below.\n\n\nCourses\n\u00b6\n\n\n\n\nFunctional Programming Principles with Scala\n by Martin Odersky.\n\n\nFunctional Program Design in Scala\n by Martin Odersky.\n\n\n\n\nVideos/Youtube\n\u00b6\n\n\n\n\nScala tutorials playlist\n\n\n\n\nBlogs\n\u00b6\n\n\n\n\nHaoyi's Programming Blog\n\n\nScala News\n\n\nTypelevel Blog\n\n\nCodacy Blog",
            "title": "Resources for Beginners"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#machine-learning",
            "text": "Machine Learning  refers to the ability to make predictions and decisions from data. It is a field that has evolved from the study of  pattern recognition  and  computational learning theory  in artificial intelligence.  Machine Learning theory and applications lie at the intersection of a number of concepts in the domains of Mathematics, Physics and Computer Science. It is no surprise that machine learning is a rich and deep domain with much intellectual and practical rewards to offer to the persistent and observant student.  The following is a non-exhaustive list of educational resources for learning ML.",
            "title": "Machine Learning"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#online-courses",
            "text": "Andrew Ng's famous  course  on  Coursera .  Intro to Machine Learning  at  Udacity  Machine Learning : MIT Open Course Ware",
            "title": "Online Courses"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#videosyoutube",
            "text": "Machine Learning Playlist  by  mathematicalmonk  Machine Learning Course  by  caltech",
            "title": "Videos/Youtube"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#books",
            "text": "Bayesian Reasoning and Machine Learning  by David Barber  Machine Learning: A Probabilistic Perspective  by Kevin P. Murphy  Pattern Recognition and Machine Learning  by Christopher Bishop  Understanding Machine Learning: From Theory to Algorithms  by Shai Shalev-Shwartz and Shai Ben-David  The Elements of Statistical Learning: Data Mining, Inference, and Prediction  by Trevor Hastie, Robert Tibshirani and Jerome Friedman",
            "title": "Books"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#forums",
            "text": "How do I learn machine learning  on  Quora",
            "title": "Forums"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#blogs",
            "text": "Deepmind Blog  Cortana Blog  Shakir Mohammad's Blog  Darren Wilkinson's research blog  John's Langford's Blog .  Kyle Kastner's Blog  Sander Dieleman's Blog",
            "title": "Blogs"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#programming-environment-scala",
            "text": "Scala  is the implementation language for DynaML, it is a hybrid language which gives the user the ability to leverage functional and object oriented programming styles. Scala code compiles to Java  byte code  giving Scala complete interoperability with Java, i.e. you can use Java libraries and classes in Scala code.  The  Java Virtual Machine  which executes  byte code  is the run time for the complete Java ecosystem. This enables Scala, Java, Groovy and Clojure programs to run on a common platform, which is a boon for Machine Learning applications as we can leverage all the libraries in the Java ecosystem.  Learning Scala can be a significant investment as the language has a large number of features which require varying levels of skill and practice to master. Some resources for learning Scala are given below.",
            "title": "Programming Environment: Scala"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#courses",
            "text": "Functional Programming Principles with Scala  by Martin Odersky.  Functional Program Design in Scala  by Martin Odersky.",
            "title": "Courses"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#videosyoutube_1",
            "text": "Scala tutorials playlist",
            "title": "Videos/Youtube"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#blogs_1",
            "text": "Haoyi's Programming Blog  Scala News  Typelevel Blog  Codacy Blog",
            "title": "Blogs"
        }
    ]
}