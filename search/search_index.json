{
    "docs": [
        {
            "location": "/",
            "text": "DynaML is a Scala environment for conducting research and education in Machine Learning. DynaML comes packaged with a powerful library of classes for various predictive models and a Scala REPL where one can not only build custom models but also play around with data work-flows.\n\n\n\n\n\n\nHello World\n\u00b6\n\n\nRefer to the \ninstallation\n guide for\ngetting up and running. Start the DynaML shell.\n\n\n\n\n\n\nThe \ndata/\n directory contains data sets, which are used by the programs in the \ndynaml-examples/\n module. Lets run a Gaussian Process (GP) regression model on the synthetic 'delve' data set.\n\n\n1\nTestGPDelve\n(\n\"RBF\"\n,\n \n2.0\n,\n \n1.0\n,\n \n500\n,\n \n1000\n)\n\n\n\n\n\n\n\n\n\nIn this example \nTestGPDelve\n we train a GP model based on the RBF Kernel with its bandwidth/length scale set to \n2.0\n and the noise level set to \n1.0\n, we use 500 input output patterns to train and test on an independent sample of 1000 data points. Apart from printing a bunch of evaluation metrics in the console DynaML also generates Javascript plots using Wisp in the browser.",
            "title": "DynaML"
        },
        {
            "location": "/#hello-world",
            "text": "Refer to the  installation  guide for\ngetting up and running. Start the DynaML shell.    The  data/  directory contains data sets, which are used by the programs in the  dynaml-examples/  module. Lets run a Gaussian Process (GP) regression model on the synthetic 'delve' data set.  1 TestGPDelve ( \"RBF\" ,   2.0 ,   1.0 ,   500 ,   1000 )     In this example  TestGPDelve  we train a GP model based on the RBF Kernel with its bandwidth/length scale set to  2.0  and the noise level set to  1.0 , we use 500 input output patterns to train and test on an independent sample of 1000 data points. Apart from printing a bunch of evaluation metrics in the console DynaML also generates Javascript plots using Wisp in the browser.",
            "title": "Hello World"
        },
        {
            "location": "/installation/installation/",
            "text": "Platform Compatibility\n\u00b6\n\n\nCurrently DynaML installs and runs on *nix platforms, though it is possible to build the project on windows, running the generated .bat file might not work and one would need to resort to using the \njava -jar\n command.\n\n\nPre-requisites\n\u00b6\n\n\n\n\nsbt\n\n\nA modern HTML5 enabled browser (to view plots generated by Wisp)\n\n\nBLAS, LAPACK and ARPACK binaries for your platform. In case they are not installed, it is possible to disable this feature by commenting out (\n//\n) the section of the build.sbt file given below.\n\n\n\n\n1\n  \n\"org.scalanlp\"\n \n%\n \n\"breeze-natives_2.11\"\n \n%\n \n\"0.11.2\"\n \n%\n \n\"compile\"\n,\n\n\n\n\n\n\n\n\n\nNote\n\n\nTensorFlow Nvidia GPU support\n\n\nIf you want to use Nvidia GPU acceleration when DynaML calls TensorFlow, you must\nbuild DynaML with a small modification to \nproject/Dependencies.scala\n\n\n1\nval\n \ngpuFlag\n:\n \nBoolean\n \n=\n \ntrue\n\n\n\n\n\n\n\nThe rest of the procedure for installation remains the same as shown below. But\nin order for your GPU to be actually utilised, you must take care to confirm that.\n\n\n\n\nYour Nvidia GPU is \ncompliant\n, \n   i.e. has a compute capability of atleast 4.1\n\n\nCuda v9.0 is installed on your system and its installation location \n   is added to \nLD_LIBRARY_PATH\n bash variable.\n\n\ncuDNN v7 is installed on your system and is appended to \nLD_LIBRARY_PATH\n\n\n\n\nUsing user compiled TensorFlow\n\n\nBy default, DynaML uses the Tensorflow dynamic library bundled with TensorFlow Scala distribution,\n but it is possible for the user to build Tensorflow from source for their platform. In this \n situation, you can make another modification to \nproject/Dependencies.scala\n.\n\n\n1\nval\n \npackagedTFFlag\n:\n \nBoolean\n \n=\n \nfalse\n\n\n\n\n\n\n\nIn most applications, the bundled TensorFlow is adequate for user requirements, and since building it\n from source is a time consuming affair, its advisable for users to start with the bundled TensorFlow\n library and compile it themselves only if absolutely necessary.\n\n\n\n\nSteps\n\u00b6\n\n\n\n\nClone this repository\n\n\n\n\nRun the following.\n\n\n1\nsbt\n\n\n\n\n\n\nThe sbt shell will open\n\n\n1\n2\n3\n[\ninfo\n]\n Loading project definition from ~/DynaML/project\n\n[\ninfo\n]\n Set current project to DynaML\n>\n\n\n\n\n\n\n\n\n\n\nBuild the source\n\n\n1\nstage\n\n\n\n\n\n\n\n\n\n\nAfter the project builds, exit from the sbt console and execute the DynaML start script from the bash shell. Make sure you have execute permissions on the DynaML start script.\n\n\n1\n./target/universal/stage/bin/dynaml\n\n\n\n\n\n\nYou will get the following prompt.\n\n\n1\n2\n3\n4\nWelcome\n \nto\n \nDynaML\n \nv1\n.\n4.1\n-\nbeta\n.\n3\n\n\nInteractive\n \nScala\n \nshell\n \nfor\n \nMachine\n \nLearning\n \nResearch\n\n\n(\nScala\n \n2.11\n.\n8\n \nJava\n \n1.8\n.\n0\n_101\n)\n\n\nDynaML\n>\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nUsing the install script\n\n\nIf you have already installed the \npre-requisites\n, \nyou can use the \ninstall.sh\n to build DynaML. It will add the a symlink to the\ndynaml\n \nexecutable in the \n/usr/local/bin\n directory. \n\n\nYou will need \nsudo\n access for \ninstall.sh\n to run correctly.\n\n\n1\n./install.sh",
            "title": "Installation"
        },
        {
            "location": "/installation/installation/#platform-compatibility",
            "text": "Currently DynaML installs and runs on *nix platforms, though it is possible to build the project on windows, running the generated .bat file might not work and one would need to resort to using the  java -jar  command.",
            "title": "Platform Compatibility"
        },
        {
            "location": "/installation/installation/#pre-requisites",
            "text": "sbt  A modern HTML5 enabled browser (to view plots generated by Wisp)  BLAS, LAPACK and ARPACK binaries for your platform. In case they are not installed, it is possible to disable this feature by commenting out ( // ) the section of the build.sbt file given below.   1    \"org.scalanlp\"   %   \"breeze-natives_2.11\"   %   \"0.11.2\"   %   \"compile\" ,     Note  TensorFlow Nvidia GPU support  If you want to use Nvidia GPU acceleration when DynaML calls TensorFlow, you must\nbuild DynaML with a small modification to  project/Dependencies.scala  1 val   gpuFlag :   Boolean   =   true    The rest of the procedure for installation remains the same as shown below. But\nin order for your GPU to be actually utilised, you must take care to confirm that.   Your Nvidia GPU is  compliant , \n   i.e. has a compute capability of atleast 4.1  Cuda v9.0 is installed on your system and its installation location \n   is added to  LD_LIBRARY_PATH  bash variable.  cuDNN v7 is installed on your system and is appended to  LD_LIBRARY_PATH   Using user compiled TensorFlow  By default, DynaML uses the Tensorflow dynamic library bundled with TensorFlow Scala distribution,\n but it is possible for the user to build Tensorflow from source for their platform. In this \n situation, you can make another modification to  project/Dependencies.scala .  1 val   packagedTFFlag :   Boolean   =   false    In most applications, the bundled TensorFlow is adequate for user requirements, and since building it\n from source is a time consuming affair, its advisable for users to start with the bundled TensorFlow\n library and compile it themselves only if absolutely necessary.",
            "title": "Pre-requisites"
        },
        {
            "location": "/installation/installation/#steps",
            "text": "Clone this repository   Run the following.  1 sbt   The sbt shell will open  1\n2\n3 [ info ]  Loading project definition from ~/DynaML/project [ info ]  Set current project to DynaML\n>     Build the source  1 stage     After the project builds, exit from the sbt console and execute the DynaML start script from the bash shell. Make sure you have execute permissions on the DynaML start script.  1 ./target/universal/stage/bin/dynaml   You will get the following prompt.  1\n2\n3\n4 Welcome   to   DynaML   v1 . 4.1 - beta . 3  Interactive   Scala   shell   for   Machine   Learning   Research  ( Scala   2.11 . 8   Java   1.8 . 0 _101 )  DynaML >       Tip  Using the install script  If you have already installed the  pre-requisites , \nyou can use the  install.sh  to build DynaML. It will add the a symlink to the dynaml  \nexecutable in the  /usr/local/bin  directory.   You will need  sudo  access for  install.sh  to run correctly.  1 ./install.sh",
            "title": "Steps"
        },
        {
            "location": "/installation/include/",
            "text": "Maven\n\u00b6\n\n\nTo include DynaML in your maven JVM project edit your \npom.xml\n file as follows\n\n\n1\n2\n3\n4\n5\n6\n<repositories>\n\n   \n<repository>\n\n       \n<id>\njitpack.io\n</id>\n\n       \n<url>\nhttps://jitpack.io\n</url>\n\n     \n</repository>\n\n\n</repositories>\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n<dependency>\n\n    \n<groupId>\ncom.github.transcendent-ai-labs\n</groupId>\n\n    \n<artifactId>\nDynaML\n</artifactId>\n\n    \n<version>\nv1.4\n</version>\n\n\n</dependency>\n\n\n\n\n\n\n\nSBT\n\u00b6\n\n\nFor sbt projects edit your \nbuild.sbt\n (see \nJitPack\n for more details)\n\n\n1\n2\n    \nresolvers\n \n+=\n \n\"jitpack\"\n \nat\n \n\"https://jitpack.io\"\n\n    \nlibraryDependencies\n \n+=\n \n\"com.github.transcendent-ai-labs\"\n \n%\n \n\"DynaML\"\n \n%\n \nversion\n\n\n\n\n\n\n\nGradle\n\u00b6\n\n\nIn your gradle project, add the following to the root \nbuild.gradle\n as follows\n\n\n1\n2\n3\n4\n5\n6\nallprojects\n \n{\n\n  \nrepositories\n \n{\n\n    \n...\n\n    \nmaven\n \n{\n \nurl\n \n\"https://jitpack.io\"\n \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\n\nand then add the dependency like\n\n\n1\n2\n3\ndependencies\n \n{\n\n    \ncompile\n \n'com.github.User:Repo:Tag'\n\n\n}\n\n\n\n\n\n\n\nLeinengen\n\u00b6\n\n\nIn \nproject.clj\n\n\n1\n:repositories\n \n[[\n\"jitpack\"\n \n\"https://jitpack.io\"\n]]\n\n\n\n\n\n\n\n1\n:dependencies\n \n[[\ncom.github.User/Repo\n \n\"Tag\"\n]]",
            "title": "Import"
        },
        {
            "location": "/installation/include/#maven",
            "text": "To include DynaML in your maven JVM project edit your  pom.xml  file as follows  1\n2\n3\n4\n5\n6 <repositories> \n    <repository> \n        <id> jitpack.io </id> \n        <url> https://jitpack.io </url> \n      </repository>  </repositories>    1\n2\n3\n4\n5 <dependency> \n     <groupId> com.github.transcendent-ai-labs </groupId> \n     <artifactId> DynaML </artifactId> \n     <version> v1.4 </version>  </dependency>",
            "title": "Maven"
        },
        {
            "location": "/installation/include/#sbt",
            "text": "For sbt projects edit your  build.sbt  (see  JitPack  for more details)  1\n2      resolvers   +=   \"jitpack\"   at   \"https://jitpack.io\" \n     libraryDependencies   +=   \"com.github.transcendent-ai-labs\"   %   \"DynaML\"   %   version",
            "title": "SBT"
        },
        {
            "location": "/installation/include/#gradle",
            "text": "In your gradle project, add the following to the root  build.gradle  as follows  1\n2\n3\n4\n5\n6 allprojects   { \n   repositories   { \n     ... \n     maven   {   url   \"https://jitpack.io\"   } \n   }  }    and then add the dependency like  1\n2\n3 dependencies   { \n     compile   'com.github.User:Repo:Tag'  }",
            "title": "Gradle"
        },
        {
            "location": "/installation/include/#leinengen",
            "text": "In  project.clj  1 :repositories   [[ \"jitpack\"   \"https://jitpack.io\" ]]    1 :dependencies   [[ com.github.User/Repo   \"Tag\" ]]",
            "title": "Leinengen"
        },
        {
            "location": "/supported_features/",
            "text": "Summary\n\n\n\"If you're not sure whether DynaML fits your requirements, this list provides a semi-comprehensive overview of available features.\"\n\n\n\n\nModels\n\u00b6\n\n\n\n\n\n\n\n\nModel Family\n\n\nSupported\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Models\n\n\nYes\n\n\nSupports regularized least squares based models for regression as well as logistic and probit models for classification.\n\n\n\n\n\n\nGeneralized Least Squares Models\n\n\nYes\n\n\n-\n\n\n\n\n\n\nLeast Squares Support Vector Machines\n\n\nYes\n\n\nContains implementation of dual LS-SVM applied to classification and regression.\n\n\n\n\n\n\nGaussian Processes\n\n\nYes\n\n\nSupports gaussian process inference models for regression and binary classification; the binary classification GP implementation uses the Laplace approximation for posterior mode computation. For regression problems, there are also multi-output and multi-task GP implementations.\n\n\n\n\n\n\nStudent T Processes\n\n\nYes\n\n\nSupports student T process inference models for regression, there are also multi-output and multi-task STP implementations.\n\n\n\n\n\n\nMulti-output Matrix T Process\n\n\nYes\n\n\n_\n\n\n\n\n\n\nSkew Gaussian Processes\n\n\nYes\n\n\nSupports extended skew gaussian process inference models for regression.\n\n\n\n\n\n\nFeed forward Neural Networks\n\n\nYes\n\n\nCan build and learn feedforward neural nets of various sizes.\n\n\n\n\n\n\nCommittee/Meta Models\n\n\nYes\n\n\nSupports creation of gating networks or committee models.\n\n\n\n\n\n\n\n\nOptimizers & Solvers\n\u00b6\n\n\nParametric Solvers\n\u00b6\n\n\n\n\n\n\n\n\nSolver\n\n\nSupported\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nRegularized Least Squares\n\n\nYes\n\n\nSolves the \nTikhonov Regularization\n problem exactly (not suitable for large data sets)\n\n\n\n\n\n\nGradient Descent\n\n\nYes\n\n\nStochastic and batch gradient descent is implemented.\n\n\n\n\n\n\nQuasi-Newton BFGS\n\n\nYes\n\n\nSecond order convex optimization (using Hessian).\n\n\n\n\n\n\nConjugate Gradient\n\n\nYes\n\n\nSupports solving of linear systems of type \nA.x = b\nA.x = b\n where \nA\nA\n is a symmetric positive definite matrix.\n\n\n\n\n\n\nCommittee Model Solver\n\n\nYes\n\n\nSolves any committee based model to calculate member model coefficients or confidences.\n\n\n\n\n\n\nBack-propagation\n\n\nYes\n\n\nLeast squares based back-propagation with momentum and regularization.\n\n\n\n\n\n\n\n\nGlobal Optimization Solvers\n\u00b6\n\n\n\n\n\n\n\n\nSolver\n\n\nSupported\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nGrid Search\n\n\nYes\n\n\nSimple search over a grid of configurations.\n\n\n\n\n\n\nCoupled Simulated Annealing\n\n\nYes\n\n\nSupports vanilla (simulated annealing) along with variants of CSA such as CSA with variance (temperature) control.\n\n\n\n\n\n\nML-II\n\n\nYes\n\n\nGradient based optimization of log marginal likelihood in Gaussian Process regression models.",
            "title": "Supported Features"
        },
        {
            "location": "/supported_features/#models",
            "text": "Model Family  Supported  Notes      Generalized Linear Models  Yes  Supports regularized least squares based models for regression as well as logistic and probit models for classification.    Generalized Least Squares Models  Yes  -    Least Squares Support Vector Machines  Yes  Contains implementation of dual LS-SVM applied to classification and regression.    Gaussian Processes  Yes  Supports gaussian process inference models for regression and binary classification; the binary classification GP implementation uses the Laplace approximation for posterior mode computation. For regression problems, there are also multi-output and multi-task GP implementations.    Student T Processes  Yes  Supports student T process inference models for regression, there are also multi-output and multi-task STP implementations.    Multi-output Matrix T Process  Yes  _    Skew Gaussian Processes  Yes  Supports extended skew gaussian process inference models for regression.    Feed forward Neural Networks  Yes  Can build and learn feedforward neural nets of various sizes.    Committee/Meta Models  Yes  Supports creation of gating networks or committee models.",
            "title": "Models"
        },
        {
            "location": "/supported_features/#optimizers-solvers",
            "text": "",
            "title": "Optimizers &amp; Solvers"
        },
        {
            "location": "/supported_features/#parametric-solvers",
            "text": "Solver  Supported  Notes      Regularized Least Squares  Yes  Solves the  Tikhonov Regularization  problem exactly (not suitable for large data sets)    Gradient Descent  Yes  Stochastic and batch gradient descent is implemented.    Quasi-Newton BFGS  Yes  Second order convex optimization (using Hessian).    Conjugate Gradient  Yes  Supports solving of linear systems of type  A.x = b A.x = b  where  A A  is a symmetric positive definite matrix.    Committee Model Solver  Yes  Solves any committee based model to calculate member model coefficients or confidences.    Back-propagation  Yes  Least squares based back-propagation with momentum and regularization.",
            "title": "Parametric Solvers"
        },
        {
            "location": "/supported_features/#global-optimization-solvers",
            "text": "Solver  Supported  Notes      Grid Search  Yes  Simple search over a grid of configurations.    Coupled Simulated Annealing  Yes  Supports vanilla (simulated annealing) along with variants of CSA such as CSA with variance (temperature) control.    ML-II  Yes  Gradient based optimization of log marginal likelihood in Gaussian Process regression models.",
            "title": "Global Optimization Solvers"
        },
        {
            "location": "/structure/",
            "text": "Motivation\n\u00b6\n\n\nDynaML was born out of the need to have a performant, extensible and easy to use Machine Learning research environment. Scala was a natural choice for these requirements due to its sprawling data science ecosystem (i.e. \nApache Spark\n), its functional object-oriented duality and its interoperability with the Java Virtual Machine.\n\n\nThe DynaML distribution is divided into four principal modules.\n\n\nModules\n\u00b6\n\n\nCore\n\u00b6\n\n\nThe heart of the DynaML distribution is the \ndynaml-core\n module.\n\n\nThe \ncore\n api consists of :\n\n\n\n\nModel implementations\n\n\nParametric Models\n\n\nStochastic Process Models\n\n\n\n\n\n\nOptimization solvers\n\n\nProbability distributions/random variables\n\n\nKernel functions for Non parametric models\n\n\n\n\nData workflows & Pipes\n\u00b6\n\n\nThe \ndynaml-pipes\n module provides an API for creating modular data processing workflows.\n\n\nThe \npipes\n module aims to separate model pre-processing tasks such as cleaning data files, replacing missing or corrupt records, applying transformations on data etc.\n\n\n\n\nAbility to create arbitrary workflows from scala functions and join them\n\n\nFeature transformations such as wavelet transform, gaussian scaling, auto-encoders etc\n\n\n\n\nDynaML REPL\n\u00b6\n\n\nThe \nread evaluate print loop\n (REPL) gives the user the ability to experiment with the data pre-processing and model building process in a mix and match fashion. The DynaML shell is based on the \nAmmonite\n project which is an augmented Scala REPL, all the features of the Ammonite REPL are a part of the DynaML REPL.\n\n\nDynaML Examples\n\u00b6\n\n\nThe module \ndynaml-examples\n contains programs which build regression and classification models on various data sets. These examples serve as case studies as well as instructional material to show the capabilities of DynaML in a hands on manner. Click \nhere\n to get started with the examples.\n\n\nLibraries Used\n\u00b6\n\n\nDynaML leverages a number of open source projects and builds on their useful features.\n\n\n\n\nBreeze\n for linear algebra operations with vectors, matrices etc.\n\n\nGremlin\n for building graphs in Neural network based models.\n\n\nSpire\n for creating algebraic entities like Fields, Groups etc.\n\n\nAmmonite\n for the shell environment.\n\n\nDynaML uses the newly minted \nWisp\n plotting library to generate aesthetic charts of common model validation metrics. There is also support for the  \nJZY3D\n scientific plotting library.",
            "title": "Structure"
        },
        {
            "location": "/structure/#motivation",
            "text": "DynaML was born out of the need to have a performant, extensible and easy to use Machine Learning research environment. Scala was a natural choice for these requirements due to its sprawling data science ecosystem (i.e.  Apache Spark ), its functional object-oriented duality and its interoperability with the Java Virtual Machine.  The DynaML distribution is divided into four principal modules.",
            "title": "Motivation"
        },
        {
            "location": "/structure/#modules",
            "text": "",
            "title": "Modules"
        },
        {
            "location": "/structure/#core",
            "text": "The heart of the DynaML distribution is the  dynaml-core  module.  The  core  api consists of :   Model implementations  Parametric Models  Stochastic Process Models    Optimization solvers  Probability distributions/random variables  Kernel functions for Non parametric models",
            "title": "Core"
        },
        {
            "location": "/structure/#data-workflows-pipes",
            "text": "The  dynaml-pipes  module provides an API for creating modular data processing workflows.  The  pipes  module aims to separate model pre-processing tasks such as cleaning data files, replacing missing or corrupt records, applying transformations on data etc.   Ability to create arbitrary workflows from scala functions and join them  Feature transformations such as wavelet transform, gaussian scaling, auto-encoders etc",
            "title": "Data workflows &amp; Pipes"
        },
        {
            "location": "/structure/#dynaml-repl",
            "text": "The  read evaluate print loop  (REPL) gives the user the ability to experiment with the data pre-processing and model building process in a mix and match fashion. The DynaML shell is based on the  Ammonite  project which is an augmented Scala REPL, all the features of the Ammonite REPL are a part of the DynaML REPL.",
            "title": "DynaML REPL"
        },
        {
            "location": "/structure/#dynaml-examples",
            "text": "The module  dynaml-examples  contains programs which build regression and classification models on various data sets. These examples serve as case studies as well as instructional material to show the capabilities of DynaML in a hands on manner. Click  here  to get started with the examples.",
            "title": "DynaML Examples"
        },
        {
            "location": "/structure/#libraries-used",
            "text": "DynaML leverages a number of open source projects and builds on their useful features.   Breeze  for linear algebra operations with vectors, matrices etc.  Gremlin  for building graphs in Neural network based models.  Spire  for creating algebraic entities like Fields, Groups etc.  Ammonite  for the shell environment.  DynaML uses the newly minted  Wisp  plotting library to generate aesthetic charts of common model validation metrics. There is also support for the   JZY3D  scientific plotting library.",
            "title": "Libraries Used"
        },
        {
            "location": "/releases/mydoc_release_notes_153/",
            "text": "Version 1.5.3 of DynaML, released August 14, 2017, introduces a new API for handling data sets. It also\nfeatures greater TensorFlow related integrations, notably the Inception v2 cell.\n\n\n\n\nAdditions\n\u00b6\n\n\nData Set API\n\u00b6\n\n\nThe \nDataSet\n family of classes helps the user to create and transform potentially large number of data instances. \nUsers can create and perform complex transformations on data sets, using the \nDataPipe\n API or simple Scala functions.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\nimport\n \n_root_.io.github.mandar2812.dynaml.probability._\n\n\nimport\n \n_root_.io.github.mandar2812.dynaml.pipes._\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\n\n\nval\n \nrandom_numbers\n \n=\n \nGaussianRV\n(\n0.0\n,\n \n1.0\n)\n \n:*\n \nGaussianRV\n(\n1.0\n,\n \n2.0\n)\n \n\n\n//Create a data set.\n\n\nval\n \ndataset1\n \n=\n \ndtfdata\n.\ndataset\n(\nrandom_numbers\n.\niid\n(\n10000\n).\ndraw\n)\n \n\n\nval\n \nfilter_gr_zero\n \n=\n \nDataPipe\n[(\nDouble\n, \nDouble\n)\n, \nBoolean\n](\n\n  \nc\n \n=>\n \nc\n.\n_1\n \n>\n \n0\nd\n \n&&\n \nc\n.\n_2\n \n>\n \n0\nd\n\n\n)\n\n\n\n//Filter elements\n\n\nval\n \ndata_gr_zero\n \n=\n \ndataset1\n.\nfilter\n(\nfilter_gr_zero\n)\n\n\n\nval\n \nabs_func\n:\n \n(\nDouble\n,\n \nDouble\n)\n \n=>\n \n(\nDouble\n,\n \nDouble\n)\n \n=\n \n  \n(\nc\n:\n \n(\nDouble\n,\n \nDouble\n))\n \n=>\n \n(\nmath\n.\nabs\n(\nc\n.\n_1\n),\n \nmath\n.\nabs\n(\nc\n.\n_2\n))\n\n\n\n//Map elements\n\n\nval\n \ndata_abs\n     \n=\n \ndataset1\n.\nmap\n(\nabs_func\n)\n\n\n\n\n\n\n\nFind out more about the \nDataSet\n API and its capabilities in the \nuser guide\n.\n\n\nTensorflow Integration\n\u00b6\n\n\nPackage\n \ndynaml.tensorflow\n \n\n\nBatch Normalisation\n\u00b6\n\n\nBatch normalisation\n is used to standardize activations of convolutional layers and \nto speed up training of deep neural nets.\n\n\nUsage\n\n\n1\n2\n3\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\n\nval\n \nbn\n \n=\n \ndtflearn\n.\nbatch_norm\n(\n\"BatchNorm1\"\n)\n\n\n\n\n\n\n\nInception v2\n\u00b6\n\n\nThe \nInception\n architecture, proposed by Google is an important \nbuilding block of \nconvolutional neural network\n architectures used in vision applications.\n\n\n\n\nIn a subsequent \npaper\n, the authors introduced optimizations in the Inception \narchitecture, known colloquially as \nInception v2\n.\n\n\nIn \nInception v2\n, larger convolutions (i.e. \n3 x 3\n and \n5 x 5\n) are implemented in a factorized manner \nto reduce the number of parameters to be learned. For example the \n3 x 3\n convolution is expressed as a \ncombination of \n1 x 3\n and \n3 x 1\n convolutions.\n\n\n\n\nSimilarly the \n5 x 5\n convolutions can be expressed a combination of two \n3 x 3\n convolutions\n\n\n\n\nDynaML now offers the Inception cell as a computational layer. \n\n\nUsage\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nimport\n \nio.github.mandar2812.dynaml.pipes._\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n\n//Create an RELU activation, given a string name/identifier.\n\n\nval\n \nrelu_act\n \n=\n \nDataPipe\n(\ntf\n.\nlearn\n.\nReLU\n(\n_\n))\n\n\n\n//Learn 10 filters in each branch of the inception cell\n\n\nval\n \nfilters\n \n=\n \nSeq\n(\n10\n,\n \n10\n,\n \n10\n,\n \n10\n)\n\n\n\nval\n \ninception_cell\n \n=\n \ndtflearn\n.\ninception_unit\n(\n\n  \nchannels\n \n=\n \n3\n,\n  \nnum_filters\n \n=\n \nfilters\n,\n \nrelu_act\n,\n\n  \n//Apply batch normalisation after each convolution\n\n  \nuse_batch_norm\n \n=\n \ntrue\n)(\nlayer_index\n \n=\n \n1\n)\n\n\n\n\n\n\n\nDynamical Systems: Continuous Time RNN\n\u00b6\n\n\nContinuous time recurrent neural networks (CTRNN) are an important class of recurrent neural networks. They enable \nthe modelling of non-linear and potentially complex dynamical systems of multiple variables, with feedback.\n\n\n\n\n\n\nAdded CTRNN layer: \ndtflearn.ctrnn\n\n\n\n\n\n\nAdded CTRNN layer with inferable time step: \ndtflearn.dctrnn\n.\n\n\n\n\n\n\nAdded a projection layer for CTRNN based models \ndtflearn.ts_linear\n.\n\n\n\n\n\n\nTraining Stopping Criteria\n\n\nCreate common and simple training stop criteria such as.\n\n\n\n\n\n\nStop after fixed number of iterations \ndtflearn.max_iter_stop(100000)\n\n\n\n\n\n\nStop after change in value of loss goes below a threshold. \ndtflearn.abs_loss_change_stop(0.0001)\n\n\n\n\n\n\nStop after change in relative value of loss goes below a threshold. \ndtflearn.rel_loss_change_stop(0.001)\n\n\n\n\n\n\nNeural Network Building Blocks\n \n\n\n\n\nAdded helper method \ndtlearn.build_tf_model()\n for training tensorflow models/estimators.\n\n\n\n\nUsage\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n \nimport\n \norg.platanios.tensorflow.api._\n\n \nimport\n \norg.platanios.tensorflow.data.image.MNISTLoader\n\n \nimport\n \nammonite.ops._\n\n\n\nval\n \ntempdir\n \n=\n \nhome\n/\n\"tmp\"\n\n\n\nval\n \ndataSet\n \n=\n \nMNISTLoader\n.\nload\n(\n\n  \njava\n.\nnio\n.\nfile\n.\nPaths\n.\nget\n(\ntempdir\n.\ntoString\n())\n\n\n)\n\n\n\nval\n \ntrainImages\n \n=\n \ntf\n.\ndata\n.\nTensorSlicesDataset\n(\ndataSet\n.\ntrainImages\n)\n\n\nval\n \ntrainLabels\n \n=\n \ntf\n.\ndata\n.\nTensorSlicesDataset\n(\ndataSet\n.\ntrainLabels\n)\n\n\n\nval\n \ntrainData\n \n=\n\n  \ntrainImages\n.\nzip\n(\ntrainLabels\n)\n\n    \n.\nrepeat\n()\n\n    \n.\nshuffle\n(\n10000\n)\n\n    \n.\nbatch\n(\n256\n)\n\n    \n.\nprefetch\n(\n10\n)\n\n\n\n// Create the MLP model.\n\n\nval\n \ninput\n \n=\n \ntf\n.\nlearn\n.\nInput\n(\n\n  \nUINT8\n,\n \n  \nShape\n(\n\n    \n-\n1\n,\n \n    \ndataSet\n.\ntrainImages\n.\nshape\n(\n1\n),\n \n    \ndataSet\n.\ntrainImages\n.\nshape\n(\n2\n))\n\n\n)\n\n\n\nval\n \ntrainInput\n \n=\n \ntf\n.\nlearn\n.\nInput\n(\nUINT8\n,\n \nShape\n(-\n1\n))\n\n\n\nval\n \narchitecture\n \n=\n \ntf\n.\nlearn\n.\nFlatten\n(\n\"Input/Flatten\"\n)\n \n>>\n \n  \ntf\n.\nlearn\n.\nCast\n(\n\"Input/Cast\"\n,\n \nFLOAT32\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nLinear\n(\n\"Layer_0/Linear\"\n,\n \n128\n)\n \n>>\n  \n  \ntf\n.\nlearn\n.\nReLU\n(\n\"Layer_0/ReLU\"\n,\n \n0.1f\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nLinear\n(\n\"Layer_1/Linear\"\n,\n \n64\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nReLU\n(\n\"Layer_1/ReLU\"\n,\n \n0.1f\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nLinear\n(\n\"Layer_2/Linear\"\n,\n \n32\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nReLU\n(\n\"Layer_2/ReLU\"\n,\n \n0.1f\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nLinear\n(\n\"OutputLayer/Linear\"\n,\n \n10\n)\n\n\n\nval\n \ntrainingInputLayer\n \n=\n \ntf\n.\nlearn\n.\nCast\n(\n\"TrainInput/Cast\"\n,\n \nINT64\n)\n\n\n\nval\n \nloss\n \n=\n\n  \ntf\n.\nlearn\n.\nSparseSoftmaxCrossEntropy\n(\n\"Loss/CrossEntropy\"\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nMean\n(\n\"Loss/Mean\"\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nScalarSummary\n(\n\"Loss/Summary\"\n,\n \n\"Loss\"\n)\n\n\n\nval\n \noptimizer\n \n=\n \ntf\n.\ntrain\n.\nAdaGrad\n(\n0.1\n)\n\n\n\n// Directory in which to save summaries and checkpoints\n\n\nval\n \nsummariesDir\n \n=\n \njava\n.\nnio\n.\nfile\n.\nPaths\n.\nget\n(\n\n  \n(\ntempdir\n/\n\"mnist_summaries\"\n).\ntoString\n()\n\n\n)\n\n\n\n\nval\n \n(\nmodel\n,\n \nestimator\n)\n \n=\n \ndtflearn\n.\nbuild_tf_model\n(\n\n  \narchitecture\n,\n \ninput\n,\n \ntrainInput\n,\n \ntrainingInputLayer\n,\n\n  \nloss\n,\n \noptimizer\n,\n \nsummariesDir\n,\n \ndtflearn\n.\nmax_iter_stop\n(\n1000\n),\n\n  \n100\n,\n \n100\n,\n \n100\n)(\ntrainData\n)\n\n\n\n\n\n\n\n\n\nBuild feedforward layers and feedforward layer stacks easier.\n\n\n\n\nUsage\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n\n//Create a single feedforward layer\n\n\nval\n \nlayer\n \n=\n \ndtflearn\n.\nfeedforward\n(\nnum_units\n \n=\n \n10\n,\n \nuseBias\n \n=\n \ntrue\n)(\nid\n \n=\n \n1\n)\n\n\n\n//Create a stack of feedforward layers\n\n\n\n\nval\n \nnet_layer_sizes\n \n=\n \nSeq\n(\n10\n,\n \n5\n,\n \n3\n)\n\n\n\nval\n \nstack\n \n=\n \ndtflearn\n.\nfeedforward_stack\n(\n\n   \n(\ni\n:\n \nInt\n)\n \n=>\n \ndtflearn\n.\nPhi\n(\n\"Act_\"\n+\ni\n),\n \nFLOAT64\n)(\n\n   \nnet_layer_sizes\n)\n\n\n\n\n\n\n\n3D Graphics\n\u00b6\n\n\nPackage\n \ndynaml.graphics\n\n\nCreate 3d plots of surfaces, for a use case, see the \njzydemo.sc\n and \ntf_wave_pde.sc\n\n\nLibrary Organisation\n\u00b6\n\n\n\n\nRemoved the \ndynaml-notebook\n module.\n\n\n\n\nBug Fixes\n\u00b6\n\n\n\n\nFixed bug related to \nscalar\n method of \nVectorField\n, \ninnerProdDouble\n and other inner product implementations.\n\n\n\n\nImprovements and Upgrades\n\u00b6\n\n\n\n\nBumped up Ammonite version to 1.1.0\n\n\nRegressionMetrics\n and \nRegressionMetricsTF\n now also compute Spearman rank correlation as\n    one of the performance metrics.\n\n\n\n\nChanges\n\u00b6",
            "title": "v1.5.3"
        },
        {
            "location": "/releases/mydoc_release_notes_153/#additions",
            "text": "",
            "title": "Additions"
        },
        {
            "location": "/releases/mydoc_release_notes_153/#data-set-api",
            "text": "The  DataSet  family of classes helps the user to create and transform potentially large number of data instances. \nUsers can create and perform complex transformations on data sets, using the  DataPipe  API or simple Scala functions.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 import   _root_.io.github.mandar2812.dynaml.probability._  import   _root_.io.github.mandar2812.dynaml.pipes._  import   io.github.mandar2812.dynaml.tensorflow._  val   random_numbers   =   GaussianRV ( 0.0 ,   1.0 )   :*   GaussianRV ( 1.0 ,   2.0 )   //Create a data set.  val   dataset1   =   dtfdata . dataset ( random_numbers . iid ( 10000 ). draw )   val   filter_gr_zero   =   DataPipe [( Double ,  Double ) ,  Boolean ]( \n   c   =>   c . _1   >   0 d   &&   c . _2   >   0 d  )  //Filter elements  val   data_gr_zero   =   dataset1 . filter ( filter_gr_zero )  val   abs_func :   ( Double ,   Double )   =>   ( Double ,   Double )   =  \n   ( c :   ( Double ,   Double ))   =>   ( math . abs ( c . _1 ),   math . abs ( c . _2 ))  //Map elements  val   data_abs       =   dataset1 . map ( abs_func )    Find out more about the  DataSet  API and its capabilities in the  user guide .",
            "title": "Data Set API"
        },
        {
            "location": "/releases/mydoc_release_notes_153/#tensorflow-integration",
            "text": "Package   dynaml.tensorflow",
            "title": "Tensorflow Integration"
        },
        {
            "location": "/releases/mydoc_release_notes_153/#batch-normalisation",
            "text": "Batch normalisation  is used to standardize activations of convolutional layers and \nto speed up training of deep neural nets.  Usage  1\n2\n3 import   io.github.mandar2812.dynaml.tensorflow._  val   bn   =   dtflearn . batch_norm ( \"BatchNorm1\" )",
            "title": "Batch Normalisation"
        },
        {
            "location": "/releases/mydoc_release_notes_153/#inception-v2",
            "text": "The  Inception  architecture, proposed by Google is an important \nbuilding block of  convolutional neural network  architectures used in vision applications.   In a subsequent  paper , the authors introduced optimizations in the Inception \narchitecture, known colloquially as  Inception v2 .  In  Inception v2 , larger convolutions (i.e.  3 x 3  and  5 x 5 ) are implemented in a factorized manner \nto reduce the number of parameters to be learned. For example the  3 x 3  convolution is expressed as a \ncombination of  1 x 3  and  3 x 1  convolutions.   Similarly the  5 x 5  convolutions can be expressed a combination of two  3 x 3  convolutions   DynaML now offers the Inception cell as a computational layer.   Usage   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 import   io.github.mandar2812.dynaml.pipes._  import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  //Create an RELU activation, given a string name/identifier.  val   relu_act   =   DataPipe ( tf . learn . ReLU ( _ ))  //Learn 10 filters in each branch of the inception cell  val   filters   =   Seq ( 10 ,   10 ,   10 ,   10 )  val   inception_cell   =   dtflearn . inception_unit ( \n   channels   =   3 ,    num_filters   =   filters ,   relu_act , \n   //Apply batch normalisation after each convolution \n   use_batch_norm   =   true )( layer_index   =   1 )",
            "title": "Inception v2"
        },
        {
            "location": "/releases/mydoc_release_notes_153/#dynamical-systems-continuous-time-rnn",
            "text": "Continuous time recurrent neural networks (CTRNN) are an important class of recurrent neural networks. They enable \nthe modelling of non-linear and potentially complex dynamical systems of multiple variables, with feedback.    Added CTRNN layer:  dtflearn.ctrnn    Added CTRNN layer with inferable time step:  dtflearn.dctrnn .    Added a projection layer for CTRNN based models  dtflearn.ts_linear .    Training Stopping Criteria  Create common and simple training stop criteria such as.    Stop after fixed number of iterations  dtflearn.max_iter_stop(100000)    Stop after change in value of loss goes below a threshold.  dtflearn.abs_loss_change_stop(0.0001)    Stop after change in relative value of loss goes below a threshold.  dtflearn.rel_loss_change_stop(0.001)    Neural Network Building Blocks     Added helper method  dtlearn.build_tf_model()  for training tensorflow models/estimators.   Usage   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61 import   io.github.mandar2812.dynaml.tensorflow._ \n  import   org.platanios.tensorflow.api._ \n  import   org.platanios.tensorflow.data.image.MNISTLoader \n  import   ammonite.ops._  val   tempdir   =   home / \"tmp\"  val   dataSet   =   MNISTLoader . load ( \n   java . nio . file . Paths . get ( tempdir . toString ())  )  val   trainImages   =   tf . data . TensorSlicesDataset ( dataSet . trainImages )  val   trainLabels   =   tf . data . TensorSlicesDataset ( dataSet . trainLabels )  val   trainData   = \n   trainImages . zip ( trainLabels ) \n     . repeat () \n     . shuffle ( 10000 ) \n     . batch ( 256 ) \n     . prefetch ( 10 )  // Create the MLP model.  val   input   =   tf . learn . Input ( \n   UINT8 ,  \n   Shape ( \n     - 1 ,  \n     dataSet . trainImages . shape ( 1 ),  \n     dataSet . trainImages . shape ( 2 ))  )  val   trainInput   =   tf . learn . Input ( UINT8 ,   Shape (- 1 ))  val   architecture   =   tf . learn . Flatten ( \"Input/Flatten\" )   >>  \n   tf . learn . Cast ( \"Input/Cast\" ,   FLOAT32 )   >> \n   tf . learn . Linear ( \"Layer_0/Linear\" ,   128 )   >>   \n   tf . learn . ReLU ( \"Layer_0/ReLU\" ,   0.1f )   >> \n   tf . learn . Linear ( \"Layer_1/Linear\" ,   64 )   >> \n   tf . learn . ReLU ( \"Layer_1/ReLU\" ,   0.1f )   >> \n   tf . learn . Linear ( \"Layer_2/Linear\" ,   32 )   >> \n   tf . learn . ReLU ( \"Layer_2/ReLU\" ,   0.1f )   >> \n   tf . learn . Linear ( \"OutputLayer/Linear\" ,   10 )  val   trainingInputLayer   =   tf . learn . Cast ( \"TrainInput/Cast\" ,   INT64 )  val   loss   = \n   tf . learn . SparseSoftmaxCrossEntropy ( \"Loss/CrossEntropy\" )   >> \n   tf . learn . Mean ( \"Loss/Mean\" )   >> \n   tf . learn . ScalarSummary ( \"Loss/Summary\" ,   \"Loss\" )  val   optimizer   =   tf . train . AdaGrad ( 0.1 )  // Directory in which to save summaries and checkpoints  val   summariesDir   =   java . nio . file . Paths . get ( \n   ( tempdir / \"mnist_summaries\" ). toString ()  )  val   ( model ,   estimator )   =   dtflearn . build_tf_model ( \n   architecture ,   input ,   trainInput ,   trainingInputLayer , \n   loss ,   optimizer ,   summariesDir ,   dtflearn . max_iter_stop ( 1000 ), \n   100 ,   100 ,   100 )( trainData )     Build feedforward layers and feedforward layer stacks easier.   Usage   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  //Create a single feedforward layer  val   layer   =   dtflearn . feedforward ( num_units   =   10 ,   useBias   =   true )( id   =   1 )  //Create a stack of feedforward layers  val   net_layer_sizes   =   Seq ( 10 ,   5 ,   3 )  val   stack   =   dtflearn . feedforward_stack ( \n    ( i :   Int )   =>   dtflearn . Phi ( \"Act_\" + i ),   FLOAT64 )( \n    net_layer_sizes )",
            "title": "Dynamical Systems: Continuous Time RNN"
        },
        {
            "location": "/releases/mydoc_release_notes_153/#3d-graphics",
            "text": "Package   dynaml.graphics  Create 3d plots of surfaces, for a use case, see the  jzydemo.sc  and  tf_wave_pde.sc",
            "title": "3D Graphics"
        },
        {
            "location": "/releases/mydoc_release_notes_153/#library-organisation",
            "text": "Removed the  dynaml-notebook  module.",
            "title": "Library Organisation"
        },
        {
            "location": "/releases/mydoc_release_notes_153/#bug-fixes",
            "text": "Fixed bug related to  scalar  method of  VectorField ,  innerProdDouble  and other inner product implementations.",
            "title": "Bug Fixes"
        },
        {
            "location": "/releases/mydoc_release_notes_153/#improvements-and-upgrades",
            "text": "Bumped up Ammonite version to 1.1.0  RegressionMetrics  and  RegressionMetricsTF  now also compute Spearman rank correlation as\n    one of the performance metrics.",
            "title": "Improvements and Upgrades"
        },
        {
            "location": "/releases/mydoc_release_notes_153/#changes",
            "text": "",
            "title": "Changes"
        },
        {
            "location": "/releases/mydoc_release_notes_152/",
            "text": "Version 1.5.2 of DynaML, released March 5, 2017, introduces functionality through improvement in the pipes API and increased integration with Tensorflow.\n\n\n\n\nAdditions\n\u00b6\n\n\nTensorflow Integration\n\u00b6\n\n\n\n\nTensorflow (beta) support now live, thanks to the \ntensorflow_scala\n project! Try it out in:\n\n\nCIFAR-10\n example script\n\n\nMNIST\n example script\n\n\n\n\n\n\n\n\nPackage\n \ndynaml.tensorflow\n\n\n\n\nThe \ndtf\n package object houses utility functions related to tensorflow primitives. Currently supports creation of tensors from arrays.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n//Create a FLOAT32 Tensor of shape (2, 2), i.e. a square matrix\n\n\nval\n \nmat\n \n=\n \ndtf\n.\ntensor_f32\n(\n2\n,\n \n2\n)(\n1\nd\n,\n \n2\nd\n,\n \n3\nd\n,\n \n4\nd\n)\n\n\n\n//Create a random 2 * 3 matrix with independent standard normal entries\n\n\nval\n \nrand_mat\n \n=\n \ndtf\n.\nrandom\n(\nFLOAT32\n,\n \n2\n,\n \n3\n)(\n\n  \nGaussianRV\n(\n0\nd\n,\n \n1\nd\n)\n \n>\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n.\ntoFloat\n)\n\n\n)\n\n\n\n//Multiply matrices\n\n\nval\n \nprod\n \n=\n \nmat\n.\nmatmul\n(\nrand_mat\n)\n\n\nprintln\n(\nprod\n.\nsummarize\n())\n\n\n\nval\n \nanother_rand_mat\n \n=\n \ndtf\n.\nrandom\n(\nFLOAT32\n,\n \n2\n,\n \n3\n)(\n\n  \nGaussianRV\n(\n0\nd\n,\n \n1\nd\n)\n \n>\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n.\ntoFloat\n)\n\n\n)\n\n\n\n//Stack tensors vertically, i.e. row wise\n\n\nval\n \nvert_tensor\n \n=\n \ndtf\n.\nstack\n(\nSeq\n(\nrand_mat\n,\n \nanother_rand_mat\n),\n \naxis\n \n=\n \n0\n)\n\n\n//Stack vectors horizontally, i.e. column wise\n\n\nval\n \nhorz_tensor\n \n=\n \ndtf\n.\nstack\n(\nSeq\n(\nrand_mat\n,\n \nanother_rand_mat\n),\n \naxis\n \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\nThe \ndtflearn\n package object deals with basic neural network building blocks which are often needed while constructing prediction architectures.\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n//Create a simple neural architecture with one convolutional layer \n\n\n//followed by a max pool and feedforward layer  \n\n\nval\n \nnet\n \n=\n \ntf\n.\nlearn\n.\nCast\n(\n\"Input/Cast\"\n,\n \nFLOAT32\n)\n \n>>\n\n  \ndtflearn\n.\nconv2d_pyramid\n(\n2\n,\n \n3\n)(\n4\n,\n \n2\n)(\n0.1f\n,\n \ntrue\n,\n \n0.6F\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nMaxPool\n(\n\"Layer_3/MaxPool\"\n,\n \nSeq\n(\n1\n,\n \n2\n,\n \n2\n,\n \n1\n),\n \n1\n,\n \n1\n,\n \nSamePadding\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nFlatten\n(\n\"Layer_3/Flatten\"\n)\n \n>>\n\n  \ndtflearn\n.\nfeedforward\n(\n256\n)(\nid\n \n=\n \n4\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nReLU\n(\n\"Layer_4/ReLU\"\n,\n \n0.1f\n)\n \n>>\n\n  \ndtflearn\n.\nfeedforward\n(\n10\n)(\nid\n \n=\n \n5\n)\n\n\n\n\n\n\n\nLibrary Organisation\n\u00b6\n\n\n\n\nAdded \ndynaml-repl\n and \ndynaml-notebook\n modules to repository.\n\n\n\n\nDynaML Server\n\u00b6\n\n\n\n\nDynaML ssh server now available (only in Local mode)\n   \n1\n$ ./target/universal/stage/bin/dynaml --server\n\n\n\n\n   To login to the server open a separate shell and type, (when prompted for password, just press ENTER)\n   \n1\n$ ssh repl@localhost -p22222\n\n\n\n\n\n\n\nBasis Generators\n\u00b6\n\n\n\n\nLegrendre polynomial basis generators\n\n\n\n\nBugfixes\n\u00b6\n\n\n\n\nAcceptance rule of \nHyperParameterMCMC\n and related classes.\n\n\n\n\nChanges\n\u00b6\n\n\n\n\nIncreased pretty printing to screen instead of logging.\n\n\n\n\nCleanup\n\u00b6\n\n\nPackage\n \ndynaml.models.svm\n\n - Removal of deprecated model classes from \nsvm\n package",
            "title": "v1.5.2"
        },
        {
            "location": "/releases/mydoc_release_notes_152/#additions",
            "text": "",
            "title": "Additions"
        },
        {
            "location": "/releases/mydoc_release_notes_152/#tensorflow-integration",
            "text": "Tensorflow (beta) support now live, thanks to the  tensorflow_scala  project! Try it out in:  CIFAR-10  example script  MNIST  example script     Package   dynaml.tensorflow   The  dtf  package object houses utility functions related to tensorflow primitives. Currently supports creation of tensors from arrays.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  //Create a FLOAT32 Tensor of shape (2, 2), i.e. a square matrix  val   mat   =   dtf . tensor_f32 ( 2 ,   2 )( 1 d ,   2 d ,   3 d ,   4 d )  //Create a random 2 * 3 matrix with independent standard normal entries  val   rand_mat   =   dtf . random ( FLOAT32 ,   2 ,   3 )( \n   GaussianRV ( 0 d ,   1 d )   >   DataPipe (( x :   Double )   =>   x . toFloat )  )  //Multiply matrices  val   prod   =   mat . matmul ( rand_mat )  println ( prod . summarize ())  val   another_rand_mat   =   dtf . random ( FLOAT32 ,   2 ,   3 )( \n   GaussianRV ( 0 d ,   1 d )   >   DataPipe (( x :   Double )   =>   x . toFloat )  )  //Stack tensors vertically, i.e. row wise  val   vert_tensor   =   dtf . stack ( Seq ( rand_mat ,   another_rand_mat ),   axis   =   0 )  //Stack vectors horizontally, i.e. column wise  val   horz_tensor   =   dtf . stack ( Seq ( rand_mat ,   another_rand_mat ),   axis   =   1 )     The  dtflearn  package object deals with basic neural network building blocks which are often needed while constructing prediction architectures.   1\n2\n3\n4\n5\n6\n7\n8\n9 //Create a simple neural architecture with one convolutional layer   //followed by a max pool and feedforward layer    val   net   =   tf . learn . Cast ( \"Input/Cast\" ,   FLOAT32 )   >> \n   dtflearn . conv2d_pyramid ( 2 ,   3 )( 4 ,   2 )( 0.1f ,   true ,   0.6F )   >> \n   tf . learn . MaxPool ( \"Layer_3/MaxPool\" ,   Seq ( 1 ,   2 ,   2 ,   1 ),   1 ,   1 ,   SamePadding )   >> \n   tf . learn . Flatten ( \"Layer_3/Flatten\" )   >> \n   dtflearn . feedforward ( 256 )( id   =   4 )   >> \n   tf . learn . ReLU ( \"Layer_4/ReLU\" ,   0.1f )   >> \n   dtflearn . feedforward ( 10 )( id   =   5 )",
            "title": "Tensorflow Integration"
        },
        {
            "location": "/releases/mydoc_release_notes_152/#library-organisation",
            "text": "Added  dynaml-repl  and  dynaml-notebook  modules to repository.",
            "title": "Library Organisation"
        },
        {
            "location": "/releases/mydoc_release_notes_152/#dynaml-server",
            "text": "DynaML ssh server now available (only in Local mode)\n    1 $ ./target/universal/stage/bin/dynaml --server  \n   To login to the server open a separate shell and type, (when prompted for password, just press ENTER)\n    1 $ ssh repl@localhost -p22222",
            "title": "DynaML Server"
        },
        {
            "location": "/releases/mydoc_release_notes_152/#basis-generators",
            "text": "Legrendre polynomial basis generators",
            "title": "Basis Generators"
        },
        {
            "location": "/releases/mydoc_release_notes_152/#bugfixes",
            "text": "Acceptance rule of  HyperParameterMCMC  and related classes.",
            "title": "Bugfixes"
        },
        {
            "location": "/releases/mydoc_release_notes_152/#changes",
            "text": "Increased pretty printing to screen instead of logging.",
            "title": "Changes"
        },
        {
            "location": "/releases/mydoc_release_notes_152/#cleanup",
            "text": "Package   dynaml.models.svm \n - Removal of deprecated model classes from  svm  package",
            "title": "Cleanup"
        },
        {
            "location": "/releases/mydoc_release_notes_151/",
            "text": "Version 1.5.1 of DynaML, released September 20, 2017, introduces bug fixes and some useful new features\n\n\n\n\nAdditions\n\u00b6\n\n\nPackage\n \ndynaml.probability.distributions\n\n\n\n\nAdded \nKumaraswamy\n distribution, an alternative to the Beta distribution.\n\n\nAdded \nErlang\n distribution, a special case of the Gamma distribution.\n\n\n\n\nPackage\n \ndynaml.analysis\n\n\n\n\n\n\nAdded Radial Basis Function generators.\n\n\n\n\nGaussian \n\n\nInverse Multi-Quadric\n\n\nMulti-Quadric\n\n\nMatern Half-Integer\n\n\n\n\n\n\n\n\nAdded an inner product space implementation for \nTuple2\n\n\n\n\n\n\nBug Fixes\n\u00b6\n\n\nPackage\n \ndynaml.kernels\n\n\n\n\nFixed bug concerning hyper-parameter blocking in \nCompositeCovariance\n and its children.\n\n\n\n\nPackage\n \ndynaml.probability.distributions\n\n\n\n\nFixed calculation error for normalisation constant of multivariate T and Gaussian family.",
            "title": "v1.5.1"
        },
        {
            "location": "/releases/mydoc_release_notes_151/#additions",
            "text": "Package   dynaml.probability.distributions   Added  Kumaraswamy  distribution, an alternative to the Beta distribution.  Added  Erlang  distribution, a special case of the Gamma distribution.   Package   dynaml.analysis    Added Radial Basis Function generators.   Gaussian   Inverse Multi-Quadric  Multi-Quadric  Matern Half-Integer     Added an inner product space implementation for  Tuple2",
            "title": "Additions"
        },
        {
            "location": "/releases/mydoc_release_notes_151/#bug-fixes",
            "text": "Package   dynaml.kernels   Fixed bug concerning hyper-parameter blocking in  CompositeCovariance  and its children.   Package   dynaml.probability.distributions   Fixed calculation error for normalisation constant of multivariate T and Gaussian family.",
            "title": "Bug Fixes"
        },
        {
            "location": "/releases/mydoc_release_notes_15/",
            "text": "Version 1.5 of DynaML, released August 11, 2017. Updates to global\noptimization api, improvements and new features in the gaussian\nprocess and stochastic process api.\n\n\n\n\nAdditions\n\u00b6\n\n\nPackage\n \ndynaml.algebra\n\n\n\n\nAdded support for \ndual numbers\n.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n//Zero Dual\n\n\nval\n \nzero\n \n=\n \nDualNumber\n.\nzero\n[\nDouble\n]\n  \n\n\nval\n \ndnum\n \n=\n \nDualNumber\n(\n1.5\n,\n \n-\n1.0\n)\n \n\nval\n \ndnum1\n \n=\n \nDualNumber\n(-\n1.5\n,\n \n1.0\n)\n \n\n\n//Algebraic operations: multiplication and addition/subtraction\n\n\ndnum1\n*\ndnum2\n\n\ndnum1\n \n-\n \ndnum\n\n\ndnum\n*\nzero\n \n\n\n\n\n\n\nPackage\n \ndynaml.probability\n\n\n\n\nAdded support for mixture distributions and mixture random variables. \nMixtureRV\n, \nContinuousDistrMixture\n for random variables and \nMixtureDistribution\n for constructing mixtures of breeze distributions.\n\n\n\n\nPackage\n \ndynaml.optimization\n\n\n\n\nAdded \nModelTuner[T, T1]\n trait as a super trait to \nGlobalOptimizer[T]\n\n\nGridSearch\n and \nCoupledSimulatedAnnealing\n now extend \nAbstractGridSearch\n and \nAbstractCSA\n respectively.\n\n\nAdded \nProbGPMixtureMachine\n: constructs a mixture model after a CSA or grid search routine by calculating the mixture probabilities of members of the final hyper-parameter ensemble.\n\n\n\n\nStochastic Mixture Models\n\u00b6\n\n\nPackage\n \ndynaml.models\n\n\n\n\nAdded \nStochasticProcessMixtureModel\nas top level class for stochastic mixture models.\n\n\nAdded \nGaussianProcessMixture\n: implementation of gaussian process\n   mixture models.\n\n\nAdded \nMVTMixture\n: implementation of mixture model over\n   multioutput matrix T processes.\n\n\n\n\nKulback-Leibler Divergence\n\u00b6\n\n\nPackage\n \ndynaml.probability\n\n\n\n\nAdded method \nKL()\n to \nprobability\n package object, to calculate\n    the Kulback Leibler divergence between two continuous random\n    variables backed by breeze distributions. \n\n\n\n\nAdaptive Metropolis Algorithms.\n\u00b6\n\n\n\n\n\n\nAdaptiveHyperParameterMCMC\n which\n   adapts the exploration covariance with each sample.\n\n\n\n\n\n\nHyperParameterSCAM\n adapts\n   the exploration covariance for each hyper-parameter independently.\n\n\n\n\n\n\nSplines and B-Spline Generators\n\u00b6\n\n\nPackage\n \ndynaml.analysis\n\n\n\n\nB-Spline generators\n\n\nBernstein\n b-spline generators.\n\n\nArbitrary spline functions can be created using the \nSplineGenerator\n class.\n\n\n\n\nCubic Spline Interpolation Kernels\n\u00b6\n\n\nPackage\n \ndynaml.kernels\n\n\n\n\nAdded cubic spline interpolation kernel \nCubicSplineKernel\n and its ARD analogue \nCubicSplineARDKernel\n \n\n\n\n\nGaussian Process Models for Linear Partial Differential Equations\n\u00b6\n\n\nBased on a legacy ICML 2003 paper by \nGraepel\n. DynaML now ships with capability of performing PDE forward and inverse inference using the Gaussian Process API.\n\n\nPackage\n \ndynaml.models.gp\n\n\n\n\nGPOperatorModel\n: models a quantity of interest which is governed by a linear PDE in space and time.\n\n\n\n\nPackage\n \ndynaml.kernels\n\n\n\n\n\n\nLinearPDEKernel\n: The core kernel primitive accepted by the \nGPOperatorModel\n class.\n\n\n\n\n\n\nGenExpSpaceTimeKernel\n: a kernel of the exponential family which can serve as a handy base kernel for \nLinearPDEKernel\n class.\n\n\n\n\n\n\nBasis Function Gaussian Processes\n\u00b6\n\n\nDynaML now supports GP models with explicitly incorporated basis\nfunctions as linear mean/trend functions.\n\n\nPackage\n \ndynaml.models.gp\n\n\n\n\nGPBasisFuncRegressionModel\n can\n   be used to create GP models with trends incorporated as a linear\n   combination of basis functions.\n\n\n\n\nLog Gaussian Processes\n\u00b6\n\n\n\n\nLogGaussianProcessModel\n represents\n   a stochastic process whose natural logarithm follows a gaussian process.\n\n\n\n\nImprovements\n\u00b6\n\n\nPackage\n \ndynaml.probability\n\n\n\n\nChanges to \nRandomVarWithDistr\n: made type parameter \nDist\n covariant.\n\n\nReform to \nIIDRandomVar\n hierarchy.\n\n\n\n\nPackage\n \ndynaml.probability.mcmc\n\n\n\n\nBug-fixes to the \nHyperParameterMCMC\n class. \n\n\n\n\nGeneral\n\n\n\n\nDynaML now ships with Ammonite \nv1.0.0\n.\n\n\n\n\nFixes\n\u00b6\n\n\nPackage\n \ndynaml.optimization\n\n\n\n\nCorrected energy calculation in \nCoupledSimulatedAnnealing\n; added\n   log likelihood due to hyper-prior. \n\n\n\n\nPackage\n \ndynaml.optimization\n\n\n\n\nCorrected energy calculation in \nCoupledSimulatedAnnealing\n; added\n   log likelihood due to hyper-prior.",
            "title": "v1.5"
        },
        {
            "location": "/releases/mydoc_release_notes_15/#additions",
            "text": "Package   dynaml.algebra   Added support for  dual numbers .    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 //Zero Dual  val   zero   =   DualNumber . zero [ Double ]    val   dnum   =   DualNumber ( 1.5 ,   - 1.0 )   val   dnum1   =   DualNumber (- 1.5 ,   1.0 )   //Algebraic operations: multiplication and addition/subtraction  dnum1 * dnum2  dnum1   -   dnum  dnum * zero     Package   dynaml.probability   Added support for mixture distributions and mixture random variables.  MixtureRV ,  ContinuousDistrMixture  for random variables and  MixtureDistribution  for constructing mixtures of breeze distributions.   Package   dynaml.optimization   Added  ModelTuner[T, T1]  trait as a super trait to  GlobalOptimizer[T]  GridSearch  and  CoupledSimulatedAnnealing  now extend  AbstractGridSearch  and  AbstractCSA  respectively.  Added  ProbGPMixtureMachine : constructs a mixture model after a CSA or grid search routine by calculating the mixture probabilities of members of the final hyper-parameter ensemble.",
            "title": "Additions"
        },
        {
            "location": "/releases/mydoc_release_notes_15/#stochastic-mixture-models",
            "text": "Package   dynaml.models   Added  StochasticProcessMixtureModel as top level class for stochastic mixture models.  Added  GaussianProcessMixture : implementation of gaussian process\n   mixture models.  Added  MVTMixture : implementation of mixture model over\n   multioutput matrix T processes.",
            "title": "Stochastic Mixture Models"
        },
        {
            "location": "/releases/mydoc_release_notes_15/#kulback-leibler-divergence",
            "text": "Package   dynaml.probability   Added method  KL()  to  probability  package object, to calculate\n    the Kulback Leibler divergence between two continuous random\n    variables backed by breeze distributions.",
            "title": "Kulback-Leibler Divergence"
        },
        {
            "location": "/releases/mydoc_release_notes_15/#adaptive-metropolis-algorithms",
            "text": "AdaptiveHyperParameterMCMC  which\n   adapts the exploration covariance with each sample.    HyperParameterSCAM  adapts\n   the exploration covariance for each hyper-parameter independently.",
            "title": "Adaptive Metropolis Algorithms."
        },
        {
            "location": "/releases/mydoc_release_notes_15/#splines-and-b-spline-generators",
            "text": "Package   dynaml.analysis   B-Spline generators  Bernstein  b-spline generators.  Arbitrary spline functions can be created using the  SplineGenerator  class.",
            "title": "Splines and B-Spline Generators"
        },
        {
            "location": "/releases/mydoc_release_notes_15/#cubic-spline-interpolation-kernels",
            "text": "Package   dynaml.kernels   Added cubic spline interpolation kernel  CubicSplineKernel  and its ARD analogue  CubicSplineARDKernel",
            "title": "Cubic Spline Interpolation Kernels"
        },
        {
            "location": "/releases/mydoc_release_notes_15/#gaussian-process-models-for-linear-partial-differential-equations",
            "text": "Based on a legacy ICML 2003 paper by  Graepel . DynaML now ships with capability of performing PDE forward and inverse inference using the Gaussian Process API.  Package   dynaml.models.gp   GPOperatorModel : models a quantity of interest which is governed by a linear PDE in space and time.   Package   dynaml.kernels    LinearPDEKernel : The core kernel primitive accepted by the  GPOperatorModel  class.    GenExpSpaceTimeKernel : a kernel of the exponential family which can serve as a handy base kernel for  LinearPDEKernel  class.",
            "title": "Gaussian Process Models for Linear Partial Differential Equations"
        },
        {
            "location": "/releases/mydoc_release_notes_15/#basis-function-gaussian-processes",
            "text": "DynaML now supports GP models with explicitly incorporated basis\nfunctions as linear mean/trend functions.  Package   dynaml.models.gp   GPBasisFuncRegressionModel  can\n   be used to create GP models with trends incorporated as a linear\n   combination of basis functions.",
            "title": "Basis Function Gaussian Processes"
        },
        {
            "location": "/releases/mydoc_release_notes_15/#log-gaussian-processes",
            "text": "LogGaussianProcessModel  represents\n   a stochastic process whose natural logarithm follows a gaussian process.",
            "title": "Log Gaussian Processes"
        },
        {
            "location": "/releases/mydoc_release_notes_15/#improvements",
            "text": "Package   dynaml.probability   Changes to  RandomVarWithDistr : made type parameter  Dist  covariant.  Reform to  IIDRandomVar  hierarchy.   Package   dynaml.probability.mcmc   Bug-fixes to the  HyperParameterMCMC  class.    General   DynaML now ships with Ammonite  v1.0.0 .",
            "title": "Improvements"
        },
        {
            "location": "/releases/mydoc_release_notes_15/#fixes",
            "text": "Package   dynaml.optimization   Corrected energy calculation in  CoupledSimulatedAnnealing ; added\n   log likelihood due to hyper-prior.    Package   dynaml.optimization   Corrected energy calculation in  CoupledSimulatedAnnealing ; added\n   log likelihood due to hyper-prior.",
            "title": "Fixes"
        },
        {
            "location": "/releases/mydoc_release_notes_143/",
            "text": "Version 1.4.3 of DynaML, released June 13, 2017. Updates, improvements and new features.\n\n\n\n\nDynaML REPL\n\u00b6\n\n\nAdditions\n\u00b6\n\n\nModule\n \nscripts\n\n\n\n\nAdded \ngp_mcmc_santafe.sc\n worksheet to try new MCMC feature on GP models; applied on the Santa Fe laser data set. \n\n\n\n\nGeneral\n\n\n\n\nUpdated Ammonite version to \n0.9.9\n\n\n\n\nPipes API\n\u00b6\n\n\nAdditions\n\u00b6\n\n\nPackage\n \ndynaml.pipes\n\n\n\n\nAdded \n._1\n and \n._2\n members in \nParallelPipe\n \n\n\n\n\nCore API\n\u00b6\n\n\nAdditions\n\u00b6\n\n\nPackage\n \ndynaml.models.neuralnets\n\n\n\n\nAdded \nSELU\n activation function proposed by \nHochreiter et. al\n\n\n\n\nPackage\n \ndynaml.models.bayes\n\n\n\n\nAdded \n*\n method to \nCoRegGPPrior\n which scales it with to a \nParallelPipe\n\n\n\n\nPackage\n \ndynaml.probability.mcmc\n\n\n\n\nAdded \nHyperParameterMCMC\n for performing MCMC sampling for models extending \nGloballyOptimizable\n.\n\n\n\n\nPackage\n \ndynaml.utils\n\n\n\n\n\n\nAdded trait \nHyperParameters\n outlining methods that must be implemented by entities having hyper-parameters\n\n\n\n\n\n\nAdded \nMeanScaler\n, \nPCAScaler\n to perform mean centering and PCA on data sets. Also added to \nDynaMLPipe\n pipe library.\n\n\n\n\n\n\nAdded tail recursive computation of the \nChebyshev\n polynomials of the first and second kind in \nutils\n.\nchebyshev\n. \n\n\n\n\n\n\nImprovements\n\u00b6\n\n\nPackage\n \ndynaml.models.bayes\n\n\n\n\nAdded \ntrendParamsEncoder\n which converts the trend/mean parameters into a scala \nMap\n[\nString\n, \nDouble\n]\n making them \n     consistent with covariance parameters. Added to \nGaussianProcessPrior\n and \nESGPPrior\n families.",
            "title": "v1.4.3"
        },
        {
            "location": "/releases/mydoc_release_notes_143/#dynaml-repl",
            "text": "",
            "title": "DynaML REPL"
        },
        {
            "location": "/releases/mydoc_release_notes_143/#additions",
            "text": "Module   scripts   Added  gp_mcmc_santafe.sc  worksheet to try new MCMC feature on GP models; applied on the Santa Fe laser data set.    General   Updated Ammonite version to  0.9.9",
            "title": "Additions"
        },
        {
            "location": "/releases/mydoc_release_notes_143/#pipes-api",
            "text": "",
            "title": "Pipes API"
        },
        {
            "location": "/releases/mydoc_release_notes_143/#additions_1",
            "text": "Package   dynaml.pipes   Added  ._1  and  ._2  members in  ParallelPipe",
            "title": "Additions"
        },
        {
            "location": "/releases/mydoc_release_notes_143/#core-api",
            "text": "",
            "title": "Core API"
        },
        {
            "location": "/releases/mydoc_release_notes_143/#additions_2",
            "text": "Package   dynaml.models.neuralnets   Added  SELU  activation function proposed by  Hochreiter et. al   Package   dynaml.models.bayes   Added  *  method to  CoRegGPPrior  which scales it with to a  ParallelPipe   Package   dynaml.probability.mcmc   Added  HyperParameterMCMC  for performing MCMC sampling for models extending  GloballyOptimizable .   Package   dynaml.utils    Added trait  HyperParameters  outlining methods that must be implemented by entities having hyper-parameters    Added  MeanScaler ,  PCAScaler  to perform mean centering and PCA on data sets. Also added to  DynaMLPipe  pipe library.    Added tail recursive computation of the  Chebyshev  polynomials of the first and second kind in  utils . chebyshev .",
            "title": "Additions"
        },
        {
            "location": "/releases/mydoc_release_notes_143/#improvements",
            "text": "Package   dynaml.models.bayes   Added  trendParamsEncoder  which converts the trend/mean parameters into a scala  Map [ String ,  Double ]  making them \n     consistent with covariance parameters. Added to  GaussianProcessPrior  and  ESGPPrior  families.",
            "title": "Improvements"
        },
        {
            "location": "/releases/mydoc_release_notes_142/",
            "text": "Version 1.4.2 of DynaML, released May 7, 2017. Updates, improvements and new features.\n\n\n\n\nCore API\n\u00b6\n\n\nAdditions\n\u00b6\n\n\nPackage\n \ndynaml\n.\nmodels\n.\nneuralnets\n\n\n\n\nAdded \nGenericAutoEncoder\n[\nLayerP\n, \nI\n]\n, the class \nAutoEncoder\n is now \ndeprecated\n\n\nAdded \nGenericNeuralStack\n[\nP\n, \nI\n, \nT\n]\n as a base class for Neural Stack API\n\n\nAdded \nLazyNeuralStack\n[\nP\n, \nI\n]\n where the layers are lazily spawned.\n\n\n\n\nPackage\n \ndynaml\n.\nkernels\n\n\n\n\nAdded \nScaledKernel\n[\nI\n]\n representing kernels of scaled Gaussian Processes.\n\n\n\n\nPackage\n \ndynaml\n.\nmodels\n.\nbayes\n\n\n\n\nAdded \n*\n method to \nGaussianProcessPrior\n[\nI\n, \nM\n]\n which creates a scaled Gaussian Process prior using the newly minted \nScaledKernel\n[\nI\n]\n class\n\n\nAdded Kronecker product GP priors with the \nCoRegGPPrior\n[\nI\n, \nJ\n, \nM\n]\n class\n\n\n\n\nPackage\n \ndynaml\n.\nmodels\n.\nstp\n\n\n\n\nAdded multi-output Students' T Regression model of \nConti & O' Hagan\n in class \nMVStudentsTModel\n\n\n\n\nPackage\n \ndynaml\n.\nprobability\n.\ndistributions\n\n\n\n\nAdded \nHasErrorBars\n[\nT\n]\n generic trait representing distributions which can generate confidence intervals around their mean value.\n\n\n\n\nImprovements\n\u00b6\n\n\nPackage\n \ndynaml\n.\nprobability\n\n\n\n\nFixed issue with creation of \nMeasurableFunction\n instances from \nRandomVariable\n instances\n\n\n\n\nPackage\n \ndynaml\n.\nprobability\n.\ndistributions\n\n\n\n\nChanged error bar calculations and sampling of Students T distributions (vector and matrix) and Matrix Normal distribution.\n\n\n\n\nPackage\n \ndynaml\n.\nmodels\n.\ngp\n\n\n\n\nAdded \nKronecker\n structure speed up to \nenergy\n (marginal likelihood) calculation of multi-output GP models\n\n\n\n\nPackage\n \ndynaml\n.\nkernels\n\n  - Improved implicit paramterization of Matern Covariance classes\n\n\nGeneral\n\n\n\n\nUpdated breeze version to latest.\n\n\nUpdated Ammonite version to latest",
            "title": "v1.4.2"
        },
        {
            "location": "/releases/mydoc_release_notes_142/#core-api",
            "text": "",
            "title": "Core API"
        },
        {
            "location": "/releases/mydoc_release_notes_142/#additions",
            "text": "Package   dynaml . models . neuralnets   Added  GenericAutoEncoder [ LayerP ,  I ] , the class  AutoEncoder  is now  deprecated  Added  GenericNeuralStack [ P ,  I ,  T ]  as a base class for Neural Stack API  Added  LazyNeuralStack [ P ,  I ]  where the layers are lazily spawned.   Package   dynaml . kernels   Added  ScaledKernel [ I ]  representing kernels of scaled Gaussian Processes.   Package   dynaml . models . bayes   Added  *  method to  GaussianProcessPrior [ I ,  M ]  which creates a scaled Gaussian Process prior using the newly minted  ScaledKernel [ I ]  class  Added Kronecker product GP priors with the  CoRegGPPrior [ I ,  J ,  M ]  class   Package   dynaml . models . stp   Added multi-output Students' T Regression model of  Conti & O' Hagan  in class  MVStudentsTModel   Package   dynaml . probability . distributions   Added  HasErrorBars [ T ]  generic trait representing distributions which can generate confidence intervals around their mean value.",
            "title": "Additions"
        },
        {
            "location": "/releases/mydoc_release_notes_142/#improvements",
            "text": "Package   dynaml . probability   Fixed issue with creation of  MeasurableFunction  instances from  RandomVariable  instances   Package   dynaml . probability . distributions   Changed error bar calculations and sampling of Students T distributions (vector and matrix) and Matrix Normal distribution.   Package   dynaml . models . gp   Added  Kronecker  structure speed up to  energy  (marginal likelihood) calculation of multi-output GP models   Package   dynaml . kernels \n  - Improved implicit paramterization of Matern Covariance classes  General   Updated breeze version to latest.  Updated Ammonite version to latest",
            "title": "Improvements"
        },
        {
            "location": "/releases/mydoc_release_notes_141/",
            "text": "Version 1.4.1 of DynaML, released March 26, 2017, implements a number of new models (Extended Skew GP, student T process, generalized least squares, etc) and features.\n\n\n\n\nPipes API\n\u00b6\n\n\nAdditions\n\u00b6\n\n\nThe pipes API has been vastly extended by creating pipes which encapsulate functions of multiple arguments leading to the following end points.\n\n\n\n\nDataPipe2\n[\nA\n, \nB\n, \nC\n]\n: Pipe which takes 2 arguments\n\n\nDataPipe3\n[\nA\n, \nB\n, \nC\n, \nD\n]\n : Pipe which takes 3 arguments\n\n\nDataPipe4\n[\nA\n, \nB\n, \nC\n, \nD\n, \nE\n]\n: Pipe which takes 4 arguments\n\n\n\n\nFurthermore there is now the ability to create pipes which return pipes, something akin to curried functions in functional programming.\n\n\n\n\nMetaPipe\n: Takes an argument returns a \nDataPipe\n\n\nMetaPipe21\n: Takes 2 arguments returns a \nDataPipe\n\n\nMetaPipe12\n: Takes an argument returns a \nDataPipe2\n\n\n\n\nA new kind of Stream data pipe, \nStreamFlatMapPipe\n is added to represent data pipelines which can perform flat map like operations on streams.\n\n\n1\n2\nval\n \nmapFunc\n:\n \n(\nI\n)\n \n=>\n \nStream\n[\nJ\n]\n \n=\n \n...\n\n\nval\n \nstreamFMPipe\n \n=\n \nStreamFlatMapPipe\n(\nmapFunc\n)\n\n\n\n\n\n\n\n\n\nAdded Data Pipes API for Apache Spark RDDs.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nval\n \nnum\n \n=\n \n20\n\n\nval\n \nnumbers\n \n=\n \nsc\n.\nparallelize\n(\n1\n \nto\n \nnum\n)\n\n\nval\n \nconvPipe\n \n=\n \nRDDPipe\n((\nn\n:\n \nInt\n)\n \n=>\n \nn\n.\ntoDouble\n)\n\n\n\nval\n \nsqPipe\n \n=\n \nRDDPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n*\nx\n)\n\n\n\nval\n \nsqrtPipe\n \n=\n \nRDDPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsqrt\n(\nx\n))\n\n\n\nval\n \nresultPipe\n \n=\n \nRDDPipe\n((\nr\n:\n \nRDD\n[\nDouble\n])\n \n=>\n \nr\n.\nreduce\n(\n_\n+\n_\n).\ntoInt\n)\n\n\n\nval\n \nnetPipeline\n \n=\n \nconvPipe\n \n>\n \nsqPipe\n \n>\n \nsqrtPipe\n \n>\n \nresultPipe\n\n\nnetPipeline\n(\nnumbers\n)\n\n\n\n\n\n\n\n\n\nAdded \nUnivariateGaussianScaler\n class for gaussian scaling of univariate data.\n\n\n\n\nCore API\n\u00b6\n\n\nAdditions\n\u00b6\n\n\nPackage\n \ndynaml\n.\nmodels\n.\nbayes\n\n\nThis new package will house stochastic prior models, currently there is support for GP and Skew GP priors, to see a starting example see \nstochasticPriors.sc\n in the \nscripts\n directory of the DynaML source.\n\n\n\n\nPackage\n \ndynaml\n.\nkernels\n\n\n\n\nAdded \nevaluateAt\n(\nh\n)(\nx\n,\ny\n)\n and \ngradientAt\n(\nh\n)(\nx\n,\ny\n)\n; expressing \nevaluate\n(\nx\n,\ny\n)\n and \ngradient\n(\nx\n,\ny\n)\n in terms of them\n\n\nAdded \nasPipe\n method for Covariance Functions\n\n\nFor backwards compatibility users are advised to extend\n   \nLocalSVMKernel\n in their custom Kernel implementations incase they do\n   not want to implement the \nevaluateAt\n API endpoints.\n\n\nAdded \nFeatureMapKernel\n, representing kernels which can be explicitly decomposed into feature mappings.\n\n\nAdded Matern half integer kernel \nGenericMaternKernel\n[\nI\n]\n\n\nAdded \nblock\n(\nS\n:\n \nString*\n)\n method to block any hyper-parameters of kernels.\n\n\nAdded \nNeuralNetworkKernel\n and \nGaussianSpectralKernel\n.\n\n\nAdded \nDecomposableCovariance\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nimport\n \nio.github.mandar2812.dynaml.DynaMLPipe._\n\n\nimport\n \nio.github.mandar2812.dynaml.kernels._\n\n\n\n\nimplicit\n \nval\n \nev\n \n=\n \nVectorField\n(\n6\n)\n\n\nimplicit\n \nval\n \nsp\n \n=\n \nbreezeDVSplitEncoder\n(\n2\n)\n\n\nimplicit\n \nval\n \nsumR\n \n=\n \nsumReducer\n\n\n\nval\n \nkernel\n \n=\n \nnew\n \nLaplacianKernel\n(\n1.5\n)\n\n\nval\n \nother_kernel\n \n=\n \nnew\n \nPolynomialKernel\n(\n1\n,\n \n0.05\n)\n\n\n\nval\n \ndecompKernel\n \n=\n \nnew\n \nDecomposableCovariance\n(\nkernel\n,\n \nother_kernel\n)(\nsp\n,\n \nsumReducer\n)\n\n\n\nval\n \nother_kernel1\n \n=\n \nnew\n \nFBMKernel\n(\n1.0\n)\n\n\n\nval\n \ndecompKernel1\n \n=\n \nnew\n \nDecomposableCovariance\n(\ndecompKernel\n,\n \nother_kernel1\n)(\nsp\n,\n \nsumReducer\n)\n\n\n\nval\n \nveca\n \n=\n \nDenseVector\n.\ntabulate\n[\nDouble\n](\n8\n)(\nmath\n.\nsin\n(\n_\n))\n\n\nval\n \nvecb\n \n=\n \nDenseVector\n.\ntabulate\n[\nDouble\n](\n8\n)(\nmath\n.\ncos\n(\n_\n))\n\n\n\ndecompKernel1\n.\nevaluate\n(\nveca\n,\n \nvecb\n)\n\n\n\n\n\n\n\n\n\nPackage\n \ndynaml\n.\nalgebra\n\n\nPartitioned Matrices/Vectors and the following operations\n\n\n\n\nAddition, Subtraction\n\n\nMatrix, vector multiplication\n\n\nLU, Cholesky\n\n\nA\n\\\ny\n, \nA\n\\\nY\n\n\n\n\nAdded calculation of quadratic forms, namely:\n\n\n\n\nquadraticForm\n which calculates \n\\mathbf{x}^\\intercal A^{-1} \\mathbf{x}\n\\mathbf{x}^\\intercal A^{-1} \\mathbf{x}\n\n\ncrossQuadraticForm\n which calculates \n\\mathbf{y}^\\intercal A^{-1} \\mathbf{x}\n\\mathbf{y}^\\intercal A^{-1} \\mathbf{x}\n\n\n\n\nWhere A is assumed to be a symmetric positive semi-definite matrix\n\n\nUsage:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nimport\n \nio.github.mandar2812.dynaml.algebra._\n\n\n\nval\n \nx\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \ny\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \na\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\n\nquadraticForm\n(\na\n,\nx\n)\n\n\ncrossQuadraticForm\n(\ny\n,\n \na\n,\n \nx\n)\n\n\n\n\n\n\n\n\n\nPackage\n \ndynaml\n.\nmodelpipe\n\n\nNew package created, moved all inheriting classes of \nModelPipe\n to this package.\n\n\nAdded the following:\n\n\n\n\nGLMPipe2\n A pipe taking two arguments and returning a \nGeneralizedLinearModel\n instance\n\n\nGeneralizedLeastSquaresPipe2\n:\n\n\nGeneralizedLeastSquaresPipe3\n:\n\n\n\n\n\n\nPackage\n \ndynaml\n.\nmodels\n\n\n\n\nAdded a new Neural Networks API: \nNeuralNet\n and \nGenericFFNeuralNet\n, for an example refer to \nTestNNDelve\n in \ndynaml\n-\nexamples\n.\n\n\nGeneralizedLeastSquaresModel\n: The  \nGLS\n model.\n\n\nESGPModel\n: The implementation of a skew gaussian process regression model\n\n\nWarped Gaussian Process\n models \nWIP\n\n\nAdded mean function capability to Gaussian Process and Student T process models.\n\n\nAdded Apache Spark implementation of Generalized Linear Models; see \nSparkGLM\n, \nSparkLogisticModel\n, \nSparkProbitGLM\n\n\n\n\n\n\nPackage\n \ndynaml.probability\n\n\n\n\nMultivariateSkewNormal\n as specified in \nAzzalani et. al\n\n\nExtendedMultivariateSkewNormal\n\n\nUESN\n and \nMESN\n representing an alternative formulation of the skew gaussian family from Adcock and Shutes.\n\n\nTruncatedGaussian\n: Truncated version of the Gaussian distribution.\n\n\nMatrix Normal Distribution\n\n\nAdded \nExpectation\n operator for \nRandomVariable\n implementations in the \nio\n.\ngithub\n.\nmandar2812\n.\ndynaml\n.\nprobability\n package object. Usage example given below.\n\n\nSkewGaussian\n, \nExtendedSkewGaussian\n: An breeze implementation of the SkewGaussian and extended Skew-Gaussian distributions respectively\n\n\nPushforwardMap\n, \nDifferentiableMap\n added: \nPushforwardMap\n enables creating new random variables with defined density from base random variables.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\nimport\n \nio.github.mandar2812.dynaml.analysis._\n\n\nimport\n \nio.github.mandar2812.dynaml.probability._\n\n\nimport\n \nio.github.mandar2812.dynaml.probability.distributions._\n\n\n\nval\n \ng\n \n=\n \nGaussianRV\n(\n0.0\n,\n \n0.25\n)\n\n\nval\n \nsg\n \n=\n \nRandomVariable\n(\nSkewGaussian\n(\n1.0\n,\n \n0.0\n,\n \n0.25\n))\n\n\n\n//Define a determinant implementation for the Jacobian type (Double in this case)\n\n\nimplicit\n \nval\n \ndetImpl\n \n=\n \nidentityPipe\n[\nDouble\n]\n\n\n\n//Defines a homeomorphism y = exp(x) x = log(y)\n\n\nval\n \nh\n:\n \nPushforwardMap\n[\nDouble\n, \nDouble\n, \nDouble\n]\n \n=\n \nPushforwardMap\n(\n\n  \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nexp\n(\nx\n)),\n\n  \nDifferentiableMap\n(\n\n    \n(\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nlog\n(\nx\n),\n\n    \n(\nx\n:\n \nDouble\n)\n \n=>\n \n1.0\n/\nx\n)\n\n\n)\n\n\n\n//Creates a log-normal random variable\n\n\nval\n \np\n \n=\n \nh\n->\ng\n\n\n\n//Creates a log-skew-gaussian random variable\n\n\nval\n \nq\n \n=\n \nh\n->\nsg\n\n\n\n//Calculate expectation of q\n\n\nprintln\n(\n\"E[Q] = \"\n+\nE\n(\nq\n))\n\n\n\n\n\n - Added \nMarkov Chain Monte Carlo\n (MCMC) based inference schemes \nContinuousMCMC\n and the underlying sampling implementation in \nGeneralMetropolisHastings\n.\n - Added implementation of \nApproximate Bayesian Computation\n (ABC) in the \nApproxBayesComputation\n class.\n\n\n1\n2\n3\n4\n5\n6\n7\n//The mean\n\n\nval\n \ncenter\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\n//Covariance (positive semi-def) matrix among rows\n\n\nval\n \nsigmaRows\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\n//Covariance (positive semi-def) matrix among columns\n\n\nval\n \nsigmaCols\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \nmatD\n \n=\n \nMatrixNormal\n(\ncenter\n,\n \nsigmaRows\n,\n \nsigmaCols\n)\n\n\n\n\n\n - \nMatrix T Distribution\n (\nExperimental\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n//The degrees of freedom (must be > 2.0 for existence of finite moments)\n\n\nval\n \nmu\n:\n \nDouble\n \n=\n \n...\n\n\n//The mean\n\n\nval\n \ncenter\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\n//Covariance (positive semi-def) matrix among rows\n\n\nval\n \nsigmaRows\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\n//Covariance (positive semi-def) matrix among columns\n\n\nval\n \nsigmaCols\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \nmatD\n \n=\n \nMatrixT\n(\nmu\n,\n \ncenter\n,\n \nsigmaCols\n,\n \nsigmaRows\n)\n\n\n\n\n\n\n\n\n\nPackage\n \ndynaml\n.\noptimization\n\n\n\n\nAdded \nProbGPCommMachine\n which performs grid search or CSA and then instead of selecting a single hyper-parameter configuration calculates a weighted Gaussian Process committee where the weights correspond to probabilities or confidence on each model instance (hyper-parameter configuration).\n\n\n\n\nPackage\n \ndynaml\n.\nutils\n\n\n\n\nAdded \nmultivariate gamma function\n\n\n\n\n1\n2\n//Returns logarithm of multivariate gamma function\n\n\nval\n \ng\n \n=\n \nmvlgamma\n(\n5\n,\n \n1.5\n)\n\n\n\n\n\n\n\n\n\nPackage\n \ndynaml\n.\ndataformat\n\n\n\n\nAdded support for reading \nMATLAB\n \n.mat\n files in the \nMAT\n object.\n\n\n\n\nImprovements/Bug Fixes\n\u00b6\n\n\nPackage\n \ndynaml\n.\nprobability\n\n\n\n\nRemoved \nProbabilityModel\n and replaced with \nJointProbabilityScheme\n and \nBayesJointProbabilityScheme\n, major refactoring to \nRandomVariable\n API.\n\n\n\n\n\n\nPackage\n \ndynaml\n.\noptimization\n\n\n\n\nImproved logging of \nCoupledSimulatedAnnealing\n\n\nRefactored \nGPMLOptimizer\n to \nGradBasedGlobalOptimizer\n\n\n\n\n\n\nPackage\n \ndynaml\n.\nutils\n\n - Correction to \nutils\n.\ngetStats\n method used for calculating mean and variance of data sets consisting of \nDenseVector\n[\nDouble\n]\n.\n - \nminMaxScalingTrainTest\n \nminMaxScaling\n of \nDynaMLPipe\n using \nGaussianScaler\n instead of \nMinMaxScaler\n for processing of features.\n\n\n\n\nPackage\n \ndynaml\n.\nkernels\n\n\n\n\nFix to \nCoRegCauchyKernel\n: corrected mismatch of hyper-parameter string\n\n\nFix to \nSVMKernel\n objects matrix gradient computation in the case when kernel dimensions are not multiples of block size.\n\n\nCorrection to gradient calculation in RBF kernel family.\n\n\nSpeed up of kernel gradient computation, kernel and kernel gradient matrices with respect to the model hyper-parameters now calculated in a single pass through the data.",
            "title": "v1.4.1"
        },
        {
            "location": "/releases/mydoc_release_notes_141/#pipes-api",
            "text": "",
            "title": "Pipes API"
        },
        {
            "location": "/releases/mydoc_release_notes_141/#additions",
            "text": "The pipes API has been vastly extended by creating pipes which encapsulate functions of multiple arguments leading to the following end points.   DataPipe2 [ A ,  B ,  C ] : Pipe which takes 2 arguments  DataPipe3 [ A ,  B ,  C ,  D ]  : Pipe which takes 3 arguments  DataPipe4 [ A ,  B ,  C ,  D ,  E ] : Pipe which takes 4 arguments   Furthermore there is now the ability to create pipes which return pipes, something akin to curried functions in functional programming.   MetaPipe : Takes an argument returns a  DataPipe  MetaPipe21 : Takes 2 arguments returns a  DataPipe  MetaPipe12 : Takes an argument returns a  DataPipe2   A new kind of Stream data pipe,  StreamFlatMapPipe  is added to represent data pipelines which can perform flat map like operations on streams.  1\n2 val   mapFunc :   ( I )   =>   Stream [ J ]   =   ...  val   streamFMPipe   =   StreamFlatMapPipe ( mapFunc )     Added Data Pipes API for Apache Spark RDDs.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 val   num   =   20  val   numbers   =   sc . parallelize ( 1   to   num )  val   convPipe   =   RDDPipe (( n :   Int )   =>   n . toDouble )  val   sqPipe   =   RDDPipe (( x :   Double )   =>   x * x )  val   sqrtPipe   =   RDDPipe (( x :   Double )   =>   math . sqrt ( x ))  val   resultPipe   =   RDDPipe (( r :   RDD [ Double ])   =>   r . reduce ( _ + _ ). toInt )  val   netPipeline   =   convPipe   >   sqPipe   >   sqrtPipe   >   resultPipe  netPipeline ( numbers )     Added  UnivariateGaussianScaler  class for gaussian scaling of univariate data.",
            "title": "Additions"
        },
        {
            "location": "/releases/mydoc_release_notes_141/#core-api",
            "text": "",
            "title": "Core API"
        },
        {
            "location": "/releases/mydoc_release_notes_141/#additions_1",
            "text": "Package   dynaml . models . bayes  This new package will house stochastic prior models, currently there is support for GP and Skew GP priors, to see a starting example see  stochasticPriors.sc  in the  scripts  directory of the DynaML source.   Package   dynaml . kernels   Added  evaluateAt ( h )( x , y )  and  gradientAt ( h )( x , y ) ; expressing  evaluate ( x , y )  and  gradient ( x , y )  in terms of them  Added  asPipe  method for Covariance Functions  For backwards compatibility users are advised to extend\n    LocalSVMKernel  in their custom Kernel implementations incase they do\n   not want to implement the  evaluateAt  API endpoints.  Added  FeatureMapKernel , representing kernels which can be explicitly decomposed into feature mappings.  Added Matern half integer kernel  GenericMaternKernel [ I ]  Added  block ( S :   String* )  method to block any hyper-parameters of kernels.  Added  NeuralNetworkKernel  and  GaussianSpectralKernel .  Added  DecomposableCovariance    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 import   io.github.mandar2812.dynaml.DynaMLPipe._  import   io.github.mandar2812.dynaml.kernels._  implicit   val   ev   =   VectorField ( 6 )  implicit   val   sp   =   breezeDVSplitEncoder ( 2 )  implicit   val   sumR   =   sumReducer  val   kernel   =   new   LaplacianKernel ( 1.5 )  val   other_kernel   =   new   PolynomialKernel ( 1 ,   0.05 )  val   decompKernel   =   new   DecomposableCovariance ( kernel ,   other_kernel )( sp ,   sumReducer )  val   other_kernel1   =   new   FBMKernel ( 1.0 )  val   decompKernel1   =   new   DecomposableCovariance ( decompKernel ,   other_kernel1 )( sp ,   sumReducer )  val   veca   =   DenseVector . tabulate [ Double ]( 8 )( math . sin ( _ ))  val   vecb   =   DenseVector . tabulate [ Double ]( 8 )( math . cos ( _ ))  decompKernel1 . evaluate ( veca ,   vecb )     Package   dynaml . algebra  Partitioned Matrices/Vectors and the following operations   Addition, Subtraction  Matrix, vector multiplication  LU, Cholesky  A \\ y ,  A \\ Y   Added calculation of quadratic forms, namely:   quadraticForm  which calculates  \\mathbf{x}^\\intercal A^{-1} \\mathbf{x} \\mathbf{x}^\\intercal A^{-1} \\mathbf{x}  crossQuadraticForm  which calculates  \\mathbf{y}^\\intercal A^{-1} \\mathbf{x} \\mathbf{y}^\\intercal A^{-1} \\mathbf{x}   Where A is assumed to be a symmetric positive semi-definite matrix  Usage:  1\n2\n3\n4\n5\n6\n7\n8 import   io.github.mandar2812.dynaml.algebra._  val   x :   DenseVector [ Double ]   =   ...  val   y :   DenseVector [ Double ]   =   ...  val   a :   DenseMatrix [ Double ]   =   ...  quadraticForm ( a , x )  crossQuadraticForm ( y ,   a ,   x )     Package   dynaml . modelpipe  New package created, moved all inheriting classes of  ModelPipe  to this package.  Added the following:   GLMPipe2  A pipe taking two arguments and returning a  GeneralizedLinearModel  instance  GeneralizedLeastSquaresPipe2 :  GeneralizedLeastSquaresPipe3 :    Package   dynaml . models   Added a new Neural Networks API:  NeuralNet  and  GenericFFNeuralNet , for an example refer to  TestNNDelve  in  dynaml - examples .  GeneralizedLeastSquaresModel : The   GLS  model.  ESGPModel : The implementation of a skew gaussian process regression model  Warped Gaussian Process  models  WIP  Added mean function capability to Gaussian Process and Student T process models.  Added Apache Spark implementation of Generalized Linear Models; see  SparkGLM ,  SparkLogisticModel ,  SparkProbitGLM    Package   dynaml.probability   MultivariateSkewNormal  as specified in  Azzalani et. al  ExtendedMultivariateSkewNormal  UESN  and  MESN  representing an alternative formulation of the skew gaussian family from Adcock and Shutes.  TruncatedGaussian : Truncated version of the Gaussian distribution.  Matrix Normal Distribution  Added  Expectation  operator for  RandomVariable  implementations in the  io . github . mandar2812 . dynaml . probability  package object. Usage example given below.  SkewGaussian ,  ExtendedSkewGaussian : An breeze implementation of the SkewGaussian and extended Skew-Gaussian distributions respectively  PushforwardMap ,  DifferentiableMap  added:  PushforwardMap  enables creating new random variables with defined density from base random variables.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 import   io.github.mandar2812.dynaml.analysis._  import   io.github.mandar2812.dynaml.probability._  import   io.github.mandar2812.dynaml.probability.distributions._  val   g   =   GaussianRV ( 0.0 ,   0.25 )  val   sg   =   RandomVariable ( SkewGaussian ( 1.0 ,   0.0 ,   0.25 ))  //Define a determinant implementation for the Jacobian type (Double in this case)  implicit   val   detImpl   =   identityPipe [ Double ]  //Defines a homeomorphism y = exp(x) x = log(y)  val   h :   PushforwardMap [ Double ,  Double ,  Double ]   =   PushforwardMap ( \n   DataPipe (( x :   Double )   =>   math . exp ( x )), \n   DifferentiableMap ( \n     ( x :   Double )   =>   math . log ( x ), \n     ( x :   Double )   =>   1.0 / x )  )  //Creates a log-normal random variable  val   p   =   h -> g  //Creates a log-skew-gaussian random variable  val   q   =   h -> sg  //Calculate expectation of q  println ( \"E[Q] = \" + E ( q ))   \n - Added  Markov Chain Monte Carlo  (MCMC) based inference schemes  ContinuousMCMC  and the underlying sampling implementation in  GeneralMetropolisHastings .\n - Added implementation of  Approximate Bayesian Computation  (ABC) in the  ApproxBayesComputation  class.  1\n2\n3\n4\n5\n6\n7 //The mean  val   center :   DenseMatrix [ Double ]   =   ...  //Covariance (positive semi-def) matrix among rows  val   sigmaRows :   DenseMatrix [ Double ]   =   ...  //Covariance (positive semi-def) matrix among columns  val   sigmaCols :   DenseMatrix [ Double ]   =   ...  val   matD   =   MatrixNormal ( center ,   sigmaRows ,   sigmaCols )   \n -  Matrix T Distribution  ( Experimental )  1\n2\n3\n4\n5\n6\n7\n8\n9 //The degrees of freedom (must be > 2.0 for existence of finite moments)  val   mu :   Double   =   ...  //The mean  val   center :   DenseMatrix [ Double ]   =   ...  //Covariance (positive semi-def) matrix among rows  val   sigmaRows :   DenseMatrix [ Double ]   =   ...  //Covariance (positive semi-def) matrix among columns  val   sigmaCols :   DenseMatrix [ Double ]   =   ...  val   matD   =   MatrixT ( mu ,   center ,   sigmaCols ,   sigmaRows )     Package   dynaml . optimization   Added  ProbGPCommMachine  which performs grid search or CSA and then instead of selecting a single hyper-parameter configuration calculates a weighted Gaussian Process committee where the weights correspond to probabilities or confidence on each model instance (hyper-parameter configuration).   Package   dynaml . utils   Added  multivariate gamma function   1\n2 //Returns logarithm of multivariate gamma function  val   g   =   mvlgamma ( 5 ,   1.5 )     Package   dynaml . dataformat   Added support for reading  MATLAB   .mat  files in the  MAT  object.",
            "title": "Additions"
        },
        {
            "location": "/releases/mydoc_release_notes_141/#improvementsbug-fixes",
            "text": "Package   dynaml . probability   Removed  ProbabilityModel  and replaced with  JointProbabilityScheme  and  BayesJointProbabilityScheme , major refactoring to  RandomVariable  API.    Package   dynaml . optimization   Improved logging of  CoupledSimulatedAnnealing  Refactored  GPMLOptimizer  to  GradBasedGlobalOptimizer    Package   dynaml . utils \n - Correction to  utils . getStats  method used for calculating mean and variance of data sets consisting of  DenseVector [ Double ] .\n -  minMaxScalingTrainTest   minMaxScaling  of  DynaMLPipe  using  GaussianScaler  instead of  MinMaxScaler  for processing of features.   Package   dynaml . kernels   Fix to  CoRegCauchyKernel : corrected mismatch of hyper-parameter string  Fix to  SVMKernel  objects matrix gradient computation in the case when kernel dimensions are not multiples of block size.  Correction to gradient calculation in RBF kernel family.  Speed up of kernel gradient computation, kernel and kernel gradient matrices with respect to the model hyper-parameters now calculated in a single pass through the data.",
            "title": "Improvements/Bug Fixes"
        },
        {
            "location": "/releases/mydoc_release_notes_14/",
            "text": "Version 1.4 of DynaML, released Sept 23, 2016, implements a number of new models (multi-output GP, student T process, random variables, etc) and features (Variance control for CSA, etc).\n\n\n\n\nModels\n\u00b6\n\n\nThe following inference models have been added.\n\n\nMeta Models & Ensembles\n\u00b6\n\n\n\n\nLSSVM committees.\n\n\n\n\nStochastic Processes\n\u00b6\n\n\n\n\nMulti-output, multi-task \nGaussian Process\n models as reviewed in \nLawrence et. al\n.\n\n\nStudent T Processes\n: single and multi output inspired from \nShah, Ghahramani et. al\n\n\nPerformance improvement to computation of \nmarginal likelihood\n and \nposterior predictive distribution\n in Gaussian Process models.\n\n\nPosterior predictive distribution outputted by the \nAbstractGPRegression\n base class is now changed to \nMultGaussianRV\n which is added to the \ndynaml\n.\nprobability\n package.\n\n\n\n\nKernels\n\u00b6\n\n\n\n\n\n\nAdded \nStationaryKernel\n and \nLocallyStationaryKernel\n classes in the kernel APIs, converted \nRBFKernel\n, \nCauchyKernel\n, \nRationalQuadraticKernel\n & \nLaplacianKernel\n to subclasses of \nStationaryKernel\n\n\n\n\n\n\nAdded \nMLPKernel\n which implements the \nmaximum likelihood perceptron\n kernel as shown \nhere\n.\n\n\n\n\n\n\nAdded \nco-regionalization kernels\n which are used in \nLawrence et. al\n to formulate kernels for vector valued functions. In this category the following co-regionalization kernels were implemented.\n\n\n\n\nCoRegRBFKernel\n\n\nCoRegCauchyKernel\n\n\nCoRegLaplaceKernel\n\n\n\n\nCoRegDiracKernel\n\n\n\n\n\n\nImproved performance when calculating kernel matrices for composite kernels.\n\n\n\n\n\n\nAdded \n:*\n operator to kernels so that one can create separable kernels used in \nco-regionalization models\n.\n\n\n\n\n\n\nOptimization\n\u00b6\n\n\n\n\nImproved performance of \nCoupledSimulatedAnnealing\n, enabled use of 4 variants of \nCoupled Simulated Annealing\n, adding the ability to set annealing schedule using so called \nvariance control\n scheme as outlined in \nde-Souza, Suykens et. al\n.\n\n\n\n\nPipes\n\u00b6\n\n\n\n\n\n\nAdded \nScaler\n and \nReversibleScaler\n traits to represent transformations which input and output into the same domain set, these traits are extensions of \nDataPipe\n.\n\n\n\n\n\n\nAdded \nDiscrete Wavelet Transform\n based on the \nHaar\n wavelet.",
            "title": "v1.4"
        },
        {
            "location": "/releases/mydoc_release_notes_14/#models",
            "text": "The following inference models have been added.",
            "title": "Models"
        },
        {
            "location": "/releases/mydoc_release_notes_14/#meta-models-ensembles",
            "text": "LSSVM committees.",
            "title": "Meta Models &amp; Ensembles"
        },
        {
            "location": "/releases/mydoc_release_notes_14/#stochastic-processes",
            "text": "Multi-output, multi-task  Gaussian Process  models as reviewed in  Lawrence et. al .  Student T Processes : single and multi output inspired from  Shah, Ghahramani et. al  Performance improvement to computation of  marginal likelihood  and  posterior predictive distribution  in Gaussian Process models.  Posterior predictive distribution outputted by the  AbstractGPRegression  base class is now changed to  MultGaussianRV  which is added to the  dynaml . probability  package.",
            "title": "Stochastic Processes"
        },
        {
            "location": "/releases/mydoc_release_notes_14/#kernels",
            "text": "Added  StationaryKernel  and  LocallyStationaryKernel  classes in the kernel APIs, converted  RBFKernel ,  CauchyKernel ,  RationalQuadraticKernel  &  LaplacianKernel  to subclasses of  StationaryKernel    Added  MLPKernel  which implements the  maximum likelihood perceptron  kernel as shown  here .    Added  co-regionalization kernels  which are used in  Lawrence et. al  to formulate kernels for vector valued functions. In this category the following co-regionalization kernels were implemented.   CoRegRBFKernel  CoRegCauchyKernel  CoRegLaplaceKernel   CoRegDiracKernel    Improved performance when calculating kernel matrices for composite kernels.    Added  :*  operator to kernels so that one can create separable kernels used in  co-regionalization models .",
            "title": "Kernels"
        },
        {
            "location": "/releases/mydoc_release_notes_14/#optimization",
            "text": "Improved performance of  CoupledSimulatedAnnealing , enabled use of 4 variants of  Coupled Simulated Annealing , adding the ability to set annealing schedule using so called  variance control  scheme as outlined in  de-Souza, Suykens et. al .",
            "title": "Optimization"
        },
        {
            "location": "/releases/mydoc_release_notes_14/#pipes",
            "text": "Added  Scaler  and  ReversibleScaler  traits to represent transformations which input and output into the same domain set, these traits are extensions of  DataPipe .    Added  Discrete Wavelet Transform  based on the  Haar  wavelet.",
            "title": "Pipes"
        },
        {
            "location": "/scaladoc/v1.5.3/core/",
            "text": "",
            "title": "dynaml-core"
        },
        {
            "location": "/scaladoc/v1.5.3/pipes/",
            "text": "",
            "title": "dynaml-pipes"
        },
        {
            "location": "/scaladoc/v1.5.3/repl/",
            "text": "",
            "title": "dynaml-repl"
        },
        {
            "location": "/scaladoc/v1.5.3/examples/",
            "text": "",
            "title": "dynaml-examples"
        },
        {
            "location": "/scaladoc/v1.5.2/core/",
            "text": "",
            "title": "dynaml-core"
        },
        {
            "location": "/scaladoc/v1.5.2/pipes/",
            "text": "",
            "title": "dynaml-pipes"
        },
        {
            "location": "/scaladoc/v1.5.2/repl/",
            "text": "",
            "title": "dynaml-repl"
        },
        {
            "location": "/scaladoc/v1.5.2/examples/",
            "text": "",
            "title": "dynaml-examples"
        },
        {
            "location": "/scaladoc/v1.5.1/core/",
            "text": "",
            "title": "dynaml-core"
        },
        {
            "location": "/scaladoc/v1.5.1/pipes/",
            "text": "",
            "title": "dynaml-pipes"
        },
        {
            "location": "/scaladoc/v1.5.1/examples/",
            "text": "",
            "title": "dynaml-examples"
        },
        {
            "location": "/scaladoc/v1.5/core/",
            "text": "",
            "title": "dynaml-core"
        },
        {
            "location": "/scaladoc/v1.5/pipes/",
            "text": "",
            "title": "dynaml-pipes"
        },
        {
            "location": "/scaladoc/v1.5/examples/",
            "text": "",
            "title": "dynaml-examples"
        },
        {
            "location": "/scaladoc/v1.4.3/core/",
            "text": "",
            "title": "dynaml-core"
        },
        {
            "location": "/scaladoc/v1.4.3/pipes/",
            "text": "",
            "title": "dynaml-pipes"
        },
        {
            "location": "/scaladoc/v1.4.3/examples/",
            "text": "",
            "title": "dynaml-examples"
        },
        {
            "location": "/scaladoc/v1.4.2/core/",
            "text": "",
            "title": "dynaml-core"
        },
        {
            "location": "/scaladoc/v1.4.2/pipes/",
            "text": "",
            "title": "dynaml-pipes"
        },
        {
            "location": "/scaladoc/v1.4.2/examples/",
            "text": "",
            "title": "dynaml-examples"
        },
        {
            "location": "/scaladoc/v1.4.1/core/",
            "text": "",
            "title": "dynaml-core"
        },
        {
            "location": "/scaladoc/v1.4.1/pipes/",
            "text": "",
            "title": "dynaml-pipes"
        },
        {
            "location": "/scaladoc/v1.4.1/examples/",
            "text": "",
            "title": "dynaml-examples"
        },
        {
            "location": "/scaladoc/v1.4/core/",
            "text": "",
            "title": "dynaml-core"
        },
        {
            "location": "/scaladoc/v1.4/pipes/",
            "text": "",
            "title": "dynaml-pipes"
        },
        {
            "location": "/scaladoc/v1.4/examples/",
            "text": "",
            "title": "dynaml-examples"
        },
        {
            "location": "/core/core_model_hierarchy/",
            "text": "Model Classes\n\u00b6\n\n\nIn DynaML all model implementations fit into a well defined class hierarchy. In fact every DynaML machine learning model is an extension of the \nModel\n[\nT\n,\nQ\n,\nR\n]\n trait.\n\n\n\n\nModel\n[\nT\n,\nQ\n,\nR\n]\n\n\nThe \nModel\n trait is quite bare bones: machine learning models are viewed as objects containing two parts or components.\n\n\n\n\n\n\nA training data set (of type \nT\n).  \n\n\n\n\n\n\nA method \npredict\n(\npoint\n:\n \nQ\n)\n:\nR\n to generate a prediction of type \nR\n given a data point of type \nQ\n.\n\n\n\n\n\n\n\n\nParameterized Models\n\u00b6\n\n\nMany predictive models calculate predictions by formulating an expression which includes a set of parameters which are used along with the data points to generate predictions, the \nParameterizedLearner[G, T, Q, R, S]\n class represents a skeleton for all parametric machine learning models such as \nGeneralized Linear Models\n, \nNeural Networks\n, etc.\n\n\n\n\nTip\n\n\nThe defining characteristic of classes which extend \nParameterizedLearner\n is that they must contain a member variable \noptimizer: RegularizedOptimizer[T, Q, R, S]\n which represents a \nregularization enabled optimizer\n implementation along with a \nlearn\n()\n method which uses the optimizer member to calculate approximate values of the model parameters given the training data.\n\n\n\n\nLinear Models\n\u00b6\n\n\nLinear models; represented by the \nLinearModel\n[\nT\n, \nP\n, \nQ\n , \nR\n, \nS\n]\n trait are extensions of \nParameterizedLearner\n, this top level trait is extended to yield many useful linear prediction models.\n\n\nGeneralized Linear Models\n which are linear in parameters expression for the predictions \ny\ny\n given a vector of processed features \n\\phi(x)\n\\phi(x)\n or basis functions.\n\n\n\n\n\n    \\begin{equation}\n        y = w^T\\varphi(x) + \\epsilon\n    \\end{equation}\n\n\n\n\n    \\begin{equation}\n        y = w^T\\varphi(x) + \\epsilon\n    \\end{equation}\n\n\n\n\n\nStochastic Processes\n\u00b6\n\n\nStochastic processes (or random functions) are general probabilistic models which can be used to construct finite dimensional distributions over a set of sampled domain points. More specifically a stochastic process is a probabilistic function \nf(.)\nf(.)\n defined on any \ndomain\n or \nindex set\n \n\\mathcal{X}\n\\mathcal{X}\n such that for any finite collection \nx_i \\in \\mathcal{X}, i = 1 \\cdots N\nx_i \\in \\mathcal{X}, i = 1 \\cdots N\n, the finite dimensional distribution \nP(f(x_1), \\cdots, f(x_N))\nP(f(x_1), \\cdots, f(x_N))\n is coherently defined.\n\n\n\n\nTip\n\n\nThe \nStochasticProcessModel\n[\nT\n, \nI\n, \nY\n, \nW\n]\n trait extends \nModel\n[\nT\n, \nI\n, \nY\n]\n and is the top level trait for the implementation of general stochastic processes. In order to extend it, one must implement among others a function to output the posterior predictive distribution \npredictiveDistribution\n()\n.\n\n\n\n\nContinuous Processes\n\u00b6\n\n\nBy continuous processes, we mean processes whose values lie on a continuous domain (such as \n\\mathbb{R}^d\n\\mathbb{R}^d\n). The \nContinuousProcessModel\n[\nT\n, \nI\n, \nY\n, \nW\n]\n abstract class provides a template which can be extended to implement continuous random process models.\n\n\n\n\nTip\n\n\nThe \nContinuousProcessModel\n class contains the method \npredictionWithErrorBars()\n which takes inputs test data and number of standard deviations, and generates predictions with upper and lower error bars around them. In order to create a sub-class of \nContinuousProcessModel\n, you must implement the method \npredictionWithErrorBars()\n.\n\n\n\n\nSecond Order Processes\n\u00b6\n\n\nSecond order stochastic processes can be described by specifying the \nmean\n (first order statistic) and \nvariance\n (second order statistic) of their finite dimensional distribution. The \nSecondOrderProcessModel\n[\nT\n, \nI\n, \nY\n, \nK\n, \nM\n, \nW\n]\n trait is an abstract skeleton which describes what elements a second order process model must have i.e. the mean and covariance functions.\n\n\nMeta Models/Model Ensembles\n\u00b6\n\n\nMeta models use predictions from several candidate models and derive a prediction that is a meaningful combination of the individual predictions. This may be achieved in several ways some of which are.\n\n\n\n\nAverage of predictions/voting\n\n\nWeighted predictions: Problem is now transferred to calculating appropriate weights.\n\n\nLearning some non-trivial functional transformation of the individual prediction, also known as \ngating networks\n.  \n\n\n\n\nCurrently the DynaML API has the following classes providing capabilities of meta models.\n\n\nAbstract Classes\n\n\n\n\nMetaModel\n[\nD\n, \nD1\n, \nBaseModel\n]\n\n\nCommitteeModel\n[\nD\n, \nD1\n, \nBaseModel\n]\n\n\n\n\nImplementations\n\n\n\n\nLS-SVM Committee\n\n\nNeural Committee",
            "title": "Model Hierarchy"
        },
        {
            "location": "/core/core_model_hierarchy/#model-classes",
            "text": "In DynaML all model implementations fit into a well defined class hierarchy. In fact every DynaML machine learning model is an extension of the  Model [ T , Q , R ]  trait.   Model [ T , Q , R ]  The  Model  trait is quite bare bones: machine learning models are viewed as objects containing two parts or components.    A training data set (of type  T ).      A method  predict ( point :   Q ) : R  to generate a prediction of type  R  given a data point of type  Q .",
            "title": "Model Classes"
        },
        {
            "location": "/core/core_model_hierarchy/#parameterized-models",
            "text": "Many predictive models calculate predictions by formulating an expression which includes a set of parameters which are used along with the data points to generate predictions, the  ParameterizedLearner[G, T, Q, R, S]  class represents a skeleton for all parametric machine learning models such as  Generalized Linear Models ,  Neural Networks , etc.   Tip  The defining characteristic of classes which extend  ParameterizedLearner  is that they must contain a member variable  optimizer: RegularizedOptimizer[T, Q, R, S]  which represents a  regularization enabled optimizer  implementation along with a  learn ()  method which uses the optimizer member to calculate approximate values of the model parameters given the training data.",
            "title": "Parameterized Models"
        },
        {
            "location": "/core/core_model_hierarchy/#linear-models",
            "text": "Linear models; represented by the  LinearModel [ T ,  P ,  Q  ,  R ,  S ]  trait are extensions of  ParameterizedLearner , this top level trait is extended to yield many useful linear prediction models.  Generalized Linear Models  which are linear in parameters expression for the predictions  y y  given a vector of processed features  \\phi(x) \\phi(x)  or basis functions.   \n    \\begin{equation}\n        y = w^T\\varphi(x) + \\epsilon\n    \\end{equation}  \n    \\begin{equation}\n        y = w^T\\varphi(x) + \\epsilon\n    \\end{equation}",
            "title": "Linear Models"
        },
        {
            "location": "/core/core_model_hierarchy/#stochastic-processes",
            "text": "Stochastic processes (or random functions) are general probabilistic models which can be used to construct finite dimensional distributions over a set of sampled domain points. More specifically a stochastic process is a probabilistic function  f(.) f(.)  defined on any  domain  or  index set   \\mathcal{X} \\mathcal{X}  such that for any finite collection  x_i \\in \\mathcal{X}, i = 1 \\cdots N x_i \\in \\mathcal{X}, i = 1 \\cdots N , the finite dimensional distribution  P(f(x_1), \\cdots, f(x_N)) P(f(x_1), \\cdots, f(x_N))  is coherently defined.   Tip  The  StochasticProcessModel [ T ,  I ,  Y ,  W ]  trait extends  Model [ T ,  I ,  Y ]  and is the top level trait for the implementation of general stochastic processes. In order to extend it, one must implement among others a function to output the posterior predictive distribution  predictiveDistribution () .",
            "title": "Stochastic Processes"
        },
        {
            "location": "/core/core_model_hierarchy/#continuous-processes",
            "text": "By continuous processes, we mean processes whose values lie on a continuous domain (such as  \\mathbb{R}^d \\mathbb{R}^d ). The  ContinuousProcessModel [ T ,  I ,  Y ,  W ]  abstract class provides a template which can be extended to implement continuous random process models.   Tip  The  ContinuousProcessModel  class contains the method  predictionWithErrorBars()  which takes inputs test data and number of standard deviations, and generates predictions with upper and lower error bars around them. In order to create a sub-class of  ContinuousProcessModel , you must implement the method  predictionWithErrorBars() .",
            "title": "Continuous Processes"
        },
        {
            "location": "/core/core_model_hierarchy/#second-order-processes",
            "text": "Second order stochastic processes can be described by specifying the  mean  (first order statistic) and  variance  (second order statistic) of their finite dimensional distribution. The  SecondOrderProcessModel [ T ,  I ,  Y ,  K ,  M ,  W ]  trait is an abstract skeleton which describes what elements a second order process model must have i.e. the mean and covariance functions.",
            "title": "Second Order Processes"
        },
        {
            "location": "/core/core_model_hierarchy/#meta-modelsmodel-ensembles",
            "text": "Meta models use predictions from several candidate models and derive a prediction that is a meaningful combination of the individual predictions. This may be achieved in several ways some of which are.   Average of predictions/voting  Weighted predictions: Problem is now transferred to calculating appropriate weights.  Learning some non-trivial functional transformation of the individual prediction, also known as  gating networks .     Currently the DynaML API has the following classes providing capabilities of meta models.  Abstract Classes   MetaModel [ D ,  D1 ,  BaseModel ]  CommitteeModel [ D ,  D1 ,  BaseModel ]   Implementations   LS-SVM Committee  Neural Committee",
            "title": "Meta Models/Model Ensembles"
        },
        {
            "location": "/core/core_glm/",
            "text": "Summary\nGeneralized Linear Models\n are a class of models which belong to the \nordinary least squares\n framework. They generally consist of a set of parameters \n\\mathbf{w}\n\\mathbf{w}\n, a feature mapping \n\\varphi()\n\\varphi()\n and a \nlink function\n which dictates how the probability distribution of the output quantity is described.\n\n\n\n\n\n\nGeneralized Linear Models\n (GLM) are available in the context of regression and binary classification, more specifically in DynaML the following members of the GLM family are implemented. The \nGeneralizedLinearModel\n[\nT\n]\n class is the base of the GLM hierarchy in DynaML, all linear models are extensions of it. It's companion object is used for the creation of GLM instances as follows.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\n\n//The task variable is a string which is set to \"regression\" or \"classification\"\n\n\nval\n \ntask\n \n=\n \n...\n\n\n\n//The map variable defines a possibly higher dimensional function of the input\n\n\n//which is akin to a basis function representation of the original features\n\n\nval\n \nmap\n:\n \nDenseVector\n[\nDouble\n]\n \n=>\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\n\n//modeltype is set to \"logit\" or \"probit\"\n\n\n//if one wishes to create a binary classification model,\n\n\n//depending on the classification model involved\n\n\nval\n \nmodeltype\n \n=\n \n\"logit\"\n\n\n\nval\n \nglm\n \n=\n \nGeneralizedLinearModel\n(\ndata\n,\n \ntask\n,\n \nmap\n,\n \nmodeltype\n)\n\n\n\n\n\n\n\nNormal GLM\n\u00b6\n\n\nThe most common regression model, also known as \nleast squares linear regression\n, implemented as the class \nRegularizedGLM\n which represents a regression model with the following prediction:\n\n\n\n\n\n    \\begin{equation}\n        y \\ | \\ \\mathbf{x} \\sim \\mathcal{N}(w^T \\varphi(\\mathbf{x}), \\sigma^{2})\n    \\end{equation}\n\n\n\n\n    \\begin{equation}\n        y \\ | \\ \\mathbf{x} \\sim \\mathcal{N}(w^T \\varphi(\\mathbf{x}), \\sigma^{2})\n    \\end{equation}\n\n\n\n\n\nHere \n\\varphi(.)\n\\varphi(.)\n is an appropriately chosen set of \nbasis functions\n. The inference problem is formulated as\n\n\n\n\n\n    \\begin{equation}\n        \\min_{w} \\ \\mathcal{J}_P(w) = \\frac{1}{2} \\gamma \\  w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2\n    \\end{equation}\n\n\n\n\n    \\begin{equation}\n        \\min_{w} \\ \\mathcal{J}_P(w) = \\frac{1}{2} \\gamma \\  w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2\n    \\end{equation}\n\n\n\n\n\nLogit GLM\n\u00b6\n\n\nIn binary classification the most common GLM used is the \nlogistic regression\n model which is given by\n$$\n    \\begin{equation}\n        P(y  = 1  |  \\mathbf{x}) = \\sigma(w^T \\varphi(\\mathbf{x}) + b)\n    \\end{equation}\n$$\n\n\nWhere \n\\sigma(z) = \\frac{1}{1 + exp(-z)}\n\\sigma(z) = \\frac{1}{1 + exp(-z)}\n is the logistic function which maps the output of the linear function \nw^T \\varphi(\\mathbf{x}) + b\nw^T \\varphi(\\mathbf{x}) + b\n to a probability value.\n\n\nProbit GLM\n\u00b6\n\n\nThe \nprobit regression\n model is an alternative to the \nlogit\n model it is represented as:\n$$\n    \\begin{equation}\n        P(y  = 1  |  \\mathbf{x}) = \\Phi(w^T \\varphi(\\mathbf{x}) + b)\n    \\end{equation}\n$$\nWhere \n\\Phi(z)\n\\Phi(z)\n is the cumulative distribution function of the standard normal distribution.\n\n\n\n\nGLS\nThe \nGeneralized Least Squares\n model which is a more broad formulation of the \nOrdinary Least Squares\n (OLS) regression model.",
            "title": "Generalized Linear Models"
        },
        {
            "location": "/core/core_glm/#normal-glm",
            "text": "The most common regression model, also known as  least squares linear regression , implemented as the class  RegularizedGLM  which represents a regression model with the following prediction:   \n    \\begin{equation}\n        y \\ | \\ \\mathbf{x} \\sim \\mathcal{N}(w^T \\varphi(\\mathbf{x}), \\sigma^{2})\n    \\end{equation}  \n    \\begin{equation}\n        y \\ | \\ \\mathbf{x} \\sim \\mathcal{N}(w^T \\varphi(\\mathbf{x}), \\sigma^{2})\n    \\end{equation}   Here  \\varphi(.) \\varphi(.)  is an appropriately chosen set of  basis functions . The inference problem is formulated as   \n    \\begin{equation}\n        \\min_{w} \\ \\mathcal{J}_P(w) = \\frac{1}{2} \\gamma \\  w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2\n    \\end{equation}  \n    \\begin{equation}\n        \\min_{w} \\ \\mathcal{J}_P(w) = \\frac{1}{2} \\gamma \\  w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2\n    \\end{equation}",
            "title": "Normal GLM"
        },
        {
            "location": "/core/core_glm/#logit-glm",
            "text": "In binary classification the most common GLM used is the  logistic regression  model which is given by\n$$\n    \\begin{equation}\n        P(y  = 1  |  \\mathbf{x}) = \\sigma(w^T \\varphi(\\mathbf{x}) + b)\n    \\end{equation}\n$$  Where  \\sigma(z) = \\frac{1}{1 + exp(-z)} \\sigma(z) = \\frac{1}{1 + exp(-z)}  is the logistic function which maps the output of the linear function  w^T \\varphi(\\mathbf{x}) + b w^T \\varphi(\\mathbf{x}) + b  to a probability value.",
            "title": "Logit GLM"
        },
        {
            "location": "/core/core_glm/#probit-glm",
            "text": "The  probit regression  model is an alternative to the  logit  model it is represented as:\n$$\n    \\begin{equation}\n        P(y  = 1  |  \\mathbf{x}) = \\Phi(w^T \\varphi(\\mathbf{x}) + b)\n    \\end{equation}\n$$\nWhere  \\Phi(z) \\Phi(z)  is the cumulative distribution function of the standard normal distribution.   GLS The  Generalized Least Squares  model which is a more broad formulation of the  Ordinary Least Squares  (OLS) regression model.",
            "title": "Probit GLM"
        },
        {
            "location": "/core/core_gls/",
            "text": "Summary\n\n\nThe \nGeneralized Least Squares\n model is a regression formulation which does not assume that the model errors/residuals are independent of each other. Rather it borrows from the \nGaussian Process\n paradigm and assigns a covariance structure to the model residuals.\n\n\n\n\n\n\nWarning\n\n\nThe nomenclature \nGeneralized Least Squares\n (GLS) and \nGeneralized Linear Models\n (GLM) can cause much confusion. It is important to remember the context of both. GLS refers to relaxing of the independence of residuals assumption while GLM refers to \nOrdinary Least Squares\n OLS based models which are extended to model regression, counts, or classification tasks.\n\n\n\n\nFormulation.\n\u00b6\n\n\nLet \n\\mathbf{X} \\in \\mathbb{R}^{n\\times m}\n\\mathbf{X} \\in \\mathbb{R}^{n\\times m}\n be a matrix containing data attributes. The GLS model builds a linear predictor of the target quantity of the following form.\n\n\n\n\n\n\\begin{equation}\n\\mathbf {y} = \\varphi(\\mathbf {X}) \\mathbf {\\beta } +\\mathbf {\\varepsilon }\n\\end{equation}\n\n\n\n\n\\begin{equation}\n\\mathbf {y} = \\varphi(\\mathbf {X}) \\mathbf {\\beta } +\\mathbf {\\varepsilon }\n\\end{equation}\n\n\n\n\n\nWhere \n\\varphi(.): \\mathbb{R}^m \\rightarrow \\mathbb{R}^d\n\\varphi(.): \\mathbb{R}^m \\rightarrow \\mathbb{R}^d\n is a feature mapping, \n\\mathbf{y} \\in \\mathbb{R}^n\n\\mathbf{y} \\in \\mathbb{R}^n\n is the vector of output values found in the training data set and \n\\mathbf{\\beta} \\in \\mathbb{R}^d\n\\mathbf{\\beta} \\in \\mathbb{R}^d\n is a set of regression parameters.\n\n\nIn the GLS framework, it is assumed that the model errors \n\\varepsilon \\in \\mathbb{R}^n\n\\varepsilon \\in \\mathbb{R}^n\n follow a multivariate gaussian distribution given by \n\\mathbb {E} [\\varepsilon |\\mathbf {X} ] = 0\n\\mathbb {E} [\\varepsilon |\\mathbf {X} ] = 0\n and \n\\operatorname{Var} [\\varepsilon |\\mathbf {X} ] = \\mathbf {\\Omega }\n\\operatorname{Var} [\\varepsilon |\\mathbf {X} ] = \\mathbf {\\Omega }\n, where \n\\mathbf{\\Omega}\n\\mathbf{\\Omega}\n is a symmetric positive semi-definite covariance matrix.\n\n\nIn order to calculate the model parameters \n\\mathbf{\\beta}\n\\mathbf{\\beta}\n, the log-likelihood of the training data outputs must be maximized with respect to the parameters \n\\mathbf{\\beta}\n\\mathbf{\\beta}\n, which leads to.\n\n\n\n\n\n\\begin{equation}\n\\min_{\\mathbf{\\beta}} \\ \\mathcal{J}_P(\\mathbf{\\beta}) = (\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )^{\\mathtt {T}}\\,\\mathbf {\\Omega } ^{-1}(\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )\n\\end{equation}\n\n\n\n\n\\begin{equation}\n\\min_{\\mathbf{\\beta}} \\ \\mathcal{J}_P(\\mathbf{\\beta}) = (\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )^{\\mathtt {T}}\\,\\mathbf {\\Omega } ^{-1}(\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )\n\\end{equation}\n\n\n\n\n\nFor the GLS problem the analytical solution of the above optimization problem can be calculated.\n\n\n\n\n\n{\\displaystyle \\mathbf {\\hat {\\beta }} =\\left(\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\varphi(\\mathbf {X})\\right)^{-1}\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\mathbf {y} .}\n\n\n\n\n{\\displaystyle \\mathbf {\\hat {\\beta }} =\\left(\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\varphi(\\mathbf {X})\\right)^{-1}\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\mathbf {y} .}\n\n\n\n\n\nGLS Models\n\u00b6\n\n\nYou can create a GLS model using the \nGeneralizedLeastSquaresModel\n class.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n//Get the training data\n\n\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n_\n\n\n//Define a feature mapping\n\n\n//If it is not defined the GLS model\n\n\n//will assume a identity feature map.\n\n\nval\n \nfeature_map\n:\n \nDenseVector\n[\nDouble\n]\n \n=>\n \nDenseVector\n[\nDouble\n]\n \n=\n \n_\n\n\n\n//Initialize a kernel function.\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n//Construct the covariance matrix for model errors.\n\n\nval\n \ncovmat\n \n=\n \nkernel\n.\nbuildBlockedKernelMatrix\n(\ndata\n,\n \ndata\n.\nlength\n)\n\n\n\nval\n \ngls_model\n \n=\n \nnew\n \nGeneralizedLeastSquaresModel\n(\n\n  \ndata\n,\n \ncovmat\n,\n\n  \nfeature_map\n)\n\n\n\n//Train the model\n\n\ngls_model\n.\nlearn\n()",
            "title": "Generalized Least Squares"
        },
        {
            "location": "/core/core_gls/#formulation",
            "text": "Let  \\mathbf{X} \\in \\mathbb{R}^{n\\times m} \\mathbf{X} \\in \\mathbb{R}^{n\\times m}  be a matrix containing data attributes. The GLS model builds a linear predictor of the target quantity of the following form.   \n\\begin{equation}\n\\mathbf {y} = \\varphi(\\mathbf {X}) \\mathbf {\\beta } +\\mathbf {\\varepsilon }\n\\end{equation}  \n\\begin{equation}\n\\mathbf {y} = \\varphi(\\mathbf {X}) \\mathbf {\\beta } +\\mathbf {\\varepsilon }\n\\end{equation}   Where  \\varphi(.): \\mathbb{R}^m \\rightarrow \\mathbb{R}^d \\varphi(.): \\mathbb{R}^m \\rightarrow \\mathbb{R}^d  is a feature mapping,  \\mathbf{y} \\in \\mathbb{R}^n \\mathbf{y} \\in \\mathbb{R}^n  is the vector of output values found in the training data set and  \\mathbf{\\beta} \\in \\mathbb{R}^d \\mathbf{\\beta} \\in \\mathbb{R}^d  is a set of regression parameters.  In the GLS framework, it is assumed that the model errors  \\varepsilon \\in \\mathbb{R}^n \\varepsilon \\in \\mathbb{R}^n  follow a multivariate gaussian distribution given by  \\mathbb {E} [\\varepsilon |\\mathbf {X} ] = 0 \\mathbb {E} [\\varepsilon |\\mathbf {X} ] = 0  and  \\operatorname{Var} [\\varepsilon |\\mathbf {X} ] = \\mathbf {\\Omega } \\operatorname{Var} [\\varepsilon |\\mathbf {X} ] = \\mathbf {\\Omega } , where  \\mathbf{\\Omega} \\mathbf{\\Omega}  is a symmetric positive semi-definite covariance matrix.  In order to calculate the model parameters  \\mathbf{\\beta} \\mathbf{\\beta} , the log-likelihood of the training data outputs must be maximized with respect to the parameters  \\mathbf{\\beta} \\mathbf{\\beta} , which leads to.   \n\\begin{equation}\n\\min_{\\mathbf{\\beta}} \\ \\mathcal{J}_P(\\mathbf{\\beta}) = (\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )^{\\mathtt {T}}\\,\\mathbf {\\Omega } ^{-1}(\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )\n\\end{equation}  \n\\begin{equation}\n\\min_{\\mathbf{\\beta}} \\ \\mathcal{J}_P(\\mathbf{\\beta}) = (\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )^{\\mathtt {T}}\\,\\mathbf {\\Omega } ^{-1}(\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )\n\\end{equation}   For the GLS problem the analytical solution of the above optimization problem can be calculated.   \n{\\displaystyle \\mathbf {\\hat {\\beta }} =\\left(\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\varphi(\\mathbf {X})\\right)^{-1}\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\mathbf {y} .}  \n{\\displaystyle \\mathbf {\\hat {\\beta }} =\\left(\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\varphi(\\mathbf {X})\\right)^{-1}\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\mathbf {y} .}",
            "title": "Formulation."
        },
        {
            "location": "/core/core_gls/#gls-models",
            "text": "You can create a GLS model using the  GeneralizedLeastSquaresModel  class.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 //Get the training data  val   data :   Stream [( DenseVector [ Double ] ,  Double )]   =   _  //Define a feature mapping  //If it is not defined the GLS model  //will assume a identity feature map.  val   feature_map :   DenseVector [ Double ]   =>   DenseVector [ Double ]   =   _  //Initialize a kernel function.  val   kernel :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  //Construct the covariance matrix for model errors.  val   covmat   =   kernel . buildBlockedKernelMatrix ( data ,   data . length )  val   gls_model   =   new   GeneralizedLeastSquaresModel ( \n   data ,   covmat , \n   feature_map )  //Train the model  gls_model . learn ()",
            "title": "GLS Models"
        },
        {
            "location": "/core/core_gp/",
            "text": "Gaussian Processes\n are stochastic processes whose finite dimensional distributions are multivariate gaussians.\n\n\nGaussian Processes\n are powerful non-parametric predictive models, which represent probability measures over spaces of functions. \nRamussen and Williams\n is the definitive guide on understanding their applications in machine learning and a gateway to their deeper theoretical foundations.\n\n\n\n\n\n\nGaussian Process\n models are well supported in DynaML, the \nAbstractGPRegressionModel\n[\nT\n, \nI\n]\n and \nAbstractGPClassification\n[\nT\n, \nI\n]\n classes which extend the \nStochasticProcessModel\n[\nT\n, \nI\n, \nY\n, \nW\n]\n base trait are the starting point for all GP implementations.\n\n\nGaussian Process Regression\n\u00b6\n\n\nThe GP regression framework aims to infer an unknown function \nf(x)\nf(x)\n given \ny_i\ny_i\n which are noise corrupted observations of this unknown function. This is done by adopting an explicit probabilistic formulation to the multi-variate distribution of the noise corrupted observations \ny_i\ny_i\n conditioned on the input features (or design matrix) \nX\nX\n\n\n\n\n\n\\begin{align}\n        & y = f(x) + \\epsilon \\\\\n        & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\\n        & \\left(\\mathbf{y} \\ \\ \\mathbf{f_*} \\right)^T | X \\sim \\mathcal{N}\\left(\\mathbf{0}, \\left[ \\begin{matrix} K(X, X) + \\sigma^{2} \\it{I} & K(X, X_*) \\\\ K(X_*, X) & K(X_*, X_*) \\end{matrix} \\right ] \\right)\n\\end{align}\n\n\n\n\n\\begin{align}\n        & y = f(x) + \\epsilon \\\\\n        & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\\n        & \\left(\\mathbf{y} \\ \\ \\mathbf{f_*} \\right)^T | X \\sim \\mathcal{N}\\left(\\mathbf{0}, \\left[ \\begin{matrix} K(X, X) + \\sigma^{2} \\it{I} & K(X, X_*) \\\\ K(X_*, X) & K(X_*, X_*) \\end{matrix} \\right ] \\right)\n\\end{align}\n\n\n\n\n\nIn the presence of training data\n\n\n\n\n\nX = (x_1, x_2, \\cdot , x_n) \\ \\mathbf{y} = (y_1, y_2, \\cdot , y_n)\n\n\n\n\nX = (x_1, x_2, \\cdot , x_n) \\ \\mathbf{y} = (y_1, y_2, \\cdot , y_n)\n\n\n\n\n\nInference is carried out by calculating the posterior predictive distribution over the unknown targets \n\\mathbf{f_*}|X,\\mathbf{y},X_*\n\\mathbf{f_*}|X,\\mathbf{y},X_*\n assuming \nX_*\nX_*\n, the test inputs are known.\n\n\n\n\n\n\\begin{align}\n        & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim \\mathcal{N}(\\mathbf{\\bar{f_*}}, cov(\\mathbf{f_*}))  \\label{eq:posterior}\\\\\n        & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\\n        & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*)\n\\end{align}\n\n\n\n\n\\begin{align}\n        & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim \\mathcal{N}(\\mathbf{\\bar{f_*}}, cov(\\mathbf{f_*}))  \\label{eq:posterior}\\\\\n        & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\\n        & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*)\n\\end{align}\n\n\n\n\n\nGP models for a single output\n\u00b6\n\n\nFor univariate GP models (single output), use the \nGPRegression\n class (an extension of \nAbstractGPRegressionModel\n). To construct a GP regression model you would need:\n\n\n\n\nTraining data\n\n\nKernel/covariance instance to model correlation between values of the latent function at each pair of input features.\n\n\nKernel instance to model the correlation of the additive noise, generally the \nDiracKernel\n (white noise) is used.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \ntrainingdata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nnum_features\n \n=\n \ntrainingdata\n.\nhead\n.\n_1\n.\nlength\n\n\n\n// Create an implicit vector field for the creation of the stationary\n\n\n// radial basis function kernel\n\n\nimplicit\n \nval\n \nfield\n \n=\n \nVectorField\n(\nnum_features\n)\n\n\n\nval\n \nkernel\n \n=\n \nnew\n \nRBFKernel\n(\n2.5\n)\n\n\nval\n \nnoiseKernel\n \n=\n \nnew\n \nDiracKernel\n(\n1.5\n)\n\n\nval\n \nmodel\n \n=\n \nnew\n \nGPRegression\n(\nkernel\n,\n \nnoiseKernel\n,\n \ntrainingData\n)\n\n\n\n\n\n\n\nGP models for multiple outputs\n\u00b6\n\n\nAs reviewed in \nLawrence et.al\n, Gaussian Processes for multiple outputs can be interpreted as single output GP models with an expanded index set. Recall that GPs are stochastic processes and thus are defined on some \nindex set\n, for example in the equations above it is noted that \nx \\in \\mathbb{R}^p\nx \\in \\mathbb{R}^p\n making \n\\mathbb{R}^p\n\\mathbb{R}^p\n the \nindex set\n of the process.\n\n\nIn case of multiple outputs the index set is expressed as a cartesian product \nx \\in \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\}\nx \\in \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\}\n, where \nd\nd\n is the number of outputs to be modeled.\n\n\nIt needs to be noted that now we will also have to define the kernel function on the same index set i.e. \n\\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\}\n\\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\}\n.\n\n\nIn multi-output GP literature a common way to construct kernels on such index sets is to multiply base kernels on each of the parts \n\\mathbb{R}^p\n\\mathbb{R}^p\n and \n\\{1,2,\\cdots,d\\}\n\\{1,2,\\cdots,d\\}\n, such kernels are known as \nseparable kernels\n.\n\n\n\n\n\n\\begin{equation}\nK((\\mathbf{x}, d), (\\mathbf{x}', d')) = K_{x}(\\mathbf{x}, \\mathbf{x}') . K_{d}(d, d')\n\\end{equation}\n\n\n\n\n\\begin{equation}\nK((\\mathbf{x}, d), (\\mathbf{x}', d')) = K_{x}(\\mathbf{x}, \\mathbf{x}') . K_{d}(d, d')\n\\end{equation}\n\n\n\n\n\nTaking this idea further \nsum of separable kernels\n (SoS) are often employed in multi-output GP models. These models are also known as \nLinear Models of Co-Regionalization\n (LMC) and the kernels which encode correlation between the outputs \nK_d(.,.)\nK_d(.,.)\n are known as \nco-regionalization kernels\n.\n\n\n\n\n\n\\begin{equation}\nK((\\mathbf{x}, d), (\\mathbf{x}', d')) = \\sum_{i = 1}^{D} K^{i}_{x}(\\mathbf{x}, \\mathbf{x}') . K^{i}_{d}(d, d')\n\\end{equation}\n\n\n\n\n\\begin{equation}\nK((\\mathbf{x}, d), (\\mathbf{x}', d')) = \\sum_{i = 1}^{D} K^{i}_{x}(\\mathbf{x}, \\mathbf{x}') . K^{i}_{d}(d, d')\n\\end{equation}\n\n\n\n\n\n\n\nCreating separable kernels\nCreating SoS kernels in DynaML is quite straightforward, use the \n:*\n operator to multiply a kernel defined on \nDenseVector\n[\nDouble\n]\n with a kernel defined on \nInt\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nval\n \nlinearK\n \n=\n \nnew\n \nPolynomialKernel\n(\n2\n,\n \n1.0\n)\n\n\nval\n \ntKernel\n \n=\n \nnew\n \nTStudentKernel\n(\n0.2\n)\n\n\nval\n \nd\n \n=\n \nnew\n \nDiracKernel\n(\n0.037\n)\n\n\n\nval\n \nmixedEffects\n \n=\n \nnew\n \nMixedEffectRegularizer\n(\n0.5\n)\n\n\nval\n \ncoRegCauchyMatrix\n \n=\n \nnew\n \nCoRegCauchyKernel\n(\n10.0\n)\n\n\nval\n \ncoRegDiracMatrix\n \n=\n \nnew\n \nCoRegDiracKernel\n\n\n\nval\n \nsos_kernel\n:\n \nCompositeCovariance\n[(\nDenseVector\n[\nDouble\n]\n, \nInt\n)]\n \n=\n\n    \n(\nlinearK\n \n:*\n \nmixedEffects\n)\n  \n+\n \n(\ntKernel\n \n:*\n \ncoRegCauchyMatrix\n)\n\n\n\nval\n \nsos_noise\n:\n \nCompositeCovariance\n[(\nDenseVector\n[\nDouble\n]\n, \nInt\n)]\n \n=\n\n    \nd\n \n:*\n \ncoRegDiracMatrix\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\nYou can use the \nMOGPRegressionModel\n[\nI\n]\n class to create multi-output GP models.\n\n\n1\n2\n3\n4\n5\n6\nval\n \ntrainingdata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]\n \n=\n \n_\n\n\n\nval\n \nmodel\n \n=\n \nnew\n \nMOGPRegressionModel\n[\nDenseVector\n[\nDouble\n]](\n\n    \nsos_kernel\n,\n \nsos_noise\n,\n \ntrainingdata\n,\n\n    \ntrainingdata\n.\nlength\n,\n\n    \ntrainingdata\n.\nhead\n.\n_2\n.\nlength\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Process Binary Classification\n\u00b6\n\n\nGaussian process models for classification are formulated using two components.\n\n\n\n\nA latent (nuisance) function \nf(x)\nf(x)\n\n\nA transfer function \n\\sigma(.)\n\\sigma(.)\n which transforms the value \nf(x)\nf(x)\n to a class probability\n\n\n\n\n\n\n\n    \\begin{align}\n        & \\pi(x) \\overset{\\triangle}{=} p(y = +1| x) = \\sigma(f(x)) \\\\\n        & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\\n    \\end{align}\n\n\n\n\n    \\begin{align}\n        & \\pi(x) \\overset{\\triangle}{=} p(y = +1| x) = \\sigma(f(x)) \\\\\n        & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\\n    \\end{align}\n\n\n\n\n\nInference is divided into two steps.\n\n\n\n\nComputing the distribution of the latent function corresponding to a test case\n\n\n\n\n\n\n\n\\begin{align}\n    & p(f_*|X, \\mathbf{y}, x_*) = \\int p(f_*|X, \\mathbf{y}, x_*, \\mathbf{f}) p(\\mathbf{f}|X, \\mathbf{y}) d\\mathbf{f} \\\\\n    & p(\\mathbf{f}|X, \\mathbf{y}) = p(\\mathbf{y}| \\mathbf{f}) p(\\mathbf{f}|X)/ p(\\mathbf{y}|X)\n\\end{align}\n\n\n\n\n\\begin{align}\n    & p(f_*|X, \\mathbf{y}, x_*) = \\int p(f_*|X, \\mathbf{y}, x_*, \\mathbf{f}) p(\\mathbf{f}|X, \\mathbf{y}) d\\mathbf{f} \\\\\n    & p(\\mathbf{f}|X, \\mathbf{y}) = p(\\mathbf{y}| \\mathbf{f}) p(\\mathbf{f}|X)/ p(\\mathbf{y}|X)\n\\end{align}\n\n\n\n\n\n\n\nGenerating probabilistic prediction for a test case.\n\n\n\n\n\n\n\n\\bar{\\pi_*} \\overset{\\triangle}{=} p(y_* = +1| X, \\mathbf{y}, x_*) = \\int \\sigma(f_*) p(f_*|X, \\mathbf{y}, x_*) df_*\n\n\n\n\n\\bar{\\pi_*} \\overset{\\triangle}{=} p(y_* = +1| X, \\mathbf{y}, x_*) = \\int \\sigma(f_*) p(f_*|X, \\mathbf{y}, x_*) df_*\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \ntrainingdata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nnum_features\n \n=\n \ntrainingdata\n.\nhead\n.\n_1\n.\nlength\n\n\n\n// Create an implicit vector field for the creation of the stationary\n\n\n// radial basis function kernel\n\n\nimplicit\n \nval\n \nfield\n \n=\n \nVectorField\n(\nnum_features\n)\n\n\n\nval\n \nkernel\n \n=\n \nnew\n \nRBFKernel\n(\n2.5\n)\n\n\nval\n \nlikelihood\n \n=\n \nnew\n \nVectorIIDSigmoid\n()\n\n\nval\n \nmodel\n \n=\n \nnew\n \nLaplaceBinaryGPC\n(\ntrainingData\n,\n \nkernel\n,\n \nlikelihood\n)\n\n\n\n\n\n\n\nExtending The GP Class\n\u00b6\n\n\nIn case you want to customize the implementation of \nGaussian Process\n models in DynaML, you can do so by extending the GP abstract skeleton \nGaussianProcessModel\n and using your own data structures for the type parameters \nI\n and \nT\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\nimport\n \nbreeze.linalg._\n\n\nimport\n \nio.github.mandar2812.dynaml.pipes._\n\n\nimport\n \nio.github.mandar2812.dynaml.probability._\n\n\nimport\n \nio.github.mandar2812.dynaml.models.gp._\n\n\nimport\n \nio.github.mandar2812.dynaml.kernels._\n\n\n\n\nclass\n \nMyGPRegressionModel\n[\nT\n, \nI\n](\n\n  \ncov\n:\n \nLocalScalarKernel\n[\nI\n],\n\n  \nn\n:\n \nLocalScalarKernel\n[\nI\n],\n\n  \ndata\n:\n \nT\n,\n \nnum\n:\n \nInt\n,\n \nmean\n:\n \nDataPipe\n[\nI\n, \nDouble\n])\n \nextends\n \n  \nAbstractGPRegressionModel\n[\nT\n, \nI\n](\ncov\n,\n \nn\n,\n \ndata\n,\n \nnum\n,\n \nmean\n)\n \n{\n\n\n  \noverride\n \nval\n \ncovariance\n \n=\n \ncov\n\n\n  \noverride\n \nval\n \nnoiseModel\n \n=\n \nn\n\n\n  \noverride\n \nprotected\n \nval\n \ng\n:\n \nT\n \n=\n \ndata\n\n\n  \n/**\n\n\n    * Convert from the underlying data structure to\n\n\n    * Seq[(I, Y)] where I is the index set of the GP\n\n\n    * and Y is the value/label type.\n\n\n    **/\n\n  \noverride\n \ndef\n \ndataAsSeq\n(\ndata\n:\n \nT\n)\n:\n \nSeq\n[(\nI\n, \nDouble\n)]\n \n=\n \n???\n\n\n\n  \n/**\n\n\n    * Calculates the energy of the configuration,\n\n\n    * in most global optimization algorithms\n\n\n    * we aim to find an approximate value of\n\n\n    * the hyper-parameters such that this function\n\n\n    * is minimized.\n\n\n    *\n\n\n    * @param h The value of the hyper-parameters in \n\n\n    *          the configuration space\n\n\n    * @param options Optional parameters\n\n\n    *                \n\n\n    * @return Configuration Energy E(h)\n\n\n    *\n\n\n    * In this particular case E(h) = -log p(Y|X,h)\n\n\n    * also known as log likelihood.\n\n\n    **/\n\n  \noverride\n \ndef\n \nenergy\n(\n\n    \nh\n:\n \nMap\n[\nString\n, \nDouble\n],\n \n    \noptions\n:\n \nMap\n[\nString\n, \nString\n])\n:\n \nDouble\n \n=\n \n???\n\n\n  \n/**\n\n\n    * Calculates the gradient energy of the configuration and\n\n\n    * subtracts this from the current value of h to yield a new\n\n\n    * hyper-parameter configuration.\n\n\n    *\n\n\n    * Over ride this function if you aim to implement a \n\n\n    * gradient based hyper-parameter optimization routine \n\n\n    * like ML-II\n\n\n    *\n\n\n    * @param h The value of the hyper-parameters in the \n\n\n    *          configuration space\n\n\n    * @return Gradient of the objective function \n\n\n    *         (marginal likelihood) as a Map\n\n\n    **/\n\n  \noverride\n \ndef\n \ngradEnergy\n(\n\n    \nh\n:\n \nMap\n[\nString\n, \nDouble\n])\n:\n \nMap\n[\nString\n, \nDouble\n]\n \n=\n \n???\n\n\n  \n/**\n\n\n   * Calculates posterior predictive distribution for\n\n\n   * a particular set of test data points.\n\n\n   *\n\n\n   * @param test A Sequence or Sequence like data structure\n\n\n   *             storing the values of the input patters.\n\n\n   **/\n\n  \noverride\n \ndef\n \npredictiveDistribution\n[\nU\n \n<:\n \nSeq\n[\nI\n]](\n\n    \ntest\n:\n \nU\n)\n:\n \nMultGaussianPRV\n \n=\n \n???\n\n\n \n//Use the predictive distribution to generate a point prediction\n\n \noverride\n \ndef\n \npredict\n(\npoint\n:\n \nI\n)\n:\n \nDouble\n \n=\n \n???\n\n\n}",
            "title": "Gaussian Processes"
        },
        {
            "location": "/core/core_gp/#gaussian-process-regression",
            "text": "The GP regression framework aims to infer an unknown function  f(x) f(x)  given  y_i y_i  which are noise corrupted observations of this unknown function. This is done by adopting an explicit probabilistic formulation to the multi-variate distribution of the noise corrupted observations  y_i y_i  conditioned on the input features (or design matrix)  X X   \n\\begin{align}\n        & y = f(x) + \\epsilon \\\\\n        & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\\n        & \\left(\\mathbf{y} \\ \\ \\mathbf{f_*} \\right)^T | X \\sim \\mathcal{N}\\left(\\mathbf{0}, \\left[ \\begin{matrix} K(X, X) + \\sigma^{2} \\it{I} & K(X, X_*) \\\\ K(X_*, X) & K(X_*, X_*) \\end{matrix} \\right ] \\right)\n\\end{align}  \n\\begin{align}\n        & y = f(x) + \\epsilon \\\\\n        & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\\n        & \\left(\\mathbf{y} \\ \\ \\mathbf{f_*} \\right)^T | X \\sim \\mathcal{N}\\left(\\mathbf{0}, \\left[ \\begin{matrix} K(X, X) + \\sigma^{2} \\it{I} & K(X, X_*) \\\\ K(X_*, X) & K(X_*, X_*) \\end{matrix} \\right ] \\right)\n\\end{align}   In the presence of training data   \nX = (x_1, x_2, \\cdot , x_n) \\ \\mathbf{y} = (y_1, y_2, \\cdot , y_n)  \nX = (x_1, x_2, \\cdot , x_n) \\ \\mathbf{y} = (y_1, y_2, \\cdot , y_n)   Inference is carried out by calculating the posterior predictive distribution over the unknown targets  \\mathbf{f_*}|X,\\mathbf{y},X_* \\mathbf{f_*}|X,\\mathbf{y},X_*  assuming  X_* X_* , the test inputs are known.   \n\\begin{align}\n        & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim \\mathcal{N}(\\mathbf{\\bar{f_*}}, cov(\\mathbf{f_*}))  \\label{eq:posterior}\\\\\n        & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\\n        & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*)\n\\end{align}  \n\\begin{align}\n        & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim \\mathcal{N}(\\mathbf{\\bar{f_*}}, cov(\\mathbf{f_*}))  \\label{eq:posterior}\\\\\n        & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\\n        & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*)\n\\end{align}",
            "title": "Gaussian Process Regression"
        },
        {
            "location": "/core/core_gp/#gp-models-for-a-single-output",
            "text": "For univariate GP models (single output), use the  GPRegression  class (an extension of  AbstractGPRegressionModel ). To construct a GP regression model you would need:   Training data  Kernel/covariance instance to model correlation between values of the latent function at each pair of input features.  Kernel instance to model the correlation of the additive noise, generally the  DiracKernel  (white noise) is used.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   trainingdata :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   num_features   =   trainingdata . head . _1 . length  // Create an implicit vector field for the creation of the stationary  // radial basis function kernel  implicit   val   field   =   VectorField ( num_features )  val   kernel   =   new   RBFKernel ( 2.5 )  val   noiseKernel   =   new   DiracKernel ( 1.5 )  val   model   =   new   GPRegression ( kernel ,   noiseKernel ,   trainingData )",
            "title": "GP models for a single output"
        },
        {
            "location": "/core/core_gp/#gp-models-for-multiple-outputs",
            "text": "As reviewed in  Lawrence et.al , Gaussian Processes for multiple outputs can be interpreted as single output GP models with an expanded index set. Recall that GPs are stochastic processes and thus are defined on some  index set , for example in the equations above it is noted that  x \\in \\mathbb{R}^p x \\in \\mathbb{R}^p  making  \\mathbb{R}^p \\mathbb{R}^p  the  index set  of the process.  In case of multiple outputs the index set is expressed as a cartesian product  x \\in \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\} x \\in \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\} , where  d d  is the number of outputs to be modeled.  It needs to be noted that now we will also have to define the kernel function on the same index set i.e.  \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\} \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\} .  In multi-output GP literature a common way to construct kernels on such index sets is to multiply base kernels on each of the parts  \\mathbb{R}^p \\mathbb{R}^p  and  \\{1,2,\\cdots,d\\} \\{1,2,\\cdots,d\\} , such kernels are known as  separable kernels .   \n\\begin{equation}\nK((\\mathbf{x}, d), (\\mathbf{x}', d')) = K_{x}(\\mathbf{x}, \\mathbf{x}') . K_{d}(d, d')\n\\end{equation}  \n\\begin{equation}\nK((\\mathbf{x}, d), (\\mathbf{x}', d')) = K_{x}(\\mathbf{x}, \\mathbf{x}') . K_{d}(d, d')\n\\end{equation}   Taking this idea further  sum of separable kernels  (SoS) are often employed in multi-output GP models. These models are also known as  Linear Models of Co-Regionalization  (LMC) and the kernels which encode correlation between the outputs  K_d(.,.) K_d(.,.)  are known as  co-regionalization kernels .   \n\\begin{equation}\nK((\\mathbf{x}, d), (\\mathbf{x}', d')) = \\sum_{i = 1}^{D} K^{i}_{x}(\\mathbf{x}, \\mathbf{x}') . K^{i}_{d}(d, d')\n\\end{equation}  \n\\begin{equation}\nK((\\mathbf{x}, d), (\\mathbf{x}', d')) = \\sum_{i = 1}^{D} K^{i}_{x}(\\mathbf{x}, \\mathbf{x}') . K^{i}_{d}(d, d')\n\\end{equation}    Creating separable kernels Creating SoS kernels in DynaML is quite straightforward, use the  :*  operator to multiply a kernel defined on  DenseVector [ Double ]  with a kernel defined on  Int .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 val   linearK   =   new   PolynomialKernel ( 2 ,   1.0 )  val   tKernel   =   new   TStudentKernel ( 0.2 )  val   d   =   new   DiracKernel ( 0.037 )  val   mixedEffects   =   new   MixedEffectRegularizer ( 0.5 )  val   coRegCauchyMatrix   =   new   CoRegCauchyKernel ( 10.0 )  val   coRegDiracMatrix   =   new   CoRegDiracKernel  val   sos_kernel :   CompositeCovariance [( DenseVector [ Double ] ,  Int )]   = \n     ( linearK   :*   mixedEffects )    +   ( tKernel   :*   coRegCauchyMatrix )  val   sos_noise :   CompositeCovariance [( DenseVector [ Double ] ,  Int )]   = \n     d   :*   coRegDiracMatrix       Tip You can use the  MOGPRegressionModel [ I ]  class to create multi-output GP models.  1\n2\n3\n4\n5\n6 val   trainingdata :   Stream [( DenseVector [ Double ] ,  DenseVector [ Double ])]   =   _  val   model   =   new   MOGPRegressionModel [ DenseVector [ Double ]]( \n     sos_kernel ,   sos_noise ,   trainingdata , \n     trainingdata . length , \n     trainingdata . head . _2 . length )",
            "title": "GP models for multiple outputs"
        },
        {
            "location": "/core/core_gp/#gaussian-process-binary-classification",
            "text": "Gaussian process models for classification are formulated using two components.   A latent (nuisance) function  f(x) f(x)  A transfer function  \\sigma(.) \\sigma(.)  which transforms the value  f(x) f(x)  to a class probability    \n    \\begin{align}\n        & \\pi(x) \\overset{\\triangle}{=} p(y = +1| x) = \\sigma(f(x)) \\\\\n        & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\\n    \\end{align}  \n    \\begin{align}\n        & \\pi(x) \\overset{\\triangle}{=} p(y = +1| x) = \\sigma(f(x)) \\\\\n        & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\\n    \\end{align}   Inference is divided into two steps.   Computing the distribution of the latent function corresponding to a test case    \n\\begin{align}\n    & p(f_*|X, \\mathbf{y}, x_*) = \\int p(f_*|X, \\mathbf{y}, x_*, \\mathbf{f}) p(\\mathbf{f}|X, \\mathbf{y}) d\\mathbf{f} \\\\\n    & p(\\mathbf{f}|X, \\mathbf{y}) = p(\\mathbf{y}| \\mathbf{f}) p(\\mathbf{f}|X)/ p(\\mathbf{y}|X)\n\\end{align}  \n\\begin{align}\n    & p(f_*|X, \\mathbf{y}, x_*) = \\int p(f_*|X, \\mathbf{y}, x_*, \\mathbf{f}) p(\\mathbf{f}|X, \\mathbf{y}) d\\mathbf{f} \\\\\n    & p(\\mathbf{f}|X, \\mathbf{y}) = p(\\mathbf{y}| \\mathbf{f}) p(\\mathbf{f}|X)/ p(\\mathbf{y}|X)\n\\end{align}    Generating probabilistic prediction for a test case.    \n\\bar{\\pi_*} \\overset{\\triangle}{=} p(y_* = +1| X, \\mathbf{y}, x_*) = \\int \\sigma(f_*) p(f_*|X, \\mathbf{y}, x_*) df_*  \n\\bar{\\pi_*} \\overset{\\triangle}{=} p(y_* = +1| X, \\mathbf{y}, x_*) = \\int \\sigma(f_*) p(f_*|X, \\mathbf{y}, x_*) df_*    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   trainingdata :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   num_features   =   trainingdata . head . _1 . length  // Create an implicit vector field for the creation of the stationary  // radial basis function kernel  implicit   val   field   =   VectorField ( num_features )  val   kernel   =   new   RBFKernel ( 2.5 )  val   likelihood   =   new   VectorIIDSigmoid ()  val   model   =   new   LaplaceBinaryGPC ( trainingData ,   kernel ,   likelihood )",
            "title": "Gaussian Process Binary Classification"
        },
        {
            "location": "/core/core_gp/#extending-the-gp-class",
            "text": "In case you want to customize the implementation of  Gaussian Process  models in DynaML, you can do so by extending the GP abstract skeleton  GaussianProcessModel  and using your own data structures for the type parameters  I  and  T .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77 import   breeze.linalg._  import   io.github.mandar2812.dynaml.pipes._  import   io.github.mandar2812.dynaml.probability._  import   io.github.mandar2812.dynaml.models.gp._  import   io.github.mandar2812.dynaml.kernels._  class   MyGPRegressionModel [ T ,  I ]( \n   cov :   LocalScalarKernel [ I ], \n   n :   LocalScalarKernel [ I ], \n   data :   T ,   num :   Int ,   mean :   DataPipe [ I ,  Double ])   extends  \n   AbstractGPRegressionModel [ T ,  I ]( cov ,   n ,   data ,   num ,   mean )   { \n\n   override   val   covariance   =   cov \n\n   override   val   noiseModel   =   n \n\n   override   protected   val   g :   T   =   data \n\n   /**      * Convert from the underlying data structure to      * Seq[(I, Y)] where I is the index set of the GP      * and Y is the value/label type.      **/ \n   override   def   dataAsSeq ( data :   T ) :   Seq [( I ,  Double )]   =   ??? \n\n\n   /**      * Calculates the energy of the configuration,      * in most global optimization algorithms      * we aim to find an approximate value of      * the hyper-parameters such that this function      * is minimized.      *      * @param h The value of the hyper-parameters in       *          the configuration space      * @param options Optional parameters      *                      * @return Configuration Energy E(h)      *      * In this particular case E(h) = -log p(Y|X,h)      * also known as log likelihood.      **/ \n   override   def   energy ( \n     h :   Map [ String ,  Double ],  \n     options :   Map [ String ,  String ]) :   Double   =   ??? \n\n   /**      * Calculates the gradient energy of the configuration and      * subtracts this from the current value of h to yield a new      * hyper-parameter configuration.      *      * Over ride this function if you aim to implement a       * gradient based hyper-parameter optimization routine       * like ML-II      *      * @param h The value of the hyper-parameters in the       *          configuration space      * @return Gradient of the objective function       *         (marginal likelihood) as a Map      **/ \n   override   def   gradEnergy ( \n     h :   Map [ String ,  Double ]) :   Map [ String ,  Double ]   =   ??? \n\n   /**     * Calculates posterior predictive distribution for     * a particular set of test data points.     *     * @param test A Sequence or Sequence like data structure     *             storing the values of the input patters.     **/ \n   override   def   predictiveDistribution [ U   <:   Seq [ I ]]( \n     test :   U ) :   MultGaussianPRV   =   ??? \n\n  //Use the predictive distribution to generate a point prediction \n  override   def   predict ( point :   I ) :   Double   =   ???  }",
            "title": "Extending The GP Class"
        },
        {
            "location": "/core/core_esgp/",
            "text": "Summary\n\n\nThe \nExtended Skew Gaussian Process\n (ESGP) uses the \nMESN\n distribution to define its finite dimensional probability distribution. It can be viewed as an generalization of the \nGaussian Process\n because when its skewness parameter approaches zero, the calculated probabilities are very close to gaussian probabilities.\n\n\n\n\nThe ESGP model uses the conditioning property of the MESN distribution, just like the multivariate normal distribution, the MESN retains its form when conditioned on a subset of its dimensions.\n\n\nCreating an ESGP model is very similar to creating a GP model in DynaML. The class \nESGPModel\n[\nT\n, \nI\n]\n can be instantiated much like the \nAbstractGPRegressionModel\n[\nT\n, \nI\n]\n, using the \napply\n method.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n//Obtain the data, some generic type\n\n\nval\n \ntrainingdata\n:\n \nDataType\n \n=\n \n...\n\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nI\n]\n \n=\n \n_\n\n\nval\n \nnoiseKernel\n:\n \nLocalScalarKernel\n[\nI\n]\n \n=\n \n_\n\n\nval\n \nmeanFunc\n:\n \nDataPipe\n[\nI\n, \nDouble\n]\n \n=\n \n_\n\n\n\nval\n \nlambda\n \n=\n \n1.5\n\n\nval\n \ntau\n \n=\n \n0.5\n\n\n\n//Define how the data is converted to a compatible type\n\n\nimplicit\n \nval\n \ntransform\n:\n \nDataPipe\n[\nDataType\n, \nSeq\n[(\nI\n, \nDouble\n)]]\n \n=\n \n_\n\n\n\nval\n \nmodel\n \n=\n \nESGPModel\n(\n\n  \nkernel\n,\n \nnoiseKernel\n,\n\n  \nmeanFunc\n,\n \nlambda\n,\n \ntau\n,\n\n  \ntrainingData\n)",
            "title": "Extended Skew Gaussian Processes"
        },
        {
            "location": "/core/core_stp/",
            "text": "Student T Processes\n (STP) can be viewed as a generalization of Gaussian Processes, in GP models we use the multivariate normal distribution to model noisy observations of an unknown function. Likewise for STP models, we employ the multivariate student t distribution. Formally a student t process is a stochastic process where the finite dimensional distribution is multivariate t.\n\n\n\n\n\n\\begin{align}\n\\mathbf{y} & \\in \\mathbb{R}^n \\\\\n\\mathbf{y} & \\sim MVT_{n}(\\nu, \\phi, K) \\\\\np(\\mathbf{y}) & = \\frac{\\Gamma(\\frac{\\nu + n}{2})}{((\\nu - 2)\\pi)^{n/2} \\Gamma(\\nu/2)} |K|^{-1/2} \\\\\n& \\times (1 + (\\mathbf{y} - \\phi)^T K^{-1} (\\mathbf{y} - \\phi))^{-\\frac{\\nu +n}{2}}\n\\end{align}\n\n\n\n\n\\begin{align}\n\\mathbf{y} & \\in \\mathbb{R}^n \\\\\n\\mathbf{y} & \\sim MVT_{n}(\\nu, \\phi, K) \\\\\np(\\mathbf{y}) & = \\frac{\\Gamma(\\frac{\\nu + n}{2})}{((\\nu - 2)\\pi)^{n/2} \\Gamma(\\nu/2)} |K|^{-1/2} \\\\\n& \\times (1 + (\\mathbf{y} - \\phi)^T K^{-1} (\\mathbf{y} - \\phi))^{-\\frac{\\nu +n}{2}}\n\\end{align}\n\n\n\n\n\nIt is known that as \n\\nu \\rightarrow \\infty\n\\nu \\rightarrow \\infty\n, the \nMVT_{n}(\\nu, \\phi, K)\nMVT_{n}(\\nu, \\phi, K)\n tends towards the multivariate normal distribution \n\\mathcal{N}_{n}(\\phi, K)\n\\mathcal{N}_{n}(\\phi, K)\n.\n\n\nRegression with Student T Processes\n\u00b6\n\n\nThe regression formulation for STP models is identical to the GP regression framework, to summarize the posterior predictive distribution takes the following form.\n\n\nSuppose \n\\mathbf{t} \\sim MVT_{n_{tr} + n_t}(\\nu, \\mathbf{0}, K)\n\\mathbf{t} \\sim MVT_{n_{tr} + n_t}(\\nu, \\mathbf{0}, K)\n is the process producing the data.\nLet \n[\\mathbf{f_*}]_{n_{t} \\times 1}\n[\\mathbf{f_*}]_{n_{t} \\times 1}\n represent the values of the function on the test inputs and \n[\\mathbf{y}]_{n_{tr} \\times 1}\n[\\mathbf{y}]_{n_{tr} \\times 1}\n represent noisy observations made on the training data points.\n\n\n\n\n\n\\begin{align}\n        & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim MVT_{\\nu + n_{tr}}(\\mathbf{\\bar{f_*}}, \\frac{\\nu + \\beta - 2}{\\nu + n_{tr} - 2} \\times cov(\\mathbf{f_*}))  \\label{eq:posterior}\\\\\n    & \\beta = \\mathbf{y}^T K^{-1} \\mathbf{y} \\\\\n        & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\\n        & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*)\n\\end{align}\n\n\n\n\n\\begin{align}\n        & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim MVT_{\\nu + n_{tr}}(\\mathbf{\\bar{f_*}}, \\frac{\\nu + \\beta - 2}{\\nu + n_{tr} - 2} \\times cov(\\mathbf{f_*}))  \\label{eq:posterior}\\\\\n    & \\beta = \\mathbf{y}^T K^{-1} \\mathbf{y} \\\\\n        & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\\n        & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*)\n\\end{align}\n\n\n\n\n\nSTP models for a single output\n\u00b6\n\n\nFor univariate GP models (single output), use the \nStudentTRegressionModel\n class (an extension of \nAbstractSTPRegressionModel\n). To construct a STP regression model you would need:\n\n\n\n\nThe degrees of freedom \n\\nu\n\\nu\n\n\nKernel/covariance instance to model correlation between values of the latent function at each pair of input features.\n\n\nKernel instance to model the correlation of the additive noise, generally the \nDiracKernel\n (white noise) is used.\n\n\nTraining data\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \ntrainingdata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nnum_features\n \n=\n \ntrainingdata\n.\nhead\n.\n_1\n.\nlength\n\n\n\n// Create an implicit vector field for the creation of the stationary\n\n\n// radial basis function kernel\n\n\nimplicit\n \nval\n \nfield\n \n=\n \nVectorField\n(\nnum_features\n)\n\n\n\nval\n \nkernel\n \n=\n \nnew\n \nRBFKernel\n(\n2.5\n)\n\n\nval\n \nnoiseKernel\n \n=\n \nnew\n \nDiracKernel\n(\n1.5\n)\n\n\nval\n \nmodel\n \n=\n \nnew\n \nStudentTRegression\n(\n1.5\n,\n \nkernel\n,\n \nnoiseKernel\n,\n \ntrainingData\n)\n\n\n\n\n\n\n\nSTP models for Multiple Outputs\n\u00b6\n\n\nYou can use the \nMOStudentTRegression\n[\nI\n]\n class to create multi-output GP models.\n\n\n1\n2\n3\n4\n5\nval\n \ntrainingdata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]\n \n=\n \n...\n\n\n\nval\n \nmodel\n \n=\n \nnew\n \nMOStudentTRegression\n[\nDenseVector\n[\nDouble\n]](\n\n    \nsos_kernel\n,\n \nsos_noise\n,\n \ntrainingdata\n,\n\n    \ntrainingdata\n.\nlength\n,\n \ntrainingdata\n.\nhead\n.\n_2\n.\nlength\n)\n\n\n\n\n\n\n\n\n\nTip\nWorking with multi-output Student T models is similar to \nmulti-output GP\n models. We need to create a kernel function over the combined index set \n(\nDenseVector\n[\nDouble\n],\n \nInt\n)\n. This can be done using the \nsum of separable\n kernel idea.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nval\n \nlinearK\n \n=\n \nnew\n \nPolynomialKernel\n(\n2\n,\n \n1.0\n)\n\n\nval\n \ntKernel\n \n=\n \nnew\n \nTStudentKernel\n(\n0.2\n)\n\n\nval\n \nd\n \n=\n \nnew\n \nDiracKernel\n(\n0.037\n)\n\n\n\nval\n \nmixedEffects\n \n=\n \nnew\n \nMixedEffectRegularizer\n(\n0.5\n)\n\n\nval\n \ncoRegCauchyMatrix\n \n=\n \nnew\n \nCoRegCauchyKernel\n(\n10.0\n)\n\n\nval\n \ncoRegDiracMatrix\n \n=\n \nnew\n \nCoRegDiracKernel\n\n\n\nval\n \nsos_kernel\n:\n \nCompositeCovariance\n[(\nDenseVector\n[\nDouble\n]\n, \nInt\n)]\n \n=\n\n    \n(\nlinearK\n \n:*\n \nmixedEffects\n)\n  \n+\n \n(\ntKernel\n \n:*\n \ncoRegCauchyMatrix\n)\n\n\n\nval\n \nsos_noise\n:\n \nCompositeCovariance\n[(\nDenseVector\n[\nDouble\n]\n, \nInt\n)]\n \n=\n\n    \nd\n \n:*\n \ncoRegDiracMatrix",
            "title": "Students T Processes"
        },
        {
            "location": "/core/core_stp/#regression-with-student-t-processes",
            "text": "The regression formulation for STP models is identical to the GP regression framework, to summarize the posterior predictive distribution takes the following form.  Suppose  \\mathbf{t} \\sim MVT_{n_{tr} + n_t}(\\nu, \\mathbf{0}, K) \\mathbf{t} \\sim MVT_{n_{tr} + n_t}(\\nu, \\mathbf{0}, K)  is the process producing the data.\nLet  [\\mathbf{f_*}]_{n_{t} \\times 1} [\\mathbf{f_*}]_{n_{t} \\times 1}  represent the values of the function on the test inputs and  [\\mathbf{y}]_{n_{tr} \\times 1} [\\mathbf{y}]_{n_{tr} \\times 1}  represent noisy observations made on the training data points.   \n\\begin{align}\n        & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim MVT_{\\nu + n_{tr}}(\\mathbf{\\bar{f_*}}, \\frac{\\nu + \\beta - 2}{\\nu + n_{tr} - 2} \\times cov(\\mathbf{f_*}))  \\label{eq:posterior}\\\\\n    & \\beta = \\mathbf{y}^T K^{-1} \\mathbf{y} \\\\\n        & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\\n        & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*)\n\\end{align}  \n\\begin{align}\n        & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim MVT_{\\nu + n_{tr}}(\\mathbf{\\bar{f_*}}, \\frac{\\nu + \\beta - 2}{\\nu + n_{tr} - 2} \\times cov(\\mathbf{f_*}))  \\label{eq:posterior}\\\\\n    & \\beta = \\mathbf{y}^T K^{-1} \\mathbf{y} \\\\\n        & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\\n        & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*)\n\\end{align}",
            "title": "Regression with Student T Processes"
        },
        {
            "location": "/core/core_stp/#stp-models-for-a-single-output",
            "text": "For univariate GP models (single output), use the  StudentTRegressionModel  class (an extension of  AbstractSTPRegressionModel ). To construct a STP regression model you would need:   The degrees of freedom  \\nu \\nu  Kernel/covariance instance to model correlation between values of the latent function at each pair of input features.  Kernel instance to model the correlation of the additive noise, generally the  DiracKernel  (white noise) is used.  Training data    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   trainingdata :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   num_features   =   trainingdata . head . _1 . length  // Create an implicit vector field for the creation of the stationary  // radial basis function kernel  implicit   val   field   =   VectorField ( num_features )  val   kernel   =   new   RBFKernel ( 2.5 )  val   noiseKernel   =   new   DiracKernel ( 1.5 )  val   model   =   new   StudentTRegression ( 1.5 ,   kernel ,   noiseKernel ,   trainingData )",
            "title": "STP models for a single output"
        },
        {
            "location": "/core/core_stp/#stp-models-for-multiple-outputs",
            "text": "You can use the  MOStudentTRegression [ I ]  class to create multi-output GP models.  1\n2\n3\n4\n5 val   trainingdata :   Stream [( DenseVector [ Double ] ,  DenseVector [ Double ])]   =   ...  val   model   =   new   MOStudentTRegression [ DenseVector [ Double ]]( \n     sos_kernel ,   sos_noise ,   trainingdata , \n     trainingdata . length ,   trainingdata . head . _2 . length )     Tip Working with multi-output Student T models is similar to  multi-output GP  models. We need to create a kernel function over the combined index set  ( DenseVector [ Double ],   Int ) . This can be done using the  sum of separable  kernel idea.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 val   linearK   =   new   PolynomialKernel ( 2 ,   1.0 )  val   tKernel   =   new   TStudentKernel ( 0.2 )  val   d   =   new   DiracKernel ( 0.037 )  val   mixedEffects   =   new   MixedEffectRegularizer ( 0.5 )  val   coRegCauchyMatrix   =   new   CoRegCauchyKernel ( 10.0 )  val   coRegDiracMatrix   =   new   CoRegDiracKernel  val   sos_kernel :   CompositeCovariance [( DenseVector [ Double ] ,  Int )]   = \n     ( linearK   :*   mixedEffects )    +   ( tKernel   :*   coRegCauchyMatrix )  val   sos_noise :   CompositeCovariance [( DenseVector [ Double ] ,  Int )]   = \n     d   :*   coRegDiracMatrix",
            "title": "STP models for Multiple Outputs"
        },
        {
            "location": "/core/core_multi_output_t/",
            "text": "Summary\n\n\nThe multi-output matrix T regression model was first described by \nConti and O' Hagan\n in their paper on Bayesian emulation of multi-output computer codes. It has been available in the \ndynaml.models.stp\n package of the \ndynaml-core\n module since \nv1.4.2\n.\n\n\n\n\nFormulation\n\u00b6\n\n\nThe model starts from the multi-output gaussian process framework. The quantity of interest is some unknown function \n\\mathbf{f}: \\mathcal{X} \\rightarrow \\mathbb{R}^q\n\\mathbf{f}: \\mathcal{X} \\rightarrow \\mathbb{R}^q\n, which maps inputs in \n\\mathcal{X}\n\\mathcal{X}\n (an arbitrary input space) to a \nq\nq\n dimensional vector outputs.\n\n\n\n\n\n\\begin{align}\n\\mathbf{f}(.)|B,\\Sigma, \\theta &\\sim \\mathcal{GP}(\\mathbf{m}(.), c(.,.)\\Sigma) \\\\\n\\mathbf{m}(x) &= B^\\intercal \\varphi(x)\n\\end{align}\n\n\n\n\n\\begin{align}\n\\mathbf{f}(.)|B,\\Sigma, \\theta &\\sim \\mathcal{GP}(\\mathbf{m}(.), c(.,.)\\Sigma) \\\\\n\\mathbf{m}(x) &= B^\\intercal \\varphi(x)\n\\end{align}\n\n\n\n\n\nThe input \nx\nx\n is transformed through \n\\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^m\n\\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^m\n which is a deterministic feature mapping which then calculates the inputs for a linear \nmean function\n \n\\mathbf{m}(.)\n\\mathbf{m}(.)\n. The parameters of this linear trend are contained in the matrix \nB \\in \\mathbb{R}^{m \\times q}\nB \\in \\mathbb{R}^{m \\times q}\n and \n\\theta\n\\theta\n contains all the covariance function hyper-parameters.\n\n\nThe prior distribution of the multi-output function is represented as a matrix normal distribution, with \nc(.,.)\nc(.,.)\n representing the covariance between two input points, and the entries of \n\\Sigma\n\\Sigma\n being the covariance between the output dimensions.\n\n\nThe predictive distribution when the output data \nD \\in \\mathbb{R}^{n\\times q}\nD \\in \\mathbb{R}^{n\\times q}\n is observed is calculated by first computing the conditional predictive distribution of \n\\mathbf{f}(.) | D, \\Sigma, B, \\theta\n\\mathbf{f}(.) | D, \\Sigma, B, \\theta\n and then integrating this distribution with respect to the posterior distributions \n\\Sigma|D\n\\Sigma|D\n and \nB|D\nB|D\n.\n\n\nThe resulting predictive distribution \n\\mathbf{f}(.)| \\theta, D\n\\mathbf{f}(.)| \\theta, D\n has the following structure.\n\n\n\n\n\n\\begin{align}\n\\mathbf{f}(.)|\\theta,D &\\sim \\mathcal{T}(\\mathbf{m}^{**}(.), c^{**}\\Sigma_{GLS};n-m) \\\\\n\\end{align}\n\n\n\n\n\\begin{align}\n\\mathbf{f}(.)|\\theta,D &\\sim \\mathcal{T}(\\mathbf{m}^{**}(.), c^{**}\\Sigma_{GLS};n-m) \\\\\n\\end{align}\n\n\n\n\n\nThe distribution is a \nmatrix variate T distribution\n. It is described by\n\n\n\n\nMean \n\\mathbf{m}^{**}(x)\n\\mathbf{m}^{**}(x)\n.\n\n\nCovariance between rows \nc^{**}(x_{1}, x_{2})\nc^{**}(x_{1}, x_{2})\n\n\nCovariance function between output columns \n\\Sigma_{GLS}\n\\Sigma_{GLS}\n\n\nDegrees of freedom \nn-m\nn-m\n.\n\n\n\n\n\n\n\\begin{align}\n\\mathbf{m}^{**}(x_{1}) &= B_{GLS}^{\\intercal}\\varphi(x_{1}) + (D-\\varphi(X)B_{GLS})^{\\intercal} C^{-1}c(x_{1},.)\\\\\nc^{**}(x_{1}, x_{2}) &= \\bar{c}(x_{1}, x_{2}) + \\hat{c}(x_{1}, x_{2})\\\\\n\\bar{c}(x_{1}, x_{2}) &= c(x_{1}, x_{2}) - C(x_{1},.)^{\\intercal}C^{-1}C(x_{2},.) \\\\\n\\hat{c}(x_{1}, x_{2}) &= H(x_{1})^{\\intercal}.A^{-1}.H(x_{2})\\\\\nH(x) &= (\\varphi(x) - \\varphi(X)C^{-1}c(x,.)) \\\\\nA &= \\varphi(X)^{\\intercal}C^{-1}\\varphi(X)\\\\\n\\end{align}\n\n\n\\begin{align}\n\\mathbf{m}^{**}(x_{1}) &= B_{GLS}^{\\intercal}\\varphi(x_{1}) + (D-\\varphi(X)B_{GLS})^{\\intercal} C^{-1}c(x_{1},.)\\\\\nc^{**}(x_{1}, x_{2}) &= \\bar{c}(x_{1}, x_{2}) + \\hat{c}(x_{1}, x_{2})\\\\\n\\bar{c}(x_{1}, x_{2}) &= c(x_{1}, x_{2}) - C(x_{1},.)^{\\intercal}C^{-1}C(x_{2},.) \\\\\n\\hat{c}(x_{1}, x_{2}) &= H(x_{1})^{\\intercal}.A^{-1}.H(x_{2})\\\\\nH(x) &= (\\varphi(x) - \\varphi(X)C^{-1}c(x,.)) \\\\\nA &= \\varphi(X)^{\\intercal}C^{-1}\\varphi(X)\\\\\n\\end{align}\n\n\n\n\nThe matrices \nB_{GLS} = (\\varphi(X)^{\\intercal}C^{-1}\\varphi(X))^{-1}\\varphi(X)^{\\intercal}C^{-1}D\nB_{GLS} = (\\varphi(X)^{\\intercal}C^{-1}\\varphi(X))^{-1}\\varphi(X)^{\\intercal}C^{-1}D\n and \n\\Sigma_{GLS} = (n-m)^{-1}(D - \\varphi(X)B_{GLS})^{\\intercal}C^{-1}(D - \\varphi(X)B_{GLS})\n\\Sigma_{GLS} = (n-m)^{-1}(D - \\varphi(X)B_{GLS})^{\\intercal}C^{-1}(D - \\varphi(X)B_{GLS})\n are the \ngeneralized least squares\n estimators for the matrices \nB\nB\n and \n\\Sigma\n\\Sigma\n which we saw in the formulation above.\n\n\nMulti-output Regression\n\u00b6\n\n\nAn implementation of the multi-output matrix T model is available via the class \nMVStudentsTModel\n. Instantiating the model is very similar to other stochastic process models in DynaML i.e. by specifying the covariance structures on signal and noise, training data, etc.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n//Obtain the data, some generic type\n\n\nval\n \ntrainingdata\n:\n \nDataType\n \n=\n \n_\n\n\nval\n \nnum_data_points\n:\n \nInt\n \n=\n \n_\n\n\nval\n \nnum_outputs\n:\nInt\n \n=\n \n_\n\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nI\n]\n \n=\n \n_\n\n\nval\n \nnoiseKernel\n:\n \nLocalScalarKernel\n[\nI\n]\n \n=\n \n_\n\n\nval\n \nfeature_map\n:\n \nDataPipe\n[\nI\n, \nDouble\n]\n \n=\n \n_\n\n\n\n//Define how the data is converted to a compatible type\n\n\nimplicit\n \nval\n \ntransform\n:\n \nDataPipe\n[\nDataType\n, \nSeq\n[(\nI\n, \nDouble\n)]]\n \n=\n \n_\n\n\n\nval\n \nmodel\n \n=\n \nMVStudentsTModel\n(\n\n  \nkernel\n,\n \nnoiseKernel\n,\n \nfeature_map\n)(\n\n  \ntrainingData\n,\n \nnum_data_points\n,\n \nnum_outputs\n)",
            "title": "Multi-output Matrix T Process"
        },
        {
            "location": "/core/core_multi_output_t/#formulation",
            "text": "The model starts from the multi-output gaussian process framework. The quantity of interest is some unknown function  \\mathbf{f}: \\mathcal{X} \\rightarrow \\mathbb{R}^q \\mathbf{f}: \\mathcal{X} \\rightarrow \\mathbb{R}^q , which maps inputs in  \\mathcal{X} \\mathcal{X}  (an arbitrary input space) to a  q q  dimensional vector outputs.   \n\\begin{align}\n\\mathbf{f}(.)|B,\\Sigma, \\theta &\\sim \\mathcal{GP}(\\mathbf{m}(.), c(.,.)\\Sigma) \\\\\n\\mathbf{m}(x) &= B^\\intercal \\varphi(x)\n\\end{align}  \n\\begin{align}\n\\mathbf{f}(.)|B,\\Sigma, \\theta &\\sim \\mathcal{GP}(\\mathbf{m}(.), c(.,.)\\Sigma) \\\\\n\\mathbf{m}(x) &= B^\\intercal \\varphi(x)\n\\end{align}   The input  x x  is transformed through  \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^m \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^m  which is a deterministic feature mapping which then calculates the inputs for a linear  mean function   \\mathbf{m}(.) \\mathbf{m}(.) . The parameters of this linear trend are contained in the matrix  B \\in \\mathbb{R}^{m \\times q} B \\in \\mathbb{R}^{m \\times q}  and  \\theta \\theta  contains all the covariance function hyper-parameters.  The prior distribution of the multi-output function is represented as a matrix normal distribution, with  c(.,.) c(.,.)  representing the covariance between two input points, and the entries of  \\Sigma \\Sigma  being the covariance between the output dimensions.  The predictive distribution when the output data  D \\in \\mathbb{R}^{n\\times q} D \\in \\mathbb{R}^{n\\times q}  is observed is calculated by first computing the conditional predictive distribution of  \\mathbf{f}(.) | D, \\Sigma, B, \\theta \\mathbf{f}(.) | D, \\Sigma, B, \\theta  and then integrating this distribution with respect to the posterior distributions  \\Sigma|D \\Sigma|D  and  B|D B|D .  The resulting predictive distribution  \\mathbf{f}(.)| \\theta, D \\mathbf{f}(.)| \\theta, D  has the following structure.   \n\\begin{align}\n\\mathbf{f}(.)|\\theta,D &\\sim \\mathcal{T}(\\mathbf{m}^{**}(.), c^{**}\\Sigma_{GLS};n-m) \\\\\n\\end{align}  \n\\begin{align}\n\\mathbf{f}(.)|\\theta,D &\\sim \\mathcal{T}(\\mathbf{m}^{**}(.), c^{**}\\Sigma_{GLS};n-m) \\\\\n\\end{align}   The distribution is a  matrix variate T distribution . It is described by   Mean  \\mathbf{m}^{**}(x) \\mathbf{m}^{**}(x) .  Covariance between rows  c^{**}(x_{1}, x_{2}) c^{**}(x_{1}, x_{2})  Covariance function between output columns  \\Sigma_{GLS} \\Sigma_{GLS}  Degrees of freedom  n-m n-m .    \\begin{align}\n\\mathbf{m}^{**}(x_{1}) &= B_{GLS}^{\\intercal}\\varphi(x_{1}) + (D-\\varphi(X)B_{GLS})^{\\intercal} C^{-1}c(x_{1},.)\\\\\nc^{**}(x_{1}, x_{2}) &= \\bar{c}(x_{1}, x_{2}) + \\hat{c}(x_{1}, x_{2})\\\\\n\\bar{c}(x_{1}, x_{2}) &= c(x_{1}, x_{2}) - C(x_{1},.)^{\\intercal}C^{-1}C(x_{2},.) \\\\\n\\hat{c}(x_{1}, x_{2}) &= H(x_{1})^{\\intercal}.A^{-1}.H(x_{2})\\\\\nH(x) &= (\\varphi(x) - \\varphi(X)C^{-1}c(x,.)) \\\\\nA &= \\varphi(X)^{\\intercal}C^{-1}\\varphi(X)\\\\\n\\end{align}  \\begin{align}\n\\mathbf{m}^{**}(x_{1}) &= B_{GLS}^{\\intercal}\\varphi(x_{1}) + (D-\\varphi(X)B_{GLS})^{\\intercal} C^{-1}c(x_{1},.)\\\\\nc^{**}(x_{1}, x_{2}) &= \\bar{c}(x_{1}, x_{2}) + \\hat{c}(x_{1}, x_{2})\\\\\n\\bar{c}(x_{1}, x_{2}) &= c(x_{1}, x_{2}) - C(x_{1},.)^{\\intercal}C^{-1}C(x_{2},.) \\\\\n\\hat{c}(x_{1}, x_{2}) &= H(x_{1})^{\\intercal}.A^{-1}.H(x_{2})\\\\\nH(x) &= (\\varphi(x) - \\varphi(X)C^{-1}c(x,.)) \\\\\nA &= \\varphi(X)^{\\intercal}C^{-1}\\varphi(X)\\\\\n\\end{align}   The matrices  B_{GLS} = (\\varphi(X)^{\\intercal}C^{-1}\\varphi(X))^{-1}\\varphi(X)^{\\intercal}C^{-1}D B_{GLS} = (\\varphi(X)^{\\intercal}C^{-1}\\varphi(X))^{-1}\\varphi(X)^{\\intercal}C^{-1}D  and  \\Sigma_{GLS} = (n-m)^{-1}(D - \\varphi(X)B_{GLS})^{\\intercal}C^{-1}(D - \\varphi(X)B_{GLS}) \\Sigma_{GLS} = (n-m)^{-1}(D - \\varphi(X)B_{GLS})^{\\intercal}C^{-1}(D - \\varphi(X)B_{GLS})  are the  generalized least squares  estimators for the matrices  B B  and  \\Sigma \\Sigma  which we saw in the formulation above.",
            "title": "Formulation"
        },
        {
            "location": "/core/core_multi_output_t/#multi-output-regression",
            "text": "An implementation of the multi-output matrix T model is available via the class  MVStudentsTModel . Instantiating the model is very similar to other stochastic process models in DynaML i.e. by specifying the covariance structures on signal and noise, training data, etc.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 //Obtain the data, some generic type  val   trainingdata :   DataType   =   _  val   num_data_points :   Int   =   _  val   num_outputs : Int   =   _  val   kernel :   LocalScalarKernel [ I ]   =   _  val   noiseKernel :   LocalScalarKernel [ I ]   =   _  val   feature_map :   DataPipe [ I ,  Double ]   =   _  //Define how the data is converted to a compatible type  implicit   val   transform :   DataPipe [ DataType ,  Seq [( I ,  Double )]]   =   _  val   model   =   MVStudentsTModel ( \n   kernel ,   noiseKernel ,   feature_map )( \n   trainingData ,   num_data_points ,   num_outputs )",
            "title": "Multi-output Regression"
        },
        {
            "location": "/core/core_ffn_new/",
            "text": "Feed forward neural networks\n\n\n\n\n\n\n\n\nFeed forward neural networks are the most common network architectures in predictive modeling, DynaML has an implementation of feed forward architectures that is trained using \nBackpropogation\n with momentum.\n\n\nIn a feed forward neural network with a single hidden layer the predicted target \ny\ny\n is expressed using the edge weights and node values in the following manner (this expression is easily extended for multi-layer nets).\n\n\n\n\n\n\\begin{equation}\ny = W_2 \\sigma(W_1 \\mathbf{x} + b_1) + b_2\n\\end{equation}\n\n\n\n\n\\begin{equation}\ny = W_2 \\sigma(W_1 \\mathbf{x} + b_1) + b_2\n\\end{equation}\n\n\n\n\n\nWhere \nW_1 , \\ W_2\nW_1 , \\ W_2\n  are matrices representing edge weights for the hidden layer and output layer respectively and \n\\sigma(.)\n\\sigma(.)\n represents a monotonic \nactivation\n function, the usual choices are \nsigmoid\n, \ntanh\n, \nlinear\n or \nrectified linear\n functions.\n\n\n\n\nThe new neural network API extends the same top level traits as the old API, i.e.\n\nNeuralNet\n[\nData\n, \nBaseGraph\n, \nInput\n, \nOutput\n,\nGraph\n \n<:\n \nNeuralGraph\n[\nBaseGraph\n, \nInput\n, \nOutput\n]]\n which itself extends the \nParameterizedLearner\n[\nData\n, \nGraph\n, \nInput\n, \nOutput\n, \nStream\n[(\nInput\n, \nOutput\n)]]\n trait.\n\n\n\n\nTip\n\n\nTo learn more about \nParameterizedLearner\n and other major model classes, refer to the \nmodel hierarchy\n specification.\n\n\n\n\nIn the case of \nNeuralNet\n, the \nparameters\n are a generic (unknown) type \nGraph\n which has to be an extension of \nNeuralGraph\n[\nBaseGraph\n, \nInput\n, \nOutput\n]\n]\n trait.\n\n\nCreating and training feed forward networks can be done by creating a back propagation instance and preparing the training data.\n\n\n\n\nTip\n\n\nFor a more in-depth picture of how the neural network API works refer to the \nneural stack\n page.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n//Data is of some generic type\n\n\nval\n \ndata\n:\n \nDataType\n \n=\n \n_\n\n\n\n//specify how this data can be\n\n\n//converted to a sequence of input and output vectors.\n\n\nval\n \ntransform\n\n\n:\n \nDataPipe\n[\nDataType\n, \nSeq\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]]\n \n=\n \n_\n\n\n\n//Create the stack factory\n\n\n//and back propagation instance\n\n\n//Input, Hidden, Output\n\n\nval\n \nbreezeStackFactory\n \n=\n \nNeuralStackFactory\n(\n\n  \nSeq\n(\n5\n,\n \n8\n,\n \n3\n))(\n\n  \nSeq\n(\nVectorSigmoid\n,\n \nVectorTansig\n)\n\n\n)\n\n\n\n//Random variable which samples layer weights\n\n\nval\n \nstackInitializer\n \n=\n \nGenericFFNeuralNet\n.\ngetWeightInitializer\n(\n\n  \nSeq\n(\n5\n,\n \n8\n,\n \n3\n)\n\n\n)\n\n\n\nval\n \nopt_backprop\n \n=\n\n  \nnew\n \nFFBackProp\n(\nbreezeStackFactory\n)\n\n    \n.\nsetNumIterations\n(\n2000\n)\n\n    \n.\nsetRegParam\n(\n0.001\n)\n\n    \n.\nsetStepSize\n(\n0.05\n)\n\n    \n.\nsetMiniBatchFraction\n(\n0.8\n)\n\n    \n.\nmomentum_\n(\n0.3\n)\n\n\n\nval\n \nff_neural_model\n \n=\n \nGenericFFNeuralNet\n(\n\n  \nopt_backprop\n,\n \ndata\n,\n\n  \ntransform\n,\n \nstackInitializer\n\n\n)\n\n\n\n//train the model\n\n\nff_neural_model\n.\nlearn\n()",
            "title": "Feed Forward Networks"
        },
        {
            "location": "/core/core_ann_new/",
            "text": "Warning\nThis API is deprecated since v1.5.2, users are advised to use the new \nTensorflow API\n.\n\n\n\n\n\n\n\n\nSummary\n\n\nIn v1.4.2, the neural stack API was introduced. It defines some primitives for modular\nconstruction of neural networks. The main idioms will be computational layers & stacks.  \n\n\n\n\nThe neural stack API extends these abstract skeletons by defining two kinds of primitives.\n\n\n\n\nComputational layers: Defining how inputs are propagated forward; \nNeuralLayer\n\n\nActivation functions: \nActivation\n[\nI\n]\n\n\nComputational stacks: composed of a number of layers; \nGenericNeuralStack\n\n\n\n\n\n\nNote\n\n\nThe classes \nNeuralLayer\n and \nGenericNeuralStack\n define layers and stacks in an abstract manner, meaning that the parameters could be in principle of any type.\n\n\nThe key point to understand is that once a layer or stack is defined, it is immutable i.e. the parameters defining its forward computation can't be changed.\n\n\nThe API rather provides \nfactory\n objects which can spawn a particular layer or stack with any parameter assignments.\n\n\n\n\nActivation Functions\n\u00b6\n\n\nActivation functions are implemented using the \nActivation\n[\nI\n]\n object, its \napply\n method requires two\narguments.\n\n\n\n\nImplementation of the activation\n\n\nImplementation of the derivative of the activation.\n\n\n\n\n1\n2\n3\n4\n5\n6\n//Define forward mapping\n\n\nval\n \nactFunc\n:\n \n(\nI\n)\n \n=>\n \nI\n \n=\n \n_\n\n\n//Define derivative of forward mapping\n\n\nval\n \ngradAct\n:\n \n(\nI\n)\n \n=>\n \nI\n \n=\n \n_\n\n\n\nval\n \nact\n \n=\n \nActivation\n(\nactFunc\n,\n \ngradAct\n)\n\n\n\n\n\n\n\nThe \ndynaml.models.neuralnets\n package also contains implementation of the following activations.\n\n\n\n\n\n\nSigmoid \ng(x) = \\frac{1}{1 + exp(-x)}\ng(x) = \\frac{1}{1 + exp(-x)}\n\n\nval\n \nact\n \n=\n \nVectorSigmoid\n\n\n\n\n\n\nTanh \ng(x) = tanh(x)\ng(x) = tanh(x)\n\n\nval\n \nact\n \n=\n \nVectorTansig\n\n\n\n\n\n\nLinear \ng(x) = x\ng(x) = x\n\n\nval\n \nact\n \n=\n \nVectorLinear\n\n\n\n\n\n\nRectified Linear \ng(x) = \\begin{cases} x & x \\geq 0\\\\0 & else\\end{cases}\ng(x) = \\begin{cases} x & x \\geq 0\\\\0 & else\\end{cases}\n\n\nval\n \nact\n \n=\n \nVectorRecLin\n\n\n\n\n\n\nComputational Layers\n\u00b6\n\n\nComputational layers are the most basic unit of neural networks. They define transformations of their inputs and with that define the forward data flow.\n\n\nEvery computational layer generally has a set of parameters describing how this transformation is going to be calculated given the inputs.\n\n\nIn DynaML, the central component of the \nNeuralLayer\n[\nParams\n, \nInput\n, \nOutput\n]\n trait is a \nMetaPipe\n[\nParams\n, \nInput\n, \nOutput\n]\n (\nhigher order pipe\n) instance.\n\n\nCreating Layers.\n\u00b6\n\n\nCreating an immutable computational layer can be done using the \nNeuralLayer\n object.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nimport\n \nscala.math._\n\n\n\nval\n \ncompute\n \n=\n \nMetaPipe\n(\n\n  \n(\nparams\n:\n \nDouble\n)\n \n=>\n \n(\nx\n:\n \nDouble\n)\n \n=>\n \n2\nd\n*\nPi\n*\nparams\n*\nx\n\n\n)\n\n\n\nval\n \nact\n \n=\n \nActivation\n(\n\n  \n(\nx\n:\n \nDouble\n)\n \n=>\n \ntanh\n(\nx\n),\n\n  \n(\nx\n:\n \nDouble\n)\n \n=>\n \ntanh\n(\nx\n)/(\nsinh\n(\nx\n)*\ncosh\n(\nx\n)))\n\n\n\nval\n \nlayer\n \n=\n \nNeuralLayer\n(\ncompute\n,\n \nact\n)(\n0.5\n)\n\n\n\n\n\n\n\n\n\nVector feed forward layers\n\n\nA common layer is the feed forward vector to vector layer which is given by.\n$$\n\\mathbf{h} = \\sigma(\\mathbf{W} \\mathbf{x} + \\mathbf{b})\n$$\n\n\n\n\nLayer Factories.\n\u00b6\n\n\nSince the computation and activation are the only two relevant inputs required to spawn any computational layer, the \nNeuralLayerFactory\n[\nParams\n, \nInput\n, \nOutput\n]\n class is the \nfactory\n for creating layers on the fly. Layer factories are data pipes which take the layer parameters as input and create computational layers on demand.\n\n\nA layer factory can be created as follows.\n\n\n1\n2\n3\nval\n \nfact\n \n=\n \nNeuralLayerFactory\n(\ncompute\n,\n \nact\n)\n\n\n\nval\n \nlayer1\n \n=\n \nfact\n(\n0.25\n)\n\n\n\n\n\n\n\n\n\nVector layer factory\n\n\nVector layers can be created using the \nVec2VecLayerFactory\n\n\n1\n2\nval\n \nlayerFactory\n \n=\n\n  \nnew\n \nVec2VecLayerFactory\n(\nVectorTansig\n)(\ninDim\n \n=\n \n4\n,\n \noutDim\n \n=\n \n5\n)\n\n\n\n\n\n\n\n\n\nNeural Stacks\n\u00b6\n\n\nA neural stack is a sequence of computational layers. Every layer represents some computation, so the neural stack is nothing but a sequence of computations or forward data flow. The top level class for neural stacks is \nGenericNeuralStack\n. Extending the base class there are two stack implementations.\n\n\n\n\n\n\nEagerly evaluated stack: Layers are spawned as soon as the stack is created.\n\n\n1\n2\n3\n4\n5\n6\nval\n \nlayers\n:\n \nSeq\n[\nNeuralLayer\n[\nP\n, \nI\n, \nI\n]]\n \n=\n \n_\n\n\n\n//Variable argument apply function\n\n\n//so the elements of the sequence\n\n\n//must be enumerated.\n\n\nval\n \nstack\n \n=\n \nNeuralStack\n(\nlayers\n:_\n*\n)\n\n\n\n\n\n\n\n\n\n\n\nLazy stack: Layers are spawned only as needed, but once created they are \nmemoized\n.\n\n\n1\n2\n3\nval\n \nlayers_func\n:\n \n(\nInt\n)\n \n=>\n \nNeuralLayer\n[\nP\n, \nI\n, \nI\n]\n \n=\n \n_\n\n\n\nval\n \nstack\n \n=\n \nLazyNeuralStack\n(\nlayers_func\n,\n \nnum_layers\n \n=\n \n4\n)\n\n\n\n\n\n\n\n\n\n\n\nStack Factories\n\u00b6\n\n\nStack factories like layer factories are pipe lines, which take as input a sequence of layer parameters and return a neural stack of the spawned layers.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nval\n \nlayerFactories\n:\n \nSeq\n[\nNeuralLayerFactory\n[\nP\n, \nI\n, \nI\n]]\n \n=\n \n_\n\n\n//Create a stack factory from a sequence of layer factories\n\n\nval\n \nstackFactory\n \n=\n \nNeuralStackFactory\n(\nlayerFactories\n:_\n*\n)\n\n\n\n//Create a stack factory that creates\n\n\n//feed forward neural stacks that take as inputs\n\n\n//breeze vectors.\n\n\n\n//Input, Hidden, Output\n\n\nval\n \nnum_units_by_layer\n \n=\n \nSeq\n(\n5\n,\n \n8\n,\n \n3\n)\n\n\nval\n \nacts\n \n=\n \nSeq\n(\nVectorSigmoid\n,\n \nVectorTansig\n)\n\n\nval\n \nbreezeStackFactory\n \n=\n \nNeuralStackFactory\n(\nnum_units_by_layer\n)(\nacts\n)",
            "title": "Neural Stack API"
        },
        {
            "location": "/core/core_ann_new/#activation-functions",
            "text": "Activation functions are implemented using the  Activation [ I ]  object, its  apply  method requires two\narguments.   Implementation of the activation  Implementation of the derivative of the activation.   1\n2\n3\n4\n5\n6 //Define forward mapping  val   actFunc :   ( I )   =>   I   =   _  //Define derivative of forward mapping  val   gradAct :   ( I )   =>   I   =   _  val   act   =   Activation ( actFunc ,   gradAct )    The  dynaml.models.neuralnets  package also contains implementation of the following activations.    Sigmoid  g(x) = \\frac{1}{1 + exp(-x)} g(x) = \\frac{1}{1 + exp(-x)}  val   act   =   VectorSigmoid    Tanh  g(x) = tanh(x) g(x) = tanh(x)  val   act   =   VectorTansig    Linear  g(x) = x g(x) = x  val   act   =   VectorLinear    Rectified Linear  g(x) = \\begin{cases} x & x \\geq 0\\\\0 & else\\end{cases} g(x) = \\begin{cases} x & x \\geq 0\\\\0 & else\\end{cases}  val   act   =   VectorRecLin",
            "title": "Activation Functions"
        },
        {
            "location": "/core/core_ann_new/#computational-layers",
            "text": "Computational layers are the most basic unit of neural networks. They define transformations of their inputs and with that define the forward data flow.  Every computational layer generally has a set of parameters describing how this transformation is going to be calculated given the inputs.  In DynaML, the central component of the  NeuralLayer [ Params ,  Input ,  Output ]  trait is a  MetaPipe [ Params ,  Input ,  Output ]  ( higher order pipe ) instance.",
            "title": "Computational Layers"
        },
        {
            "location": "/core/core_ann_new/#creating-layers",
            "text": "Creating an immutable computational layer can be done using the  NeuralLayer  object.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 import   scala.math._  val   compute   =   MetaPipe ( \n   ( params :   Double )   =>   ( x :   Double )   =>   2 d * Pi * params * x  )  val   act   =   Activation ( \n   ( x :   Double )   =>   tanh ( x ), \n   ( x :   Double )   =>   tanh ( x )/( sinh ( x )* cosh ( x )))  val   layer   =   NeuralLayer ( compute ,   act )( 0.5 )     Vector feed forward layers  A common layer is the feed forward vector to vector layer which is given by.\n$$\n\\mathbf{h} = \\sigma(\\mathbf{W} \\mathbf{x} + \\mathbf{b})\n$$",
            "title": "Creating Layers."
        },
        {
            "location": "/core/core_ann_new/#layer-factories",
            "text": "Since the computation and activation are the only two relevant inputs required to spawn any computational layer, the  NeuralLayerFactory [ Params ,  Input ,  Output ]  class is the  factory  for creating layers on the fly. Layer factories are data pipes which take the layer parameters as input and create computational layers on demand.  A layer factory can be created as follows.  1\n2\n3 val   fact   =   NeuralLayerFactory ( compute ,   act )  val   layer1   =   fact ( 0.25 )     Vector layer factory  Vector layers can be created using the  Vec2VecLayerFactory  1\n2 val   layerFactory   = \n   new   Vec2VecLayerFactory ( VectorTansig )( inDim   =   4 ,   outDim   =   5 )",
            "title": "Layer Factories."
        },
        {
            "location": "/core/core_ann_new/#neural-stacks",
            "text": "A neural stack is a sequence of computational layers. Every layer represents some computation, so the neural stack is nothing but a sequence of computations or forward data flow. The top level class for neural stacks is  GenericNeuralStack . Extending the base class there are two stack implementations.    Eagerly evaluated stack: Layers are spawned as soon as the stack is created.  1\n2\n3\n4\n5\n6 val   layers :   Seq [ NeuralLayer [ P ,  I ,  I ]]   =   _  //Variable argument apply function  //so the elements of the sequence  //must be enumerated.  val   stack   =   NeuralStack ( layers :_ * )      Lazy stack: Layers are spawned only as needed, but once created they are  memoized .  1\n2\n3 val   layers_func :   ( Int )   =>   NeuralLayer [ P ,  I ,  I ]   =   _  val   stack   =   LazyNeuralStack ( layers_func ,   num_layers   =   4 )",
            "title": "Neural Stacks"
        },
        {
            "location": "/core/core_ann_new/#stack-factories",
            "text": "Stack factories like layer factories are pipe lines, which take as input a sequence of layer parameters and return a neural stack of the spawned layers.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 val   layerFactories :   Seq [ NeuralLayerFactory [ P ,  I ,  I ]]   =   _  //Create a stack factory from a sequence of layer factories  val   stackFactory   =   NeuralStackFactory ( layerFactories :_ * )  //Create a stack factory that creates  //feed forward neural stacks that take as inputs  //breeze vectors.  //Input, Hidden, Output  val   num_units_by_layer   =   Seq ( 5 ,   8 ,   3 )  val   acts   =   Seq ( VectorSigmoid ,   VectorTansig )  val   breezeStackFactory   =   NeuralStackFactory ( num_units_by_layer )( acts )",
            "title": "Stack Factories"
        },
        {
            "location": "/core/core_ann/",
            "text": "Warning\nThis API is deprecated since v1.4.2, users are advised to use the new \nneural stack API\n.\n\n\n\n\n\n\nFeed-forward Network\n\u00b6\n\n\nTo create a feedforward network we need three entities.\n\n\n\n\nThe training data (type parameter \nD\n)\n\n\nA data pipe which transforms the original data into a data structure that understood by the \nFeedForwardNetwork\n\n\nThe network architecture (i.e. the network as a graph object)\n\n\n\n\nNetwork graph\n\u00b6\n\n\nA standard feedforward network can be created by first initializing the network architecture/graph.\n\n\n1\n2\nval\n \ngr\n \n=\n \nFFNeuralGraph\n(\nnum_inputs\n \n=\n \n3\n,\n \nnum_outputs\n \n=\n \n1\n,\n\n\nhidden_layers\n \n=\n \n1\n,\n \nList\n(\n\"logsig\"\n,\n \n\"linear\"\n),\n \nList\n(\n5\n))\n\n\n\n\n\n\n\nThis creates a neural network graph with one hidden layer, 3 input nodes, 1 output node and assigns sigmoid activation in the hidden layer. It also creates 5 neurons in the hidden layer.\n\n\nNext we create a data transform pipe which converts instances of the data input-output patterns to \n(DenseVector[Double], DenseVector[Double])\n, this is required in many data processing applications where the data structure storing the training data is not a \nbreeze\n vector.\n\n\nLets say we have data in the form \ntrainingdata: Stream[(DenseVector[Double], Double)]\n, i.e. we have input features as breeze vectors and scalar output values which help the network learn an unknown function. We can write the transform as.\n\n\n1\n2\n3\n4\nval\n \ntransform\n \n=\n \nDataPipe\n(\n\n    \n(\nd\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)])\n \n=>\n\n        \nd\n.\nmap\n(\nel\n \n=>\n \n(\nel\n.\n_1\n,\n \nDenseVector\n(\nel\n.\n_2\n)))\n\n\n)\n\n\n\n\n\n\n\nModel Building\n\u00b6\n\n\nWe are now in a position to initialize a feed forward neural network model.\n\n\n1\n2\n3\nval\n \nmodel\n \n=\n \nnew\n \nFeedForwardNetwork\n[\n\n    \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n\n\n](\ntrainingdata\n,\n \ngr\n,\n \ntransform\n)\n\n\n\n\n\n\n\nHere the variable \ntrainingdata\n represents the training input output pairs, which must conform to the type argument given in square brackets (i.e. \nStream[(DenseVector[Double], Double)]\n).\n\n\nTraining the model using back propagation can be done as follows, you can set custom values for the backpropagation parameters like the learning rate, momentum factor, mini batch fraction, regularization and number of learning iterations.\n\n\n1\n2\n3\n4\n5\n6\nmodel\n.\nsetLearningRate\n(\n0.09\n)\n\n   \n.\nsetMaxIterations\n(\n100\n)\n\n   \n.\nsetBatchFraction\n(\n0.85\n)\n\n   \n.\nsetMomentum\n(\n0.45\n)\n\n   \n.\nsetRegParam\n(\n0.0001\n)\n\n   \n.\nlearn\n()\n\n\n\n\n\n\n\nThe trained model can now be used for prediction, by using either the \npredict()\n method or the \nfeedForward()\n value member both of which are members of \nFeedForwardNetwork\n (refer to the \napi\n docs for more details).\n\n\n1\n2\nval\n \npattern\n \n=\n \nDenseVector\n(\n2.0\n,\n \n3.5\n,\n \n2.5\n)\n\n\nval\n \nprediction\n \n=\n \nmodel\n.\npredict\n(\npattern\n)\n\n\n\n\n\n\n\nSparse Autoencoder\n\u00b6\n\n\nSparse autoencoders\n are a feedforward architecture that are useful for unsupervised feature learning. They learn a compressed (or expanded) vector representation of the original data features. This process is known by various terms like \nfeature learning\n, \nfeature engineering\n, \nrepresentation learning\n etc. Autoencoders are amongst several models used for feature learning. Other notable examples include \nconvolutional neural networks\n (CNN), \nprincipal component analysis\n (PCA), \nSingular Value Decomposition\n (PCA) (a variant of  PCA), \nDiscrete Wavelet Transform\n (DWT), etc.\n\n\nCreation\n\u00b6\n\n\nAutoencoders can be created using the \nAutoEncoder\n class. Its constructor has the following arguments.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nimport\n \nio.github.mandar2812.dynaml.models.neuralnets._\n\n\nimport\n \nio.github.mandar2812.dynaml.models.neuralnets.TransferFunctions._\n\n\nimport\n \nio.github.mandar2812.dynaml.optimization.BackPropagation\n\n\n\n//Cast the training data as a stream of (x,x),\n\n\n//where x are the DenseVector of features\n\n\nval\n \ntrainingData\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]\n \n=\n \n...\n\n\n\nval\n \ntestData\n \n=\n \n...\n\n\n\nval\n \nenc\n \n=\n \nnew\n \nAutoEncoder\n(\n\n    \ninDim\n \n=\n \ntrainingData\n.\nhead\n.\n_1\n.\nlength\n,\n\n    \noutDim\n \n=\n \n4\n,\n \nacts\n \n=\n \nList\n(\nSIGMOID\n,\n \nLIN\n))\n\n\n\n\n\n\n\nTraining\n\u00b6\n\n\nThe training algorithm used is a modified version of standard back-propagation. The objective function can be seen as an addition of three terms.\n\n\n\n\n\n\\begin{align}\n\\mathcal{J}(\\mathbf{W}, \\mathbf{X}; \\lambda, \\rho) &= \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) + \\lambda \\mathcal{R}(\\mathbf{W}) + KL(\\hat{\\rho}\\ ||\\ \\rho) \\\\\nKL(\\hat{\\rho}\\ ||\\ \\rho) &= \\sum_{i = 1}^{n_h} \\rho log(\\frac{\\rho}{\\hat{\\rho}_i}) + (1 - \\rho) log(\\frac{1-\\rho}{1-\\hat{\\rho}_i}) \\\\\n\\hat{\\rho}_i &= \\frac{1}{m} \\sum_{j = 1}^{N} a_{i}(x_j)\n\\end{align}\n\n\n\n\n\\begin{align}\n\\mathcal{J}(\\mathbf{W}, \\mathbf{X}; \\lambda, \\rho) &= \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) + \\lambda \\mathcal{R}(\\mathbf{W}) + KL(\\hat{\\rho}\\ ||\\ \\rho) \\\\\nKL(\\hat{\\rho}\\ ||\\ \\rho) &= \\sum_{i = 1}^{n_h} \\rho log(\\frac{\\rho}{\\hat{\\rho}_i}) + (1 - \\rho) log(\\frac{1-\\rho}{1-\\hat{\\rho}_i}) \\\\\n\\hat{\\rho}_i &= \\frac{1}{m} \\sum_{j = 1}^{N} a_{i}(x_j)\n\\end{align}\n\n\n\n\n\n\n\n\n\n\\mathcal{L}(\\mathbf{W}, \\mathbf{X})\n\\mathcal{L}(\\mathbf{W}, \\mathbf{X})\n is the least squares loss.\n\n\n\n\n\n\n\\mathcal{R}(\\mathbf{W})\n\\mathcal{R}(\\mathbf{W})\n is the regularization penalty, with parameter \n\\lambda\n\\lambda\n.\n\n\n\n\n\n\nKL(\\hat{\\rho} \\| \\rho)\nKL(\\hat{\\rho} \\| \\rho)\n is the \nKullback Leibler\n divergence, between the average activation (over all data instances \nx \\in \\mathbf{X}\nx \\in \\mathbf{X}\n) of each hidden node and a specified value \n\\rho \\in [0,1]\n\\rho \\in [0,1]\n which is also known as the \nsparsity weight\n.\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n//Set sparsity parameter for back propagation\n\n\nBackPropagation\n.\nrho\n \n=\n \n0.5\n\n\n\nenc\n.\noptimizer\n\n  \n.\nsetRegParam\n(\n0.0\n)\n\n  \n.\nsetStepSize\n(\n1.5\n)\n\n  \n.\nsetNumIterations\n(\n200\n)\n\n  \n.\nsetMomentum\n(\n0.4\n)\n\n  \n.\nsetSparsityWeight\n(\n0.9\n)\n\n\n\nenc\n.\nlearn\n(\ntrainingData\n.\ntoStream\n)\n\n\n\nval\n \nmetrics\n \n=\n \nnew\n \nMultiRegressionMetrics\n(\n\n    \ntestData\n.\nmap\n(\nc\n \n=>\n \n(\nenc\n.\ni\n(\nenc\n(\nc\n.\n_1\n)),\n \nc\n.\n_2\n)).\ntoList\n,\n\n    \ntestData\n.\nlength\n)",
            "title": "Old Neural Net API"
        },
        {
            "location": "/core/core_ann/#feed-forward-network",
            "text": "To create a feedforward network we need three entities.   The training data (type parameter  D )  A data pipe which transforms the original data into a data structure that understood by the  FeedForwardNetwork  The network architecture (i.e. the network as a graph object)",
            "title": "Feed-forward Network"
        },
        {
            "location": "/core/core_ann/#network-graph",
            "text": "A standard feedforward network can be created by first initializing the network architecture/graph.  1\n2 val   gr   =   FFNeuralGraph ( num_inputs   =   3 ,   num_outputs   =   1 ,  hidden_layers   =   1 ,   List ( \"logsig\" ,   \"linear\" ),   List ( 5 ))    This creates a neural network graph with one hidden layer, 3 input nodes, 1 output node and assigns sigmoid activation in the hidden layer. It also creates 5 neurons in the hidden layer.  Next we create a data transform pipe which converts instances of the data input-output patterns to  (DenseVector[Double], DenseVector[Double]) , this is required in many data processing applications where the data structure storing the training data is not a  breeze  vector.  Lets say we have data in the form  trainingdata: Stream[(DenseVector[Double], Double)] , i.e. we have input features as breeze vectors and scalar output values which help the network learn an unknown function. We can write the transform as.  1\n2\n3\n4 val   transform   =   DataPipe ( \n     ( d :   Stream [( DenseVector [ Double ] ,  Double )])   => \n         d . map ( el   =>   ( el . _1 ,   DenseVector ( el . _2 )))  )",
            "title": "Network graph"
        },
        {
            "location": "/core/core_ann/#model-building",
            "text": "We are now in a position to initialize a feed forward neural network model.  1\n2\n3 val   model   =   new   FeedForwardNetwork [ \n     Stream [( DenseVector [ Double ] ,  Double )]  ]( trainingdata ,   gr ,   transform )    Here the variable  trainingdata  represents the training input output pairs, which must conform to the type argument given in square brackets (i.e.  Stream[(DenseVector[Double], Double)] ).  Training the model using back propagation can be done as follows, you can set custom values for the backpropagation parameters like the learning rate, momentum factor, mini batch fraction, regularization and number of learning iterations.  1\n2\n3\n4\n5\n6 model . setLearningRate ( 0.09 ) \n    . setMaxIterations ( 100 ) \n    . setBatchFraction ( 0.85 ) \n    . setMomentum ( 0.45 ) \n    . setRegParam ( 0.0001 ) \n    . learn ()    The trained model can now be used for prediction, by using either the  predict()  method or the  feedForward()  value member both of which are members of  FeedForwardNetwork  (refer to the  api  docs for more details).  1\n2 val   pattern   =   DenseVector ( 2.0 ,   3.5 ,   2.5 )  val   prediction   =   model . predict ( pattern )",
            "title": "Model Building"
        },
        {
            "location": "/core/core_ann/#sparse-autoencoder",
            "text": "Sparse autoencoders  are a feedforward architecture that are useful for unsupervised feature learning. They learn a compressed (or expanded) vector representation of the original data features. This process is known by various terms like  feature learning ,  feature engineering ,  representation learning  etc. Autoencoders are amongst several models used for feature learning. Other notable examples include  convolutional neural networks  (CNN),  principal component analysis  (PCA),  Singular Value Decomposition  (PCA) (a variant of  PCA),  Discrete Wavelet Transform  (DWT), etc.",
            "title": "Sparse Autoencoder"
        },
        {
            "location": "/core/core_ann/#creation",
            "text": "Autoencoders can be created using the  AutoEncoder  class. Its constructor has the following arguments.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 import   io.github.mandar2812.dynaml.models.neuralnets._  import   io.github.mandar2812.dynaml.models.neuralnets.TransferFunctions._  import   io.github.mandar2812.dynaml.optimization.BackPropagation  //Cast the training data as a stream of (x,x),  //where x are the DenseVector of features  val   trainingData :   Stream [( DenseVector [ Double ] ,  DenseVector [ Double ])]   =   ...  val   testData   =   ...  val   enc   =   new   AutoEncoder ( \n     inDim   =   trainingData . head . _1 . length , \n     outDim   =   4 ,   acts   =   List ( SIGMOID ,   LIN ))",
            "title": "Creation"
        },
        {
            "location": "/core/core_ann/#training",
            "text": "The training algorithm used is a modified version of standard back-propagation. The objective function can be seen as an addition of three terms.   \n\\begin{align}\n\\mathcal{J}(\\mathbf{W}, \\mathbf{X}; \\lambda, \\rho) &= \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) + \\lambda \\mathcal{R}(\\mathbf{W}) + KL(\\hat{\\rho}\\ ||\\ \\rho) \\\\\nKL(\\hat{\\rho}\\ ||\\ \\rho) &= \\sum_{i = 1}^{n_h} \\rho log(\\frac{\\rho}{\\hat{\\rho}_i}) + (1 - \\rho) log(\\frac{1-\\rho}{1-\\hat{\\rho}_i}) \\\\\n\\hat{\\rho}_i &= \\frac{1}{m} \\sum_{j = 1}^{N} a_{i}(x_j)\n\\end{align}  \n\\begin{align}\n\\mathcal{J}(\\mathbf{W}, \\mathbf{X}; \\lambda, \\rho) &= \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) + \\lambda \\mathcal{R}(\\mathbf{W}) + KL(\\hat{\\rho}\\ ||\\ \\rho) \\\\\nKL(\\hat{\\rho}\\ ||\\ \\rho) &= \\sum_{i = 1}^{n_h} \\rho log(\\frac{\\rho}{\\hat{\\rho}_i}) + (1 - \\rho) log(\\frac{1-\\rho}{1-\\hat{\\rho}_i}) \\\\\n\\hat{\\rho}_i &= \\frac{1}{m} \\sum_{j = 1}^{N} a_{i}(x_j)\n\\end{align}     \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) \\mathcal{L}(\\mathbf{W}, \\mathbf{X})  is the least squares loss.    \\mathcal{R}(\\mathbf{W}) \\mathcal{R}(\\mathbf{W})  is the regularization penalty, with parameter  \\lambda \\lambda .    KL(\\hat{\\rho} \\| \\rho) KL(\\hat{\\rho} \\| \\rho)  is the  Kullback Leibler  divergence, between the average activation (over all data instances  x \\in \\mathbf{X} x \\in \\mathbf{X} ) of each hidden node and a specified value  \\rho \\in [0,1] \\rho \\in [0,1]  which is also known as the  sparsity weight .     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 //Set sparsity parameter for back propagation  BackPropagation . rho   =   0.5  enc . optimizer \n   . setRegParam ( 0.0 ) \n   . setStepSize ( 1.5 ) \n   . setNumIterations ( 200 ) \n   . setMomentum ( 0.4 ) \n   . setSparsityWeight ( 0.9 )  enc . learn ( trainingData . toStream )  val   metrics   =   new   MultiRegressionMetrics ( \n     testData . map ( c   =>   ( enc . i ( enc ( c . _1 )),   c . _2 )). toList , \n     testData . length )",
            "title": "Training"
        },
        {
            "location": "/core/core_lssvm/",
            "text": "Least Squares Support Vector Machines are a modification of the classical Support Vector Machine, please see \nSuykens et. al\n for a complete background.\n\n\n\n\nLSSVM Regression\n\u00b6\n\n\nIn case of LSSVM regression one solves (by applying the \nKKT\n conditions) the following constrained optimization problem.\n\n\n\n\n\n    \\begin{align}\n        & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\\n        & y_k = w^\\intercal \\varphi(x) + b + e_k, \\ k =1, \\cdots, N\n    \\end{align}\n\n\n\n\n    \\begin{align}\n        & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\\n        & y_k = w^\\intercal \\varphi(x) + b + e_k, \\ k =1, \\cdots, N\n    \\end{align}\n\n\n\n\n\nLeading to a predictive model of the form.\n\n\n\n\n\n    \\begin{equation}\n        y(x) = \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b\n    \\end{equation}\n\n\n\n\n    \\begin{equation}\n        y(x) = \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b\n    \\end{equation}\n\n\n\n\n\nWhere the values \n\\alpha \\ \\& \\ b\n\\alpha \\ \\& \\ b\n are the solution of\n\n\n\n\n\n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\end{equation}\n\n\n\n\n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\end{equation}\n\n\n\n\n\nHere \nK\n is the \nN \\times N\nN \\times N\n kernel matrix whose entries are given by \nK_{kl} = \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N\nK_{kl} = \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N\n and \nI\nI\n is the identity matrix of order \nN\nN\n.\n\n\nLSSVM Classification\n\u00b6\n\n\nIn case of LSSVM for binary classification one solves (by applying the \nKKT\n conditions) the following constrained optimization problem.\n\n\n\n\n\n    \\begin{align}\n        & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\\n        & y_k[w^\\intercal \\varphi(x) + b] = 1 - e_k, \\ k =1, \\cdots, N\n    \\end{align}\n\n\n\n\n    \\begin{align}\n        & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\\n        & y_k[w^\\intercal \\varphi(x) + b] = 1 - e_k, \\ k =1, \\cdots, N\n    \\end{align}\n\n\n\n\n\nLeading to a classifier of the form.\n\n\n\n\n\n    \\begin{equation}\n        y(x) = sign \\left[ \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\right]\n    \\end{equation}\n\n\n\n\n    \\begin{equation}\n        y(x) = sign \\left[ \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\right]\n    \\end{equation}\n\n\n\n\n\nWhere the values \n\\alpha \\ \\& \\ b\n\\alpha \\ \\& \\ b\n are the solution of\n\n\n\n\n\n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & y^\\intercal   \\\\ \\hline\n   y & \\Omega + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   1_v  \n\\end{array}\\right]\n\\end{equation}\n\n\n\n\n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & y^\\intercal   \\\\ \\hline\n   y & \\Omega + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   1_v  \n\\end{array}\\right]\n\\end{equation}\n\n\n\n\n\nHere \n\\Omega\n\\Omega\n is the \nN \\times N\nN \\times N\n matrix whose entries are given by\n\n\n\n\n\n\\begin{align}\n \\Omega_{kl} & = y_{k} y_{l} \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N \\\\\n             & = y_{k} y_{l} K(x_k, x_l)\n\\end{align}\n\n\n\n\n\\begin{align}\n \\Omega_{kl} & = y_{k} y_{l} \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N \\\\\n             & = y_{k} y_{l} K(x_k, x_l)\n\\end{align}\n\n\n\n\n\nand \nI\nI\n is the identity matrix of order \nN\nN\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n// Create the training data set\n\n\n\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nnumPoints\n \n=\n \ndata\n.\nlength\n\n\nval\n \nnum_features\n \n=\n \ndata\n.\nhead\n.\n_1\n.\nlength\n\n\n\n// Create an implicit vector field for the creation of the stationary\n\n\n// radial basis function kernel\n\n\n\nimplicit\n \nval\n \nfield\n \n=\n \nVectorField\n(\nnum_features\n)\n\n\nval\n \nkern\n \n=\n \nnew\n \nRBFKernel\n(\n2.0\n)\n\n\n\n//Create the model\n\n\nval\n \nlssvmModel\n \n=\n \nnew\n \nDLSSVM\n(\ndata\n,\n \nnumPoints\n,\n \nkern\n,\n \nmodelTask\n \n=\n \n\"regression\"\n)\n\n\n\n//Set the regularization parameter and learn the model\n\n\nmodel\n.\nsetRegParam\n(\n1.5\n).\nlearn\n()",
            "title": "Least Squares SVM"
        },
        {
            "location": "/core/core_lssvm/#lssvm-regression",
            "text": "In case of LSSVM regression one solves (by applying the  KKT  conditions) the following constrained optimization problem.   \n    \\begin{align}\n        & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\\n        & y_k = w^\\intercal \\varphi(x) + b + e_k, \\ k =1, \\cdots, N\n    \\end{align}  \n    \\begin{align}\n        & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\\n        & y_k = w^\\intercal \\varphi(x) + b + e_k, \\ k =1, \\cdots, N\n    \\end{align}   Leading to a predictive model of the form.   \n    \\begin{equation}\n        y(x) = \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b\n    \\end{equation}  \n    \\begin{equation}\n        y(x) = \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b\n    \\end{equation}   Where the values  \\alpha \\ \\& \\ b \\alpha \\ \\& \\ b  are the solution of   \n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\end{equation}  \n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\end{equation}   Here  K  is the  N \\times N N \\times N  kernel matrix whose entries are given by  K_{kl} = \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N K_{kl} = \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N  and  I I  is the identity matrix of order  N N .",
            "title": "LSSVM Regression"
        },
        {
            "location": "/core/core_lssvm/#lssvm-classification",
            "text": "In case of LSSVM for binary classification one solves (by applying the  KKT  conditions) the following constrained optimization problem.   \n    \\begin{align}\n        & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\\n        & y_k[w^\\intercal \\varphi(x) + b] = 1 - e_k, \\ k =1, \\cdots, N\n    \\end{align}  \n    \\begin{align}\n        & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\\n        & y_k[w^\\intercal \\varphi(x) + b] = 1 - e_k, \\ k =1, \\cdots, N\n    \\end{align}   Leading to a classifier of the form.   \n    \\begin{equation}\n        y(x) = sign \\left[ \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\right]\n    \\end{equation}  \n    \\begin{equation}\n        y(x) = sign \\left[ \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\right]\n    \\end{equation}   Where the values  \\alpha \\ \\& \\ b \\alpha \\ \\& \\ b  are the solution of   \n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & y^\\intercal   \\\\ \\hline\n   y & \\Omega + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   1_v  \n\\end{array}\\right]\n\\end{equation}  \n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & y^\\intercal   \\\\ \\hline\n   y & \\Omega + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   1_v  \n\\end{array}\\right]\n\\end{equation}   Here  \\Omega \\Omega  is the  N \\times N N \\times N  matrix whose entries are given by   \n\\begin{align}\n \\Omega_{kl} & = y_{k} y_{l} \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N \\\\\n             & = y_{k} y_{l} K(x_k, x_l)\n\\end{align}  \n\\begin{align}\n \\Omega_{kl} & = y_{k} y_{l} \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N \\\\\n             & = y_{k} y_{l} K(x_k, x_l)\n\\end{align}   and  I I  is the identity matrix of order  N N .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 // Create the training data set  val   data :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   numPoints   =   data . length  val   num_features   =   data . head . _1 . length  // Create an implicit vector field for the creation of the stationary  // radial basis function kernel  implicit   val   field   =   VectorField ( num_features )  val   kern   =   new   RBFKernel ( 2.0 )  //Create the model  val   lssvmModel   =   new   DLSSVM ( data ,   numPoints ,   kern ,   modelTask   =   \"regression\" )  //Set the regularization parameter and learn the model  model . setRegParam ( 1.5 ). learn ()",
            "title": "LSSVM Classification"
        },
        {
            "location": "/core/core_dynaml_tf/",
            "text": "Summary\n\n\nSince v1.5.2, DynaML has moved towards closer integration with Google Tensorflow. This is done via the\n\nTensorflow for Scala\n project. The DynaML \n\ntensorflow\n \npackage builds on \nTensorflow for Scala\n and provides a high level API containing several convenience routines and \nbuilding blocks for deep learning.\n\n\n\n\nGoogle Tensorflow\n\u00b6\n\n\n\n\n\n\ncourtesy Google.\n\n\n\n\nTensorflow\n is a versatile and general computational framework \nfor working with tensors and \ncomputational graphs\n. \nIt provides tensor primitives as well as the ability to define transformations on them. Under the hood, \nthese transformations are baked into a \ncomputational graph\n. Obtaining results of computations now \nbecomes a job of \nevaluating\n the relevant nodes of these graphs.\n\n\n\n\n\n\ncourtesy Wikipedia.\n\n\n\n\nIt turns out that representing computation in this manner is advantageous when you need to compute derivatives of\narbitrary functions with respect to any inputs. Tensorflow has the ability to evaluate/execute computational graphs \non any hardware architecture, freeing the user from worrying about those details.\n\n\nThe tensorflow API is roughly divided into two levels.\n\n\n\n\nLow level: Tensor primitives, variables, placeholders, constants, computational graphs.\n\n\nHigh level: Models, estimators etc.\n\n\n\n\nTensorflow for Scala\n\u00b6\n\n\n\n\nThe \ntensorflow for scala\n library provides scala users with access to the low \nas well as high level API's. Among its packages include.\n\n\n\n\n\n\nLow level:\n\n\n\n\nvariables, tensors, placeholders, constants, computational graphs\n\n\n\n\n\n\n\n\nHigh level:\n\n\n\n\nlayers, models, estimators, etc\n\n\n\n\n\n\n\n\nDynaML Tensorflow\n\u00b6\n\n\nDynaML interfaces with the tensorflow scala API and provides a number of convenience features.\n\n\n\n\nThe tensorflow pointer \ndtf\n\n\nNeural network building blocks \ndtflearn\n\n\nSupporting utilities\n\n\nData pipes acting on tensorflow based data, \ndtfpipe\n\n\nThe \ndynaml.tensorflow.utils\n package.\n\n\nMiscellaneous utilities, \ndtfutils\n.",
            "title": "Introduction"
        },
        {
            "location": "/core/core_dynaml_tf/#google-tensorflow",
            "text": "courtesy Google.   Tensorflow  is a versatile and general computational framework \nfor working with tensors and  computational graphs . \nIt provides tensor primitives as well as the ability to define transformations on them. Under the hood, \nthese transformations are baked into a  computational graph . Obtaining results of computations now \nbecomes a job of  evaluating  the relevant nodes of these graphs.    courtesy Wikipedia.   It turns out that representing computation in this manner is advantageous when you need to compute derivatives of\narbitrary functions with respect to any inputs. Tensorflow has the ability to evaluate/execute computational graphs \non any hardware architecture, freeing the user from worrying about those details.  The tensorflow API is roughly divided into two levels.   Low level: Tensor primitives, variables, placeholders, constants, computational graphs.  High level: Models, estimators etc.",
            "title": "Google Tensorflow"
        },
        {
            "location": "/core/core_dynaml_tf/#tensorflow-for-scala",
            "text": "The  tensorflow for scala  library provides scala users with access to the low \nas well as high level API's. Among its packages include.    Low level:   variables, tensors, placeholders, constants, computational graphs     High level:   layers, models, estimators, etc",
            "title": "Tensorflow for Scala"
        },
        {
            "location": "/core/core_dynaml_tf/#dynaml-tensorflow",
            "text": "DynaML interfaces with the tensorflow scala API and provides a number of convenience features.   The tensorflow pointer  dtf  Neural network building blocks  dtflearn  Supporting utilities  Data pipes acting on tensorflow based data,  dtfpipe  The  dynaml.tensorflow.utils  package.  Miscellaneous utilities,  dtfutils .",
            "title": "DynaML Tensorflow"
        },
        {
            "location": "/core/core_dtf/",
            "text": "Summary\n\n\nThe \ndtf\n \nobject can be used to create and transform tensors.\n\n\n\n\nTo use DynaML's tensorflow API, import it in your code/script/DynaML shell session.\n\n\n1\n2\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n\n\n\n\n\nCreating Tensors.\n\u00b6\n\n\nCreating tensors using the \ndtf\n \nobject is easy, the user needs to provide a scala collection containing the\nthe data, the shape and data-type of the tensor. \n\n\nThere is more than one way to instantiate a tensor.\n\n\nEnumeration of Values\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n\n//Create a float tensor\n\n\nval\n \ntensor_float\n \n=\n \ndtf\n.\ntensor_from\n[\nFloat\n](\n\n  \nFLOAT32\n,\n \nShape\n(\n2\n,\n \n2\n))(\n\n  \n1\nf\n,\n \n2\nf\n,\n \n3\nf\n,\n \n4\nf\n)\n\n\n\n//Prints out a summary of the values in tensor1\n\n\ntensor_float\n.\nsummarize\n()\n\n\n\nval\n \ntensor_double\n \n=\n \ndtf\n.\ntensor_from\n[\nDouble\n](\n\n  \nFLOAT64\n,\n \nShape\n(\n2\n,\n \n2\n))(\n\n  \n1.0\n,\n \n2.0\n,\n \n3.0\n,\n \n4.0\n)\n\n\n\ntensor_double\n.\nsummarize\n()\n\n\n\n\n\n\n\nFrom a Scala Sequence\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n\nval\n \nfloat_seq\n \n=\n \nSeq\n(\n1\nf\n,\n \n2\nf\n,\n \n3\nf\n,\n \n4\nf\n)\n\n\nval\n \ndouble_seq\n \n=\n \nSeq\n(\n1.0\n,\n \n2.0\n,\n \n3.0\n,\n \n4.0\n)\n\n\n\n//Specify data type as a string, and enumerate the shape\n\n\nval\n \ntensor_float\n \n=\n \ndtf\n.\ntensor_from\n[\nFloat\n](\n\"FLOAT32\"\n,\n \n2\n,\n \n2\n)(\nfloat_seq\n)\n\n\n\n//Prints out a summary of the values in tensor1\n\n\ntensor_float\n.\nsummarize\n()\n\n\n\nval\n \ntensor_double\n \n=\n \ndtf\n.\ntensor_from\n[\nDouble\n](\n\"FLOAT64\"\n,\n \n2\n,\n \n2\n)(\ndouble_seq\n)\n\n\n\ntensor_double\n.\nsummarize\n()\n\n\n\n\n\n\n\nFrom an Array of Bytes.\n\u00b6\n\n\nWhen dealing with binary data formats, such as images and other binary numerical formats, \nit is useful to be able to instantiate tensors from buffers of raw bytes.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n\nval\n \nbyte_buffer\n:\n \nArray\n[\nByte\n]\n \n=\n \n_\n\n\n\nval\n \nshape\n:\n \nShape\n \n=\n \n_\n\n\n\nval\n \nbyte_tensor\n \n=\n \ndtf\n.\ntensor_from_buffer\n(\nINT32\n,\n \nshape\n)(\nbyte_buffer\n)\n\n\n\n\n\n\n\nApart from these functions, there are.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n\n//Double tensor\n\n\nval\n \nt\n \n=\n \ndtf\n.\ntensor_f64\n(\n2\n,\n \n2\n)(\n1.0\n,\n \n2.0\n,\n \n3.0\n,\n \n4.0\n)\n\n\n\n//32 bit Integer tensor\n\n\nval\n \nt_int\n \n=\n \ndtf\n.\ntensor_i32\n(\n2\n,\n \n3\n)(\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n)\n\n\n\n//Fill a (3, 2, 5) tensor, with the value 1.\n\n\nval\n \nt_fill\n \n=\n \ndtf\n.\nfill\n(\nFLOAT32\n,\n \n3\n,\n \n2\n,\n \n5\n)(\n1\nf\n)\n\n\n\n\n\n\n\nRandom Tensors\n\u00b6\n\n\nIt is also possible to create tensors whose elements are \nindependent and identically distributed\n, by using the DynaML\nprobability API.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nimport\n \nbreeze.stats.distributions._\n\n\n\nimport\n \nio.github.mandar2812.dynaml.probability._\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n\nval\n \nrv\n \n=\n \nRandomVariable\n(\nnew\n \nLogNormal\n(\n0.0\n,\n \n1.5\n))\n\n\n\nval\n \nrandom_tensor\n \n=\n \ndtf\n.\nrandom\n(\nFLOAT64\n,\n \n3\n,\n \n5\n,\n \n2\n)(\nrv\n)\n\n\n\n\n\n\n\nOperations on Tensors\n\u00b6\n\n\nStack\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\nDynaML\n>\nval\n \nrandom_tensor1\n \n=\n \ndtf\n.\nrandom\n(\nFLOAT64\n,\n \n2\n,\n \n3\n)(\nrv\n)\n \n\nrandom_tensor1\n:\n \nTensor\n \n=\n \nFLOAT64\n[\n2\n, \n3\n]\n\n\n\nDynaML\n>\nval\n \nrandom_tensor2\n \n=\n \ndtf\n.\nrandom\n(\nFLOAT64\n,\n \n2\n,\n \n3\n)(\nrv\n)\n \n\nrandom_tensor2\n:\n \nTensor\n \n=\n \nFLOAT64\n[\n2\n, \n3\n]\n\n\n\nDynaML\n>\nval\n \nt\n \n=\n \ndtf\n.\nstack\n(\nSeq\n(\nrandom_tensor1\n,\n \nrandom_tensor2\n),\n \naxis\n \n=\n \n1\n)\n \n\nt\n:\n \nTensor\n \n=\n \nFLOAT64\n[\n2\n, \n2\n, \n3\n]\n\n\n\nDynaML\n>\nval\n \nt0\n \n=\n \ndtf\n.\nstack\n(\nSeq\n(\nrandom_tensor1\n,\n \nrandom_tensor2\n),\n \naxis\n \n=\n \n0\n)\n \n\nt0\n:\n \nTensor\n \n=\n \nFLOAT64\n[\n2\n, \n2\n, \n3\n]\n\n\n\nDynaML\n>\nrandom_tensor1\n.\nsummarize\n(\n100\n,\n \nfalse\n)\n \n\nres18\n:\n \nString\n \n=\n \n\"\"\"FLOAT64[2, 3]\n\n\n[[0.3501699906342581, 0.2900664662305818, 0.42806656451314345],\n\n\n [0.3066005571688877, 1.3931959054429162, 0.6366232162759474]]\"\"\"\n\n\n\nDynaML\n>\nrandom_tensor2\n.\nsummarize\n(\n100\n,\n \nfalse\n)\n \n\nres19\n:\n \nString\n \n=\n \n\"\"\"FLOAT64[2, 3]\n\n\n[[0.21565105620570899, 0.5267519630011802, 6.817248106561024],\n\n\n [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]\"\"\"\n\n\n\nDynaML\n>\nt\n.\nsummarize\n(\n100\n,\n \nfalse\n)\n \n\nres16\n:\n \nString\n \n=\n \n\"\"\"FLOAT64[2, 2, 3]\n\n\n[[[0.3501699906342581, 0.2900664662305818, 0.42806656451314345],\n\n\n  [0.21565105620570899, 0.5267519630011802, 6.817248106561024]],\n\n\n\n [[0.3066005571688877, 1.3931959054429162, 0.6366232162759474],\n\n\n  [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]]\"\"\"\n\n\n\nDynaML\n>\nt0\n.\nsummarize\n(\n100\n,\n \nfalse\n)\n \n\nres17\n:\n \nString\n \n=\n \n\"\"\"FLOAT64[2, 2, 3]\n\n\n[[[0.3501699906342581, 0.2900664662305818, 0.42806656451314345],\n\n\n  [0.3066005571688877, 1.3931959054429162, 0.6366232162759474]],\n\n\n\n [[0.21565105620570899, 0.5267519630011802, 6.817248106561024],\n\n\n  [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]]\"\"\"\n\n\n\n\n\n\n\nConcatenate\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nDynaML\n>\nval\n \nt\n \n=\n \ndtf\n.\nconcatenate\n(\nSeq\n(\nrandom_tensor1\n,\n \nrandom_tensor2\n),\n \naxis\n \n=\n \n0\n)\n \n\nt\n:\n \nTensor\n \n=\n \nFLOAT64\n[\n4\n, \n3\n]\n\n\n\nDynaML\n>\nval\n \nt1\n \n=\n \ndtf\n.\nconcatenate\n(\nSeq\n(\nrandom_tensor1\n,\n \nrandom_tensor2\n),\n \naxis\n \n=\n \n1\n)\n \n\nt1\n:\n \nTensor\n \n=\n \nFLOAT64\n[\n2\n, \n6\n]\n\n\n\nDynaML\n>\nt\n.\nsummarize\n(\n100\n,\n \nfalse\n)\n \n\nres28\n:\n \nString\n \n=\n \n\"\"\"FLOAT64[4, 3]\n\n\n[[0.3501699906342581, 0.2900664662305818, 0.42806656451314345],\n\n\n [0.3066005571688877, 1.3931959054429162, 0.6366232162759474],\n\n\n [0.21565105620570899, 0.5267519630011802, 6.817248106561024],\n\n\n [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]\"\"\"\n\n\n\nDynaML\n>\nt1\n.\nsummarize\n(\n100\n,\n \nfalse\n)\n \n\nres29\n:\n \nString\n \n=\n \n\"\"\"FLOAT64[2, 6]\n\n\n[[0.3501699906342581, 0.2900664662305818, 0.42806656451314345, 0.21565105620570899, 0.5267519630011802, 6.817248106561024],\n\n\n [0.3066005571688877, 1.3931959054429162, 0.6366232162759474, 0.35121879449734744, 5.487926862392467, 2.3538094624119177]]\"\"\"\n\n\n\n\n\n\n\nUnstack\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nDynaML\n>\ndtf\n.\nunstack\n(\nt1\n,\n \naxis\n \n=\n \n1\n)\n \n\nres31\n:\n \nSeq\n[\nTensor\n]\n \n=\n \nArraySeq\n(\nFLOAT64\n[\n2\n],\n \nFLOAT64\n[\n2\n],\n \nFLOAT64\n[\n2\n],\n \nFLOAT64\n[\n2\n],\n \nFLOAT64\n[\n2\n],\n \nFLOAT64\n[\n2\n])\n\n\n\nDynaML\n>\nres31\n.\nmap\n(\nt\n \n=>\n \nt\n.\nsummarize\n(\n100\n,\n \nfalse\n))\n \n\nres33\n:\n \nSeq\n[\nString\n]\n \n=\n \nArraySeq\n(\n\n  \n\"\"\"FLOAT64[2]\n\n\n[0.3501699906342581, 0.3066005571688877]\"\"\"\n,\n\n  \n\"\"\"FLOAT64[2]\n\n\n[0.2900664662305818, 1.3931959054429162]\"\"\"\n,\n\n  \n\"\"\"FLOAT64[2]\n\n\n[0.42806656451314345, 0.6366232162759474]\"\"\"\n,\n\n  \n\"\"\"FLOAT64[2]\n\n\n[0.21565105620570899, 0.35121879449734744]\"\"\"\n,\n\n  \n\"\"\"FLOAT64[2]\n\n\n[0.5267519630011802, 5.487926862392467]\"\"\"\n,\n\n  \n\"\"\"FLOAT64[2]\n\n\n[6.817248106561024, 2.3538094624119177]\"\"\"\n\n\n)",
            "title": "Tensorflow Pointer"
        },
        {
            "location": "/core/core_dtf/#creating-tensors",
            "text": "Creating tensors using the  dtf  \nobject is easy, the user needs to provide a scala collection containing the\nthe data, the shape and data-type of the tensor.   There is more than one way to instantiate a tensor.",
            "title": "Creating Tensors."
        },
        {
            "location": "/core/core_dtf/#enumeration-of-values",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  //Create a float tensor  val   tensor_float   =   dtf . tensor_from [ Float ]( \n   FLOAT32 ,   Shape ( 2 ,   2 ))( \n   1 f ,   2 f ,   3 f ,   4 f )  //Prints out a summary of the values in tensor1  tensor_float . summarize ()  val   tensor_double   =   dtf . tensor_from [ Double ]( \n   FLOAT64 ,   Shape ( 2 ,   2 ))( \n   1.0 ,   2.0 ,   3.0 ,   4.0 )  tensor_double . summarize ()",
            "title": "Enumeration of Values"
        },
        {
            "location": "/core/core_dtf/#from-a-scala-sequence",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  val   float_seq   =   Seq ( 1 f ,   2 f ,   3 f ,   4 f )  val   double_seq   =   Seq ( 1.0 ,   2.0 ,   3.0 ,   4.0 )  //Specify data type as a string, and enumerate the shape  val   tensor_float   =   dtf . tensor_from [ Float ]( \"FLOAT32\" ,   2 ,   2 )( float_seq )  //Prints out a summary of the values in tensor1  tensor_float . summarize ()  val   tensor_double   =   dtf . tensor_from [ Double ]( \"FLOAT64\" ,   2 ,   2 )( double_seq )  tensor_double . summarize ()",
            "title": "From a Scala Sequence"
        },
        {
            "location": "/core/core_dtf/#from-an-array-of-bytes",
            "text": "When dealing with binary data formats, such as images and other binary numerical formats, \nit is useful to be able to instantiate tensors from buffers of raw bytes.  1\n2\n3\n4\n5\n6\n7\n8 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  val   byte_buffer :   Array [ Byte ]   =   _  val   shape :   Shape   =   _  val   byte_tensor   =   dtf . tensor_from_buffer ( INT32 ,   shape )( byte_buffer )    Apart from these functions, there are.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  //Double tensor  val   t   =   dtf . tensor_f64 ( 2 ,   2 )( 1.0 ,   2.0 ,   3.0 ,   4.0 )  //32 bit Integer tensor  val   t_int   =   dtf . tensor_i32 ( 2 ,   3 )( 1 ,   2 ,   3 ,   4 ,   5 ,   6 )  //Fill a (3, 2, 5) tensor, with the value 1.  val   t_fill   =   dtf . fill ( FLOAT32 ,   3 ,   2 ,   5 )( 1 f )",
            "title": "From an Array of Bytes."
        },
        {
            "location": "/core/core_dtf/#random-tensors",
            "text": "It is also possible to create tensors whose elements are  independent and identically distributed , by using the DynaML\nprobability API.  1\n2\n3\n4\n5\n6\n7\n8\n9 import   breeze.stats.distributions._  import   io.github.mandar2812.dynaml.probability._  import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  val   rv   =   RandomVariable ( new   LogNormal ( 0.0 ,   1.5 ))  val   random_tensor   =   dtf . random ( FLOAT64 ,   3 ,   5 ,   2 )( rv )",
            "title": "Random Tensors"
        },
        {
            "location": "/core/core_dtf/#operations-on-tensors",
            "text": "",
            "title": "Operations on Tensors"
        },
        {
            "location": "/core/core_dtf/#stack",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37 DynaML > val   random_tensor1   =   dtf . random ( FLOAT64 ,   2 ,   3 )( rv )   random_tensor1 :   Tensor   =   FLOAT64 [ 2 ,  3 ]  DynaML > val   random_tensor2   =   dtf . random ( FLOAT64 ,   2 ,   3 )( rv )   random_tensor2 :   Tensor   =   FLOAT64 [ 2 ,  3 ]  DynaML > val   t   =   dtf . stack ( Seq ( random_tensor1 ,   random_tensor2 ),   axis   =   1 )   t :   Tensor   =   FLOAT64 [ 2 ,  2 ,  3 ]  DynaML > val   t0   =   dtf . stack ( Seq ( random_tensor1 ,   random_tensor2 ),   axis   =   0 )   t0 :   Tensor   =   FLOAT64 [ 2 ,  2 ,  3 ]  DynaML > random_tensor1 . summarize ( 100 ,   false )   res18 :   String   =   \"\"\"FLOAT64[2, 3]  [[0.3501699906342581, 0.2900664662305818, 0.42806656451314345],   [0.3066005571688877, 1.3931959054429162, 0.6366232162759474]]\"\"\"  DynaML > random_tensor2 . summarize ( 100 ,   false )   res19 :   String   =   \"\"\"FLOAT64[2, 3]  [[0.21565105620570899, 0.5267519630011802, 6.817248106561024],   [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]\"\"\"  DynaML > t . summarize ( 100 ,   false )   res16 :   String   =   \"\"\"FLOAT64[2, 2, 3]  [[[0.3501699906342581, 0.2900664662305818, 0.42806656451314345],    [0.21565105620570899, 0.5267519630011802, 6.817248106561024]],   [[0.3066005571688877, 1.3931959054429162, 0.6366232162759474],    [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]]\"\"\"  DynaML > t0 . summarize ( 100 ,   false )   res17 :   String   =   \"\"\"FLOAT64[2, 2, 3]  [[[0.3501699906342581, 0.2900664662305818, 0.42806656451314345],    [0.3066005571688877, 1.3931959054429162, 0.6366232162759474]],   [[0.21565105620570899, 0.5267519630011802, 6.817248106561024],    [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]]\"\"\"",
            "title": "Stack"
        },
        {
            "location": "/core/core_dtf/#concatenate",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 DynaML > val   t   =   dtf . concatenate ( Seq ( random_tensor1 ,   random_tensor2 ),   axis   =   0 )   t :   Tensor   =   FLOAT64 [ 4 ,  3 ]  DynaML > val   t1   =   dtf . concatenate ( Seq ( random_tensor1 ,   random_tensor2 ),   axis   =   1 )   t1 :   Tensor   =   FLOAT64 [ 2 ,  6 ]  DynaML > t . summarize ( 100 ,   false )   res28 :   String   =   \"\"\"FLOAT64[4, 3]  [[0.3501699906342581, 0.2900664662305818, 0.42806656451314345],   [0.3066005571688877, 1.3931959054429162, 0.6366232162759474],   [0.21565105620570899, 0.5267519630011802, 6.817248106561024],   [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]\"\"\"  DynaML > t1 . summarize ( 100 ,   false )   res29 :   String   =   \"\"\"FLOAT64[2, 6]  [[0.3501699906342581, 0.2900664662305818, 0.42806656451314345, 0.21565105620570899, 0.5267519630011802, 6.817248106561024],   [0.3066005571688877, 1.3931959054429162, 0.6366232162759474, 0.35121879449734744, 5.487926862392467, 2.3538094624119177]]\"\"\"",
            "title": "Concatenate"
        },
        {
            "location": "/core/core_dtf/#unstack",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 DynaML > dtf . unstack ( t1 ,   axis   =   1 )   res31 :   Seq [ Tensor ]   =   ArraySeq ( FLOAT64 [ 2 ],   FLOAT64 [ 2 ],   FLOAT64 [ 2 ],   FLOAT64 [ 2 ],   FLOAT64 [ 2 ],   FLOAT64 [ 2 ])  DynaML > res31 . map ( t   =>   t . summarize ( 100 ,   false ))   res33 :   Seq [ String ]   =   ArraySeq ( \n   \"\"\"FLOAT64[2]  [0.3501699906342581, 0.3066005571688877]\"\"\" , \n   \"\"\"FLOAT64[2]  [0.2900664662305818, 1.3931959054429162]\"\"\" , \n   \"\"\"FLOAT64[2]  [0.42806656451314345, 0.6366232162759474]\"\"\" , \n   \"\"\"FLOAT64[2]  [0.21565105620570899, 0.35121879449734744]\"\"\" , \n   \"\"\"FLOAT64[2]  [0.5267519630011802, 5.487926862392467]\"\"\" , \n   \"\"\"FLOAT64[2]  [6.817248106561024, 2.3538094624119177]\"\"\"  )",
            "title": "Unstack"
        },
        {
            "location": "/core/core_dtflearn/",
            "text": "Summary\n\n\nThe \ndtflearn\n \nobject makes it easy to create and train neural networks of \nvarying complexity.\n\n\n\n\nActivation Functions\n\u00b6\n\n\nApart from the activation functions defined in tensorflow for scala, DynaML defines some additional activations.\n\n\n\n\nHyperbolic Tangent\n \n    \n1\nval\n \nact\n \n=\n \ndtflearn\n.\nTanh\n(\n\"SomeIdentifier\"\n)\n\n\n\n\n\n\nCumulative Gaussian\n\n    \n1\nval\n \nact\n \n=\n \ndtflearn\n.\nPhi\n(\n\"OtherIdentifier\"\n)\n\n\n\n\n\n\nGeneralized Logistic\n\n    \n1\nval\n \nact\n \n=\n \ndtflearn\n.\nGeneralizedLogistic\n(\n\"AnotherId\"\n)\n\n\n\n\n\n\n\n\nLayers\n\u00b6\n\n\nDynaML aims to supplement and extend the collection of layers available in \norg.platanios.tensorflow.api.layers\n, \nall the layers defined in DynaML's \ntensorflow\n package extend the \nLayer[T, R]\n class in \n\norg.platanios.tensorflow.api.layers\n.\n\n\nRadial Basis Function Network\n\u00b6\n\n\nRadial Basis Function\n (RBF) networks are an important class of basis functions, each of which are expressed as \ndecaying with distance from a defined central node.\n\n\n\n\n\n\\begin{align}\nf(x) & = \\sum_{i}{w_{i} \\varphi(||x - c_{i}||/\\sigma)} \\\\\n\\varphi(u) & = exp(-u^2/2)\n\\end{align}\n\n\n\n\n\\begin{align}\nf(x) & = \\sum_{i}{w_{i} \\varphi(||x - c_{i}||/\\sigma)} \\\\\n\\varphi(u) & = exp(-u^2/2)\n\\end{align}\n\n\n\n\n\nThe RBF layer implementation in DynaML treats the node center positions \nc_i\nc_i\n and length scales \n\\sigma_i\n\\sigma_i\n as \nparameters to be learned via gradient based back-propagation.\n\n\n1\n2\n3\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\n\nval\n \nrbf\n \n=\n \ndtflearn\n.\nrbf_layer\n(\nname\n \n=\n \n\"rbf1\"\n,\n \nnum_units\n \n=\n \n10\n)\n\n\n\n\n\n\n\nContinuous Time RNN\n\u00b6\n\n\nContinuous time recurrent neural networks (CTRNN) are an important class of recurrent neural networks. They enable\nthe modelling of non-linear and potentially complex dynamical systems of multiple variables, with feedback.\n\n\nEach state variable is modeled by a single neuron \ny_i\ny_i\n, the evolution of the system \ny = (y_1, \\cdots, y_n)^T\ny = (y_1, \\cdots, y_n)^T\n \nis governed by a set of coupled ordinary differential equations. These equations can be expressed in vector form as \nfollows.\n\n\n\n\n\n\\begin{align}\ndy/dt & = - \\Lambda . y + W . \\sigma(G.y + b) \\\\ \n\\end{align}\n\n\n\n\n\\begin{align}\ndy/dt & = - \\Lambda . y + W . \\sigma(G.y + b) \\\\ \n\\end{align}\n\n\n\n\n\nThe parameters of the system above are.\n\n\n\n\nTime Constant/Decay Rate\n\n\n\n\n\n\n\n\\begin{equation}\n\\Lambda = \\begin{pmatrix}\n        \\lambda_1 & \\cdots  & 0 \\\\ \n        \\vdots & \\ddots  & \\vdots \\\\ \n        0 & \\cdots  & \\lambda_n  \n        \\end{pmatrix}\n\\end{equation}\n\n\n\n\n\\begin{equation}\n\\Lambda = \\begin{pmatrix}\n        \\lambda_1 & \\cdots  & 0 \\\\ \n        \\vdots & \\ddots  & \\vdots \\\\ \n        0 & \\cdots  & \\lambda_n  \n        \\end{pmatrix}\n\\end{equation}\n\n\n\n\n\n\n\nGain\n\n\n\n\n\n\n\n\\begin{equation}\nG = \\begin{pmatrix}\n  g_{11} & \\cdots  & g_{1n} \\\\ \n  \\vdots & \\ddots  & \\vdots \\\\ \n  g_{n1} & \\cdots  & g_{nn}  \n  \\end{pmatrix}\n\\end{equation}\n\n\n\n\n\\begin{equation}\nG = \\begin{pmatrix}\n  g_{11} & \\cdots  & g_{1n} \\\\ \n  \\vdots & \\ddots  & \\vdots \\\\ \n  g_{n1} & \\cdots  & g_{nn}  \n  \\end{pmatrix}\n\\end{equation}\n\n\n\n\n\n\n\nBias\n\n\n\n\n\n\n\n\\begin{equation}\nb = \\begin{pmatrix}\n  b_{1}\\\\ \n  \\vdots\\\\ \n  b_{n}  \n  \\end{pmatrix}\n\\end{equation}\n\n\n\n\n\\begin{equation}\nb = \\begin{pmatrix}\n  b_{1}\\\\ \n  \\vdots\\\\ \n  b_{n}  \n  \\end{pmatrix}\n\\end{equation}\n\n\n\n\n\n\n\nWeights\n\n\n\n\n\n\n\n\\begin{equation}\nW = \\begin{pmatrix}\n  w_{11} & \\cdots  & w_{1n} \\\\ \n  \\vdots & \\ddots  & \\vdots \\\\ \n  w_{n1} & \\cdots  & w_{nn}  \n  \\end{pmatrix}\n\\end{equation}\n\n\n\n\n\\begin{equation}\nW = \\begin{pmatrix}\n  w_{11} & \\cdots  & w_{1n} \\\\ \n  \\vdots & \\ddots  & \\vdots \\\\ \n  w_{n1} & \\cdots  & w_{nn}  \n  \\end{pmatrix}\n\\end{equation}\n\n\n\n\n\nIn order to use the CTRNN model in a modelling sequences of finite length, we need to solve its \ngoverning equations numerically. This gives us the trajectory of the state upto \nT\nT\n steps \n\ny^{0}, \\cdots, y^{T}\ny^{0}, \\cdots, y^{T}\n.\n\n\n\n\n\ny^{k+1} = y^{k} + \\Delta t (- \\Lambda . y^{k} + W . \\sigma(G.y^{k} + b))\n\n\n\n\ny^{k+1} = y^{k} + \\Delta t (- \\Lambda . y^{k} + W . \\sigma(G.y^{k} + b))\n\n\n\n\n\nDynaML's implementation of the CTRNN can be used to learn the trajectory of\ndynamical systems upto a predefined time horizon. The parameters \n\\Lambda, G, b, W\n\\Lambda, G, b, W\n are\nlearned using gradient based loss minimization. \n\n\nThe CTRNN implementations are also instances of \nLayer[Output, Output]\n, which take as input\ntensors of shape \nn\nn\n and produce tensors of shape \n(n, T)\n(n, T)\n, there are two variants that users\ncan choose from.\n\n\nFixed Time Step Integration\n\u00b6\n\n\nWhen the integration time step \n\\Delta t\n\\Delta t\n is user defined and fixed.\n\n\n1\n2\n3\n4\n5\n6\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n \n\n\nval\n \nctrnn_layer\n \n=\n \ndtflearn\n.\nctrnn\n(\n\n\nname\n \n=\n \n\"CTRNN_1\"\n,\n \nunits\n \n=\n \n10\n,\n \n\nhorizon\n \n=\n \n5\n,\n \ntimestep\n \n=\n \n0.1\n)\n\n\n\n\n\n\n\nDynamic Time Step Integration\n\u00b6\n\n\nWhen the integration time step \n\\Delta t\n\\Delta t\n is a parameter that can be learned during the\ntraining process.\n\n\n1\n2\n3\n4\n5\n6\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n \n\n\nval\n \ndctrnn_layer\n \n=\n \ndtflearn\n.\ndctrnn\n(\n\n\nname\n \n=\n \n\"DCTRNN_1\"\n,\n \nunits\n \n=\n \n10\n,\n \n\nhorizon\n \n=\n \n5\n)\n\n\n\n\n\n\n\nStack & Concatenate\n\u00b6\n\n\nOften one would need to combine inputs of previous layers in some manner, the following layers enable these operations.\n\n\nStack Inputs\n\u00b6\n\n\nThis is a computational layer which performs the function of \ndtf.stack()\n.\n\n\n1\n2\n3\n4\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n \n\n\nval\n \nstk_layer\n \n=\n \ndtflearn\n.\nstack_outputs\n(\n\"StackTensors\"\n,\n \naxis\n \n=\n \n1\n)\n\n\n\n\n\n\n\nConcatenate Inputs\n\u00b6\n\n\nThis is a computational layer which performs the function of \ndtf.concatenate()\n.\n\n\n1\n2\n3\n4\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n \n\n\nval\n \nconcat_layer\n \n=\n \ndtflearn\n.\nstack_outputs\n(\n\"ConcatenateTensors\"\n,\n \naxis\n \n=\n \n1\n)\n\n\n\n\n\n\n\nCollect Layers\n\u00b6\n\n\nA sequence of layers can be collected into a single layer which accepts a sequence of symbolic tensors.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n \n\n\nval\n \nlayers\n \n=\n \nSeq\n(\n\n  \ntf\n.\nlearn\n.\nLinear\n(\n\"l1\"\n,\n \n10\n),\n\n  \ndtflearn\n.\nidentity\n(\n\"Identity\"\n),\n\n  \ndtflearn\n.\nctrnn\n(\n\n    \nname\n \n=\n \n\"CTRNN_1\"\n,\n \nunits\n \n=\n \n10\n,\n \n    \nhorizon\n \n=\n \n5\n,\n \ntimestep\n \n=\n \n0.1\n\n  \n)\n\n\n)\n\n\n\nval\n \ncombined_layer\n \n=\n \ndtflearn\n.\nstack_layers\n(\n\"Collect\"\n,\n \nlayers\n)\n\n\n\n\n\n\n\nInput Pairs\n\u00b6\n\n\nTo handle inputs consisting of pairs of elements, one can provide a separate layer for processing each of the elements.\n\n\n1\n2\n3\n4\n5\n6\n7\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n\nval\n \nsl\n \n=\n \ndtflearn\n.\ntuple2_layer\n(\n\n  \n\"tuple2layer\"\n,\n \n  \ndtflearn\n.\nrbf_layer\n(\n\"rbf1\"\n,\n \n10\n),\n \n  \ntf\n.\nlearn\n.\nLinear\n(\n\"lin1\"\n,\n \n10\n))\n \n\n\n\n\n\n\nCombining the elements of Tuple2 can be done as follows.\n\n\n1\n2\n3\n4\n5\n6\n7\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n\n//Stack elements of the tuple into one tensor\n\n\nval\n \nlayer1\n \n=\n \ndtflearn\n.\nstack_tuple2\n(\n\"tuple2layer\"\n,\n \naxis\n \n=\n \n1\n)\n \n\n//Concatenate elements of the tuple into one tensor\n\n\nval\n \nlayer2\n \n=\n \ndtflearn\n.\nconcat_tuple2\n(\n\"tuple2layer\"\n,\n \naxis\n \n=\n \n1\n)\n \n\n\n\n\n\n\nStoppage Criteria\n\u00b6\n\n\nIn order to train tensorflow models using iterative gradient based models, the user must \ndefine some stoppage criteria for the training process. This can be done via the method \n\ntf.learn.StopCriteria()\n. The following preset stop criteria call \ntf.learn.StopCriteria()\n under the hood. \n\n\nIterations Based\n\u00b6\n\n\n1\nval\n \nstopc1\n \n=\n \ndtflearn\n.\nmax_iter_stop\n(\n10000\n)\n\n\n\n\n\n\n\nChange in Loss\n\u00b6\n\n\nAbsolute Value of Loss\n\u00b6\n\n\n1\nval\n \nstopc2\n \n=\n \ndtflearn\n.\nabs_loss_change_stop\n(\n0.1\n)\n\n\n\n\n\n\n\nRelative Value of Loss\n\u00b6\n\n\n1\nval\n \nstopc2\n \n=\n \ndtflearn\n.\nrel_loss_change_stop\n(\n0.1\n)\n\n\n\n\n\n\n\nNetwork Building Blocks\n\u00b6\n\n\nTo make it convenient to build deeper stacks of neural networks, DynaML includes some common layer design patterns\nas ready made easy to use methods.\n\n\nConvolutional Neural Nets\n\u00b6\n\n\nConvolutional neural networks\n (CNN) are a crucial building block\nof deep neural architectures for visual pattern recognition. It turns out that CNN layers must be combined with\nother computational units such as \nrectified linear\n (ReLU) activations, \n\ndropout\n and \nmax pool\n layers.\n\n\nCurrently two abstractions are offered for building large CNN based network stacks\n\n\nConvolutional Unit\n\u00b6\n\n\nA single CNN unit is expressed as a convolutional layer followed by a ReLU activation and proceeded by a dropout layer.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n\n//Learn 16 filters of shape (2, 2, 4), suitable for 4 channel jpeg images.\n\n\n//Slide the filters over the image in steps of 1 pixel in each direction.\n\n\nval\n \ncnn_unit\n \n=\n \ndtflearn\n.\nconv2d_unit\n(\n\n    \nshape\n \n=\n \nShape\n(\n2\n,\n \n2\n,\n \n4\n,\n \n16\n),\n \nstride\n \n=\n \n(\n1\n,\n \n1\n),\n\n    \nrelu_param\n \n=\n \n0.05f\n,\n \ndropout\n \n=\n \ntrue\n,\n\n    \nkeep_prob\n \n=\n \n0.55f\n)(\ni\n \n=\n \n1\n)\n\n\n\n\n\n\n\nConvolutional Pyramid\n\u00b6\n\n\nA CNN pyramid builds a stack of CNN units each with a stride multiplied by a factor of 2 and depth divided\nby a factor of 2 with respect to the previous unit.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n\n//Start with a CNN unit of shape (2, 2, 3, 16) stride (1, 1)\n\n\n//End with a CNN unit of shape (2, 2, 8, 4) and stride of (8, 8)\n\n\nval\n \ncnn_stack\n \n=\n \ndtflearn\n.\nconv2d_pyramid\n(\n\n  \nsize\n \n=\n \n2\n,\n \nnum_channels_input\n \n=\n \n3\n)(\n\n  \nstart_num_bits\n \n=\n \n4\n,\n \nend_num_bits\n \n=\n \n2\n)(\n\n  \nrelu_param\n \n=\n \n0.1f\n,\n \ndropout\n \n=\n \ntrue\n,\n \n  \nkeep_prob\n \n=\n \n0.6F\n)\n\n\n\n\n\n\n\nFeed-forward Neural Nets\n\u00b6\n\n\nFeed-forward networks are the oldest and most frequently used components of neural network architectures, they are often\nstacked into a number of layers. With \ndtflearn.feedforward_stack()\n, you can define feed-forward stacks of arbitrary\nwidth and depth.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\n\nval\n \nnet_layer_sizes\n \n=\n \nSeq\n(\n10\n,\n \n20\n,\n \n13\n,\n \n15\n)\n\n\n\nval\n \narchitecture\n \n=\n \ndtflearn\n.\nfeedforward_stack\n(\n\n    \n(\ni\n:\n \nInt\n)\n \n=>\n \ndtflearn\n.\nPhi\n(\n\"Act_\"\n+\ni\n),\n \nFLOAT64\n)(\n\n    \nnet_layer_sizes\n)\n\n\n\n\n\n\n\nBuilding Tensorflow Models\n\u00b6\n\n\nAfter defining the key ingredients needed to build a tensorflow model, \ndtflearn.build_tf_model()\n builds a new \ncomputational graph and creates a tensorflow model and estimator which is trained on the provided data. In the \nfollowing example, we bring together all the elements of model training: data, architecture, loss etc.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\nimport\n \nammonite.ops._\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow.dtflearn\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\nimport\n \norg.platanios.tensorflow.api.ops.NN.SamePadding\n\n\nimport\n \norg.platanios.tensorflow.data.image.CIFARLoader\n\n\nimport\n \njava.nio.file.Paths\n\n\n\n\nval\n \ntempdir\n \n=\n \nhome\n/\n\"tmp\"\n\n\n\nval\n \ndataSet\n \n=\n \nCIFARLoader\n.\nload\n(\n\n  \nPaths\n.\nget\n(\ntempdir\n.\ntoString\n()),\n \n  \nCIFARLoader\n.\nCIFAR_10\n)\n\n\n\nval\n \ntrainImages\n \n=\n \ntf\n.\ndata\n.\nTensorSlicesDataset\n(\ndataSet\n.\ntrainImages\n)\n\n\nval\n \ntrainLabels\n \n=\n \ntf\n.\ndata\n.\nTensorSlicesDataset\n(\ndataSet\n.\ntrainLabels\n)\n\n\n\nval\n \ntrainData\n \n=\n \n  \ntrainImages\n.\nzip\n(\ntrainLabels\n)\n\n    \n.\nrepeat\n()\n\n    \n.\nshuffle\n(\n10000\n)\n\n    \n.\nbatch\n(\n128\n)\n\n    \n.\nprefetch\n(\n10\n)\n\n\n\n\nprintln\n(\n\"Building the classification model.\"\n)\n\n\nval\n \ninput\n \n=\n \ntf\n.\nlearn\n.\nInput\n(\n\n  \nUINT8\n,\n \n  \nShape\n(\n\n    \n-\n1\n,\n \n    \ndataSet\n.\ntrainImages\n.\nshape\n(\n1\n),\n \n    \ndataSet\n.\ntrainImages\n.\nshape\n(\n2\n),\n \n    \ndataSet\n.\ntrainImages\n.\nshape\n(\n3\n))\n\n\n)\n\n\n\nval\n \ntrainInput\n \n=\n \ntf\n.\nlearn\n.\nInput\n(\nUINT8\n,\n \nShape\n(-\n1\n))\n\n\n\nval\n \narchitecture\n \n=\n \ntf\n.\nlearn\n.\nCast\n(\n\"Input/Cast\"\n,\n \nFLOAT32\n)\n \n>>\n\n  \ndtflearn\n.\nconv2d_pyramid\n(\n2\n,\n \n3\n)(\n4\n,\n \n2\n)(\n0.1f\n,\n \ntrue\n,\n \n0.6F\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nMaxPool\n(\n\n    \n\"Layer_3/MaxPool\"\n,\n \n    \nSeq\n(\n1\n,\n \n2\n,\n \n2\n,\n \n1\n),\n \n    \n1\n,\n \n1\n,\n \nSamePadding\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nFlatten\n(\n\"Layer_3/Flatten\"\n)\n \n>>\n\n  \ndtflearn\n.\nfeedforward\n(\n256\n)(\nid\n \n=\n \n4\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nReLU\n(\n\"Layer_4/ReLU\"\n,\n \n0.1f\n)\n \n>>\n\n  \ndtflearn\n.\nfeedforward\n(\n10\n)(\nid\n \n=\n \n5\n)\n\n\n\nval\n \ntrainingInputLayer\n \n=\n \ntf\n.\nlearn\n.\nCast\n(\n\"TrainInput/Cast\"\n,\n \nINT64\n)\n\n\n\nval\n \nloss\n \n=\n \ntf\n.\nlearn\n.\nSparseSoftmaxCrossEntropy\n(\n\"Loss/CrossEntropy\"\n)\n \n>>\n\n  \ntf\n.\nlearn\n.\nMean\n(\n\"Loss/Mean\"\n)\n \n>>\n \n  \ntf\n.\nlearn\n.\nScalarSummary\n(\n\"Loss/Summary\"\n,\n \n\"Loss\"\n)\n\n\n\nval\n \noptimizer\n \n=\n \ntf\n.\ntrain\n.\nAdaGrad\n(\n0.1\n)\n\n\n\nprintln\n(\n\"Training the linear regression model.\"\n)\n\n\nval\n \nsummariesDir\n \n=\n \njava\n.\nnio\n.\nfile\n.\nPaths\n.\nget\n(\n\n  \n(\ntempdir\n/\n\"cifar_summaries\"\n).\ntoString\n()\n\n\n)\n\n\n\nval\n \n(\nmodel\n,\n \nestimator\n)\n \n=\n \ndtflearn\n.\nbuild_tf_model\n(\n\n  \narchitecture\n,\n \ninput\n,\n \ntrainInput\n,\n \ntrainingInputLayer\n,\n\n  \nloss\n,\n \noptimizer\n,\n \nsummariesDir\n,\n \n  \ndtflearn\n.\nmax_iter_stop\n(\n1000\n),\n\n  \n100\n,\n \n100\n,\n \n100\n)(\ntrainData\n)",
            "title": "Building Blocks"
        },
        {
            "location": "/core/core_dtflearn/#activation-functions",
            "text": "Apart from the activation functions defined in tensorflow for scala, DynaML defines some additional activations.   Hyperbolic Tangent  \n     1 val   act   =   dtflearn . Tanh ( \"SomeIdentifier\" )    Cumulative Gaussian \n     1 val   act   =   dtflearn . Phi ( \"OtherIdentifier\" )    Generalized Logistic \n     1 val   act   =   dtflearn . GeneralizedLogistic ( \"AnotherId\" )",
            "title": "Activation Functions"
        },
        {
            "location": "/core/core_dtflearn/#layers",
            "text": "DynaML aims to supplement and extend the collection of layers available in  org.platanios.tensorflow.api.layers , \nall the layers defined in DynaML's  tensorflow  package extend the  Layer[T, R]  class in  org.platanios.tensorflow.api.layers .",
            "title": "Layers"
        },
        {
            "location": "/core/core_dtflearn/#radial-basis-function-network",
            "text": "Radial Basis Function  (RBF) networks are an important class of basis functions, each of which are expressed as \ndecaying with distance from a defined central node.   \n\\begin{align}\nf(x) & = \\sum_{i}{w_{i} \\varphi(||x - c_{i}||/\\sigma)} \\\\\n\\varphi(u) & = exp(-u^2/2)\n\\end{align}  \n\\begin{align}\nf(x) & = \\sum_{i}{w_{i} \\varphi(||x - c_{i}||/\\sigma)} \\\\\n\\varphi(u) & = exp(-u^2/2)\n\\end{align}   The RBF layer implementation in DynaML treats the node center positions  c_i c_i  and length scales  \\sigma_i \\sigma_i  as \nparameters to be learned via gradient based back-propagation.  1\n2\n3 import   io.github.mandar2812.dynaml.tensorflow._  val   rbf   =   dtflearn . rbf_layer ( name   =   \"rbf1\" ,   num_units   =   10 )",
            "title": "Radial Basis Function Network"
        },
        {
            "location": "/core/core_dtflearn/#continuous-time-rnn",
            "text": "Continuous time recurrent neural networks (CTRNN) are an important class of recurrent neural networks. They enable\nthe modelling of non-linear and potentially complex dynamical systems of multiple variables, with feedback.  Each state variable is modeled by a single neuron  y_i y_i , the evolution of the system  y = (y_1, \\cdots, y_n)^T y = (y_1, \\cdots, y_n)^T  \nis governed by a set of coupled ordinary differential equations. These equations can be expressed in vector form as \nfollows.   \n\\begin{align}\ndy/dt & = - \\Lambda . y + W . \\sigma(G.y + b) \\\\ \n\\end{align}  \n\\begin{align}\ndy/dt & = - \\Lambda . y + W . \\sigma(G.y + b) \\\\ \n\\end{align}   The parameters of the system above are.   Time Constant/Decay Rate    \n\\begin{equation}\n\\Lambda = \\begin{pmatrix}\n        \\lambda_1 & \\cdots  & 0 \\\\ \n        \\vdots & \\ddots  & \\vdots \\\\ \n        0 & \\cdots  & \\lambda_n  \n        \\end{pmatrix}\n\\end{equation}  \n\\begin{equation}\n\\Lambda = \\begin{pmatrix}\n        \\lambda_1 & \\cdots  & 0 \\\\ \n        \\vdots & \\ddots  & \\vdots \\\\ \n        0 & \\cdots  & \\lambda_n  \n        \\end{pmatrix}\n\\end{equation}    Gain    \n\\begin{equation}\nG = \\begin{pmatrix}\n  g_{11} & \\cdots  & g_{1n} \\\\ \n  \\vdots & \\ddots  & \\vdots \\\\ \n  g_{n1} & \\cdots  & g_{nn}  \n  \\end{pmatrix}\n\\end{equation}  \n\\begin{equation}\nG = \\begin{pmatrix}\n  g_{11} & \\cdots  & g_{1n} \\\\ \n  \\vdots & \\ddots  & \\vdots \\\\ \n  g_{n1} & \\cdots  & g_{nn}  \n  \\end{pmatrix}\n\\end{equation}    Bias    \n\\begin{equation}\nb = \\begin{pmatrix}\n  b_{1}\\\\ \n  \\vdots\\\\ \n  b_{n}  \n  \\end{pmatrix}\n\\end{equation}  \n\\begin{equation}\nb = \\begin{pmatrix}\n  b_{1}\\\\ \n  \\vdots\\\\ \n  b_{n}  \n  \\end{pmatrix}\n\\end{equation}    Weights    \n\\begin{equation}\nW = \\begin{pmatrix}\n  w_{11} & \\cdots  & w_{1n} \\\\ \n  \\vdots & \\ddots  & \\vdots \\\\ \n  w_{n1} & \\cdots  & w_{nn}  \n  \\end{pmatrix}\n\\end{equation}  \n\\begin{equation}\nW = \\begin{pmatrix}\n  w_{11} & \\cdots  & w_{1n} \\\\ \n  \\vdots & \\ddots  & \\vdots \\\\ \n  w_{n1} & \\cdots  & w_{nn}  \n  \\end{pmatrix}\n\\end{equation}   In order to use the CTRNN model in a modelling sequences of finite length, we need to solve its \ngoverning equations numerically. This gives us the trajectory of the state upto  T T  steps  y^{0}, \\cdots, y^{T} y^{0}, \\cdots, y^{T} .   \ny^{k+1} = y^{k} + \\Delta t (- \\Lambda . y^{k} + W . \\sigma(G.y^{k} + b))  \ny^{k+1} = y^{k} + \\Delta t (- \\Lambda . y^{k} + W . \\sigma(G.y^{k} + b))   DynaML's implementation of the CTRNN can be used to learn the trajectory of\ndynamical systems upto a predefined time horizon. The parameters  \\Lambda, G, b, W \\Lambda, G, b, W  are\nlearned using gradient based loss minimization.   The CTRNN implementations are also instances of  Layer[Output, Output] , which take as input\ntensors of shape  n n  and produce tensors of shape  (n, T) (n, T) , there are two variants that users\ncan choose from.",
            "title": "Continuous Time RNN"
        },
        {
            "location": "/core/core_dtflearn/#fixed-time-step-integration",
            "text": "When the integration time step  \\Delta t \\Delta t  is user defined and fixed.  1\n2\n3\n4\n5\n6 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._   val   ctrnn_layer   =   dtflearn . ctrnn (  name   =   \"CTRNN_1\" ,   units   =   10 ,   horizon   =   5 ,   timestep   =   0.1 )",
            "title": "Fixed Time Step Integration"
        },
        {
            "location": "/core/core_dtflearn/#dynamic-time-step-integration",
            "text": "When the integration time step  \\Delta t \\Delta t  is a parameter that can be learned during the\ntraining process.  1\n2\n3\n4\n5\n6 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._   val   dctrnn_layer   =   dtflearn . dctrnn (  name   =   \"DCTRNN_1\" ,   units   =   10 ,   horizon   =   5 )",
            "title": "Dynamic Time Step Integration"
        },
        {
            "location": "/core/core_dtflearn/#stack-concatenate",
            "text": "Often one would need to combine inputs of previous layers in some manner, the following layers enable these operations.",
            "title": "Stack &amp; Concatenate"
        },
        {
            "location": "/core/core_dtflearn/#stack-inputs",
            "text": "This is a computational layer which performs the function of  dtf.stack() .  1\n2\n3\n4 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._   val   stk_layer   =   dtflearn . stack_outputs ( \"StackTensors\" ,   axis   =   1 )",
            "title": "Stack Inputs"
        },
        {
            "location": "/core/core_dtflearn/#concatenate-inputs",
            "text": "This is a computational layer which performs the function of  dtf.concatenate() .  1\n2\n3\n4 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._   val   concat_layer   =   dtflearn . stack_outputs ( \"ConcatenateTensors\" ,   axis   =   1 )",
            "title": "Concatenate Inputs"
        },
        {
            "location": "/core/core_dtflearn/#collect-layers",
            "text": "A sequence of layers can be collected into a single layer which accepts a sequence of symbolic tensors.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._   val   layers   =   Seq ( \n   tf . learn . Linear ( \"l1\" ,   10 ), \n   dtflearn . identity ( \"Identity\" ), \n   dtflearn . ctrnn ( \n     name   =   \"CTRNN_1\" ,   units   =   10 ,  \n     horizon   =   5 ,   timestep   =   0.1 \n   )  )  val   combined_layer   =   dtflearn . stack_layers ( \"Collect\" ,   layers )",
            "title": "Collect Layers"
        },
        {
            "location": "/core/core_dtflearn/#input-pairs",
            "text": "To handle inputs consisting of pairs of elements, one can provide a separate layer for processing each of the elements.  1\n2\n3\n4\n5\n6\n7 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  val   sl   =   dtflearn . tuple2_layer ( \n   \"tuple2layer\" ,  \n   dtflearn . rbf_layer ( \"rbf1\" ,   10 ),  \n   tf . learn . Linear ( \"lin1\" ,   10 ))     Combining the elements of Tuple2 can be done as follows.  1\n2\n3\n4\n5\n6\n7 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  //Stack elements of the tuple into one tensor  val   layer1   =   dtflearn . stack_tuple2 ( \"tuple2layer\" ,   axis   =   1 )   //Concatenate elements of the tuple into one tensor  val   layer2   =   dtflearn . concat_tuple2 ( \"tuple2layer\" ,   axis   =   1 )",
            "title": "Input Pairs"
        },
        {
            "location": "/core/core_dtflearn/#stoppage-criteria",
            "text": "In order to train tensorflow models using iterative gradient based models, the user must \ndefine some stoppage criteria for the training process. This can be done via the method  tf.learn.StopCriteria() . The following preset stop criteria call  tf.learn.StopCriteria()  under the hood.",
            "title": "Stoppage Criteria"
        },
        {
            "location": "/core/core_dtflearn/#iterations-based",
            "text": "1 val   stopc1   =   dtflearn . max_iter_stop ( 10000 )",
            "title": "Iterations Based"
        },
        {
            "location": "/core/core_dtflearn/#change-in-loss",
            "text": "",
            "title": "Change in Loss"
        },
        {
            "location": "/core/core_dtflearn/#absolute-value-of-loss",
            "text": "1 val   stopc2   =   dtflearn . abs_loss_change_stop ( 0.1 )",
            "title": "Absolute Value of Loss"
        },
        {
            "location": "/core/core_dtflearn/#relative-value-of-loss",
            "text": "1 val   stopc2   =   dtflearn . rel_loss_change_stop ( 0.1 )",
            "title": "Relative Value of Loss"
        },
        {
            "location": "/core/core_dtflearn/#network-building-blocks",
            "text": "To make it convenient to build deeper stacks of neural networks, DynaML includes some common layer design patterns\nas ready made easy to use methods.",
            "title": "Network Building Blocks"
        },
        {
            "location": "/core/core_dtflearn/#convolutional-neural-nets",
            "text": "Convolutional neural networks  (CNN) are a crucial building block\nof deep neural architectures for visual pattern recognition. It turns out that CNN layers must be combined with\nother computational units such as  rectified linear  (ReLU) activations,  dropout  and  max pool  layers.  Currently two abstractions are offered for building large CNN based network stacks",
            "title": "Convolutional Neural Nets"
        },
        {
            "location": "/core/core_dtflearn/#convolutional-unit",
            "text": "A single CNN unit is expressed as a convolutional layer followed by a ReLU activation and proceeded by a dropout layer.  1\n2\n3\n4\n5\n6\n7\n8\n9 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  //Learn 16 filters of shape (2, 2, 4), suitable for 4 channel jpeg images.  //Slide the filters over the image in steps of 1 pixel in each direction.  val   cnn_unit   =   dtflearn . conv2d_unit ( \n     shape   =   Shape ( 2 ,   2 ,   4 ,   16 ),   stride   =   ( 1 ,   1 ), \n     relu_param   =   0.05f ,   dropout   =   true , \n     keep_prob   =   0.55f )( i   =   1 )",
            "title": "Convolutional Unit"
        },
        {
            "location": "/core/core_dtflearn/#convolutional-pyramid",
            "text": "A CNN pyramid builds a stack of CNN units each with a stride multiplied by a factor of 2 and depth divided\nby a factor of 2 with respect to the previous unit.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  //Start with a CNN unit of shape (2, 2, 3, 16) stride (1, 1)  //End with a CNN unit of shape (2, 2, 8, 4) and stride of (8, 8)  val   cnn_stack   =   dtflearn . conv2d_pyramid ( \n   size   =   2 ,   num_channels_input   =   3 )( \n   start_num_bits   =   4 ,   end_num_bits   =   2 )( \n   relu_param   =   0.1f ,   dropout   =   true ,  \n   keep_prob   =   0.6F )",
            "title": "Convolutional Pyramid"
        },
        {
            "location": "/core/core_dtflearn/#feed-forward-neural-nets",
            "text": "Feed-forward networks are the oldest and most frequently used components of neural network architectures, they are often\nstacked into a number of layers. With  dtflearn.feedforward_stack() , you can define feed-forward stacks of arbitrary\nwidth and depth.  1\n2\n3\n4\n5\n6\n7\n8 import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  val   net_layer_sizes   =   Seq ( 10 ,   20 ,   13 ,   15 )  val   architecture   =   dtflearn . feedforward_stack ( \n     ( i :   Int )   =>   dtflearn . Phi ( \"Act_\" + i ),   FLOAT64 )( \n     net_layer_sizes )",
            "title": "Feed-forward Neural Nets"
        },
        {
            "location": "/core/core_dtflearn/#building-tensorflow-models",
            "text": "After defining the key ingredients needed to build a tensorflow model,  dtflearn.build_tf_model()  builds a new \ncomputational graph and creates a tensorflow model and estimator which is trained on the provided data. In the \nfollowing example, we bring together all the elements of model training: data, architecture, loss etc.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66 import   ammonite.ops._  import   io.github.mandar2812.dynaml.tensorflow.dtflearn  import   org.platanios.tensorflow.api._  import   org.platanios.tensorflow.api.ops.NN.SamePadding  import   org.platanios.tensorflow.data.image.CIFARLoader  import   java.nio.file.Paths  val   tempdir   =   home / \"tmp\"  val   dataSet   =   CIFARLoader . load ( \n   Paths . get ( tempdir . toString ()),  \n   CIFARLoader . CIFAR_10 )  val   trainImages   =   tf . data . TensorSlicesDataset ( dataSet . trainImages )  val   trainLabels   =   tf . data . TensorSlicesDataset ( dataSet . trainLabels )  val   trainData   =  \n   trainImages . zip ( trainLabels ) \n     . repeat () \n     . shuffle ( 10000 ) \n     . batch ( 128 ) \n     . prefetch ( 10 )  println ( \"Building the classification model.\" )  val   input   =   tf . learn . Input ( \n   UINT8 ,  \n   Shape ( \n     - 1 ,  \n     dataSet . trainImages . shape ( 1 ),  \n     dataSet . trainImages . shape ( 2 ),  \n     dataSet . trainImages . shape ( 3 ))  )  val   trainInput   =   tf . learn . Input ( UINT8 ,   Shape (- 1 ))  val   architecture   =   tf . learn . Cast ( \"Input/Cast\" ,   FLOAT32 )   >> \n   dtflearn . conv2d_pyramid ( 2 ,   3 )( 4 ,   2 )( 0.1f ,   true ,   0.6F )   >> \n   tf . learn . MaxPool ( \n     \"Layer_3/MaxPool\" ,  \n     Seq ( 1 ,   2 ,   2 ,   1 ),  \n     1 ,   1 ,   SamePadding )   >> \n   tf . learn . Flatten ( \"Layer_3/Flatten\" )   >> \n   dtflearn . feedforward ( 256 )( id   =   4 )   >> \n   tf . learn . ReLU ( \"Layer_4/ReLU\" ,   0.1f )   >> \n   dtflearn . feedforward ( 10 )( id   =   5 )  val   trainingInputLayer   =   tf . learn . Cast ( \"TrainInput/Cast\" ,   INT64 )  val   loss   =   tf . learn . SparseSoftmaxCrossEntropy ( \"Loss/CrossEntropy\" )   >> \n   tf . learn . Mean ( \"Loss/Mean\" )   >>  \n   tf . learn . ScalarSummary ( \"Loss/Summary\" ,   \"Loss\" )  val   optimizer   =   tf . train . AdaGrad ( 0.1 )  println ( \"Training the linear regression model.\" )  val   summariesDir   =   java . nio . file . Paths . get ( \n   ( tempdir / \"cifar_summaries\" ). toString ()  )  val   ( model ,   estimator )   =   dtflearn . build_tf_model ( \n   architecture ,   input ,   trainInput ,   trainingInputLayer , \n   loss ,   optimizer ,   summariesDir ,  \n   dtflearn . max_iter_stop ( 1000 ), \n   100 ,   100 ,   100 )( trainData )",
            "title": "Building Tensorflow Models"
        },
        {
            "location": "/core/core_dtfdata/",
            "text": "Summary\n\n\nThe \nDataSet\n API added in v1.5.3, makes it easy to work with potentially large data sets, \nperform complex pre-processing tasks and feed these data sets into TensorFlow models.\n\n\n\n\nData Set\n\u00b6\n\n\nBasics\n\u00b6\n\n\nA \nDataSet[X]\n instance is simply a wrapper over an \nIterable[X]\n object, although the user still has \naccess to the underlying collection.    \n\n\n\n\nTip\n\n\nThe \ndtfdata\n \nobject gives the user easy access to the \nDataSet\n API.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nimport\n \n_root_.io.github.mandar2812.dynaml.probability._\n\n\nimport\n \n_root_.io.github.mandar2812.dynaml.pipes._\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\n\n\nval\n \nrandom_numbers\n \n=\n \nGaussianRV\n(\n0.0\n,\n \n1.0\n)\n \n:*\n \nGaussianRV\n(\n1.0\n,\n \n2.0\n)\n \n\n\n//Create a data set.\n\n\nval\n \ndataset1\n \n=\n \ndtfdata\n.\ndataset\n(\nrandom_numbers\n.\niid\n(\n10000\n).\ndraw\n)\n\n\n\n//Access underlying data\n\n\ndataset1\n.\ndata\n\n\n\n\n\n\n\n\n\nTransformations\n\u00b6\n\n\nDynaML data sets support several operations of the \nmap-reduce\n philosophy.\n\n\nMap\n\u00b6\n\n\nTransform each element of type \nX\n into some other element of type \nY\n (\nY\n can possibly be the same as \nX\n).\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nimport\n \n_root_.io.github.mandar2812.dynaml.probability._\n\n\nimport\n \n_root_.io.github.mandar2812.dynaml.pipes._\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\n\n\nval\n \nrandom_numbers\n \n=\n \nGaussianRV\n(\n0.0\n,\n \n1.0\n)\n\n\n//A data set of random gaussian numbers.     \n\n\nval\n \nrandom_gaussian_dataset\n \n=\n \ndtfdata\n.\ndataset\n(\n\n  \nrandom_numbers\n.\niid\n(\n10000\n).\ndraw\n\n\n)\n\n\n\n//Transform data set by applying a scala function\n\n\nval\n \nrandom_chisq_dataset\n \n=\n \nrandom_gaussian_dataset\n.\nmap\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n*\nx\n)\n\n\n\nval\n \nexp_tr\n \n=\n \nDataPipe\n[\nDouble\n, \nDouble\n](\nmath\n.\nexp\n \n_\n)\n\n\n//Can pass a DataPipe instead of a function\n\n\nval\n \nrandom_log_gaussian_dataset\n \n=\n \nrandom_gaussian_dataset\n.\nmap\n(\nexp_tr\n)\n\n\n\n\n\n\n\nFlat Map\n\u00b6\n\n\nProcess each element by applying a function which transforms each element into an \nIterable\n, \nthis operation is followed by flattening of the top level \nIterable\n.\n\n\nSchematically, this process is\n\n\nIterable[X] -> Iterable[Iterable[Y]] -> Iterable[Y]\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nimport\n \n_root_.io.github.mandar2812.dynaml.probability._\n\n\nimport\n \n_root_.io.github.mandar2812.dynaml.pipes._\n\n\nimport\n \nscala.util.Random\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\n\nval\n \nrandom_gaussian_dataset\n \n=\n \ndtfdata\n.\ndataset\n(\n\n  \nGaussianRV\n(\n0.0\n,\n \n1.0\n).\niid\n(\n10000\n).\ndraw\n\n\n)\n\n\n\n//Transform data set by applying a scala function\n\n\nval\n \ngaussian_mixture\n \n=\n \nrandom_gaussian_dataset\n.\nflatMap\n(\n\n  \n(\nx\n:\n \nDouble\n)\n \n=>\n \nGaussianRV\n(\n0.0\n,\n \nx\n*\nx\n).\niid\n(\n10\n).\ndraw\n\n\n)\n\n\n\n\n\n\n\nFilter\n\u00b6\n\n\nCollect only the elements which satisfy some predicate, i.e. a function which returns \ntrue\n for the\nelements to be selected (filtered) and \nfalse\n for the ones which should be discarded.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nimport\n \n_root_.io.github.mandar2812.dynaml.probability._\n\n\nimport\n \n_root_.io.github.mandar2812.dynaml.pipes._\n\n\nimport\n \nscala.util.Random\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\n\nval\n \ngaussian_dataset\n \n=\n \ndtfdata\n.\ndataset\n(\n\n  \nGaussianRV\n(\n0.0\n,\n \n1.0\n).\niid\n(\n10000\n).\ndraw\n\n\n)\n\n\n\nval\n \nonlyPositive\n \n=\n \nDataPipe\n[\nDouble\n, \nBoolean\n](\n_\n \n>\n \n0.0\n)\n\n\n\nval\n \ntruncated_gaussian\n \n=\n \ngaussian_dataset\n.\nfilter\n(\nonlyPositive\n)\n\n\n\nval\n \nzeroOrGreater\n \n=\n \n(\nx\n:\n \nDouble\n)\n \n=>\n \nx\n \n>=\n \n0.0\n\n\n//filterNot works in the opposite manner to filter\n\n\nval\n \nneg_truncated_gaussian\n \n=\n \ngaussian_dataset\n.\nfilterNot\n(\nzeroOrGreater\n)\n\n\n\n\n\n\n\nScan & Friends\n\u00b6\n\n\nSometimes, we need to perform operations on a data set which are sequential in nature. In this situation, \nthe \nscanLeft()\n and \nscanRight()\n are useful.\n\n\nLets simulate a random walk, we start with \nx_0\nx_0\n, a number and add independent gaussian increments to it.\n\n\n\n\n\n\\begin{align*}\nx_t &= x_{t-1} + \\epsilon \\\\\n\\epsilon &\\sim \\mathcal{N}(0, 1)\n\\end{align*}\n\n\n\n\n\\begin{align*}\nx_t &= x_{t-1} + \\epsilon \\\\\n\\epsilon &\\sim \\mathcal{N}(0, 1)\n\\end{align*}\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nimport\n \n_root_.io.github.mandar2812.dynaml.probability._\n\n\nimport\n \n_root_.io.github.mandar2812.dynaml.pipes._\n\n\nimport\n \nscala.util.Random\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\n\nval\n \ngaussian_increments\n \n=\n \ndtfdata\n.\ndataset\n(\n\n  \nGaussianRV\n(\n0.0\n,\n \n1.0\n).\niid\n(\n10000\n).\ndraw\n\n\n)\n\n\n\nval\n \nincrement\n \n=\n \nDataPipe2\n[\nDouble\n, \nDouble\n, \nDouble\n]((\nx\n,\n \ni\n)\n \n=>\n \nx\n \n+\n \ni\n)\n\n\n\n//Start the random walk from zero, and keep adding increments.\n\n\nval\n \nrandom_walk\n \n=\n \ngaussian_increments\n.\nscanLeft\n(\n0.0\n)(\nincrement\n)\n\n\n\n\n\n\n\nThe \nscanRight()\n works just like the \nscanLeft()\n method, except it begins from the last element \nof the collection.\n\n\nReduce & Reduce Left\n\u00b6\n\n\nThe \nreduce()\n and \nreduceLeft()\n methods help in computing summary values from the entire data \ncollection.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nimport\n \n_root_.io.github.mandar2812.dynaml.probability._\n\n\nimport\n \n_root_.io.github.mandar2812.dynaml.pipes._\n\n\nimport\n \nscala.util.Random\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\n\nval\n \ngaussian_increments\n \n=\n \ndtfdata\n.\ndataset\n(\n\n  \nGaussianRV\n(\n0.0\n,\n \n1.0\n).\niid\n(\n10000\n).\ndraw\n\n\n)\n\n\n\nval\n \nincrement\n \n=\n \nDataPipe2\n[\nDouble\n, \nDouble\n, \nDouble\n]((\nx\n,\n \ni\n)\n \n=>\n \nx\n \n+\n \ni\n)\n\n\n\nval\n \nrandom_walk\n \n=\n \ngaussian_increments\n.\nscanLeft\n(\n0.0\n)(\nincrement\n)\n\n\n\nval\n \naverage\n \n=\n \nrandom_walk\n.\nreduce\n(\n\n  \nDataPipe2\n[\nDouble\n, \nDouble\n, \nDouble\n]((\nx\n,\n \ny\n)\n \n=>\n \nx\n \n+\n \ny\n)\n\n\n)/\n10000.0\n\n\n\n\n\n\n\nOther Transformations\n\u00b6\n\n\nSome times transformations on data sets cannot be applied on each element individually, but the \nentire data collection is required for such a transformation.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nimport\n \n_root_.io.github.mandar2812.dynaml.probability._\n\n\nimport\n \n_root_.io.github.mandar2812.dynaml.pipes._\n\n\nimport\n \nscala.util.Random\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\n\nval\n \ngaussian_data\n \n=\n \ndtfdata\n.\ndataset\n(\n\n  \nGaussianRV\n(\n0.0\n,\n \n1.0\n).\niid\n(\n10000\n).\ndraw\n\n\n)\n\n\n\nval\n \nresample\n \n=\n \nDataPipe\n[\nIterable\n[\nDouble\n]\n, \nIterable\n[\nDouble\n]](\n\n  \ncoll\n \n=>\n \n(\n0\n \nuntil\n \n10000\n).\nmap\n(\n_\n \n=>\n \ncoll\n(\nRandom\n.\nnextInt\n(\n10000\n)))\n\n\n)\n\n\n\nval\n \nresampled_data\n \n=\n \ngaussian_data\n.\ntransform\n(\nresample\n)\n\n\n\n\n\n\n\n\n\nNote\n\n\nConversion to TF-Scala \nDataset\n class\n\n\nThe TensorFlow scala API also has a \nDataset\n class, from a DynaML \nDataSet\n \ninstance, it is possible to obtain a TensorFlow \nDataset\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nimport\n \n_root_.io.github.mandar2812.dynaml.probability._\n\n\nimport\n \n_root_.io.github.mandar2812.dynaml.pipes._\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\nimport\n \norg.platanios.tensorflow.api._\n\n\nimport\n \norg.platanios.tensorflow.api.types._\n\n\n\n\nval\n \nrandom_numbers\n \n=\n \nGaussianRV\n(\n0.0\n,\n \n1.0\n)\n\n\n\n//Create a data set.\n\n\nval\n \ndataset1\n \n=\n \ndtfdata\n.\ndataset\n(\nrandom_numbers\n.\niid\n(\n10000\n).\ndraw\n)\n\n\n\n//Convert to TensorFlow data set\n\n\ndataset1\n.\nbuild\n[\nTensor\n, \nOutput\n, \nDataType.Aux\n[\nDouble\n]\n, \nDataType\n, \nShape\n](\n\n  \nLeft\n(\nDataPipe\n[\nDouble\n, \nTensor\n](\nx\n \n=>\n \ndtf\n.\ntensor_f64\n(\n1\n)(\nx\n))),\n\n  \nFLOAT64\n,\n \nShape\n(\n1\n)\n    \n\n)\n\n\n\n\n\n\n\n\n\nTuple Data & Supervised Data\n\u00b6\n\n\nThe classes \nZipDataSet[X, Y]\n and \nSupervisedDataSet[X, Y]\n both represent data collections which consist of \n\n(X, Y)\n tuples. They can be created in a number of ways.\n\n\nZip Data\n\u00b6\n\n\nThe \nzip()\n method can be used to create data sets consisting of tuples.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nimport\n \n_root_.io.github.mandar2812.dynaml.probability._\n\n\nimport\n \n_root_.io.github.mandar2812.dynaml.pipes._\n\n\nimport\n \nscala.util.Random\n\n\nimport\n \n_root_.breeze.stats.distributions._\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\n\nval\n \ngaussian_data\n \n=\n \ndtfdata\n.\ndataset\n(\n\n  \nGaussianRV\n(\n0.0\n,\n \n1.0\n).\niid\n(\n10000\n).\ndraw\n\n\n)\n\n\n\nval\n \nlog_normal_data\n \n=\n \ngaussian_data\n.\nmap\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nexp\n(\nx\n))\n\n\n\nval\n \npoisson_data\n  \n=\n \ndtfdata\n.\ndataset\n(\n\n  \nRandomVariable\n(\nPoisson\n(\n2.5\n)).\niid\n(\n10000\n).\ndraw\n\n\n)\n \n\n\nval\n \ntuple_data1\n \n=\n \npoisson_data\n.\nzip\n(\ngaussian_data\n)\n\n\n\nval\n \ntuple_data2\n \n=\n \npoisson_data\n.\nzip\n(\nlog_normal_data\n)\n\n\n\n//Join on the keys, in this case the \n\n\n//Poisson distributed integers\n\n\n\ntuple_data1\n.\njoin\n(\ntuple_data2\n)\n\n\n\n\n\n\n\nSupervised Data\n\u00b6\n\n\nFor supervised learning operations, we can use the \nSupervisedDataSet\n class, which can be instantiated \nin the following ways.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nimport\n \n_root_.io.github.mandar2812.dynaml.probability._\n\n\nimport\n \n_root_.io.github.mandar2812.dynaml.pipes._\n\n\nimport\n \nscala.util.Random\n\n\nimport\n \n_root_.breeze.stats.distributions._\n\n\nimport\n \nio.github.mandar2812.dynaml.tensorflow._\n\n\n\nval\n \ngaussian_data\n \n=\n \ndtfdata\n.\ndataset\n(\n\n  \nGaussianRV\n(\n0.0\n,\n \n1.0\n).\niid\n(\n10000\n).\ndraw\n\n\n)\n\n\n\nval\n \nsup_data1\n \n=\n \ngaussian_data\n.\nto_supervised\n(\n\n  \nDataPipe\n[\nDouble\n, \n(\nDouble\n, \nDouble\n)](\nx\n \n=>\n \n(\nx\n,\n \nGaussianRV\n(\n0.0\n,\n \nx\n*\nx\n).\ndraw\n))\n\n\n)\n\n\n\nval\n \ntargets\n \n=\n \ngaussian_data\n.\nmap\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nexp\n(\nx\n))\n\n\n\nval\n \nsup_data2\n \n=\n \ndtfdata\n.\nsupervised_dataset\n(\ngaussian_data\n,\n \ntargets\n)",
            "title": "Data Set API"
        },
        {
            "location": "/core/core_dtfdata/#data-set",
            "text": "",
            "title": "Data Set"
        },
        {
            "location": "/core/core_dtfdata/#basics",
            "text": "A  DataSet[X]  instance is simply a wrapper over an  Iterable[X]  object, although the user still has \naccess to the underlying collection.       Tip  The  dtfdata  \nobject gives the user easy access to the  DataSet  API.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 import   _root_.io.github.mandar2812.dynaml.probability._  import   _root_.io.github.mandar2812.dynaml.pipes._  import   io.github.mandar2812.dynaml.tensorflow._  val   random_numbers   =   GaussianRV ( 0.0 ,   1.0 )   :*   GaussianRV ( 1.0 ,   2.0 )   //Create a data set.  val   dataset1   =   dtfdata . dataset ( random_numbers . iid ( 10000 ). draw )  //Access underlying data  dataset1 . data",
            "title": "Basics"
        },
        {
            "location": "/core/core_dtfdata/#transformations",
            "text": "DynaML data sets support several operations of the  map-reduce  philosophy.",
            "title": "Transformations"
        },
        {
            "location": "/core/core_dtfdata/#map",
            "text": "Transform each element of type  X  into some other element of type  Y  ( Y  can possibly be the same as  X ).   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 import   _root_.io.github.mandar2812.dynaml.probability._  import   _root_.io.github.mandar2812.dynaml.pipes._  import   io.github.mandar2812.dynaml.tensorflow._  val   random_numbers   =   GaussianRV ( 0.0 ,   1.0 )  //A data set of random gaussian numbers.       val   random_gaussian_dataset   =   dtfdata . dataset ( \n   random_numbers . iid ( 10000 ). draw  )  //Transform data set by applying a scala function  val   random_chisq_dataset   =   random_gaussian_dataset . map (( x :   Double )   =>   x * x )  val   exp_tr   =   DataPipe [ Double ,  Double ]( math . exp   _ )  //Can pass a DataPipe instead of a function  val   random_log_gaussian_dataset   =   random_gaussian_dataset . map ( exp_tr )",
            "title": "Map"
        },
        {
            "location": "/core/core_dtfdata/#flat-map",
            "text": "Process each element by applying a function which transforms each element into an  Iterable , \nthis operation is followed by flattening of the top level  Iterable .  Schematically, this process is  Iterable[X] -> Iterable[Iterable[Y]] -> Iterable[Y]   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 import   _root_.io.github.mandar2812.dynaml.probability._  import   _root_.io.github.mandar2812.dynaml.pipes._  import   scala.util.Random  import   io.github.mandar2812.dynaml.tensorflow._  val   random_gaussian_dataset   =   dtfdata . dataset ( \n   GaussianRV ( 0.0 ,   1.0 ). iid ( 10000 ). draw  )  //Transform data set by applying a scala function  val   gaussian_mixture   =   random_gaussian_dataset . flatMap ( \n   ( x :   Double )   =>   GaussianRV ( 0.0 ,   x * x ). iid ( 10 ). draw  )",
            "title": "Flat Map"
        },
        {
            "location": "/core/core_dtfdata/#filter",
            "text": "Collect only the elements which satisfy some predicate, i.e. a function which returns  true  for the\nelements to be selected (filtered) and  false  for the ones which should be discarded.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 import   _root_.io.github.mandar2812.dynaml.probability._  import   _root_.io.github.mandar2812.dynaml.pipes._  import   scala.util.Random  import   io.github.mandar2812.dynaml.tensorflow._  val   gaussian_dataset   =   dtfdata . dataset ( \n   GaussianRV ( 0.0 ,   1.0 ). iid ( 10000 ). draw  )  val   onlyPositive   =   DataPipe [ Double ,  Boolean ]( _   >   0.0 )  val   truncated_gaussian   =   gaussian_dataset . filter ( onlyPositive )  val   zeroOrGreater   =   ( x :   Double )   =>   x   >=   0.0  //filterNot works in the opposite manner to filter  val   neg_truncated_gaussian   =   gaussian_dataset . filterNot ( zeroOrGreater )",
            "title": "Filter"
        },
        {
            "location": "/core/core_dtfdata/#scan-friends",
            "text": "Sometimes, we need to perform operations on a data set which are sequential in nature. In this situation, \nthe  scanLeft()  and  scanRight()  are useful.  Lets simulate a random walk, we start with  x_0 x_0 , a number and add independent gaussian increments to it.   \n\\begin{align*}\nx_t &= x_{t-1} + \\epsilon \\\\\n\\epsilon &\\sim \\mathcal{N}(0, 1)\n\\end{align*}  \n\\begin{align*}\nx_t &= x_{t-1} + \\epsilon \\\\\n\\epsilon &\\sim \\mathcal{N}(0, 1)\n\\end{align*}    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 import   _root_.io.github.mandar2812.dynaml.probability._  import   _root_.io.github.mandar2812.dynaml.pipes._  import   scala.util.Random  import   io.github.mandar2812.dynaml.tensorflow._  val   gaussian_increments   =   dtfdata . dataset ( \n   GaussianRV ( 0.0 ,   1.0 ). iid ( 10000 ). draw  )  val   increment   =   DataPipe2 [ Double ,  Double ,  Double ](( x ,   i )   =>   x   +   i )  //Start the random walk from zero, and keep adding increments.  val   random_walk   =   gaussian_increments . scanLeft ( 0.0 )( increment )    The  scanRight()  works just like the  scanLeft()  method, except it begins from the last element \nof the collection.",
            "title": "Scan &amp; Friends"
        },
        {
            "location": "/core/core_dtfdata/#reduce-reduce-left",
            "text": "The  reduce()  and  reduceLeft()  methods help in computing summary values from the entire data \ncollection.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 import   _root_.io.github.mandar2812.dynaml.probability._  import   _root_.io.github.mandar2812.dynaml.pipes._  import   scala.util.Random  import   io.github.mandar2812.dynaml.tensorflow._  val   gaussian_increments   =   dtfdata . dataset ( \n   GaussianRV ( 0.0 ,   1.0 ). iid ( 10000 ). draw  )  val   increment   =   DataPipe2 [ Double ,  Double ,  Double ](( x ,   i )   =>   x   +   i )  val   random_walk   =   gaussian_increments . scanLeft ( 0.0 )( increment )  val   average   =   random_walk . reduce ( \n   DataPipe2 [ Double ,  Double ,  Double ](( x ,   y )   =>   x   +   y )  )/ 10000.0",
            "title": "Reduce &amp; Reduce Left"
        },
        {
            "location": "/core/core_dtfdata/#other-transformations",
            "text": "Some times transformations on data sets cannot be applied on each element individually, but the \nentire data collection is required for such a transformation.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 import   _root_.io.github.mandar2812.dynaml.probability._  import   _root_.io.github.mandar2812.dynaml.pipes._  import   scala.util.Random  import   io.github.mandar2812.dynaml.tensorflow._  val   gaussian_data   =   dtfdata . dataset ( \n   GaussianRV ( 0.0 ,   1.0 ). iid ( 10000 ). draw  )  val   resample   =   DataPipe [ Iterable [ Double ] ,  Iterable [ Double ]]( \n   coll   =>   ( 0   until   10000 ). map ( _   =>   coll ( Random . nextInt ( 10000 )))  )  val   resampled_data   =   gaussian_data . transform ( resample )     Note  Conversion to TF-Scala  Dataset  class  The TensorFlow scala API also has a  Dataset  class, from a DynaML  DataSet  \ninstance, it is possible to obtain a TensorFlow  Dataset .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 import   _root_.io.github.mandar2812.dynaml.probability._  import   _root_.io.github.mandar2812.dynaml.pipes._  import   io.github.mandar2812.dynaml.tensorflow._  import   org.platanios.tensorflow.api._  import   org.platanios.tensorflow.api.types._  val   random_numbers   =   GaussianRV ( 0.0 ,   1.0 )  //Create a data set.  val   dataset1   =   dtfdata . dataset ( random_numbers . iid ( 10000 ). draw )  //Convert to TensorFlow data set  dataset1 . build [ Tensor ,  Output ,  DataType.Aux [ Double ] ,  DataType ,  Shape ]( \n   Left ( DataPipe [ Double ,  Tensor ]( x   =>   dtf . tensor_f64 ( 1 )( x ))), \n   FLOAT64 ,   Shape ( 1 )      )",
            "title": "Other Transformations"
        },
        {
            "location": "/core/core_dtfdata/#tuple-data-supervised-data",
            "text": "The classes  ZipDataSet[X, Y]  and  SupervisedDataSet[X, Y]  both represent data collections which consist of  (X, Y)  tuples. They can be created in a number of ways.",
            "title": "Tuple Data &amp; Supervised Data"
        },
        {
            "location": "/core/core_dtfdata/#zip-data",
            "text": "The  zip()  method can be used to create data sets consisting of tuples.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 import   _root_.io.github.mandar2812.dynaml.probability._  import   _root_.io.github.mandar2812.dynaml.pipes._  import   scala.util.Random  import   _root_.breeze.stats.distributions._  import   io.github.mandar2812.dynaml.tensorflow._  val   gaussian_data   =   dtfdata . dataset ( \n   GaussianRV ( 0.0 ,   1.0 ). iid ( 10000 ). draw  )  val   log_normal_data   =   gaussian_data . map (( x :   Double )   =>   math . exp ( x ))  val   poisson_data    =   dtfdata . dataset ( \n   RandomVariable ( Poisson ( 2.5 )). iid ( 10000 ). draw  )   val   tuple_data1   =   poisson_data . zip ( gaussian_data )  val   tuple_data2   =   poisson_data . zip ( log_normal_data )  //Join on the keys, in this case the   //Poisson distributed integers  tuple_data1 . join ( tuple_data2 )",
            "title": "Zip Data"
        },
        {
            "location": "/core/core_dtfdata/#supervised-data",
            "text": "For supervised learning operations, we can use the  SupervisedDataSet  class, which can be instantiated \nin the following ways.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 import   _root_.io.github.mandar2812.dynaml.probability._  import   _root_.io.github.mandar2812.dynaml.pipes._  import   scala.util.Random  import   _root_.breeze.stats.distributions._  import   io.github.mandar2812.dynaml.tensorflow._  val   gaussian_data   =   dtfdata . dataset ( \n   GaussianRV ( 0.0 ,   1.0 ). iid ( 10000 ). draw  )  val   sup_data1   =   gaussian_data . to_supervised ( \n   DataPipe [ Double ,  ( Double ,  Double )]( x   =>   ( x ,   GaussianRV ( 0.0 ,   x * x ). draw ))  )  val   targets   =   gaussian_data . map (( x :   Double )   =>   math . exp ( x ))  val   sup_data2   =   dtfdata . supervised_dataset ( gaussian_data ,   targets )",
            "title": "Supervised Data"
        },
        {
            "location": "/core/core_kernels/",
            "text": "The \ndynaml.kernels\n package has a highly developed API for creating kernel functions for machine learning applications. Here\nwe give the user an in-depth introduction to its capabilities.\n\n\n\n\n\n\n\n\nPositive definite\n functions or \npositive type\n functions occupy an important place in various areas of mathematics, from the construction of covariances of random variables to quantifying distance measures in \nHilbert spaces\n. Symmetric positive type functions defined on the cartesian product of a set with itself \nK: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}\nK: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}\n are also known as \nkernel\n functions in machine learning. They are applied extensively in problems such as.\n\n\n\n\nModel non-linear behavior in SVM models: \nSVM\n and \nLSSVM\n\n\nQuantify covariance between input patterns: \nGaussian Processes\n\n\nRepresent degree of 'closeness' or affinity in unsupervised learning: \nKernel Spectral Clustering\n\n\n\n\nFor an in depth review of the various applications of kernels in the machine learning domain, refer to \nScholkopf et. al\n\n\n\n\nNomenclature\n\n\nIn the machine learning community the words \nkernel\n and \ncovariance function\n are used interchangeably.\n\n\n\n\nKernel API\n\u00b6\n\n\nThe kernel class hierarchy all stems from a simple trait shown here.\n\n\n1\n2\n3\ntrait\n \nKernel\n[\nT\n, \nV\n]\n \n{\n\n  \ndef\n \nevaluate\n(\nx\n:\n \nT\n,\n \ny\n:\n \nT\n)\n:\n \nV\n\n\n}\n\n\n\n\n\n\n\nThis outlines only one key feature for kernel functions i.e. their evaluation functional which takes two inputs from \n\\mathcal{X}\n\\mathcal{X}\n and yields a scalar value.\n\n\n\n\nKernel\n vs \nCovarianceFunction\n\n\n\nFor practical purposes, the \nKernel\n[\nT\n, \nV\n]\n trait does not have enough functionality for usage in varied models like \nGaussian Processes\n, \nStudent's T Processes\n, \nLS-SVM\n etc.\n\n\nFor this reason there is the \nCovarianceFunction\n[\nT\n, \nV\n, \nM\n]\n abstract class. It contains methods to construct kernel matrices, keep track of hyper-parameter assignments among other things.\n\n\n\n\nCreating arbitrary kernel functions\n\u00b6\n\n\nApart from off the shelf kernel functions, it is also possible to create custom kernels on the fly by using the \nCovarianceFunction\n object.\n\n\nConstructing kernels via feature maps\n\u00b6\n\n\nIt is known from \nMercer's theorem\n that any valid kernel function must be decomposable as a dot product between certain \nbasis function\n representation of the inputs. This translates mathematically into.\n\n\n\n\n\n\\begin{align}\n    & K(\\mathbf{x}, \\mathbf{y}) = \\varphi^{T}(\\mathbf{x}) . \\varphi(\\mathbf{y}) \\\\\n    & \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^n\n\\end{align}\n\n\n\n\n\\begin{align}\n    & K(\\mathbf{x}, \\mathbf{y}) = \\varphi^{T}(\\mathbf{x}) . \\varphi(\\mathbf{y}) \\\\\n    & \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^n\n\\end{align}\n\n\n\n\n\nThe function \n\\varphi(.)\n\\varphi(.)\n is some higher (possibly infinite) dimensional representation of the input features of a data point. Note that the input space \n\\mathcal{X}\n\\mathcal{X}\n could be any of the following (but not limited to).\n\n\n\n\n\n\nThe space of all connection graphs with specific number of nodes.\n\n\n\n\n\n\nA multi-dimensional vector.\n\n\n\n\n\n\nThe space of all character sequences (binary or otherwise) up to a certain length.\n\n\n\n\n\n\nThe set of all integer tuples e.g. \n(1,2), (6,10), \\cdots\n(1,2), (6,10), \\cdots\n\n\n\n\n\n\nWe can use any function from some domain \n\\mathcal{X}\n\\mathcal{X}\n yielding a \nDenseVector\n[\nDouble\n]\n to define a particular inner product/kernel function.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n// First create a function mapping from some input space to\n\n\n// Breeze dense vectors.\n\n\n\nval\n \nmapFunc\n \n=\n \n(\nvec\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n \n{\n\n    \nval\n \nmat\n \n=\n \nvec\n \n*\n \nvec\n.\nt\n\n    \nmat\n.\ntoDenseVector\n\n\n}\n\n\n\nval\n \nkernel\n \n=\n \nCovarianceFunction\n(\nmapFunc\n)\n\n\n\n\n\n\n\n\n\nFeature map kernels\n\n\nCovariance functions constructed using feature mappings as shown above return a special object; an instance of the \nFeatureMapCovariance\n[\nT\n, \nDenseVector\n[\nDouble\n]]\n class. In the section on composite kernels we will see why this is important.\n\n\n\n\nConstructing kernels via direct evaluation\n\u00b6\n\n\nInstead of defining a feature representation like \n\\varphi(.)\n\\varphi(.)\n as in the section above, you can also directly define the evaluation expression of the kernel.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n// Create the expression for the required kernel.\n\n\nval\n \nmapFunc\n \n=\n\n\n(\nstate\n:\n \nMap\n[\nString\n, \nDouble\n])\n \n=>\n\n  \n(\nx\n:\n \nDenseVector\n[\nDouble\n],\n \ny\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n \n{\n\n       \nstate\n(\n\"alpha\"\n)*(\nx\n \ndot\n \ny\n)\n \n+\n \nstate\n(\n\"intercept\"\n)\n\n  \n}\n\n\n\n//Creates kernel with two hyper-parameters: alpha and intercept\n\n\nval\n \nkernel\n \n=\n \nCovarianceFunction\n(\nmapFunc\n)(\n\n  \nMap\n(\n\"alpha\"\n \n->\n \n1.5\n,\n \n\"intercept\"\n \n->\n \n0.01\n)\n\n\n)\n\n\n\n\n\n\n\n\n\nCreating Composite Kernels\n\u00b6\n\n\nAlgebraic Operations\n\u00b6\n\n\nIn machine learning it is well known that kernels can be combined to give other valid kernels. The symmetric positive semi-definite property of a kernel is preserved as long as it is added or multiplied to another valid kernel. In DynaML adding and multiplying kernels is elementary.\n\n\n1\n2\n3\n4\n5\nval\n \nk1\n \n=\n \nnew\n \nRBFKernel\n(\n2.5\n)\n\n\nval\n \nk2\n \n=\n \nnew\n \nRationalQuadraticKernel\n(\n2.0\n)\n\n\n\nval\n \nk\n \n=\n \nk1\n \n+\n \nk2\n\n\nval\n \nk3\n \n=\n \nk\n*\nk2\n\n\n\n\n\n\n\nComposition\n\u00b6\n\n\nFrom Mercer's theorem, every kernel can be expressed as a dot product of feature mappings evaluated at the respective data points. We can use this to construct more complex covariances i.e. by successively applying feature mappings.\n\n\n\n\n\n\\begin{align}\nC_{a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{a}(\\mathbf{x})^\\intercal \\varphi_{a}(\\mathbf{y}) \\\\\nC_{b}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\mathbf{x})^\\intercal \\varphi_{b}(\\mathbf{y}) \\\\\nC_{b . a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\varphi_{a}(\\mathbf{x}))^\\intercal \\varphi_{b}(\\varphi_{a}(\\mathbf{y}))\n\\end{align}\n\n\n\n\n\\begin{align}\nC_{a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{a}(\\mathbf{x})^\\intercal \\varphi_{a}(\\mathbf{y}) \\\\\nC_{b}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\mathbf{x})^\\intercal \\varphi_{b}(\\mathbf{y}) \\\\\nC_{b . a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\varphi_{a}(\\mathbf{x}))^\\intercal \\varphi_{b}(\\varphi_{a}(\\mathbf{y}))\n\\end{align}\n\n\n\n\n\nIn DynaML, we can create a composite kernel if the kernel represented by the map \n\\varphi_{a}\n\\varphi_{a}\n, is explicitly of type \nFeatureMapCovariance\n[\nT\n, \nDenseVector\n[\nDouble\n]]\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \nmapFunc\n \n=\n \n(\nvec\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n \n{\n\n    \nvec\n/\n2\nd\n\n\n}\n\n\n\nval\n \nk1\n \n=\n \nCovarianceFunction\n(\nmapFunc\n)\n\n\n\nval\n \nk2\n \n=\n \nnew\n \nRationalQuadraticKernel\n(\n2.0\n)\n\n\n\n//Composite kernel\n\n\nval\n \nk3\n \n=\n \nk2\n \n>\n \nk1\n\n\n\n\n\n\n\nScaling Covariances\n\u00b6\n\n\nIf \nC(\\mathbf{x}, \\mathbf{y})\nC(\\mathbf{x}, \\mathbf{y})\n is a valid covariance function, then \ng(\\mathbf{x}) C(\\mathbf{x}, \\mathbf{y}) g(\\mathbf{x})\ng(\\mathbf{x}) C(\\mathbf{x}, \\mathbf{y}) g(\\mathbf{x})\n is also a valid covariance function, where \ng(.): \\mathcal{X} \\rightarrow \\mathbb{R}\ng(.): \\mathcal{X} \\rightarrow \\mathbb{R}\n is a non-negative function from the domain of the inputs \n\\mathcal{X}\n\\mathcal{X}\n to the real number line. We call these covariances \nscaled covariance functions\n.\n\n\n1\n2\n3\n4\n5\n6\n7\n//Instantiate some kernel\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nI\n]\n \n=\n \n_\n\n\n\nval\n \nscalingFunction\n:\n \n(\nI\n)\n \n=>\n \nDouble\n \n=\n \n_\n\n\n\nval\n \nscKernel\n \n=\n \nScaledKernel\n(\n\n  \nkernel\n,\n \nDataPipe\n(\nscalingFunction\n))\n\n\n\n\n\n\n\nAdvanced Composite Kernels\n\u00b6\n\n\nSometimes we would like to express a kernel function as a product (or sum) of component kernels each of which act on\na sub-set of the dimensions (degree of freedom) of the input attributes.\n\n\nFor example; for 4 dimensional input vector, we may define two component kernels acting on the first two and\nlast two dimensions respectively and combine their evaluations via addition or multiplication. For this purpose the\n\ndynaml.kernels\n package has the \nDecomposableCovariance\n[\nS\n]\n class.\n\n\nIn order to create a decomposable kernel you need three components.\n\n\n\n\nThe component kernels (order matters)\n\n\nAn \nEncoder\n[\nS\n, \nArray\n[\nS\n]]\n instance which splits the input into an array of components\n\n\nA \nReducer\n which combines the individual kernel evaluations.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n//Not required in REPL, already imported\n\n\nimport\n \nio.github.mandar2812.dynaml.DynaMLPipe._\n\n\nimport\n \nio.github.mandar2812.dynaml.pipes._\n\n\n\nval\n \nkernel1\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\nval\n \nkernel2\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n\n//Default Reducer is addition\n\n\nval\n \ndecomp_kernel\n \n=\n\n  \nnew\n \nDecomposableCovariance\n[\nDenseVector\n[\nDouble\n]](\n\n    \nkernel1\n,\n \nkernel2\n)(\n\n    \nbreezeDVSplitEncoder\n(\n2\n))\n\n\n\nval\n \ndecomp_kernel_mult\n \n=\n\n  \nnew\n \nDecomposableCovariance\n[\nDenseVector\n[\nDouble\n]](\n\n    \nkernel1\n,\n \nkernel2\n)(\n\n    \nbreezeDVSplitEncoder\n(\n2\n),\n\n    \nReducer\n.:*:)\n\n\n\n\n\n\n\n\n\n\n\nImplementing Custom Kernels\n\n\nYou can implement your own custom kernels by extending the \nLocalScalarKernel[T]\n interface, for example:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\nimport\n \nbreeze.linalg.\n{\nDenseMatrix\n,\n \nnorm\n,\n \nDenseVector\n}\n\n\n\n//You can have any number of constructor parameters\n\n\nclass\n \nMyNewKernel\n(\nth\n:\n \nDouble\n \n=\n \n1.0\n)\n\n  \nextends\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n\n  \nwith\n \nSerializable\n \n{\n\n\n  \n//One must specify the names of each hyper-parameter\n\n  \noverride\n \nval\n \nhyper_parameters\n \n=\n \nList\n(\n\"theta\"\n)\n\n\n  \n//The state variable stores the \n\n  \n//current value of all kernel hyper-parameters\n\n  \nstate\n \n=\n \nMap\n(\n\"theta\"\n \n->\n \nth\n)\n\n\n  \n// The implementation of the actual kernel function\n\n  \noverride\n \ndef\n \nevaluateAt\n(\n\n    \nconfig\n:\n \nMap\n[\nString\n, \nDouble\n])(\n\n    \nx\n:\n \nDenseVector\n[\nDouble\n],\n\n    \ny\n:\n \nDenseVector\n[\nDouble\n])\n:\n \nDouble\n \n=\n \n???\n\n\n  \n// Return the gradient of the kernel for each hyper-parameter\n\n  \n// for a particular pair of points x,y\n\n  \noverride\n \ndef\n \ngradientAt\n(\n\n    \nconfig\n:\n \nMap\n[\nString\n, \nDouble\n])(\n\n    \nx\n:\n \nDenseVector\n[\nDouble\n],\n\n    \ny\n:\n \nDenseVector\n[\nDouble\n])\n:\n \nMap\n[\nString\n, \nDouble\n]\n \n=\n \n???\n\n\n}",
            "title": "Kernel API"
        },
        {
            "location": "/core/core_kernels/#kernel-api",
            "text": "The kernel class hierarchy all stems from a simple trait shown here.  1\n2\n3 trait   Kernel [ T ,  V ]   { \n   def   evaluate ( x :   T ,   y :   T ) :   V  }    This outlines only one key feature for kernel functions i.e. their evaluation functional which takes two inputs from  \\mathcal{X} \\mathcal{X}  and yields a scalar value.   Kernel  vs  CovarianceFunction  \nFor practical purposes, the  Kernel [ T ,  V ]  trait does not have enough functionality for usage in varied models like  Gaussian Processes ,  Student's T Processes ,  LS-SVM  etc.  For this reason there is the  CovarianceFunction [ T ,  V ,  M ]  abstract class. It contains methods to construct kernel matrices, keep track of hyper-parameter assignments among other things.",
            "title": "Kernel API"
        },
        {
            "location": "/core/core_kernels/#creating-arbitrary-kernel-functions",
            "text": "Apart from off the shelf kernel functions, it is also possible to create custom kernels on the fly by using the  CovarianceFunction  object.",
            "title": "Creating arbitrary kernel functions"
        },
        {
            "location": "/core/core_kernels/#constructing-kernels-via-feature-maps",
            "text": "It is known from  Mercer's theorem  that any valid kernel function must be decomposable as a dot product between certain  basis function  representation of the inputs. This translates mathematically into.   \n\\begin{align}\n    & K(\\mathbf{x}, \\mathbf{y}) = \\varphi^{T}(\\mathbf{x}) . \\varphi(\\mathbf{y}) \\\\\n    & \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^n\n\\end{align}  \n\\begin{align}\n    & K(\\mathbf{x}, \\mathbf{y}) = \\varphi^{T}(\\mathbf{x}) . \\varphi(\\mathbf{y}) \\\\\n    & \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^n\n\\end{align}   The function  \\varphi(.) \\varphi(.)  is some higher (possibly infinite) dimensional representation of the input features of a data point. Note that the input space  \\mathcal{X} \\mathcal{X}  could be any of the following (but not limited to).    The space of all connection graphs with specific number of nodes.    A multi-dimensional vector.    The space of all character sequences (binary or otherwise) up to a certain length.    The set of all integer tuples e.g.  (1,2), (6,10), \\cdots (1,2), (6,10), \\cdots    We can use any function from some domain  \\mathcal{X} \\mathcal{X}  yielding a  DenseVector [ Double ]  to define a particular inner product/kernel function.  1\n2\n3\n4\n5\n6\n7\n8\n9 // First create a function mapping from some input space to  // Breeze dense vectors.  val   mapFunc   =   ( vec :   DenseVector [ Double ])   =>   { \n     val   mat   =   vec   *   vec . t \n     mat . toDenseVector  }  val   kernel   =   CovarianceFunction ( mapFunc )     Feature map kernels  Covariance functions constructed using feature mappings as shown above return a special object; an instance of the  FeatureMapCovariance [ T ,  DenseVector [ Double ]]  class. In the section on composite kernels we will see why this is important.",
            "title": "Constructing kernels via feature maps"
        },
        {
            "location": "/core/core_kernels/#constructing-kernels-via-direct-evaluation",
            "text": "Instead of defining a feature representation like  \\varphi(.) \\varphi(.)  as in the section above, you can also directly define the evaluation expression of the kernel.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 // Create the expression for the required kernel.  val   mapFunc   =  ( state :   Map [ String ,  Double ])   => \n   ( x :   DenseVector [ Double ],   y :   DenseVector [ Double ])   =>   { \n        state ( \"alpha\" )*( x   dot   y )   +   state ( \"intercept\" ) \n   }  //Creates kernel with two hyper-parameters: alpha and intercept  val   kernel   =   CovarianceFunction ( mapFunc )( \n   Map ( \"alpha\"   ->   1.5 ,   \"intercept\"   ->   0.01 )  )",
            "title": "Constructing kernels via direct evaluation"
        },
        {
            "location": "/core/core_kernels/#creating-composite-kernels",
            "text": "",
            "title": "Creating Composite Kernels"
        },
        {
            "location": "/core/core_kernels/#algebraic-operations",
            "text": "In machine learning it is well known that kernels can be combined to give other valid kernels. The symmetric positive semi-definite property of a kernel is preserved as long as it is added or multiplied to another valid kernel. In DynaML adding and multiplying kernels is elementary.  1\n2\n3\n4\n5 val   k1   =   new   RBFKernel ( 2.5 )  val   k2   =   new   RationalQuadraticKernel ( 2.0 )  val   k   =   k1   +   k2  val   k3   =   k * k2",
            "title": "Algebraic Operations"
        },
        {
            "location": "/core/core_kernels/#composition",
            "text": "From Mercer's theorem, every kernel can be expressed as a dot product of feature mappings evaluated at the respective data points. We can use this to construct more complex covariances i.e. by successively applying feature mappings.   \n\\begin{align}\nC_{a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{a}(\\mathbf{x})^\\intercal \\varphi_{a}(\\mathbf{y}) \\\\\nC_{b}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\mathbf{x})^\\intercal \\varphi_{b}(\\mathbf{y}) \\\\\nC_{b . a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\varphi_{a}(\\mathbf{x}))^\\intercal \\varphi_{b}(\\varphi_{a}(\\mathbf{y}))\n\\end{align}  \n\\begin{align}\nC_{a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{a}(\\mathbf{x})^\\intercal \\varphi_{a}(\\mathbf{y}) \\\\\nC_{b}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\mathbf{x})^\\intercal \\varphi_{b}(\\mathbf{y}) \\\\\nC_{b . a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\varphi_{a}(\\mathbf{x}))^\\intercal \\varphi_{b}(\\varphi_{a}(\\mathbf{y}))\n\\end{align}   In DynaML, we can create a composite kernel if the kernel represented by the map  \\varphi_{a} \\varphi_{a} , is explicitly of type  FeatureMapCovariance [ T ,  DenseVector [ Double ]]   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   mapFunc   =   ( vec :   DenseVector [ Double ])   =>   { \n     vec / 2 d  }  val   k1   =   CovarianceFunction ( mapFunc )  val   k2   =   new   RationalQuadraticKernel ( 2.0 )  //Composite kernel  val   k3   =   k2   >   k1",
            "title": "Composition"
        },
        {
            "location": "/core/core_kernels/#scaling-covariances",
            "text": "If  C(\\mathbf{x}, \\mathbf{y}) C(\\mathbf{x}, \\mathbf{y})  is a valid covariance function, then  g(\\mathbf{x}) C(\\mathbf{x}, \\mathbf{y}) g(\\mathbf{x}) g(\\mathbf{x}) C(\\mathbf{x}, \\mathbf{y}) g(\\mathbf{x})  is also a valid covariance function, where  g(.): \\mathcal{X} \\rightarrow \\mathbb{R} g(.): \\mathcal{X} \\rightarrow \\mathbb{R}  is a non-negative function from the domain of the inputs  \\mathcal{X} \\mathcal{X}  to the real number line. We call these covariances  scaled covariance functions .  1\n2\n3\n4\n5\n6\n7 //Instantiate some kernel  val   kernel :   LocalScalarKernel [ I ]   =   _  val   scalingFunction :   ( I )   =>   Double   =   _  val   scKernel   =   ScaledKernel ( \n   kernel ,   DataPipe ( scalingFunction ))",
            "title": "Scaling Covariances"
        },
        {
            "location": "/core/core_kernels/#advanced-composite-kernels",
            "text": "Sometimes we would like to express a kernel function as a product (or sum) of component kernels each of which act on\na sub-set of the dimensions (degree of freedom) of the input attributes.  For example; for 4 dimensional input vector, we may define two component kernels acting on the first two and\nlast two dimensions respectively and combine their evaluations via addition or multiplication. For this purpose the dynaml.kernels  package has the  DecomposableCovariance [ S ]  class.  In order to create a decomposable kernel you need three components.   The component kernels (order matters)  An  Encoder [ S ,  Array [ S ]]  instance which splits the input into an array of components  A  Reducer  which combines the individual kernel evaluations.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 //Not required in REPL, already imported  import   io.github.mandar2812.dynaml.DynaMLPipe._  import   io.github.mandar2812.dynaml.pipes._  val   kernel1 :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  val   kernel2 :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  //Default Reducer is addition  val   decomp_kernel   = \n   new   DecomposableCovariance [ DenseVector [ Double ]]( \n     kernel1 ,   kernel2 )( \n     breezeDVSplitEncoder ( 2 ))  val   decomp_kernel_mult   = \n   new   DecomposableCovariance [ DenseVector [ Double ]]( \n     kernel1 ,   kernel2 )( \n     breezeDVSplitEncoder ( 2 ), \n     Reducer .:*:)      Implementing Custom Kernels  You can implement your own custom kernels by extending the  LocalScalarKernel[T]  interface, for example:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27 import   breeze.linalg. { DenseMatrix ,   norm ,   DenseVector }  //You can have any number of constructor parameters  class   MyNewKernel ( th :   Double   =   1.0 ) \n   extends   LocalScalarKernel [ DenseVector [ Double ]] \n   with   Serializable   { \n\n   //One must specify the names of each hyper-parameter \n   override   val   hyper_parameters   =   List ( \"theta\" ) \n\n   //The state variable stores the  \n   //current value of all kernel hyper-parameters \n   state   =   Map ( \"theta\"   ->   th ) \n\n   // The implementation of the actual kernel function \n   override   def   evaluateAt ( \n     config :   Map [ String ,  Double ])( \n     x :   DenseVector [ Double ], \n     y :   DenseVector [ Double ]) :   Double   =   ??? \n\n   // Return the gradient of the kernel for each hyper-parameter \n   // for a particular pair of points x,y \n   override   def   gradientAt ( \n     config :   Map [ String ,  Double ])( \n     x :   DenseVector [ Double ], \n     y :   DenseVector [ Double ]) :   Map [ String ,  Double ]   =   ???  }",
            "title": "Advanced Composite Kernels"
        },
        {
            "location": "/core/core_kernel_stat/",
            "text": "Stationary kernels can be expressed as a function of the difference between their inputs.\n\n\n\n\n\n    C(\\mathbf{x}, \\mathbf{y}) = K(||\\mathbf{x} - \\mathbf{y}||_{p})\n\n\n\n\n    C(\\mathbf{x}, \\mathbf{y}) = K(||\\mathbf{x} - \\mathbf{y}||_{p})\n\n\n\n\n\nNote that any norm may be used to quantify the distance between the two vectors \n\\mathbf{x} \\ \\& \\ \\mathbf{y}\n\\mathbf{x} \\ \\& \\ \\mathbf{y}\n. The values \np = 1\np = 1\n and \np = 2\np = 2\n represent the \nManhattan distance\n and \nEuclidean distance\n respectively.\n\n\n\n\nInstantiating Stationary Kernels\nStationary kernels are implemented as a subset of the \nStationaryKernel[T, V, M]\n class which requires a \nField[T]\n implicit object (an algebraic field which has definitions for addition, subtraction, multiplication and division of its elements much like the number system). You may also import \nspire.implicits._\n in order to load the default field implementations for basic data types like \nInt\n, \nDouble\n and so on. Before instantiating any child class of \nStationaryKernel\n one needs to enter the following code.\n\n\n1\n2\n3\n4\n5\n6\n    \nimport\n \nspire.algebra.Field\n\n    \nimport\n \nio.github.mandar2812.dynaml.analysis.VectorField\n\n    \n//Calculate the number of input features\n\n    \n//and create a vector field of that dimension\n\n    \nval\n \nnum_features\n:\n \nInt\n \n=\n \n...\n\n    \nimplicit\n \nval\n \nf\n \n=\n \nVectorField\n(\nnum_features\n)\n\n\n\n\n\n\n\n\n\n\n\nRadial Basis Function Kernel\n\u00b6\n\n\n\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2\\sigma^2}\\right)\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2\\sigma^2}\\right)\n\n\n\n\n\nThe RBF kernel is the most popular kernel function applied in machine learning, it represents an inner product space which is spanned by the \nHermite\n polynomials and as such is suitable to model smooth functions. The RBF kernel is also called a \nuniversal\n kernel for the reason that any smooth function can be represented with a high degree of accuracy assuming we can find a suitable value of the bandwidth.\n\n\n1\nval\n \nrbf\n \n=\n \nnew\n \nRBFKernel\n(\n4.0\n)\n\n\n\n\n\n\n\nSquared Exponential Kernel\n\u00b6\n\n\nA generalization of the RBF Kernel is the Squared Exponential Kernel\n\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2l^2}\\right)\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2l^2}\\right)\n\n\n\n\n\n1\nval\n \nrbf\n \n=\n \nnew\n \nSEKernel\n(\n4.0\n,\n \n2.0\n)\n\n\n\n\n\n\n\nMahalanobis Kernel\n\u00b6\n\n\nThis kernel is a further generalization of the SE kernel. It uses the \nMahalanobis distance\n instead of the Euclidean distance between the inputs.\n\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\\right)\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\\right)\n\n\n\n\n\nThe Mahalanobis distance \n(\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\n(\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\n is characterized by a symmetric positive definite matrix \n\\Sigma\n\\Sigma\n. This distance metric reduces to the Euclidean distance if \n\\Sigma\n\\Sigma\n is the \nidentity matrix\n. Further, if \n\\Sigma\n\\Sigma\n is diagonal, the \nMahalanobis kernel\n becomes the \nAutomatic Relevance Determination\n version of the SE kernel (SE-ARD).\n\n\nIn DynaML the class \nMahalanobisKernel\n implements the SE-ARD kernel with diagonal \n\\Sigma\n\\Sigma\n.\n\n\n1\n2\n3\n4\nval\n \nbandwidths\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n_\n\n\nval\n \namp\n \n=\n \n1.5\n\n\n\nval\n \nmaha_kernel\n \n=\n \nnew\n \nMahalanobisKernel\n(\nbandwidths\n,\n \namp\n)\n\n\n\n\n\n\n\nStudent T Kernel\n\u00b6\n\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + ||\\mathbf{x}-\\mathbf{y}||^d}\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + ||\\mathbf{x}-\\mathbf{y}||^d}\n\n\n\n\n\n1\nval\n \ntstud\n \n=\n \nnew\n \nTStudentKernel\n(\n2.0\n)\n\n\n\n\n\n\n\nRational Quadratic Kernel\n\u00b6\n\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = \\left( 1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2 \\mu \\ell^2} \\right)^{-\\frac{1}{2}  (dim(\\mathbf{x})+\\mu)}\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = \\left( 1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2 \\mu \\ell^2} \\right)^{-\\frac{1}{2}  (dim(\\mathbf{x})+\\mu)}\n\n\n\n\n\n1\nval\n \nrat\n \n=\n \nnew\n \nRationalQuadraticKernel\n(\nshape\n \n=\n \n1.5\n,\n \nl\n \n=\n \n1.5\n)\n\n\n\n\n\n\n\nCauchy Kernel\n\u00b6\n\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{\\sigma^2}}\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{\\sigma^2}}\n\n\n\n\n\n1\nval\n \ncau\n \n=\n \nnew\n \nCauchyKernel\n(\n2.5\n)\n\n\n\n\n\n\n\nGaussian Spectral Kernel\n\u00b6\n\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = cos(2\\pi \\mu ||\\mathbf{x}-\\mathbf{y}||) \\ exp(-2\\pi^{2} \\sigma^{2} ||\\mathbf{x}-\\mathbf{y}||^{2} )\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = cos(2\\pi \\mu ||\\mathbf{x}-\\mathbf{y}||) \\ exp(-2\\pi^{2} \\sigma^{2} ||\\mathbf{x}-\\mathbf{y}||^{2} )\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n//Define how the hyper-parameter Map gets transformed to the kernel parameters\n\n\nval\n \nencoder\n \n=\n \nEncoder\n(\n\n  \n(\nconf\n:\n \nMap\n[\nString\n, \nDouble\n])\n \n=>\n \n(\nconf\n(\n\"c\"\n),\n \nconf\n(\n\"s\"\n)),\n\n  \n(\ncs\n:\n \n(\nDouble\n,\n \nDouble\n))\n \n=>\n \nMap\n(\n\"c\"\n \n->\n \ncs\n.\n_1\n,\n \n\"s\"\n \n->\n \ncs\n.\n_2\n))\n\n\n\nval\n \ngsmKernel\n \n=\n \nGaussianSpectralKernel\n[\nDouble\n](\n3.5\n,\n \n2.0\n,\n \nencoder\n)\n\n\n\n\n\n\n\nMatern Half Integer\n\u00b6\n\n\nThe Matern kernel is an important family of covariance functions. Matern covariances are parameterized via two quantities i.e. order \n\\nu\n\\nu\n and \n\\rho\n\\rho\n the characteristic length scale. The general matern covariance is defined in terms of modified \nBessel\n functions.\n\n\n\n\n\nC_{\\nu}(\\mathbf{x},\\mathbf{y}) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)^{\\nu} K_{\\nu}\\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)\n\n\n\n\nC_{\\nu}(\\mathbf{x},\\mathbf{y}) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)^{\\nu} K_{\\nu}\\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)\n\n\n\n\n\nWhere \nd = ||\\mathbf{x} - \\mathbf{y}||\nd = ||\\mathbf{x} - \\mathbf{y}||\n is the Euclidean (\nL_2\nL_2\n) distance between points.\n\n\nFor the case \n\\nu = p + \\frac{1}{2}, p \\in \\mathbb{N}\n\\nu = p + \\frac{1}{2}, p \\in \\mathbb{N}\n the expression becomes.\n\n\n\n\n\nC_{\\nu}(\\mathbf{x},\\mathbf{y}) =  exp\\left(-\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) \\frac{\\Gamma(p+1)}{\\Gamma(2p+1)} \\sum_{i = 0}^{p}{\\frac{(p+1)!}{i!(p-i)!}\\left(\\sqrt{8\\nu}\\frac{d}{\\rho}\\right)^{p-i}}\n\n\n\n\nC_{\\nu}(\\mathbf{x},\\mathbf{y}) =  exp\\left(-\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) \\frac{\\Gamma(p+1)}{\\Gamma(2p+1)} \\sum_{i = 0}^{p}{\\frac{(p+1)!}{i!(p-i)!}\\left(\\sqrt{8\\nu}\\frac{d}{\\rho}\\right)^{p-i}}\n\n\n\n\n\nCurrently there is only support for matern half integer kernels.\n\n\n1\n2\nimplicit\n \nev\n \n=\n \nVectorField\n(\n2\n)\n\n\nval\n \nmatKern\n \n=\n \nnew\n \nGenericMaternKernel\n(\n1.5\n,\n \np\n \n=\n \n1\n)\n\n\n\n\n\n\n\nWavelet Kernel\n\u00b6\n\n\nThe Wavelet kernel (\nZhang et al, 2004\n) comes from Wavelet theory and is given as\n\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = \\prod_{i = 1}^{d} h\\left(\\frac{x_i-y_i}{a}\\right)\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = \\prod_{i = 1}^{d} h\\left(\\frac{x_i-y_i}{a}\\right)\n\n\n\n\n\nWhere the function \nh\n is known as the mother wavelet function, Zhang et. al suggest the following expression for the mother wavelet function.\n\n\n\n\n\n    h(x) = cos(1.75x)exp(-\\frac{1}{2}x^2)\n\n\n\n\n    h(x) = cos(1.75x)exp(-\\frac{1}{2}x^2)\n\n\n\n\n\n1\nval\n \nwv\n \n=\n \nnew\n \nWaveletKernel\n(\nx\n \n=>\n \nmath\n.\ncos\n(\n1.75\n*\nx\n)*\nmath\n.\nexp\n(-\n1.0\n*\nx\n*\nx\n/\n2.0\n))(\n1.5\n)\n\n\n\n\n\n\n\nPeriodic Kernel\n\u00b6\n\n\nThe periodic kernel has \nFourier\n series as its orthogonal eigenfunctions. It is used when constructing predictive models over quantities which are known to have some periodic behavior.\n\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = exp\\left(-2 \\ sin^{2}\\left(\\frac{\\pi \\omega ||\\mathbf{x}-\\mathbf{y}||}{l^2}\\right)\\right)\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = exp\\left(-2 \\ sin^{2}\\left(\\frac{\\pi \\omega ||\\mathbf{x}-\\mathbf{y}||}{l^2}\\right)\\right)\n\n\n\n\n\n1\nval\n \nperiodic_kernel\n \n=\n \nnew\n \nPeriodicKernel\n(\nlengthscale\n \n=\n \n1.5\n,\n \nfreq\n \n=\n \n2.5\n)\n\n\n\n\n\n\n\nWave Kernel\n\u00b6\n\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = \\frac{\\theta}{||\\mathbf{x} - \\mathbf{y}||^2} \\times sin(\\frac{||\\mathbf{x} - \\mathbf{y}||^2}{\\theta})\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = \\frac{\\theta}{||\\mathbf{x} - \\mathbf{y}||^2} \\times sin(\\frac{||\\mathbf{x} - \\mathbf{y}||^2}{\\theta})\n\n\n\n\n\n1\nval\n \nwv_kernel\n \n=\n \nWaveKernel\n(\nth\n \n=\n \n1.0\n)\n\n\n\n\n\n\n\nLaplacian Kernel\n\u00b6\n\n\nThe Laplacian kernel is the covariance function of the well known \nOrnstein Ulhenbeck process\n, samples drawn from this process are continuous and only once differentiable.\n\n\n\n\n\n\\begin{equation}\nC(\\mathbf{x},\\mathbf{y}) = exp \\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||_{1}}{2\\beta}\\right)\n\\end{equation}\n\n\n\n\n\\begin{equation}\nC(\\mathbf{x},\\mathbf{y}) = exp \\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||_{1}}{2\\beta}\\right)\n\\end{equation}\n\n\n\n\n\n1\nval\n \nlap\n \n=\n \nnew\n \nLaplacianKernel\n(\n4.0\n)",
            "title": "Stationary Kernels"
        },
        {
            "location": "/core/core_kernel_stat/#radial-basis-function-kernel",
            "text": "C(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2\\sigma^2}\\right)  \nC(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2\\sigma^2}\\right)   The RBF kernel is the most popular kernel function applied in machine learning, it represents an inner product space which is spanned by the  Hermite  polynomials and as such is suitable to model smooth functions. The RBF kernel is also called a  universal  kernel for the reason that any smooth function can be represented with a high degree of accuracy assuming we can find a suitable value of the bandwidth.  1 val   rbf   =   new   RBFKernel ( 4.0 )",
            "title": "Radial Basis Function Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#squared-exponential-kernel",
            "text": "A generalization of the RBF Kernel is the Squared Exponential Kernel   \n    C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2l^2}\\right)  \n    C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2l^2}\\right)   1 val   rbf   =   new   SEKernel ( 4.0 ,   2.0 )",
            "title": "Squared Exponential Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#mahalanobis-kernel",
            "text": "This kernel is a further generalization of the SE kernel. It uses the  Mahalanobis distance  instead of the Euclidean distance between the inputs.   \n    C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\\right)  \n    C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\\right)   The Mahalanobis distance  (\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y}) (\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})  is characterized by a symmetric positive definite matrix  \\Sigma \\Sigma . This distance metric reduces to the Euclidean distance if  \\Sigma \\Sigma  is the  identity matrix . Further, if  \\Sigma \\Sigma  is diagonal, the  Mahalanobis kernel  becomes the  Automatic Relevance Determination  version of the SE kernel (SE-ARD).  In DynaML the class  MahalanobisKernel  implements the SE-ARD kernel with diagonal  \\Sigma \\Sigma .  1\n2\n3\n4 val   bandwidths :   DenseVector [ Double ]   =   _  val   amp   =   1.5  val   maha_kernel   =   new   MahalanobisKernel ( bandwidths ,   amp )",
            "title": "Mahalanobis Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#student-t-kernel",
            "text": "C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + ||\\mathbf{x}-\\mathbf{y}||^d}  \n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + ||\\mathbf{x}-\\mathbf{y}||^d}   1 val   tstud   =   new   TStudentKernel ( 2.0 )",
            "title": "Student T Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#rational-quadratic-kernel",
            "text": "C(\\mathbf{x},\\mathbf{y}) = \\left( 1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2 \\mu \\ell^2} \\right)^{-\\frac{1}{2}  (dim(\\mathbf{x})+\\mu)}  \n    C(\\mathbf{x},\\mathbf{y}) = \\left( 1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2 \\mu \\ell^2} \\right)^{-\\frac{1}{2}  (dim(\\mathbf{x})+\\mu)}   1 val   rat   =   new   RationalQuadraticKernel ( shape   =   1.5 ,   l   =   1.5 )",
            "title": "Rational Quadratic Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#cauchy-kernel",
            "text": "C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{\\sigma^2}}  \n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{\\sigma^2}}   1 val   cau   =   new   CauchyKernel ( 2.5 )",
            "title": "Cauchy Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#gaussian-spectral-kernel",
            "text": "C(\\mathbf{x},\\mathbf{y}) = cos(2\\pi \\mu ||\\mathbf{x}-\\mathbf{y}||) \\ exp(-2\\pi^{2} \\sigma^{2} ||\\mathbf{x}-\\mathbf{y}||^{2} )  \nC(\\mathbf{x},\\mathbf{y}) = cos(2\\pi \\mu ||\\mathbf{x}-\\mathbf{y}||) \\ exp(-2\\pi^{2} \\sigma^{2} ||\\mathbf{x}-\\mathbf{y}||^{2} )   1\n2\n3\n4\n5\n6 //Define how the hyper-parameter Map gets transformed to the kernel parameters  val   encoder   =   Encoder ( \n   ( conf :   Map [ String ,  Double ])   =>   ( conf ( \"c\" ),   conf ( \"s\" )), \n   ( cs :   ( Double ,   Double ))   =>   Map ( \"c\"   ->   cs . _1 ,   \"s\"   ->   cs . _2 ))  val   gsmKernel   =   GaussianSpectralKernel [ Double ]( 3.5 ,   2.0 ,   encoder )",
            "title": "Gaussian Spectral Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#matern-half-integer",
            "text": "The Matern kernel is an important family of covariance functions. Matern covariances are parameterized via two quantities i.e. order  \\nu \\nu  and  \\rho \\rho  the characteristic length scale. The general matern covariance is defined in terms of modified  Bessel  functions.   \nC_{\\nu}(\\mathbf{x},\\mathbf{y}) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)^{\\nu} K_{\\nu}\\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)  \nC_{\\nu}(\\mathbf{x},\\mathbf{y}) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)^{\\nu} K_{\\nu}\\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)   Where  d = ||\\mathbf{x} - \\mathbf{y}|| d = ||\\mathbf{x} - \\mathbf{y}||  is the Euclidean ( L_2 L_2 ) distance between points.  For the case  \\nu = p + \\frac{1}{2}, p \\in \\mathbb{N} \\nu = p + \\frac{1}{2}, p \\in \\mathbb{N}  the expression becomes.   \nC_{\\nu}(\\mathbf{x},\\mathbf{y}) =  exp\\left(-\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) \\frac{\\Gamma(p+1)}{\\Gamma(2p+1)} \\sum_{i = 0}^{p}{\\frac{(p+1)!}{i!(p-i)!}\\left(\\sqrt{8\\nu}\\frac{d}{\\rho}\\right)^{p-i}}  \nC_{\\nu}(\\mathbf{x},\\mathbf{y}) =  exp\\left(-\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) \\frac{\\Gamma(p+1)}{\\Gamma(2p+1)} \\sum_{i = 0}^{p}{\\frac{(p+1)!}{i!(p-i)!}\\left(\\sqrt{8\\nu}\\frac{d}{\\rho}\\right)^{p-i}}   Currently there is only support for matern half integer kernels.  1\n2 implicit   ev   =   VectorField ( 2 )  val   matKern   =   new   GenericMaternKernel ( 1.5 ,   p   =   1 )",
            "title": "Matern Half Integer"
        },
        {
            "location": "/core/core_kernel_stat/#wavelet-kernel",
            "text": "The Wavelet kernel ( Zhang et al, 2004 ) comes from Wavelet theory and is given as   \n    C(\\mathbf{x},\\mathbf{y}) = \\prod_{i = 1}^{d} h\\left(\\frac{x_i-y_i}{a}\\right)  \n    C(\\mathbf{x},\\mathbf{y}) = \\prod_{i = 1}^{d} h\\left(\\frac{x_i-y_i}{a}\\right)   Where the function  h  is known as the mother wavelet function, Zhang et. al suggest the following expression for the mother wavelet function.   \n    h(x) = cos(1.75x)exp(-\\frac{1}{2}x^2)  \n    h(x) = cos(1.75x)exp(-\\frac{1}{2}x^2)   1 val   wv   =   new   WaveletKernel ( x   =>   math . cos ( 1.75 * x )* math . exp (- 1.0 * x * x / 2.0 ))( 1.5 )",
            "title": "Wavelet Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#periodic-kernel",
            "text": "The periodic kernel has  Fourier  series as its orthogonal eigenfunctions. It is used when constructing predictive models over quantities which are known to have some periodic behavior.   \nC(\\mathbf{x},\\mathbf{y}) = exp\\left(-2 \\ sin^{2}\\left(\\frac{\\pi \\omega ||\\mathbf{x}-\\mathbf{y}||}{l^2}\\right)\\right)  \nC(\\mathbf{x},\\mathbf{y}) = exp\\left(-2 \\ sin^{2}\\left(\\frac{\\pi \\omega ||\\mathbf{x}-\\mathbf{y}||}{l^2}\\right)\\right)   1 val   periodic_kernel   =   new   PeriodicKernel ( lengthscale   =   1.5 ,   freq   =   2.5 )",
            "title": "Periodic Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#wave-kernel",
            "text": "C(\\mathbf{x},\\mathbf{y}) = \\frac{\\theta}{||\\mathbf{x} - \\mathbf{y}||^2} \\times sin(\\frac{||\\mathbf{x} - \\mathbf{y}||^2}{\\theta})  \nC(\\mathbf{x},\\mathbf{y}) = \\frac{\\theta}{||\\mathbf{x} - \\mathbf{y}||^2} \\times sin(\\frac{||\\mathbf{x} - \\mathbf{y}||^2}{\\theta})   1 val   wv_kernel   =   WaveKernel ( th   =   1.0 )",
            "title": "Wave Kernel"
        },
        {
            "location": "/core/core_kernel_stat/#laplacian-kernel",
            "text": "The Laplacian kernel is the covariance function of the well known  Ornstein Ulhenbeck process , samples drawn from this process are continuous and only once differentiable.   \n\\begin{equation}\nC(\\mathbf{x},\\mathbf{y}) = exp \\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||_{1}}{2\\beta}\\right)\n\\end{equation}  \n\\begin{equation}\nC(\\mathbf{x},\\mathbf{y}) = exp \\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||_{1}}{2\\beta}\\right)\n\\end{equation}   1 val   lap   =   new   LaplacianKernel ( 4.0 )",
            "title": "Laplacian Kernel"
        },
        {
            "location": "/core/core_kernel_nonstat/",
            "text": "Non-stationary covariance functions cannot be expressed as simply a function of the distance between their inputs \n\\mathbf{x} - \\mathbf{y}\n\\mathbf{x} - \\mathbf{y}\n.\n\n\nLocally Stationary Kernels\n\u00b6\n\n\nA simple way to construct non-stationary covariances from stationary ones is by scaling the original stationary covariance; \nK(\\mathbf{x} - \\mathbf{y})\nK(\\mathbf{x} - \\mathbf{y})\n, by a function of \n\\mathbf{x} + \\mathbf{y}\n\\mathbf{x} + \\mathbf{y}\n.\n\n\n\n\n\nC(\\mathbf{x}, \\mathbf{y}) = G(\\mathbf{x} + \\mathbf{y}) K(\\mathbf{x} - \\mathbf{y})\n\n\n\n\nC(\\mathbf{x}, \\mathbf{y}) = G(\\mathbf{x} + \\mathbf{y}) K(\\mathbf{x} - \\mathbf{y})\n\n\n\n\n\nHere \nG(.): \\mathcal{X} \\rightarrow \\mathbb{R}\nG(.): \\mathcal{X} \\rightarrow \\mathbb{R}\n is a non-negative function of its inputs. These kernels are called \nlocally stationary kernels\n. For an in-depth review of locally stationary kernels refer to \nGenton et. al\n.\n\n\n1\n2\n3\n4\n5\n6\n7\n//Instantiate the base kernel\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nI\n]\n \n=\n \n_\n\n\n\nval\n \nscalingFunction\n:\n \n(\nI\n)\n \n=>\n \nDouble\n \n=\n \n_\n\n\n\nval\n \nscKernel\n \n=\n \nnew\n \nLocallyStationaryKernel\n(\n\n    \nkernel\n,\n \nDataPipe\n(\nscalingFunction\n))\n\n\n\n\n\n\n\nPolynomial Kernel\n\u00b6\n\n\nA very popular non-stationary kernel used in machine learning, the polynomial represents the data features as polynomial expansions up to an index \nd\nd\n.\n\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^\\intercal \\mathbf{y} + a)^{d}\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^\\intercal \\mathbf{y} + a)^{d}\n\n\n\n\n\n1\nval\n \nfbm\n \n=\n \nnew\n \nPolynomialKernel\n(\n2\n,\n \n0.99\n)\n\n\n\n\n\n\n\nFractional Brownian Field (FBM) Kernel\n\u00b6\n\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{2}\\left(||\\mathbf{x}||_{2}^{2H} + ||\\mathbf{y}||_{2}^{2H} - ||\\mathbf{x}-\\mathbf{y}||_{2}^{2H}\\right)\n\n\n\n\n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{2}\\left(||\\mathbf{x}||_{2}^{2H} + ||\\mathbf{y}||_{2}^{2H} - ||\\mathbf{x}-\\mathbf{y}||_{2}^{2H}\\right)\n\n\n\n\n\n1\nval\n \nfbm\n \n=\n \nnew\n \nFBMKernel\n(\n0.99\n)\n\n\n\n\n\n\n\nThe FBM kernel is the generalization of fractional Brownian motion to multi-variate index sets. Fractional Brownian motion is a stochastic process which is the generalization of Brownian motion, it was first studied by \nMandelbrot and Von Ness\n. It is a \nself similar\n stochastic process, with stationary increments. However the process itself is non-stationary (as can be seen from the expression for the kernel) and has long range non vanishing covariance.\n\n\nMaximum Likelihood Perceptron Kernel\n\u00b6\n\n\nThe \nmaximum likelihood perceptron\n (MLP) kernel, was first arrived at in Radford Neal's \nthesis\n, by considering the limiting case of a bayesian feed forward neural network with sigmoid activation.\n\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = sin^{-1} \\left (\\frac{w \\mathbf{x}^\\intercal \\mathbf{y} + b}{(w \\mathbf{x}^\\intercal \\mathbf{x} + b) (w \\mathbf{y}^\\intercal \\mathbf{y} + b)} \\right )\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = sin^{-1} \\left (\\frac{w \\mathbf{x}^\\intercal \\mathbf{y} + b}{(w \\mathbf{x}^\\intercal \\mathbf{x} + b) (w \\mathbf{y}^\\intercal \\mathbf{y} + b)} \\right )\n\n\n\n\n\nNeural Network Kernel\n\u00b6\n\n\nAlso a result of limiting case of bayesian neural networks, albeit with \nerf(.)\nerf(.)\n as the transfer function.\n\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = \\frac{2}{\\pi} sin \\left (\\frac{2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{y}}{(2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{x} + 1) (2 \\mathbf{y}^\\intercal \\Sigma \\mathbf{y} + 1)} \\right )\n\n\n\n\nC(\\mathbf{x},\\mathbf{y}) = \\frac{2}{\\pi} sin \\left (\\frac{2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{y}}{(2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{x} + 1) (2 \\mathbf{y}^\\intercal \\Sigma \\mathbf{y} + 1)} \\right )",
            "title": "Non Stationary Kernels"
        },
        {
            "location": "/core/core_kernel_nonstat/#locally-stationary-kernels",
            "text": "A simple way to construct non-stationary covariances from stationary ones is by scaling the original stationary covariance;  K(\\mathbf{x} - \\mathbf{y}) K(\\mathbf{x} - \\mathbf{y}) , by a function of  \\mathbf{x} + \\mathbf{y} \\mathbf{x} + \\mathbf{y} .   \nC(\\mathbf{x}, \\mathbf{y}) = G(\\mathbf{x} + \\mathbf{y}) K(\\mathbf{x} - \\mathbf{y})  \nC(\\mathbf{x}, \\mathbf{y}) = G(\\mathbf{x} + \\mathbf{y}) K(\\mathbf{x} - \\mathbf{y})   Here  G(.): \\mathcal{X} \\rightarrow \\mathbb{R} G(.): \\mathcal{X} \\rightarrow \\mathbb{R}  is a non-negative function of its inputs. These kernels are called  locally stationary kernels . For an in-depth review of locally stationary kernels refer to  Genton et. al .  1\n2\n3\n4\n5\n6\n7 //Instantiate the base kernel  val   kernel :   LocalScalarKernel [ I ]   =   _  val   scalingFunction :   ( I )   =>   Double   =   _  val   scKernel   =   new   LocallyStationaryKernel ( \n     kernel ,   DataPipe ( scalingFunction ))",
            "title": "Locally Stationary Kernels"
        },
        {
            "location": "/core/core_kernel_nonstat/#polynomial-kernel",
            "text": "A very popular non-stationary kernel used in machine learning, the polynomial represents the data features as polynomial expansions up to an index  d d .   \nC(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^\\intercal \\mathbf{y} + a)^{d}  \nC(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^\\intercal \\mathbf{y} + a)^{d}   1 val   fbm   =   new   PolynomialKernel ( 2 ,   0.99 )",
            "title": "Polynomial Kernel"
        },
        {
            "location": "/core/core_kernel_nonstat/#fractional-brownian-field-fbm-kernel",
            "text": "C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{2}\\left(||\\mathbf{x}||_{2}^{2H} + ||\\mathbf{y}||_{2}^{2H} - ||\\mathbf{x}-\\mathbf{y}||_{2}^{2H}\\right)  \n    C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{2}\\left(||\\mathbf{x}||_{2}^{2H} + ||\\mathbf{y}||_{2}^{2H} - ||\\mathbf{x}-\\mathbf{y}||_{2}^{2H}\\right)   1 val   fbm   =   new   FBMKernel ( 0.99 )    The FBM kernel is the generalization of fractional Brownian motion to multi-variate index sets. Fractional Brownian motion is a stochastic process which is the generalization of Brownian motion, it was first studied by  Mandelbrot and Von Ness . It is a  self similar  stochastic process, with stationary increments. However the process itself is non-stationary (as can be seen from the expression for the kernel) and has long range non vanishing covariance.",
            "title": "Fractional Brownian Field (FBM) Kernel"
        },
        {
            "location": "/core/core_kernel_nonstat/#maximum-likelihood-perceptron-kernel",
            "text": "The  maximum likelihood perceptron  (MLP) kernel, was first arrived at in Radford Neal's  thesis , by considering the limiting case of a bayesian feed forward neural network with sigmoid activation.   \nC(\\mathbf{x},\\mathbf{y}) = sin^{-1} \\left (\\frac{w \\mathbf{x}^\\intercal \\mathbf{y} + b}{(w \\mathbf{x}^\\intercal \\mathbf{x} + b) (w \\mathbf{y}^\\intercal \\mathbf{y} + b)} \\right )  \nC(\\mathbf{x},\\mathbf{y}) = sin^{-1} \\left (\\frac{w \\mathbf{x}^\\intercal \\mathbf{y} + b}{(w \\mathbf{x}^\\intercal \\mathbf{x} + b) (w \\mathbf{y}^\\intercal \\mathbf{y} + b)} \\right )",
            "title": "Maximum Likelihood Perceptron Kernel"
        },
        {
            "location": "/core/core_kernel_nonstat/#neural-network-kernel",
            "text": "Also a result of limiting case of bayesian neural networks, albeit with  erf(.) erf(.)  as the transfer function.   \nC(\\mathbf{x},\\mathbf{y}) = \\frac{2}{\\pi} sin \\left (\\frac{2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{y}}{(2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{x} + 1) (2 \\mathbf{y}^\\intercal \\Sigma \\mathbf{y} + 1)} \\right )  \nC(\\mathbf{x},\\mathbf{y}) = \\frac{2}{\\pi} sin \\left (\\frac{2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{y}}{(2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{x} + 1) (2 \\mathbf{y}^\\intercal \\Sigma \\mathbf{y} + 1)} \\right )",
            "title": "Neural Network Kernel"
        },
        {
            "location": "/core/core_kernel_nystrom/",
            "text": "Summary\n\n\n\"\nAutomatic Feature Extraction\n (AFE) is a technique by which approximate eigen-functions can be extracted from gram matrices constructed via kernel functions. These eigen-functions are \nfeatures\n engineered by a particular kernel.\"\"\n\n\n\n\nSome Mathematical Background\n\u00b6\n\n\nDefinitions\n\u00b6\n\n\nLet \nX_k \\ \\in \\ \\mathbb{R}^d \\ , \\ k = 1, \\cdots ,n\nX_k \\ \\in \\ \\mathbb{R}^d \\ , \\ k = 1, \\cdots ,n\n be a random sample drawn from a distribution \nF(x)\nF(x)\n. Let \nC \\in \\mathbb{R}^d\nC \\in \\mathbb{R}^d\n be a compact set such that, \n\\mathcal{H} = \\mathcal{L}^2(C)\n\\mathcal{H} = \\mathcal{L}^2(C)\n be a Hilbert space of functions given by the inner product below.\n\n\n\n\n\n\\begin{equation}\n<f,g>_{\\mathcal{H}} = \\int f(x)g(x) dF(x)\n\\end{equation}\n\n\n\n\n\\begin{equation}\n<f,g>_{\\mathcal{H}} = \\int f(x)g(x) dF(x)\n\\end{equation}\n\n\n\n\n\nFurther let \nM(\\mathcal{H}, \\mathcal{H})\nM(\\mathcal{H}, \\mathcal{H})\n be a class of linear operators from \n\\mathcal{H}\n\\mathcal{H}\n to \n\\mathcal{H}\n\\mathcal{H}\n.  \n\n\nNystr\u00f6m method\n\u00b6\n\n\nAutomatic Feature Extraction (AFE) using the \nNystr\u00f6m method\n aims at finding a finite dimensional approximation to the kernel eigenfunction expansion of Mercer kernels, as shown below.\n\n\n\n\n\n\\begin{equation}\nK(x,t) = \\sum_i{\\lambda_i \\phi(x)\\phi(t)}\n\\end{equation}\n\n\n\n\n\\begin{equation}\nK(x,t) = \\sum_i{\\lambda_i \\phi(x)\\phi(t)}\n\\end{equation}\n\n\n\n\n\nIt is well known that Mercer kernels form a \nReproducing Kernel Hilbert Space\n (\nRHKS\n) of functions. Every Mercer kernel defines a unique \nRHKS\n of functions as shown by the Moore-Aronszajn theorem. For a more involved treatment of \nRHKS\n and their applications the reader may refer to the book written by Bertinet et.al.\n\n\nMercer's theorem states that the spectral decomposition of integral operator of \nK\nK\n, \n\\mathcal{T} \\in M(\\mathcal{H},\\mathcal{H})\n\\mathcal{T} \\in M(\\mathcal{H},\\mathcal{H})\n defined below yields the eigenfunctions which span the RHKS generated by \nK\nK\n and having an inner product defined as above.\n\n\n\n\n\n\\begin{equation}\n(\\mathcal{T}\\phi_i)(t) = \\int K(x,t) \\phi(x) dF(x)\n\\end{equation}\n\n\n\n\n\\begin{equation}\n(\\mathcal{T}\\phi_i)(t) = \\int K(x,t) \\phi(x) dF(x)\n\\end{equation}\n\n\n\n\n\nEquation above is more commonly also known as the \nFredholm integral equation\n of the first kind. Nystr\u00f6m's method method approximates this integral using the quadrature constructed by considering a finite kernel matrix constructed out of a prototype set \nX_k \\ k = 1, \\cdots, m\nX_k \\ k = 1, \\cdots, m\n and calculating its spectral decomposition consisting of eigenvalues \n\\lambda_k\n\\lambda_k\n and eigen-vectors \nu_k\nu_k\n. This yields an expression for the approximate non-linear feature map \n\\hat{\\phi} : \\mathbb{R}^d \\longrightarrow \\mathbb{R}^m\n\\hat{\\phi} : \\mathbb{R}^d \\longrightarrow \\mathbb{R}^m\n.\n\n\n\n\n\n\\begin{equation}\n\\hat{\\phi}_{i}(t) = \\frac{\\sqrt{m}}{\\lambda_i}\\sum_{k=1}^{m}K(X_k, t)u_{k,i}\n\\end{equation}\n\n\n\n\n\\begin{equation}\n\\hat{\\phi}_{i}(t) = \\frac{\\sqrt{m}}{\\lambda_i}\\sum_{k=1}^{m}K(X_k, t)u_{k,i}\n\\end{equation}\n\n\n\n\n\nAFE in DynaML Kernels\n\u00b6\n\n\nThe \nSVMKernel[M]\n contains an implementation of AFE in the method\n\n\n1\n2\n3\n4\nfeatureMapping\n(\n\n  \ndecomposition\n:\n \n(\nDenseVector\n[\nDouble\n],\n \nDenseMatrix\n[\nDouble\n]))(\n\n    \nprototypes\n:\n \nList\n[\nDenseVector\n[\nDouble\n]])(\n\n      \ndata\n:\n \nDenseVector\n[\nDouble\n])\n:\n \nDenseVector\n[\nDouble\n]\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe \nSVMKernel\n class is extended by all the implemented library kernels in DynaML thereby enabling the use of AFE in potentially any model employing kernels.",
            "title": "Automatic Feature Extraction"
        },
        {
            "location": "/core/core_kernel_nystrom/#some-mathematical-background",
            "text": "",
            "title": "Some Mathematical Background"
        },
        {
            "location": "/core/core_kernel_nystrom/#definitions",
            "text": "Let  X_k \\ \\in \\ \\mathbb{R}^d \\ , \\ k = 1, \\cdots ,n X_k \\ \\in \\ \\mathbb{R}^d \\ , \\ k = 1, \\cdots ,n  be a random sample drawn from a distribution  F(x) F(x) . Let  C \\in \\mathbb{R}^d C \\in \\mathbb{R}^d  be a compact set such that,  \\mathcal{H} = \\mathcal{L}^2(C) \\mathcal{H} = \\mathcal{L}^2(C)  be a Hilbert space of functions given by the inner product below.   \n\\begin{equation}\n<f,g>_{\\mathcal{H}} = \\int f(x)g(x) dF(x)\n\\end{equation}  \n\\begin{equation}\n<f,g>_{\\mathcal{H}} = \\int f(x)g(x) dF(x)\n\\end{equation}   Further let  M(\\mathcal{H}, \\mathcal{H}) M(\\mathcal{H}, \\mathcal{H})  be a class of linear operators from  \\mathcal{H} \\mathcal{H}  to  \\mathcal{H} \\mathcal{H} .",
            "title": "Definitions"
        },
        {
            "location": "/core/core_kernel_nystrom/#nystrom-method",
            "text": "Automatic Feature Extraction (AFE) using the  Nystr\u00f6m method  aims at finding a finite dimensional approximation to the kernel eigenfunction expansion of Mercer kernels, as shown below.   \n\\begin{equation}\nK(x,t) = \\sum_i{\\lambda_i \\phi(x)\\phi(t)}\n\\end{equation}  \n\\begin{equation}\nK(x,t) = \\sum_i{\\lambda_i \\phi(x)\\phi(t)}\n\\end{equation}   It is well known that Mercer kernels form a  Reproducing Kernel Hilbert Space  ( RHKS ) of functions. Every Mercer kernel defines a unique  RHKS  of functions as shown by the Moore-Aronszajn theorem. For a more involved treatment of  RHKS  and their applications the reader may refer to the book written by Bertinet et.al.  Mercer's theorem states that the spectral decomposition of integral operator of  K K ,  \\mathcal{T} \\in M(\\mathcal{H},\\mathcal{H}) \\mathcal{T} \\in M(\\mathcal{H},\\mathcal{H})  defined below yields the eigenfunctions which span the RHKS generated by  K K  and having an inner product defined as above.   \n\\begin{equation}\n(\\mathcal{T}\\phi_i)(t) = \\int K(x,t) \\phi(x) dF(x)\n\\end{equation}  \n\\begin{equation}\n(\\mathcal{T}\\phi_i)(t) = \\int K(x,t) \\phi(x) dF(x)\n\\end{equation}   Equation above is more commonly also known as the  Fredholm integral equation  of the first kind. Nystr\u00f6m's method method approximates this integral using the quadrature constructed by considering a finite kernel matrix constructed out of a prototype set  X_k \\ k = 1, \\cdots, m X_k \\ k = 1, \\cdots, m  and calculating its spectral decomposition consisting of eigenvalues  \\lambda_k \\lambda_k  and eigen-vectors  u_k u_k . This yields an expression for the approximate non-linear feature map  \\hat{\\phi} : \\mathbb{R}^d \\longrightarrow \\mathbb{R}^m \\hat{\\phi} : \\mathbb{R}^d \\longrightarrow \\mathbb{R}^m .   \n\\begin{equation}\n\\hat{\\phi}_{i}(t) = \\frac{\\sqrt{m}}{\\lambda_i}\\sum_{k=1}^{m}K(X_k, t)u_{k,i}\n\\end{equation}  \n\\begin{equation}\n\\hat{\\phi}_{i}(t) = \\frac{\\sqrt{m}}{\\lambda_i}\\sum_{k=1}^{m}K(X_k, t)u_{k,i}\n\\end{equation}",
            "title": "Nystr\u00f6m method"
        },
        {
            "location": "/core/core_kernel_nystrom/#afe-in-dynaml-kernels",
            "text": "The  SVMKernel[M]  contains an implementation of AFE in the method  1\n2\n3\n4 featureMapping ( \n   decomposition :   ( DenseVector [ Double ],   DenseMatrix [ Double ]))( \n     prototypes :   List [ DenseVector [ Double ]])( \n       data :   DenseVector [ Double ]) :   DenseVector [ Double ]     Note  The  SVMKernel  class is extended by all the implemented library kernels in DynaML thereby enabling the use of AFE in potentially any model employing kernels.",
            "title": "AFE in DynaML Kernels"
        },
        {
            "location": "/core/partitioned_objects/",
            "text": "Partitioned Vectors & Dual Vectors\n\n\nThe \ndynaml\n.\nalgebra\n package contains a number of classes and utilities\nfor constructing blocked linear algebra objects.\n\n\n\n\nDynaML makes extensive use of the \nbreeze\n linear algebra library for matrix-vector\noperations. Breeze is an attractive option because it is easy to use and has the ability to use low level implementations\nsuch as \nLAPACK\n and \nBLAS\n for performance speed-up.\n\n\nIf we are working with linear algebra objects which are large in size as compared to the available JVM memory, it may be necessary\nto not construct the entire vector in an eager fashion.\n\n\nIn the Scala language, there are \nlazy\n data structures, which are not computed unless they are required in further\ncomputation. The \ndynaml.algebra\n package leverages lazy data structures to create blocked vectors and matrices.\nEach partition/block of a blocked object is a breeze vector or matrix.\n\n\nThe proceeding pages give the user a glimpse of how to use and manipulate objects of the \nalgebra\n package.",
            "title": "Blocked Algebraic Objects"
        },
        {
            "location": "/core/partitioned_vectors/",
            "text": "Summary\n\n\nHere we show how to use blocked vectors and blocked dual vectors.\n\n\n\n\nBlocked vectors and dual vectors in the \nalgebra\n package are wrappers around \nStream\n[(\nLong\n, \nDenseVector\n[\nDouble\n])]\n\neach partition consists of an ordered index and the partition content which is in the form of a breeze vector.\n\n\nThe relevant API endpoints are \nPartitionedVector\n and \nPartitionedDualVector\n. In order to access these\nobjects, you must do \nimport\n \nio.github.mandar2812.dynaml.algebra._\n (already loaded by default in the DynaML shell).\n\n\nCreation\n\u00b6\n\n\nBlock vectors can be created in a number of ways. \nPartitionedVector\n and \nPartitionedDualVector\n\nare column row vectors respectively and treated as transposes of each other.\n\n\nFrom Input Blocks\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n//Create the data blocks\n\n\nval\n \ndata_blocks\n:\n \nStream\n[(\nLong\n, \nDenseVector\n[\nDouble\n])]\n \n=\n\n  \n(\n0L\n \nuntil\n \n10L\n).\ntoStream\n.\nmap\n(\nindex\n \n=>\n \n(\nindex\n,\n \nDenseVector\n.\nones\n[\nDouble\n](\n500\n)))\n\n\n\n//Instantiate the partitioned vector\n\n\n\nval\n \npart_vector\n \n=\n \nPartitionedVector\n(\ndata_blocks\n)\n\n\n\n//Optionally you may also provide the total length\n\n\n//of the partitioned vector\n\n\n\nval\n \npart_vector\n \n=\n \nPartitionedVector\n(\ndata_blocks\n,\n \nnum_rows\n:\n \nLong\n \n=\n \n5000L\n)\n\n\n\n\n//Created Block Dual Vector\n\n\n\nval\n \npart_dvector\n \n=\n \nPartitionedDualVector\n(\ndata_blocks\n)\n\n\n\n//Optionally you may also provide the total length\n\n\n//of the partitioned dual vector\n\n\n\nval\n \npart_dvector\n \n=\n \nPartitionedDualVector\n(\ndata_blocks\n,\n \nnum_rows\n:\n \nLong\n \n=\n \n5000L\n)\n\n\n\n\n\n\n\nFrom Tabulating Functions\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nval\n \ntabFunc\n:\n \n(\nLong\n)\n \n=>\n \nDouble\n \n=\n\n  \n(\nindex\n:\n \nLong\n)\n \n=>\n \nmath\n.\nsin\n(\n2\nd\n*\nmath\n.\nPi\n*\nindex\n/\n5000\nd\n)\n\n\n\n//Instantiate the partitioned vector\n\n\nval\n \npart_vector\n \n=\n \nPartitionedVector\n(\n\n  \nlength\n \n=\n \n5000L\n,\n \nnumElementsPerBlock\n \n=\n \n500\n,\n\n  \ntabFunc\n)\n\n\n\n//Instantiate the partitioned dual vector\n\n\nval\n \npart_dvector\n \n=\n \nPartitionedDualVector\n(\n\n  \nlength\n \n=\n \n5000L\n,\n \nnumElementsPerBlock\n \n=\n \n500\n,\n  \ntabFunc\n)\n\n\n\n\n\n\n\nFrom a Stream\n\u00b6\n\n\n1\n2\n3\n4\n5\n//Create the data stream\n\n\nval\n \ndata\n:\n \nStream\n[\nDouble\n]\n \n=\n \nStream\n.\nfill\n[\nDouble\n](\n5000\n)(\n1.0\n)\n\n\n\n//Instantiate the partitioned vector\n\n\nval\n \npart_vector\n \n=\n \nPartitionedVector\n(\ndata\n,\n \nlength\n \n=\n \n5000L\n,\n \nnum_elements_per_block\n \n=\n \n500\n)\n\n\n\n\n\n\n\nFrom a Breeze Vector\n\u00b6\n\n\n1\n2\n3\n4\n5\n//Create the data blocks\n\n\nval\n \ndata_vector\n \n=\n \nDenseVector\n.\nones\n[\nDouble\n](\n5000\n)\n\n\n\n//Instantiate the partitioned vector\n\n\nval\n \npart_vector\n \n=\n \nPartitionedVector\n(\ndata_vector\n,\n \nnum_elements_per_block\n \n=\n \n500\n)\n\n\n\n\n\n\n\nApart from the above methods of creation there are a number of convenience functions available.\n\n\nVector with Filled Values\n\u00b6\n\n\nVector of zeros\n\u00b6\n\n\n1\nval\n \nones_vec\n \n=\n \nPartitionedVector\n.\nzeros\n(\n5000L\n,\n \n500\n)\n\n\n\n\n\n\n\nVector of Ones\n\u00b6\n\n\n1\nval\n \nones_vec\n \n=\n \nPartitionedVector\n.\nones\n(\n5000L\n,\n \n500\n)\n\n\n\n\n\n\n\nVector of Random Values\n\u00b6\n\n\n1\n2\n3\nval\n \nrandom_var\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \nrand_vec\n \n=\n \nPartitionedVector\n.\nrand\n(\n5000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\n\n\n\n\n\nVector Concatenation\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\nval\n \nrandom_var\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \nrand_vec1\n \n=\n \nPartitionedVector\n.\nrand\n(\n2000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\nval\n \nrand_vec2\n \n=\n \nPartitionedVector\n.\nrand\n(\n2000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\n\n//Vector of length 4000, having 8 blocks of 500 elements each\n\n\nval\n \nvec\n \n=\n \nPartitionedVector\n.\nvertcat\n(\nrand_vec1\n,\n \nrand_vec2\n)\n\n\n\n\n\n\n\n\n\nTip\n\n\nA \nPartitionedDualVector\n can be created via the transpose operation\non a \nPartitionedVector\n instance and vice versa.\n\n\n1\n2\n3\n4\n5\nval\n \nrandom_var\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \np_vec\n \n=\n \nPartitionedVector\n.\nrand\n(\n5000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\n\nval\n \np_dvec\n \n=\n \np_vec\n.\nt\n\n\n\n\n\n\n\n\n\nAlgebraic Operations\n\u00b6\n\n\nPartitioned vectors and dual vectors have a number of algebraic operations available in the API.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nval\n \nbeta_var\n \n=\n \nRandomVariable\n(\nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\nval\n \ngamma_var\n \n=\n \nRandomVariable\n(\nGamma\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \np_vec_beta\n \n=\n \nPartitionedVector\n.\nrand\n(\n5000L\n,\n \n500\n,\n \nbeta_var\n)\n\n\nval\n \np_vec_gamma\n \n=\n \nPartitionedVector\n.\nrand\n(\n5000L\n,\n \n500\n,\n \ngamma_var\n)\n\n\n\nval\n \ndvec_beta\n \n=\n \np_vec_beta\n.\nt\n\n\nval\n \ndvec_gamma\n \n=\n \np_vec_gamma\n.\nt\n\n\n\n//Addition\n\n\nval\n \nadd_vec\n \n=\n \np_vec_beta\n \n+\n \np_vec_gamma\n\n\nval\n \nadd_dvec\n \n=\n \ndvec_beta\n \n+\n \ndvec_gamma\n\n\n\n//Subtraction\n\n\nval\n \nsub_vec\n \n=\n \np_vec_beta\n \n-\n \np_vec_gamma\n\n\nval\n \nsub_dvec\n \n=\n \ndvec_beta\n \n-\n \ndvec_gamma\n\n\n\n//Element wise multiplication\n\n\nval\n \nmult_vec\n \n=\n \np_vec_beta\n \n:*\n \np_vec_gamma\n\n\n\n//Element wise division\n\n\nval\n \ndiv_vec\n \n=\n \np_vec_beta\n \n:/\n \np_vec_gamma\n\n\n\n//Inner Product\n\n\nval\n \nprod\n \n=\n \ndvec_gamma\n*\np_vec_beta\n\n\n\n//Scaler multiplication\n\n\nval\n \nsc_vec\n \n=\n \nadd_vec\n*\n1.5\n\n\nval\n \nsc_dvec\n \n=\n \nadd_dvec\n*\n2.5\n\n\n\n\n\n\n\nMisc. Operations\n\u00b6\n\n\nMap Partitions\n\u00b6\n\n\nMap each index, partition pair by a scala function.\n\n\n1\n2\n3\n4\n5\nval\n \nvec\n:\n \nPartitionedVector\n \n=\n \n_\n\n\n\nval\n \nother_vec\n \n=\n \nvec\n.\nmap\n(\n\n   \n(\npair\n:\n \n(\nLong\n,\n \nDenseVector\n[\nDouble\n]))\n \n=>\n \n(\npair\n.\n_1\n,\n \npair\n.\n_2\n*\n1.5\n)\n\n\n)\n\n\n\n\n\n\n\nSlice\n\u00b6\n\n\nObtain subset of elements, the new vector is repartitioned and re-indexed accordingly.\n\n\n1\n2\n3\nval\n \nvec\n:\n \nPartitionedVector\n \n=\n \nPartitionedVector\n.\nones\n(\n5000L\n,\n \n500\n)\n\n\n\nval\n \nother_vec\n \n=\n \nvec\n(\n999L\n \nuntil\n \n2000L\n)\n\n\n\n\n\n\n\nReverse\n\u00b6\n\n\nReverse a block vector\n\n\n1\n2\n3\nval\n \nvec\n:\n \nPartitionedVector\n \n=\n \nPartitionedVector\n.\nones\n(\n5000L\n,\n \n500\n)\n\n\n\nval\n \nreverse_vec\n \n=\n \nvec\n.\nreverse\n\n\n\n\n\n\n\nConvert to Breeze Vector\n\u00b6\n\n\n1\n2\n3\n4\n5\nval\n \nvec\n:\n \nPartitionedVector\n \n=\n \nPartitionedVector\n.\nones\n(\n5000L\n,\n \n500\n)\n\n\n\n//Do not use on large vectors as\n\n\n//it might lead to overflow of memory.\n\n\nval\n \nbreeze_vec\n \n=\n \nvec\n.\ntoBreezeVector",
            "title": "Block Vectors"
        },
        {
            "location": "/core/partitioned_vectors/#creation",
            "text": "Block vectors can be created in a number of ways.  PartitionedVector  and  PartitionedDualVector \nare column row vectors respectively and treated as transposes of each other.",
            "title": "Creation"
        },
        {
            "location": "/core/partitioned_vectors/#from-input-blocks",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 //Create the data blocks  val   data_blocks :   Stream [( Long ,  DenseVector [ Double ])]   = \n   ( 0L   until   10L ). toStream . map ( index   =>   ( index ,   DenseVector . ones [ Double ]( 500 )))  //Instantiate the partitioned vector  val   part_vector   =   PartitionedVector ( data_blocks )  //Optionally you may also provide the total length  //of the partitioned vector  val   part_vector   =   PartitionedVector ( data_blocks ,   num_rows :   Long   =   5000L )  //Created Block Dual Vector  val   part_dvector   =   PartitionedDualVector ( data_blocks )  //Optionally you may also provide the total length  //of the partitioned dual vector  val   part_dvector   =   PartitionedDualVector ( data_blocks ,   num_rows :   Long   =   5000L )",
            "title": "From Input Blocks"
        },
        {
            "location": "/core/partitioned_vectors/#from-tabulating-functions",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 val   tabFunc :   ( Long )   =>   Double   = \n   ( index :   Long )   =>   math . sin ( 2 d * math . Pi * index / 5000 d )  //Instantiate the partitioned vector  val   part_vector   =   PartitionedVector ( \n   length   =   5000L ,   numElementsPerBlock   =   500 , \n   tabFunc )  //Instantiate the partitioned dual vector  val   part_dvector   =   PartitionedDualVector ( \n   length   =   5000L ,   numElementsPerBlock   =   500 ,    tabFunc )",
            "title": "From Tabulating Functions"
        },
        {
            "location": "/core/partitioned_vectors/#from-a-stream",
            "text": "1\n2\n3\n4\n5 //Create the data stream  val   data :   Stream [ Double ]   =   Stream . fill [ Double ]( 5000 )( 1.0 )  //Instantiate the partitioned vector  val   part_vector   =   PartitionedVector ( data ,   length   =   5000L ,   num_elements_per_block   =   500 )",
            "title": "From a Stream"
        },
        {
            "location": "/core/partitioned_vectors/#from-a-breeze-vector",
            "text": "1\n2\n3\n4\n5 //Create the data blocks  val   data_vector   =   DenseVector . ones [ Double ]( 5000 )  //Instantiate the partitioned vector  val   part_vector   =   PartitionedVector ( data_vector ,   num_elements_per_block   =   500 )    Apart from the above methods of creation there are a number of convenience functions available.",
            "title": "From a Breeze Vector"
        },
        {
            "location": "/core/partitioned_vectors/#vector-with-filled-values",
            "text": "",
            "title": "Vector with Filled Values"
        },
        {
            "location": "/core/partitioned_vectors/#vector-of-zeros",
            "text": "1 val   ones_vec   =   PartitionedVector . zeros ( 5000L ,   500 )",
            "title": "Vector of zeros"
        },
        {
            "location": "/core/partitioned_vectors/#vector-of-ones",
            "text": "1 val   ones_vec   =   PartitionedVector . ones ( 5000L ,   500 )",
            "title": "Vector of Ones"
        },
        {
            "location": "/core/partitioned_vectors/#vector-of-random-values",
            "text": "1\n2\n3 val   random_var   =   RandomVariable ( new   Beta ( 1.5 ,   2.5 ))  val   rand_vec   =   PartitionedVector . rand ( 5000L ,   500 ,   random_var )",
            "title": "Vector of Random Values"
        },
        {
            "location": "/core/partitioned_vectors/#vector-concatenation",
            "text": "1\n2\n3\n4\n5\n6\n7 val   random_var   =   RandomVariable ( new   Beta ( 1.5 ,   2.5 ))  val   rand_vec1   =   PartitionedVector . rand ( 2000L ,   500 ,   random_var )  val   rand_vec2   =   PartitionedVector . rand ( 2000L ,   500 ,   random_var )  //Vector of length 4000, having 8 blocks of 500 elements each  val   vec   =   PartitionedVector . vertcat ( rand_vec1 ,   rand_vec2 )     Tip  A  PartitionedDualVector  can be created via the transpose operation\non a  PartitionedVector  instance and vice versa.  1\n2\n3\n4\n5 val   random_var   =   RandomVariable ( new   Beta ( 1.5 ,   2.5 ))  val   p_vec   =   PartitionedVector . rand ( 5000L ,   500 ,   random_var )  val   p_dvec   =   p_vec . t",
            "title": "Vector Concatenation"
        },
        {
            "location": "/core/partitioned_vectors/#algebraic-operations",
            "text": "Partitioned vectors and dual vectors have a number of algebraic operations available in the API.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29 val   beta_var   =   RandomVariable ( Beta ( 1.5 ,   2.5 ))  val   gamma_var   =   RandomVariable ( Gamma ( 1.5 ,   2.5 ))  val   p_vec_beta   =   PartitionedVector . rand ( 5000L ,   500 ,   beta_var )  val   p_vec_gamma   =   PartitionedVector . rand ( 5000L ,   500 ,   gamma_var )  val   dvec_beta   =   p_vec_beta . t  val   dvec_gamma   =   p_vec_gamma . t  //Addition  val   add_vec   =   p_vec_beta   +   p_vec_gamma  val   add_dvec   =   dvec_beta   +   dvec_gamma  //Subtraction  val   sub_vec   =   p_vec_beta   -   p_vec_gamma  val   sub_dvec   =   dvec_beta   -   dvec_gamma  //Element wise multiplication  val   mult_vec   =   p_vec_beta   :*   p_vec_gamma  //Element wise division  val   div_vec   =   p_vec_beta   :/   p_vec_gamma  //Inner Product  val   prod   =   dvec_gamma * p_vec_beta  //Scaler multiplication  val   sc_vec   =   add_vec * 1.5  val   sc_dvec   =   add_dvec * 2.5",
            "title": "Algebraic Operations"
        },
        {
            "location": "/core/partitioned_vectors/#misc-operations",
            "text": "",
            "title": "Misc. Operations"
        },
        {
            "location": "/core/partitioned_vectors/#map-partitions",
            "text": "Map each index, partition pair by a scala function.  1\n2\n3\n4\n5 val   vec :   PartitionedVector   =   _  val   other_vec   =   vec . map ( \n    ( pair :   ( Long ,   DenseVector [ Double ]))   =>   ( pair . _1 ,   pair . _2 * 1.5 )  )",
            "title": "Map Partitions"
        },
        {
            "location": "/core/partitioned_vectors/#slice",
            "text": "Obtain subset of elements, the new vector is repartitioned and re-indexed accordingly.  1\n2\n3 val   vec :   PartitionedVector   =   PartitionedVector . ones ( 5000L ,   500 )  val   other_vec   =   vec ( 999L   until   2000L )",
            "title": "Slice"
        },
        {
            "location": "/core/partitioned_vectors/#reverse",
            "text": "Reverse a block vector  1\n2\n3 val   vec :   PartitionedVector   =   PartitionedVector . ones ( 5000L ,   500 )  val   reverse_vec   =   vec . reverse",
            "title": "Reverse"
        },
        {
            "location": "/core/partitioned_vectors/#convert-to-breeze-vector",
            "text": "1\n2\n3\n4\n5 val   vec :   PartitionedVector   =   PartitionedVector . ones ( 5000L ,   500 )  //Do not use on large vectors as  //it might lead to overflow of memory.  val   breeze_vec   =   vec . toBreezeVector",
            "title": "Convert to Breeze Vector"
        },
        {
            "location": "/core/partitioned_matrices/",
            "text": "Summary\n\n\nHere we show how to use the block matrix API\n\n\n\n\nThe \nalgebra\n package\ncontains a number of block matrix implementations.\n\n\n\n\n\n\n\n\nClass\n\n\nRepresents\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nPartitionedMatrix\n\n\nA general block matrix\n\n\nUser facing i.e. can be instantiated directly\n\n\n\n\n\n\nLowerTriPartitionedMatrix\n\n\nLower triangular block matrix\n\n\nResult of \nalgebra\n API calls\n\n\n\n\n\n\nUpperTriPartitionedMatrix\n\n\nUpper triangular block matrix\n\n\nResult of \nalgebra\n API calls\n\n\n\n\n\n\nPartitionedPSDMatrix\n\n\nSymmetric positive semi-definite matrix\n\n\nResult of applying kernel function on data.\n\n\n\n\n\n\n\n\nCreation\n\u00b6\n\n\nBlock can be created in two major ways.\n\n\nFrom Input Blocks\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nval\n \nindex_set\n \n=\n \n(\n0L\n \nuntil\n \n10L\n).\ntoStream\n\n\n//Create the data blocks\n\n\nval\n \ndata_blocks\n:\n \nStream\n[((\nLong\n, \nLong\n)\n, \nDenseVector\n[\nDouble\n])]\n \n=\n\n  \nutils\n.\ncombine\n(\nindex_set\n).\nmap\n(\n\n    \nindices\n \n=>\n \n(\n\n      \n(\nindices\n.\nhead\n,\n \nindices\n.\nlast\n),\n \nDenseMatrix\n.\nones\n[\nDouble\n](\n500\n,\n \n500\n)\n\n    \n)\n\n  \n)\n\n\n\n//Instantiate the partitioned matrix\n\n\n//must provide dimensions\n\n\nval\n \npart_matrix\n \n=\n \nPartitionedMatrix\n(\n\n  \ndata_blocks\n,\n \nnumrows\n \n=\n \n5000L\n,\n \nnumcols\n \n=\n \n5000L\n)\n\n\n\n\n\n\n\nFrom Tabulating Functions\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nval\n \ntabFunc\n:\n \n(\nLong\n,\n \nLong\n)\n \n=>\n \nDouble\n \n=\n\n  \n(\nindexR\n:\n \nLong\n,\n \nindexC\n:\n \nLong\n)\n \n=>\n \n{\n\n    \nmath\n.\nsin\n(\n2\nd\n*\nmath\n.\nPi\n*\nindexR\n/\n5000\nd\n)*\nmath\n.\ncos\n(\n2\nd\n*\nmath\n.\nPi\n*\nindexC\n/\n5000\nd\n)\n\n  \n}\n\n\n\n//Instantiate the partitioned matrix\n\n\nval\n \npart_matrix\n \n=\n \nPartitionedMatrix\n(\n\n  \nnRows\n \n=\n \n5000L\n,\n \nnCols\n \n=\n \n5000L\n,\n\n  \nnumElementsPerRBlock\n \n=\n \n1000\n,\n\n  \nnumElementsPerCBlock\n \n=\n \n1000\n,\n\n  \ntabFunc\n)\n\n\n\n\n\n\n\nFrom Outer Product\n\u00b6\n\n\nA \nPartitionedMatrix\n can also be constructed from the product of a \nPartitionedDualVector\n and\n\nPartitionedVector\n.\n\n\n1\n2\n3\n4\n5\n6\nval\n \nrandom_var\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \nrand_vec1\n \n=\n \nPartitionedVector\n.\nrand\n(\n2000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\nval\n \nrand_vec2\n \n=\n \nPartitionedVector\n.\nrand\n(\n2000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\n\nval\n \np_mat\n \n=\n \nrand_vec1\n*\nrand_vec2\n.\nt\n\n\n\n\n\n\n\nMatrix Concatenation\n\u00b6\n\n\nYou can vertically join matrices, as long as the number of rows and row blocks match.\n\n\n1\n2\n3\n4\nval\n \nmat1\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\nval\n \nmat2\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\nval\n \nmat3\n \n=\n \nPartitionedMatrix\n.\nvertcat\n(\nmat1\n,\n \nmat2\n)\n\n\n\n\n\n\n\n\n\nPositive Semi-Definite Matrices\n\n\nThe class \nPartitionedPSDMatrix\n can be instantiated in two ways.\n\n\n\n\n\n\nFrom outer product.\n\n\n1\n2\n3\n4\nval\n \nrandom_var\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \nrand_vec\n \n=\n \nPartitionedVector\n.\nrand\n(\n2000L\n,\n \n500\n,\n \nrandom_var\n)\n\n\nval\n \npsd_mat\n \n=\n \nPartitionedPSDMatrix\n.\nfromOuterProduct\n(\nrand_vec\n)\n\n\n\n\n\n\n\n\n\n\n\nFrom kernel evaluation\n\n\n1\n2\n3\n4\n5\n6\n//Obtain data\n\n\nval\n \ndata\n:\n \nSeq\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n//Create kernel instance\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n\nval\n \npsd_gram_mat\n \n=\n \nkernel\n.\nbuildBlockedKernelMatrix\n(\ndata\n,\n \ndata\n.\nlength\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgebraic Operations\n\u00b6\n\n\nPartitioned vectors and dual vectors have a number of algebraic operations available in the API.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nval\n \nbeta_var\n \n=\n \nRandomVariable\n(\nBeta\n(\n1.5\n,\n \n2.5\n))\n\n\nval\n \ngamma_var\n \n=\n \nRandomVariable\n(\nGamma\n(\n1.5\n,\n \n2.5\n))\n\n\n\nval\n \np_vec_beta\n \n=\n \nPartitionedVector\n.\nrand\n(\n5000L\n,\n \n1000\n,\n \nbeta_var\n)\n\n\nval\n \np_vec_gamma\n \n=\n \nPartitionedVector\n.\nrand\n(\n5000L\n,\n \n1000\n,\n \ngamma_var\n)\n\n\n\nval\n \ndvec_beta\n \n=\n \np_vec_beta\n.\nt\n\n\nval\n \ndvec_gamma\n \n=\n \np_vec_gamma\n.\nt\n\n\n\nval\n \nmat1\n \n=\n \np_vec_gamma\n*\ndvec_gamma\n\n\nval\n \nmat2\n \n=\n \np_vec_beta\n*\ndvec_beta\n\n\n\n//Addition\n\n\nval\n \nadd_mat\n \n=\n \nmat1\n \n+\n \nmat2\n\n\n\n//Subtraction\n\n\nval\n \nsub_mat\n \n=\n \nmat2\n \n-\n \nmat1\n\n\n\n//Element wise multiplication\n\n\nval\n \nmult_mat\n \n=\n \nmat1\n \n:*\n \nmat2\n\n\n\n//Matrix matrix product\n\n\n\nval\n \nprod_mat\n \n=\n \nmat1\n*\nmat2\n\n\n\n//matrix vector Product\n\n\nval\n \nprod\n \n=\n \nmat1\n*\np_vec_beta\n\n\nval\n \nprod_dual\n \n=\n \ndvec_gamma\n*\nmat2\n\n\n\n//Scaler multiplication\n\n\nval\n \nsc_mat\n \n=\n \nmat1\n*\n1.5\n\n\n\n\n\n\n\nMisc. Operations\n\u00b6\n\n\nMap Partitions\n\u00b6\n\n\nMap each index, partition pair by a Scala function.\n\n\n1\n2\n3\n4\n5\nval\n \nvec\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\nval\n \nother_vec\n \n=\n \nvec\n.\nmap\n(\n\n   \n(\npair\n:\n \n((\nLong\n,\n \nLong\n),\n \nDenseMatrix\n[\nDouble\n]))\n \n=>\n \n(\npair\n.\n_1\n,\n \npair\n.\n_2\n*\n1.5\n)\n\n\n)\n\n\n\n\n\n\n\nSlice\n\u00b6\n\n\nObtain subset of elements, the new matrix is repartitioned and re-indexed accordingly.\n\n\n1\n2\n3\n4\n5\nval\n \nvec\n:\n \nPartitionedVector\n \n=\n \nPartitionedVector\n.\nones\n(\n5000L\n,\n \n1000\n)\n\n\n\nval\n \nmat\n \n=\n \nvec\n*\nvec\n.\nt\n\n\n\nval\n \nother_mat\n \n=\n \nvec\n(\n999L\n \nuntil\n \n2000L\n,\n \n0L\n \nuntil\n \n999L\n)\n\n\n\n\n\n\n\nUpper and Lower Triangular Sections\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\nval\n \nvec\n:\n \nPartitionedVector\n \n=\n \nPartitionedVector\n.\nones\n(\n5000L\n,\n \n1000\n)\n\n\n\nval\n \nmat\n \n=\n \nvec\n*\nvec\n.\nt\n\n\n\nval\n \nlower_tri\n:\n \nLowerTriPartitionedMatrix\n \n=\n \nmat\n.\nL\n\n\nval\n \nupper_tri\n:\n \nUpperTriPartitionedMatrix\n \n=\n \nmat\n.\nU\n\n\n\n\n\n\n\nConvert to Breeze Matrix\n\u00b6\n\n\n1\n2\n3\n4\n5\nval\n \nmat\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\n//Do not use on large vectors as\n\n\n//it might lead to overflow of memory.\n\n\nval\n \nbreeze_mat\n \n=\n \nmat\n.\ntoBreezeMatrix",
            "title": "Block Matrices"
        },
        {
            "location": "/core/partitioned_matrices/#creation",
            "text": "Block can be created in two major ways.",
            "title": "Creation"
        },
        {
            "location": "/core/partitioned_matrices/#from-input-blocks",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 val   index_set   =   ( 0L   until   10L ). toStream  //Create the data blocks  val   data_blocks :   Stream [(( Long ,  Long ) ,  DenseVector [ Double ])]   = \n   utils . combine ( index_set ). map ( \n     indices   =>   ( \n       ( indices . head ,   indices . last ),   DenseMatrix . ones [ Double ]( 500 ,   500 ) \n     ) \n   )  //Instantiate the partitioned matrix  //must provide dimensions  val   part_matrix   =   PartitionedMatrix ( \n   data_blocks ,   numrows   =   5000L ,   numcols   =   5000L )",
            "title": "From Input Blocks"
        },
        {
            "location": "/core/partitioned_matrices/#from-tabulating-functions",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 val   tabFunc :   ( Long ,   Long )   =>   Double   = \n   ( indexR :   Long ,   indexC :   Long )   =>   { \n     math . sin ( 2 d * math . Pi * indexR / 5000 d )* math . cos ( 2 d * math . Pi * indexC / 5000 d ) \n   }  //Instantiate the partitioned matrix  val   part_matrix   =   PartitionedMatrix ( \n   nRows   =   5000L ,   nCols   =   5000L , \n   numElementsPerRBlock   =   1000 , \n   numElementsPerCBlock   =   1000 , \n   tabFunc )",
            "title": "From Tabulating Functions"
        },
        {
            "location": "/core/partitioned_matrices/#from-outer-product",
            "text": "A  PartitionedMatrix  can also be constructed from the product of a  PartitionedDualVector  and PartitionedVector .  1\n2\n3\n4\n5\n6 val   random_var   =   RandomVariable ( new   Beta ( 1.5 ,   2.5 ))  val   rand_vec1   =   PartitionedVector . rand ( 2000L ,   500 ,   random_var )  val   rand_vec2   =   PartitionedVector . rand ( 2000L ,   500 ,   random_var )  val   p_mat   =   rand_vec1 * rand_vec2 . t",
            "title": "From Outer Product"
        },
        {
            "location": "/core/partitioned_matrices/#matrix-concatenation",
            "text": "You can vertically join matrices, as long as the number of rows and row blocks match.  1\n2\n3\n4 val   mat1 :   PartitionedMatrix   =   _  val   mat2 :   PartitionedMatrix   =   _  val   mat3   =   PartitionedMatrix . vertcat ( mat1 ,   mat2 )     Positive Semi-Definite Matrices  The class  PartitionedPSDMatrix  can be instantiated in two ways.    From outer product.  1\n2\n3\n4 val   random_var   =   RandomVariable ( new   Beta ( 1.5 ,   2.5 ))  val   rand_vec   =   PartitionedVector . rand ( 2000L ,   500 ,   random_var )  val   psd_mat   =   PartitionedPSDMatrix . fromOuterProduct ( rand_vec )      From kernel evaluation  1\n2\n3\n4\n5\n6 //Obtain data  val   data :   Seq [ DenseVector [ Double ]]   =   _  //Create kernel instance  val   kernel :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  val   psd_gram_mat   =   kernel . buildBlockedKernelMatrix ( data ,   data . length )",
            "title": "Matrix Concatenation"
        },
        {
            "location": "/core/partitioned_matrices/#algebraic-operations",
            "text": "Partitioned vectors and dual vectors have a number of algebraic operations available in the API.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31 val   beta_var   =   RandomVariable ( Beta ( 1.5 ,   2.5 ))  val   gamma_var   =   RandomVariable ( Gamma ( 1.5 ,   2.5 ))  val   p_vec_beta   =   PartitionedVector . rand ( 5000L ,   1000 ,   beta_var )  val   p_vec_gamma   =   PartitionedVector . rand ( 5000L ,   1000 ,   gamma_var )  val   dvec_beta   =   p_vec_beta . t  val   dvec_gamma   =   p_vec_gamma . t  val   mat1   =   p_vec_gamma * dvec_gamma  val   mat2   =   p_vec_beta * dvec_beta  //Addition  val   add_mat   =   mat1   +   mat2  //Subtraction  val   sub_mat   =   mat2   -   mat1  //Element wise multiplication  val   mult_mat   =   mat1   :*   mat2  //Matrix matrix product  val   prod_mat   =   mat1 * mat2  //matrix vector Product  val   prod   =   mat1 * p_vec_beta  val   prod_dual   =   dvec_gamma * mat2  //Scaler multiplication  val   sc_mat   =   mat1 * 1.5",
            "title": "Algebraic Operations"
        },
        {
            "location": "/core/partitioned_matrices/#misc-operations",
            "text": "",
            "title": "Misc. Operations"
        },
        {
            "location": "/core/partitioned_matrices/#map-partitions",
            "text": "Map each index, partition pair by a Scala function.  1\n2\n3\n4\n5 val   vec :   PartitionedMatrix   =   _  val   other_vec   =   vec . map ( \n    ( pair :   (( Long ,   Long ),   DenseMatrix [ Double ]))   =>   ( pair . _1 ,   pair . _2 * 1.5 )  )",
            "title": "Map Partitions"
        },
        {
            "location": "/core/partitioned_matrices/#slice",
            "text": "Obtain subset of elements, the new matrix is repartitioned and re-indexed accordingly.  1\n2\n3\n4\n5 val   vec :   PartitionedVector   =   PartitionedVector . ones ( 5000L ,   1000 )  val   mat   =   vec * vec . t  val   other_mat   =   vec ( 999L   until   2000L ,   0L   until   999L )",
            "title": "Slice"
        },
        {
            "location": "/core/partitioned_matrices/#upper-and-lower-triangular-sections",
            "text": "1\n2\n3\n4\n5\n6 val   vec :   PartitionedVector   =   PartitionedVector . ones ( 5000L ,   1000 )  val   mat   =   vec * vec . t  val   lower_tri :   LowerTriPartitionedMatrix   =   mat . L  val   upper_tri :   UpperTriPartitionedMatrix   =   mat . U",
            "title": "Upper and Lower Triangular Sections"
        },
        {
            "location": "/core/partitioned_matrices/#convert-to-breeze-matrix",
            "text": "1\n2\n3\n4\n5 val   mat :   PartitionedMatrix   =   _  //Do not use on large vectors as  //it might lead to overflow of memory.  val   breeze_mat   =   mat . toBreezeMatrix",
            "title": "Convert to Breeze Matrix"
        },
        {
            "location": "/core/partitioned_alg_utils/",
            "text": "Summary\n\n\nThe \nalgebra\n package has utility functions for commonly used operations on block matrices and vectors. We give\nthe user a glimpse here.\n\n\n\n\n\n\nWarning\n\n\nThe routines in this section assume that the block sizes of the input matrix are homogenous i.e. the number of row blocks\nis equal to number of column blocks.\n\n\n\n\nBlocked Operations\n\u00b6\n\n\nFollowing operations are the blocked implementations of standard algorithms used for matrices.\n\n\nLU\nLU\n Decomposition\n\u00b6\n\n\nLU\nLU\n decomposition\n consists of decomposing a square matrix into\nlower and upper triangular factors.\n\n\n1\n2\n3\n4\n//Initialize a square matrix\n\n\nval\n \nsq_mat\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\nval\n \n(\nlower\n,\n \nupper\n)\n \n=\n \nbLU\n(\nsq_mat\n)\n\n\n\n\n\n\n\nCholesky Decomposition\n\u00b6\n\n\nCholesky decomposition\n consists of decomposing a\nsymmetric positive semi-definite matrix uniquely into lower and upper triangular factors.\n\n\n1\n2\n3\n4\n//Initialize a psd matrix\n\n\nval\n \npsd_mat\n:\n \nPartitionedPSDMatrix\n \n=\n \n_\n\n\n\nval\n \n(\nlower\n,\n \nupper\n)\n \n=\n \nbcholesky\n(\npsd_mat\n)\n\n\n\n\n\n\n\nTrace\n\u00b6\n\n\nTrace of a square matrix is the sum of the diagonal elements.\n\n\n1\n2\n3\n4\n//Initialize a square matrix\n\n\nval\n \nsq_mat\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\nval\n \ntr\n \n=\n \nbtrace\n(\nsq_mat\n)\n\n\n\n\n\n\n\nDeterminant\n\u00b6\n\n\nThe \ndeterminant\n of a square matrix represents the scaling factor of the\ntransformation described by the matrix\n\n\n1\n2\n3\n4\n//Initialize a square matrix\n\n\nval\n \nsq_mat\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\nval\n \nde\n \n=\n \nbdet\n(\nsq_mat\n)\n\n\n\n\n\n\n\nDiagonal\n\u00b6\n\n\nObtain diagonal elements of a square block matrix in the form of a block vector.\n\n\n1\n2\n3\n4\n//Initialize a square matrix\n\n\nval\n \nsq_mat\n:\n \nPartitionedMatrix\n \n=\n \n_\n\n\n\nval\n \ndia\n:\n \nPartitionedVector\n \n=\n \nbdiagonal\n(\nsq_mat\n)\n\n\n\n\n\n\n\nQuadratic Forms\n\u00b6\n\n\nQuadratic forms are often encountered in algebra, they involve products on inverse positive semi-definite matrices with\nvectors. The two common quadratic forms are.\n\n\n\n\n\n\nSelf Quadratic Forms\n:  \n\\mathbf{x}^\\intercal \\Omega^{-1} \\mathbf{x}\n\\mathbf{x}^\\intercal \\Omega^{-1} \\mathbf{x}\n\n\n\n\n\n\nCross Quadratic Form\n:  \n\\mathbf{y}^\\intercal \\Omega^{-1} \\mathbf{x}\n\\mathbf{y}^\\intercal \\Omega^{-1} \\mathbf{x}\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nval\n \n(\nx\n,\ny\n)\n:\n \n(\nDenseVector\n[\nDouble\n],\n \nDenseVector\n[\nDouble\n])\n \n=\n \n(\n_\n,\n_\n)\n\n\n\nval\n \nomega\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n_\n\n\n\n//Use breeze function\n\n\nval\n \nlower\n \n=\n \ncholesky\n(\nomega\n)\n\n\n\nval\n \nx_omega_x\n \n=\n \nquadraticForm\n(\nlower\n,\n \nx\n)\n\n\n\nval\n \ny_omega_x\n \n=\n \ncrossQuadraticForm\n(\ny\n,\n \nlower\n,\n \nx\n)\n\n\n\n//Blocked Version of the same.\n\n\n\nval\n \n(\nxb\n,\nyb\n)\n:\n \n(\nPartitionedVector\n,\n \nPartitionedVector\n)\n \n=\n \n(\n_\n,\n_\n)\n\n\n\nval\n \nomegab\n:\n \nPartitionedPSDMatrix\n \n=\n \n_\n\n\n\n//Use DynaML algebra function\n\n\nval\n \nlowerb\n \n=\n \nbcholesky\n(\nomegab\n)\n\n\n\nval\n \nx_omega_x_b\n \n=\n \nblockedQuadraticForm\n(\nlowerb\n,\n \nxb\n)\n\n\n\nval\n \ny_omega_x_b\n \n=\n \nblockedCrossQuadraticForm\n(\nyb\n,\n \nlowerb\n,\n \nxb\n)",
            "title": "Block Algebra Utilities"
        },
        {
            "location": "/core/partitioned_alg_utils/#blocked-operations",
            "text": "Following operations are the blocked implementations of standard algorithms used for matrices.",
            "title": "Blocked Operations"
        },
        {
            "location": "/core/partitioned_alg_utils/#lulu-decomposition",
            "text": "LU LU  decomposition  consists of decomposing a square matrix into\nlower and upper triangular factors.  1\n2\n3\n4 //Initialize a square matrix  val   sq_mat :   PartitionedMatrix   =   _  val   ( lower ,   upper )   =   bLU ( sq_mat )",
            "title": "LULU Decomposition"
        },
        {
            "location": "/core/partitioned_alg_utils/#cholesky-decomposition",
            "text": "Cholesky decomposition  consists of decomposing a\nsymmetric positive semi-definite matrix uniquely into lower and upper triangular factors.  1\n2\n3\n4 //Initialize a psd matrix  val   psd_mat :   PartitionedPSDMatrix   =   _  val   ( lower ,   upper )   =   bcholesky ( psd_mat )",
            "title": "Cholesky Decomposition"
        },
        {
            "location": "/core/partitioned_alg_utils/#trace",
            "text": "Trace of a square matrix is the sum of the diagonal elements.  1\n2\n3\n4 //Initialize a square matrix  val   sq_mat :   PartitionedMatrix   =   _  val   tr   =   btrace ( sq_mat )",
            "title": "Trace"
        },
        {
            "location": "/core/partitioned_alg_utils/#determinant",
            "text": "The  determinant  of a square matrix represents the scaling factor of the\ntransformation described by the matrix  1\n2\n3\n4 //Initialize a square matrix  val   sq_mat :   PartitionedMatrix   =   _  val   de   =   bdet ( sq_mat )",
            "title": "Determinant"
        },
        {
            "location": "/core/partitioned_alg_utils/#diagonal",
            "text": "Obtain diagonal elements of a square block matrix in the form of a block vector.  1\n2\n3\n4 //Initialize a square matrix  val   sq_mat :   PartitionedMatrix   =   _  val   dia :   PartitionedVector   =   bdiagonal ( sq_mat )",
            "title": "Diagonal"
        },
        {
            "location": "/core/partitioned_alg_utils/#quadratic-forms",
            "text": "Quadratic forms are often encountered in algebra, they involve products on inverse positive semi-definite matrices with\nvectors. The two common quadratic forms are.    Self Quadratic Forms :   \\mathbf{x}^\\intercal \\Omega^{-1} \\mathbf{x} \\mathbf{x}^\\intercal \\Omega^{-1} \\mathbf{x}    Cross Quadratic Form :   \\mathbf{y}^\\intercal \\Omega^{-1} \\mathbf{x} \\mathbf{y}^\\intercal \\Omega^{-1} \\mathbf{x}     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 val   ( x , y ) :   ( DenseVector [ Double ],   DenseVector [ Double ])   =   ( _ , _ )  val   omega :   DenseMatrix [ Double ]   =   _  //Use breeze function  val   lower   =   cholesky ( omega )  val   x_omega_x   =   quadraticForm ( lower ,   x )  val   y_omega_x   =   crossQuadraticForm ( y ,   lower ,   x )  //Blocked Version of the same.  val   ( xb , yb ) :   ( PartitionedVector ,   PartitionedVector )   =   ( _ , _ )  val   omegab :   PartitionedPSDMatrix   =   _  //Use DynaML algebra function  val   lowerb   =   bcholesky ( omegab )  val   x_omega_x_b   =   blockedQuadraticForm ( lowerb ,   xb )  val   y_omega_x_b   =   blockedCrossQuadraticForm ( yb ,   lowerb ,   xb )",
            "title": "Quadratic Forms"
        },
        {
            "location": "/core/core_prob_randomvar/",
            "text": "What I cannot create, I do not understand - Richard Feynman\n\n\n\n\n\n\n\n\nSummary\n\n\nSince version 1.4 a new package called \nprobability\n has been added to the core api with an aim to aid in the modeling of random variables and measurable functions.\n\n\n\n\nRandom variables and probability distributions form the bedrock of modern statistical based approaches to inference. Furthermore, analytically tractable inference is only possible for a small number of models while a wealth of interesting model structures don't yield themselves to analytical inference and approximate sampling based approaches are often employed.\n\n\nRandom Variable API\n\u00b6\n\n\nAlthough both random variable with tractable and intractable distributions can be constructed, the emphasis is on the sampling capabilities of random variable objects.\n\n\nThe \nprobability\n package class hierarchy consists of classes and traits which represent continuous and discrete random variables along with ability to endow them with distributions.\n\n\nDynaML Random Variable\n\u00b6\n\n\nThe \nRandomVariable\n[\nDomain\n]\n forms the top of the class hierarchy in the \nprobability\n package. It is a light weight trait which takes a form like so.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nabstract\n \nclass\n \nRandomVariable\n[\nDomain\n]\n \n{\n\n\n  \nval\n \nsample\n:\n \nDataPipe\n[\nUnit\n, \nDomain\n]\n\n\n  \ndef\n \n:*[\nDomain1\n](\nother\n:\n \nRandomVariable\n[\nDomain1\n])\n:\n \nRandomVariable\n[(\nDomain\n, \nDomain1\n)]\n \n=\n \n{\n\n    \nval\n \nsam\n \n=\n \nthis\n.\nsample\n\n    \nRandomVariable\n(\nBifurcationPipe\n(\nsam\n,\nother\n.\nsample\n))\n\n  \n}\n\n\n}\n\n\n\n\n\n\n\nA \nRandomVariable\n instance is defined by its type parameter \nDomain\n, in Mathematics this is the underlying space (referred to as the \nsupport\n) over which the random variable is defined (\n\\mathbb{R}^p\n\\mathbb{R}^p\n for continuos variables, \n\\mathbb{N}\n\\mathbb{N}\n for discrete variables).\n\n\nThe two main functionalities are as follows.\n\n\n\n\n\n\nsample\n which is a data pipe having no input and outputs a sample from the random variables distribution whenever invoked.\n\n\n\n\n\n\n:*\n the 'composition' operator between random variables, evaluating an expression like \nrandomVar1\n \n:*\n \nrandomVar2\n creates a new random variable whose domain is a cartesian product of the domains of \nrandomVar1\n and \nrandomVar2\n.\n\n\n\n\n\n\nContinuous and discrete distribution random variables are implemented through the \nContinuousDistrRV\n[\nDomain\n]\n and \nDiscreteDistrRV\n[\nDomain\n]\n respectively.\n\n\nCreating Random Variables\n\u00b6\n\n\nCreating random variables can be created by a number of ways.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nimport\n \nbreeze.stats.distributions._\n\n\nimport\n \nspire.implicits._\n\n\n\n\n//Create a sampling function\n\n\nval\n \nsampF\n:\n \n()\n \n=>\n \nDouble\n \n=\n \n...\n\n\nval\n \nrv\n \n=\n \nRandomVariable\n(\nsampF\n)\n\n\n\n//Also works with a pipe\n\n\nval\n \nsampF\n:\n \nDataPipe\n[\nUnit\n, \nDouble\n]\n \n=\n \n...\n\n\nval\n \nrv\n \n=\n \nRandomVariable\n(\nsampF\n)\n\n\n\n\n\n\n\n\n\nSampling is the core functionality of the classes extending \nRandomVariable\n but in some cases representing random variables having an underlying (tractable and known) distribution is a requirement, for that purpose there exists the  \nRandomVarWithDistr\n[\nDomain\n, \nDist\n]\n trait which is a bare bones extension of \nRandomVariable\n; it contains only one other member, \nunderlyingDist\n which is of abstract type \nDist\n.\n\n\nThe type \nDist\n can be any breeze distribution, which is either contained in the package \nbreeze\n.\nstats\n.\ndistributions\n or a user written extension of a breeze probability distribution.\n\n\n\n\n\n\nCreating random variables from breeze distributions\n\n\nCreating a random variable backed by a breeze distribution is easy, simply pass the breeze distribution to the \nRandomVariable\n companion object.\n\n\n1\nval\n \np\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n7.5\n,\n \n7.5\n))\n\n\n\n\n\n\n\nThe \nRandomVariable\n object recognizes the breeze distribution passed to it and creates a continuous or discrete random variable accordingly.",
            "title": "Random Variables"
        },
        {
            "location": "/core/core_prob_randomvar/#random-variable-api",
            "text": "Although both random variable with tractable and intractable distributions can be constructed, the emphasis is on the sampling capabilities of random variable objects.  The  probability  package class hierarchy consists of classes and traits which represent continuous and discrete random variables along with ability to endow them with distributions.",
            "title": "Random Variable API"
        },
        {
            "location": "/core/core_prob_randomvar/#dynaml-random-variable",
            "text": "The  RandomVariable [ Domain ]  forms the top of the class hierarchy in the  probability  package. It is a light weight trait which takes a form like so.  1\n2\n3\n4\n5\n6\n7\n8\n9 abstract   class   RandomVariable [ Domain ]   { \n\n   val   sample :   DataPipe [ Unit ,  Domain ] \n\n   def   :*[ Domain1 ]( other :   RandomVariable [ Domain1 ]) :   RandomVariable [( Domain ,  Domain1 )]   =   { \n     val   sam   =   this . sample \n     RandomVariable ( BifurcationPipe ( sam , other . sample )) \n   }  }    A  RandomVariable  instance is defined by its type parameter  Domain , in Mathematics this is the underlying space (referred to as the  support ) over which the random variable is defined ( \\mathbb{R}^p \\mathbb{R}^p  for continuos variables,  \\mathbb{N} \\mathbb{N}  for discrete variables).  The two main functionalities are as follows.    sample  which is a data pipe having no input and outputs a sample from the random variables distribution whenever invoked.    :*  the 'composition' operator between random variables, evaluating an expression like  randomVar1   :*   randomVar2  creates a new random variable whose domain is a cartesian product of the domains of  randomVar1  and  randomVar2 .    Continuous and discrete distribution random variables are implemented through the  ContinuousDistrRV [ Domain ]  and  DiscreteDistrRV [ Domain ]  respectively.",
            "title": "DynaML Random Variable"
        },
        {
            "location": "/core/core_prob_randomvar/#creating-random-variables",
            "text": "Creating random variables can be created by a number of ways.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 import   breeze.stats.distributions._  import   spire.implicits._  //Create a sampling function  val   sampF :   ()   =>   Double   =   ...  val   rv   =   RandomVariable ( sampF )  //Also works with a pipe  val   sampF :   DataPipe [ Unit ,  Double ]   =   ...  val   rv   =   RandomVariable ( sampF )     Sampling is the core functionality of the classes extending  RandomVariable  but in some cases representing random variables having an underlying (tractable and known) distribution is a requirement, for that purpose there exists the   RandomVarWithDistr [ Domain ,  Dist ]  trait which is a bare bones extension of  RandomVariable ; it contains only one other member,  underlyingDist  which is of abstract type  Dist .  The type  Dist  can be any breeze distribution, which is either contained in the package  breeze . stats . distributions  or a user written extension of a breeze probability distribution.    Creating random variables from breeze distributions  Creating a random variable backed by a breeze distribution is easy, simply pass the breeze distribution to the  RandomVariable  companion object.  1 val   p   =   RandomVariable ( new   Beta ( 7.5 ,   7.5 ))    The  RandomVariable  object recognizes the breeze distribution passed to it and creates a continuous or discrete random variable accordingly.",
            "title": "Creating Random Variables"
        },
        {
            "location": "/core/core_prob_operations/",
            "text": "Apart from just creating wrapper code around sampling procedures which represent random variables, it is also important to do transformations on random variables to yield new more interesting random variables and distributions. In statistics one often formulates certain random variables as algebraic operations on other simpler random variables.\n\n\n\n\nAlgebraic Operations\n\u00b6\n\n\nIt is possible to do common algebraic operations on instances of continuous random variables.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nimport\n \nspire.implicits._\n\n\n\nval\n \nb\n \n=\n \nRandomVariable\n(\nnew\n \nBeta\n(\n7.5\n,\n \n7.5\n))\n\n\nval\n \ng\n \n=\n \nRandomVariable\n(\nnew\n \nGamma\n(\n1.5\n,\n \n1.2\n))\n\n\nval\n \nn\n \n=\n \nGaussianRV\n(\n0.0\n,\n \n1.0\n)\n\n\n\nval\n \naddR\n \n=\n \nb\n \n+\n \nn\n \n-\n \ng\n\n\n\nval\n \nmultR\n \n=\n \nb\n \n*\n \n(\nn\n \n+\n \ng\n)\n\n\n\nhistogram\n((\n1\n \nto\n \n1000\n).\nmap\n(\n_\n \n=>\n \nmultR\n.\nsample\n()))\n\n\n\n\n\n\n\n\n\nMeasurable Functions\n\u00b6\n\n\nIn many cases random variables can be expressed as functions of one another, for example chi square random variables are obtained by squaring normally distributed samples.\n\n\n1\n2\n3\n4\nval\n \nchsq\n \n=\n \nMeasurableFunction\n(\nn\n,\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n*\nx\n))\n\n\n\n//Generate a chi square distribution with one degree of freedom\n\n\nhistogram\n((\n1\n \nto\n \n1000\n).\nmap\n(\n_\n \n=>\n \nchsq\n.\nsample\n()))\n\n\n\n\n\n\n\n\n\nPush-forward Maps\n\u00b6",
            "title": "Operations on Random Variables"
        },
        {
            "location": "/core/core_prob_operations/#algebraic-operations",
            "text": "It is possible to do common algebraic operations on instances of continuous random variables.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 import   spire.implicits._  val   b   =   RandomVariable ( new   Beta ( 7.5 ,   7.5 ))  val   g   =   RandomVariable ( new   Gamma ( 1.5 ,   1.2 ))  val   n   =   GaussianRV ( 0.0 ,   1.0 )  val   addR   =   b   +   n   -   g  val   multR   =   b   *   ( n   +   g )  histogram (( 1   to   1000 ). map ( _   =>   multR . sample ()))",
            "title": "Algebraic Operations"
        },
        {
            "location": "/core/core_prob_operations/#measurable-functions",
            "text": "In many cases random variables can be expressed as functions of one another, for example chi square random variables are obtained by squaring normally distributed samples.  1\n2\n3\n4 val   chsq   =   MeasurableFunction ( n ,   DataPipe (( x :   Double )   =>   x * x ))  //Generate a chi square distribution with one degree of freedom  histogram (( 1   to   1000 ). map ( _   =>   chsq . sample ()))",
            "title": "Measurable Functions"
        },
        {
            "location": "/core/core_prob_operations/#push-forward-maps",
            "text": "",
            "title": "Push-forward Maps"
        },
        {
            "location": "/core/core_prob_dist/",
            "text": "Summary\n\n\nThe DynaML \ndynaml.probability.distributions\n package leverages and extends the \nbreeze.stats.distributions\n package. Below is a list of distributions implemented.\n\n\n\n\nSpecifying Distributions\n\u00b6\n\n\nEvery probability density function \n\\rho(x)\n\\rho(x)\n defined over some domain \nx \\in \\mathcal{X}\nx \\in \\mathcal{X}\n can be represented as \n\\rho(x) = \\frac{1}{Z} f(x)\n\\rho(x) = \\frac{1}{Z} f(x)\n, where \nf(x)\nf(x)\n is the un-normalized probability weight and \nZ\nZ\n is the normalization constant. The normalization constant ensures that the density function sums to \n1\n1\n over the whole domain \n\\mathcal{X}\n\\mathcal{X}\n.\n\n\nDescribing Skewness\n\u00b6\n\n\nAn important analytical way to create skewed distributions was described by \nAzzalani et. al\n. It consists of four components.\n\n\n\n\nA symmetric probability density \n\\varphi(.)\n\\varphi(.)\n\n\nAn odd function \nw(.)\nw(.)\n\n\nA cumulative distribution function \nG(.)\nG(.)\n of some symmetric density\n\n\nA cut-off parameter \n\\tau\n\\tau\n\n\n\n\n\n\n\n\\rho(x) = \\frac{1}{G(\\tau)} \\times \\varphi(x)\\times G(w(x) + \\tau)\n\n\n\n\n\\rho(x) = \\frac{1}{G(\\tau)} \\times \\varphi(x)\\times G(w(x) + \\tau)\n\n\n\n\n\nDistributions API\n\u00b6\n\n\nThe \nDensity\n[\nT\n]\n and \nRand\n[\nT\n]\n traits form the API entry points for implementing probability distributions in breeze. In the \ndynaml.probability.distributions\n package, these two traits are inherited by \nGenericDistribution\n[\nT\n]\n which is extended by \nAbstractContinuousDistr\n[\nT\n]\n and \nAbstractDiscreteDistr\n[\nT\n]\n classes.\n\n\n\n\nDistributions which can produce confidence intervals\n\n\nThe trait \nHasErrorBars\n[\nT\n]\n can be used as a mix in to provide the ability of producing error bars to distributions. To extend it, one has to implement the \nconfidenceInterval\n(\ns\n:\n \nDouble\n)\n:\n \n(\nT\n,\n \nT\n)\n method.\n\n\n\n\n\n\nSkewness\n\n\nThe \nSkewSymmDistribution\n[\nT\n]\n class is the generic base implementations for skew symmetric family of distributions in DynaML.\n\n\n\n\nDistributions Library\n\u00b6\n\n\nApart from the distributions defined in the \nbreeze\n.\nstats\n.\ndistributions\n, users have access to the following distributions implemented in the \ndynaml\n.\nprobability\n.\ndistributions\n.\n\n\nMultivariate Students T\n\u00b6\n\n\nDefines a \nStudents' T\n distribution over the domain of finite dimensional vectors.\n\n\n\\mathcal{X} \\equiv  \\mathbb{R}^{n}\n\\mathcal{X} \\equiv  \\mathbb{R}^{n}\n\n\nf(x) = \\left[1+{\\frac {1}{\\nu }}({\\mathbf {x} }-{\\boldsymbol {\\mu }})^{\\rm {T}}{\\boldsymbol {\\Sigma }}^{-1}({\\mathbf {x} }-{\\boldsymbol {\\mu }})\\right]^{-(\\nu +p)/2}\nf(x) = \\left[1+{\\frac {1}{\\nu }}({\\mathbf {x} }-{\\boldsymbol {\\mu }})^{\\rm {T}}{\\boldsymbol {\\Sigma }}^{-1}({\\mathbf {x} }-{\\boldsymbol {\\mu }})\\right]^{-(\\nu +p)/2}\n  \n\n\nZ = \\frac{\\Gamma \\left[(\\nu +p)/2\\right]}{\\Gamma (\\nu /2)\\nu ^{p/2}\\pi ^{p/2}\\left|{\\boldsymbol {\\Sigma }}\\right|^{1/2}}\nZ = \\frac{\\Gamma \\left[(\\nu +p)/2\\right]}{\\Gamma (\\nu /2)\\nu ^{p/2}\\pi ^{p/2}\\left|{\\boldsymbol {\\Sigma }}\\right|^{1/2}}\n\n\nUsage\n:\n\n1\n2\n3\n4\nval\n \nmu\n \n=\n \n2.5\n\n\nval\n \nmean\n \n=\n \nDenseVector\n(\n1.0\n,\n \n0.0\n)\n\n\nval\n \ncov\n \n=\n \nDenseMatrix\n((\n1.5\n,\n \n0.5\n),\n \n(\n0.5\n,\n \n2.5\n))\n\n\nval\n \nd\n \n=\n \nMultivariateStudentsT\n(\nmu\n,\n \nmean\n,\n \ncov\n)\n\n\n\n\n\n\nMatrix T\n\u00b6\n\n\nDefines a \nStudents' T\n distribution over the domain of matrices.\n\n\n\\mathcal{X} \\equiv  \\mathbb{R}^{n \\times p}\n\\mathcal{X} \\equiv  \\mathbb{R}^{n \\times p}\n\n\nf(x) = \\left|{\\mathbf {I}}_{n}+{\\boldsymbol \\Sigma }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}}){\\boldsymbol \\Omega }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}})^{{{\\rm {T}}}}\\right|^{{-{\\frac {\\nu +n+p-1}{2}}}}\nf(x) = \\left|{\\mathbf {I}}_{n}+{\\boldsymbol \\Sigma }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}}){\\boldsymbol \\Omega }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}})^{{{\\rm {T}}}}\\right|^{{-{\\frac {\\nu +n+p-1}{2}}}}\n  \n\n\nZ = {\\frac {\\Gamma_{p}\\left({\\frac {\\nu +n+p-1}{2}}\\right)}{(\\pi )^{{\\frac {np}{2}}}\\Gamma _{p}\\left({\\frac {\\nu +p-1}{2}}\\right)}}|{\\boldsymbol \\Omega }|^{{-{\\frac {n}{2}}}}|{\\boldsymbol \\Sigma }|^{{-{\\frac {p}{2}}}}\nZ = {\\frac {\\Gamma_{p}\\left({\\frac {\\nu +n+p-1}{2}}\\right)}{(\\pi )^{{\\frac {np}{2}}}\\Gamma _{p}\\left({\\frac {\\nu +p-1}{2}}\\right)}}|{\\boldsymbol \\Omega }|^{{-{\\frac {n}{2}}}}|{\\boldsymbol \\Sigma }|^{{-{\\frac {p}{2}}}}\n\n\nUsage\n:\n\n1\n2\n3\n4\n5\nval\n \nmu\n \n=\n \n2.5\n\n\nval\n \nmean\n \n=\n \nDenseMatrix\n((-\n1.5\n,\n \n-\n0.5\n),\n \n(\n3.5\n,\n \n-\n2.5\n))\n\n\nval\n \ncov_rows\n \n=\n \nDenseMatrix\n((\n1.5\n,\n \n0.5\n),\n \n(\n0.5\n,\n \n2.5\n))\n\n\nval\n \ncov_cols\n \n=\n \nDenseMatrix\n((\n0.5\n,\n \n0.1\n),\n \n(\n0.1\n,\n \n1.5\n))\n\n\nval\n \nd\n \n=\n \nMatrixT\n(\nmu\n,\n \nmean\n,\n \ncov_rows\n,\n \ncov_cols\n)\n\n\n\n\n\n\nMatrix Normal\n\u00b6\n\n\nDefines a \nGaussian\n distribution over the domain of matrices.\n\n\n\\mathcal{X} \\equiv  \\mathbb{R}^{n \\times p}\n\\mathcal{X} \\equiv  \\mathbb{R}^{n \\times p}\n\n\nf(x) = \\exp\\left( -\\frac{1}{2} \\, \\mathrm{tr}\\left[ \\mathbf{V}^{-1} (\\mathbf{X} - \\mathbf{M})^{T} \\mathbf{U}^{-1} (\\mathbf{X} - \\mathbf{M}) \\right] \\right)\nf(x) = \\exp\\left( -\\frac{1}{2} \\, \\mathrm{tr}\\left[ \\mathbf{V}^{-1} (\\mathbf{X} - \\mathbf{M})^{T} \\mathbf{U}^{-1} (\\mathbf{X} - \\mathbf{M}) \\right] \\right)\n  \n\n\nZ = (2\\pi)^{np/2} |\\mathbf{V}|^{n/2} |\\mathbf{U}|^{p/2}\nZ = (2\\pi)^{np/2} |\\mathbf{V}|^{n/2} |\\mathbf{U}|^{p/2}\n\n\nUsage\n:\n\n1\n2\n3\n4\nval\n \nmean\n \n=\n \nDenseMatrix\n((-\n1.5\n,\n \n-\n0.5\n),\n \n(\n3.5\n,\n \n-\n2.5\n))\n\n\nval\n \ncov_rows\n \n=\n \nDenseMatrix\n((\n1.5\n,\n \n0.5\n),\n \n(\n0.5\n,\n \n2.5\n))\n\n\nval\n \ncov_cols\n \n=\n \nDenseMatrix\n((\n0.5\n,\n \n0.1\n),\n \n(\n0.1\n,\n \n1.5\n))\n\n\nval\n \nd\n \n=\n \nMatrixNormal\n(\nmean\n,\n \ncov_rows\n,\n \ncov_cols\n)\n\n\n\n\n\n\nTruncated Normal\n\u00b6\n\n\nDefines a univariate \nGaussian\n distribution that is defined in a finite domain.\n\n\n\\mathcal{X} \\equiv  [a, b]\n\\mathcal{X} \\equiv  [a, b]\n\n\nf(x) = \\begin{cases} \\phi ({\\frac {x-\\mu }{\\sigma }}) & a \\leq x \\leq b\\\\0 & else\\end{cases}\nf(x) = \\begin{cases} \\phi ({\\frac {x-\\mu }{\\sigma }}) & a \\leq x \\leq b\\\\0 & else\\end{cases}\n  \n\n\nZ = \\sigma \\left(\\Phi ({\\frac {b-\\mu }{\\sigma }})-\\Phi ({\\frac {a-\\mu }{\\sigma }})\\right)\nZ = \\sigma \\left(\\Phi ({\\frac {b-\\mu }{\\sigma }})-\\Phi ({\\frac {a-\\mu }{\\sigma }})\\right)\n\n\n\\phi()\n\\phi()\n and \n\\Phi()\n\\Phi()\n being the gaussian density function and cumulative distribution function respectively\n\n\nUsage\n:\n\n1\n2\n3\n4\nval\n \nmean\n \n=\n \n1.5\n\n\nval\n \nsigma\n \n=\n \n1.5\n\n\nval\n \n(\na\n,\nb\n)\n \n=\n \n(-\n0.5\n,\n \n2.5\n)\n\n\nval\n \nd\n \n=\n \nTruncatedGaussian\n(\nmean\n,\n \nsigma\n,\n \na\n,\n \nb\n)\n\n\n\n\n\n\nSkew Gaussian\n\u00b6\n\n\nUnivariate\n\u00b6\n\n\n\\mathcal{X} \\equiv  \\mathbb{R}\n\\mathcal{X} \\equiv  \\mathbb{R}\n\n\nf(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}))\nf(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}))\n  \n\n\nZ = \\frac{1}{2}\nZ = \\frac{1}{2}\n\n\n\\phi()\n\\phi()\n and \n\\Phi()\n\\Phi()\n being the standard gaussian density function and cumulative distribution function respectively\n\n\nMultivariate\n\u00b6\n\n\n\\mathcal{X} \\equiv  \\mathbb{R}^d\n\\mathcal{X} \\equiv  \\mathbb{R}^d\n\n\nf(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}))\nf(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}))\n  \n\n\nZ = \\frac{1}{2}\nZ = \\frac{1}{2}\n\n\n\\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})\n\\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})\n and \n\\Phi()\n\\Phi()\n are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively and \nL\nL\n is the lower triangular Cholesky decomposition of \n\\Sigma\n\\Sigma\n.\n\n\n\n\nSkewness parameter \n\\alpha\n\\alpha\n\n\nThe parameter \n\\alpha\n\\alpha\n determines the skewness of the distribution and its sign tells us in which direction the distribution has a fatter tail. In the univariate case the parameter \n\\alpha\n\\alpha\n is a scalar, while in the multivariate case \n\\alpha \\in \\mathbb{R}^d\n\\alpha \\in \\mathbb{R}^d\n, so for the multivariate skew gaussian distribution, there is a skewness value for each dimension.\n\n\n\n\nUsage\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n//Univariate\n\n\nval\n \nmean\n \n=\n \n1.5\n\n\nval\n \nsigma\n \n=\n \n1.5\n\n\nval\n \na\n \n=\n \n-\n0.5\n\n\nval\n \nd\n \n=\n \nSkewGaussian\n(\na\n,\n \nmean\n,\n \nsigma\n)\n\n\n\n//Multivariate\n\n\nval\n \nmu\n \n=\n \nDenseVector\n.\nones\n[\nDouble\n](\n4\n)\n\n\nval\n \nalpha\n \n=\n \nDenseVector\n.\nfill\n[\nDouble\n](\n4\n)(\n1.2\n)\n\n\nval\n \ncov\n \n=\n \nDenseMatrix\n.\neye\n[\nDouble\n](\n4\n)*\n1.5\n\n\nval\n \nmd\n \n=\n \nMultivariateSkewNormal\n(\nalpha\n,\n \nmu\n,\n \ncov\n)\n\n\n\n\n\n\nExtended Skew Gaussian\n\u00b6\n\n\nUnivariate\n\u00b6\n\n\nThe generalization of the univariate skew \nGaussian\n distribution.\n\n\n\\mathcal{X} \\equiv  \\mathbb{R}\n\\mathcal{X} \\equiv  \\mathbb{R}\n\n\nf(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}) + \\tau\\sqrt{1 + \\alpha^{2}})\nf(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}) + \\tau\\sqrt{1 + \\alpha^{2}})\n  \n\n\nZ = \\Phi(\\tau)\nZ = \\Phi(\\tau)\n\n\n\\phi()\n\\phi()\n and \n\\Phi()\n\\Phi()\n being the standard gaussian density function and cumulative distribution function respectively\n\n\nMultivariate\n\u00b6\n\n\n\\mathcal{X} \\equiv  \\mathbb{R}^d\n\\mathcal{X} \\equiv  \\mathbb{R}^d\n\n\nf(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\mathbf{\\alpha}})\nf(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\mathbf{\\alpha}})\n  \n\n\nZ = \\Phi(\\tau)\nZ = \\Phi(\\tau)\n\n\n\\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})\n\\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})\n and \n\\Phi()\n\\Phi()\n are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively and \nL\nL\n is the lower triangular Cholesky decomposition of \n\\Sigma\n\\Sigma\n.\n\n\nUsage\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n//Univariate\n\n\nval\n \nmean\n \n=\n \n1.5\n\n\nval\n \nsigma\n \n=\n \n1.5\n\n\nval\n \na\n \n=\n \n-\n0.5\n\n\nval\n \nc\n \n=\n \n0.5\n\n\nval\n \nd\n \n=\n \nExtendedSkewGaussian\n(\nc\n,\n \na\n,\n \nmean\n,\n \nsigma\n)\n\n\n\n//Multivariate\n\n\nval\n \nmu\n \n=\n \nDenseVector\n.\nones\n[\nDouble\n](\n4\n)\n\n\nval\n \nalpha\n \n=\n \nDenseVector\n.\nfill\n[\nDouble\n](\n4\n)(\n1.2\n)\n\n\nval\n \ncov\n \n=\n \nDenseMatrix\n.\neye\n[\nDouble\n](\n4\n)*\n1.5\n\n\nval\n \ntau\n \n=\n \n0.2\n\n\nval\n \nmd\n \n=\n \nExtendedMultivariateSkewNormal\n(\ntau\n,\n \nalpha\n,\n \nmu\n,\n \ncov\n)\n\n\n\n\n\n\n\n\nConfusing Nomenclature\n\n\nThe following distribution has a very similar form and name to the \nextended skew gaussian\n distribution shown above. But despite its deceptively similar formula, it is a very different object.\n\n\nWe use the name MESN to denote the variant below instead of its expanded form.\n\n\n\n\nMESN\n\u00b6\n\n\nThe  \nMultivariate Extended Skew Normal\n or MESN distribution was formulated by \nAdcock and Schutes\n. It is given by\n\n\n\\mathcal{X} \\equiv  \\mathbb{R}^d\n\\mathcal{X} \\equiv  \\mathbb{R}^d\n\n\nf(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu} + \\mathbf{\\alpha}\\tau, {\\Sigma} + \\mathbf{\\alpha}\\mathbf{\\alpha}^\\intercal) \\Phi\\left(\\frac{\\mathbf{\\alpha}^{\\intercal} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau}{\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\Sigma^{-1}\\mathbf{\\alpha}}}\\right)\nf(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu} + \\mathbf{\\alpha}\\tau, {\\Sigma} + \\mathbf{\\alpha}\\mathbf{\\alpha}^\\intercal) \\Phi\\left(\\frac{\\mathbf{\\alpha}^{\\intercal} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau}{\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\Sigma^{-1}\\mathbf{\\alpha}}}\\right)\n  \n\n\nZ = \\Phi(\\tau)\nZ = \\Phi(\\tau)\n\n\n\\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})\n\\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})\n and \n\\Phi()\n\\Phi()\n are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively.\n\n\nUsage\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n//Univariate\n\n\nval\n \nmean\n \n=\n \n1.5\n\n\nval\n \nsigma\n \n=\n \n1.5\n\n\nval\n \na\n \n=\n \n-\n0.5\n\n\nval\n \nc\n \n=\n \n0.5\n\n\nval\n \nd\n \n=\n \nUESN\n(\nc\n,\n \na\n,\n \nmean\n,\n \nsigma\n)\n\n\n\n//Multivariate\n\n\nval\n \nmu\n \n=\n \nDenseVector\n.\nones\n[\nDouble\n](\n4\n)\n\n\nval\n \nalpha\n \n=\n \nDenseVector\n.\nfill\n[\nDouble\n](\n4\n)(\n1.2\n)\n\n\nval\n \ncov\n \n=\n \nDenseMatrix\n.\neye\n[\nDouble\n](\n4\n)*\n1.5\n\n\nval\n \ntau\n \n=\n \n0.2\n\n\nval\n \nmd\n \n=\n \nMESN\n(\ntau\n,\n \nalpha\n,\n \nmu\n,\n \ncov\n)\n\n\n\n\n\n\n\n\nExtended Skew Gaussian Process\n ESGP\n\n\nThe MESN distribution is used to define the finite dimensional probabilities for the \nESGP\n process.",
            "title": "Probability Distributions"
        },
        {
            "location": "/core/core_prob_dist/#specifying-distributions",
            "text": "Every probability density function  \\rho(x) \\rho(x)  defined over some domain  x \\in \\mathcal{X} x \\in \\mathcal{X}  can be represented as  \\rho(x) = \\frac{1}{Z} f(x) \\rho(x) = \\frac{1}{Z} f(x) , where  f(x) f(x)  is the un-normalized probability weight and  Z Z  is the normalization constant. The normalization constant ensures that the density function sums to  1 1  over the whole domain  \\mathcal{X} \\mathcal{X} .",
            "title": "Specifying Distributions"
        },
        {
            "location": "/core/core_prob_dist/#describing-skewness",
            "text": "An important analytical way to create skewed distributions was described by  Azzalani et. al . It consists of four components.   A symmetric probability density  \\varphi(.) \\varphi(.)  An odd function  w(.) w(.)  A cumulative distribution function  G(.) G(.)  of some symmetric density  A cut-off parameter  \\tau \\tau    \n\\rho(x) = \\frac{1}{G(\\tau)} \\times \\varphi(x)\\times G(w(x) + \\tau)  \n\\rho(x) = \\frac{1}{G(\\tau)} \\times \\varphi(x)\\times G(w(x) + \\tau)",
            "title": "Describing Skewness"
        },
        {
            "location": "/core/core_prob_dist/#distributions-api",
            "text": "The  Density [ T ]  and  Rand [ T ]  traits form the API entry points for implementing probability distributions in breeze. In the  dynaml.probability.distributions  package, these two traits are inherited by  GenericDistribution [ T ]  which is extended by  AbstractContinuousDistr [ T ]  and  AbstractDiscreteDistr [ T ]  classes.   Distributions which can produce confidence intervals  The trait  HasErrorBars [ T ]  can be used as a mix in to provide the ability of producing error bars to distributions. To extend it, one has to implement the  confidenceInterval ( s :   Double ) :   ( T ,   T )  method.    Skewness  The  SkewSymmDistribution [ T ]  class is the generic base implementations for skew symmetric family of distributions in DynaML.",
            "title": "Distributions API"
        },
        {
            "location": "/core/core_prob_dist/#distributions-library",
            "text": "Apart from the distributions defined in the  breeze . stats . distributions , users have access to the following distributions implemented in the  dynaml . probability . distributions .",
            "title": "Distributions Library"
        },
        {
            "location": "/core/core_prob_dist/#multivariate-students-t",
            "text": "Defines a  Students' T  distribution over the domain of finite dimensional vectors.  \\mathcal{X} \\equiv  \\mathbb{R}^{n} \\mathcal{X} \\equiv  \\mathbb{R}^{n}  f(x) = \\left[1+{\\frac {1}{\\nu }}({\\mathbf {x} }-{\\boldsymbol {\\mu }})^{\\rm {T}}{\\boldsymbol {\\Sigma }}^{-1}({\\mathbf {x} }-{\\boldsymbol {\\mu }})\\right]^{-(\\nu +p)/2} f(x) = \\left[1+{\\frac {1}{\\nu }}({\\mathbf {x} }-{\\boldsymbol {\\mu }})^{\\rm {T}}{\\boldsymbol {\\Sigma }}^{-1}({\\mathbf {x} }-{\\boldsymbol {\\mu }})\\right]^{-(\\nu +p)/2}     Z = \\frac{\\Gamma \\left[(\\nu +p)/2\\right]}{\\Gamma (\\nu /2)\\nu ^{p/2}\\pi ^{p/2}\\left|{\\boldsymbol {\\Sigma }}\\right|^{1/2}} Z = \\frac{\\Gamma \\left[(\\nu +p)/2\\right]}{\\Gamma (\\nu /2)\\nu ^{p/2}\\pi ^{p/2}\\left|{\\boldsymbol {\\Sigma }}\\right|^{1/2}}  Usage : 1\n2\n3\n4 val   mu   =   2.5  val   mean   =   DenseVector ( 1.0 ,   0.0 )  val   cov   =   DenseMatrix (( 1.5 ,   0.5 ),   ( 0.5 ,   2.5 ))  val   d   =   MultivariateStudentsT ( mu ,   mean ,   cov )",
            "title": "Multivariate Students T"
        },
        {
            "location": "/core/core_prob_dist/#matrix-t",
            "text": "Defines a  Students' T  distribution over the domain of matrices.  \\mathcal{X} \\equiv  \\mathbb{R}^{n \\times p} \\mathcal{X} \\equiv  \\mathbb{R}^{n \\times p}  f(x) = \\left|{\\mathbf {I}}_{n}+{\\boldsymbol \\Sigma }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}}){\\boldsymbol \\Omega }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}})^{{{\\rm {T}}}}\\right|^{{-{\\frac {\\nu +n+p-1}{2}}}} f(x) = \\left|{\\mathbf {I}}_{n}+{\\boldsymbol \\Sigma }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}}){\\boldsymbol \\Omega }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}})^{{{\\rm {T}}}}\\right|^{{-{\\frac {\\nu +n+p-1}{2}}}}     Z = {\\frac {\\Gamma_{p}\\left({\\frac {\\nu +n+p-1}{2}}\\right)}{(\\pi )^{{\\frac {np}{2}}}\\Gamma _{p}\\left({\\frac {\\nu +p-1}{2}}\\right)}}|{\\boldsymbol \\Omega }|^{{-{\\frac {n}{2}}}}|{\\boldsymbol \\Sigma }|^{{-{\\frac {p}{2}}}} Z = {\\frac {\\Gamma_{p}\\left({\\frac {\\nu +n+p-1}{2}}\\right)}{(\\pi )^{{\\frac {np}{2}}}\\Gamma _{p}\\left({\\frac {\\nu +p-1}{2}}\\right)}}|{\\boldsymbol \\Omega }|^{{-{\\frac {n}{2}}}}|{\\boldsymbol \\Sigma }|^{{-{\\frac {p}{2}}}}  Usage : 1\n2\n3\n4\n5 val   mu   =   2.5  val   mean   =   DenseMatrix ((- 1.5 ,   - 0.5 ),   ( 3.5 ,   - 2.5 ))  val   cov_rows   =   DenseMatrix (( 1.5 ,   0.5 ),   ( 0.5 ,   2.5 ))  val   cov_cols   =   DenseMatrix (( 0.5 ,   0.1 ),   ( 0.1 ,   1.5 ))  val   d   =   MatrixT ( mu ,   mean ,   cov_rows ,   cov_cols )",
            "title": "Matrix T"
        },
        {
            "location": "/core/core_prob_dist/#matrix-normal",
            "text": "Defines a  Gaussian  distribution over the domain of matrices.  \\mathcal{X} \\equiv  \\mathbb{R}^{n \\times p} \\mathcal{X} \\equiv  \\mathbb{R}^{n \\times p}  f(x) = \\exp\\left( -\\frac{1}{2} \\, \\mathrm{tr}\\left[ \\mathbf{V}^{-1} (\\mathbf{X} - \\mathbf{M})^{T} \\mathbf{U}^{-1} (\\mathbf{X} - \\mathbf{M}) \\right] \\right) f(x) = \\exp\\left( -\\frac{1}{2} \\, \\mathrm{tr}\\left[ \\mathbf{V}^{-1} (\\mathbf{X} - \\mathbf{M})^{T} \\mathbf{U}^{-1} (\\mathbf{X} - \\mathbf{M}) \\right] \\right)     Z = (2\\pi)^{np/2} |\\mathbf{V}|^{n/2} |\\mathbf{U}|^{p/2} Z = (2\\pi)^{np/2} |\\mathbf{V}|^{n/2} |\\mathbf{U}|^{p/2}  Usage : 1\n2\n3\n4 val   mean   =   DenseMatrix ((- 1.5 ,   - 0.5 ),   ( 3.5 ,   - 2.5 ))  val   cov_rows   =   DenseMatrix (( 1.5 ,   0.5 ),   ( 0.5 ,   2.5 ))  val   cov_cols   =   DenseMatrix (( 0.5 ,   0.1 ),   ( 0.1 ,   1.5 ))  val   d   =   MatrixNormal ( mean ,   cov_rows ,   cov_cols )",
            "title": "Matrix Normal"
        },
        {
            "location": "/core/core_prob_dist/#truncated-normal",
            "text": "Defines a univariate  Gaussian  distribution that is defined in a finite domain.  \\mathcal{X} \\equiv  [a, b] \\mathcal{X} \\equiv  [a, b]  f(x) = \\begin{cases} \\phi ({\\frac {x-\\mu }{\\sigma }}) & a \\leq x \\leq b\\\\0 & else\\end{cases} f(x) = \\begin{cases} \\phi ({\\frac {x-\\mu }{\\sigma }}) & a \\leq x \\leq b\\\\0 & else\\end{cases}     Z = \\sigma \\left(\\Phi ({\\frac {b-\\mu }{\\sigma }})-\\Phi ({\\frac {a-\\mu }{\\sigma }})\\right) Z = \\sigma \\left(\\Phi ({\\frac {b-\\mu }{\\sigma }})-\\Phi ({\\frac {a-\\mu }{\\sigma }})\\right)  \\phi() \\phi()  and  \\Phi() \\Phi()  being the gaussian density function and cumulative distribution function respectively  Usage : 1\n2\n3\n4 val   mean   =   1.5  val   sigma   =   1.5  val   ( a , b )   =   (- 0.5 ,   2.5 )  val   d   =   TruncatedGaussian ( mean ,   sigma ,   a ,   b )",
            "title": "Truncated Normal"
        },
        {
            "location": "/core/core_prob_dist/#skew-gaussian",
            "text": "",
            "title": "Skew Gaussian"
        },
        {
            "location": "/core/core_prob_dist/#univariate",
            "text": "\\mathcal{X} \\equiv  \\mathbb{R} \\mathcal{X} \\equiv  \\mathbb{R}  f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma})) f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}))     Z = \\frac{1}{2} Z = \\frac{1}{2}  \\phi() \\phi()  and  \\Phi() \\Phi()  being the standard gaussian density function and cumulative distribution function respectively",
            "title": "Univariate"
        },
        {
            "location": "/core/core_prob_dist/#multivariate",
            "text": "\\mathcal{X} \\equiv  \\mathbb{R}^d \\mathcal{X} \\equiv  \\mathbb{R}^d  f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu})) f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}))     Z = \\frac{1}{2} Z = \\frac{1}{2}  \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})  and  \\Phi() \\Phi()  are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively and  L L  is the lower triangular Cholesky decomposition of  \\Sigma \\Sigma .   Skewness parameter  \\alpha \\alpha  The parameter  \\alpha \\alpha  determines the skewness of the distribution and its sign tells us in which direction the distribution has a fatter tail. In the univariate case the parameter  \\alpha \\alpha  is a scalar, while in the multivariate case  \\alpha \\in \\mathbb{R}^d \\alpha \\in \\mathbb{R}^d , so for the multivariate skew gaussian distribution, there is a skewness value for each dimension.   Usage :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 //Univariate  val   mean   =   1.5  val   sigma   =   1.5  val   a   =   - 0.5  val   d   =   SkewGaussian ( a ,   mean ,   sigma )  //Multivariate  val   mu   =   DenseVector . ones [ Double ]( 4 )  val   alpha   =   DenseVector . fill [ Double ]( 4 )( 1.2 )  val   cov   =   DenseMatrix . eye [ Double ]( 4 )* 1.5  val   md   =   MultivariateSkewNormal ( alpha ,   mu ,   cov )",
            "title": "Multivariate"
        },
        {
            "location": "/core/core_prob_dist/#extended-skew-gaussian",
            "text": "",
            "title": "Extended Skew Gaussian"
        },
        {
            "location": "/core/core_prob_dist/#univariate_1",
            "text": "The generalization of the univariate skew  Gaussian  distribution.  \\mathcal{X} \\equiv  \\mathbb{R} \\mathcal{X} \\equiv  \\mathbb{R}  f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}) + \\tau\\sqrt{1 + \\alpha^{2}}) f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}) + \\tau\\sqrt{1 + \\alpha^{2}})     Z = \\Phi(\\tau) Z = \\Phi(\\tau)  \\phi() \\phi()  and  \\Phi() \\Phi()  being the standard gaussian density function and cumulative distribution function respectively",
            "title": "Univariate"
        },
        {
            "location": "/core/core_prob_dist/#multivariate_1",
            "text": "\\mathcal{X} \\equiv  \\mathbb{R}^d \\mathcal{X} \\equiv  \\mathbb{R}^d  f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\mathbf{\\alpha}}) f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\mathbf{\\alpha}})     Z = \\Phi(\\tau) Z = \\Phi(\\tau)  \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})  and  \\Phi() \\Phi()  are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively and  L L  is the lower triangular Cholesky decomposition of  \\Sigma \\Sigma .  Usage :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 //Univariate  val   mean   =   1.5  val   sigma   =   1.5  val   a   =   - 0.5  val   c   =   0.5  val   d   =   ExtendedSkewGaussian ( c ,   a ,   mean ,   sigma )  //Multivariate  val   mu   =   DenseVector . ones [ Double ]( 4 )  val   alpha   =   DenseVector . fill [ Double ]( 4 )( 1.2 )  val   cov   =   DenseMatrix . eye [ Double ]( 4 )* 1.5  val   tau   =   0.2  val   md   =   ExtendedMultivariateSkewNormal ( tau ,   alpha ,   mu ,   cov )     Confusing Nomenclature  The following distribution has a very similar form and name to the  extended skew gaussian  distribution shown above. But despite its deceptively similar formula, it is a very different object.  We use the name MESN to denote the variant below instead of its expanded form.",
            "title": "Multivariate"
        },
        {
            "location": "/core/core_prob_dist/#mesn",
            "text": "The   Multivariate Extended Skew Normal  or MESN distribution was formulated by  Adcock and Schutes . It is given by  \\mathcal{X} \\equiv  \\mathbb{R}^d \\mathcal{X} \\equiv  \\mathbb{R}^d  f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu} + \\mathbf{\\alpha}\\tau, {\\Sigma} + \\mathbf{\\alpha}\\mathbf{\\alpha}^\\intercal) \\Phi\\left(\\frac{\\mathbf{\\alpha}^{\\intercal} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau}{\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\Sigma^{-1}\\mathbf{\\alpha}}}\\right) f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu} + \\mathbf{\\alpha}\\tau, {\\Sigma} + \\mathbf{\\alpha}\\mathbf{\\alpha}^\\intercal) \\Phi\\left(\\frac{\\mathbf{\\alpha}^{\\intercal} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau}{\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\Sigma^{-1}\\mathbf{\\alpha}}}\\right)     Z = \\Phi(\\tau) Z = \\Phi(\\tau)  \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma})  and  \\Phi() \\Phi()  are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively.  Usage :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 //Univariate  val   mean   =   1.5  val   sigma   =   1.5  val   a   =   - 0.5  val   c   =   0.5  val   d   =   UESN ( c ,   a ,   mean ,   sigma )  //Multivariate  val   mu   =   DenseVector . ones [ Double ]( 4 )  val   alpha   =   DenseVector . fill [ Double ]( 4 )( 1.2 )  val   cov   =   DenseMatrix . eye [ Double ]( 4 )* 1.5  val   tau   =   0.2  val   md   =   MESN ( tau ,   alpha ,   mu ,   cov )     Extended Skew Gaussian Process  ESGP  The MESN distribution is used to define the finite dimensional probabilities for the  ESGP  process.",
            "title": "MESN"
        },
        {
            "location": "/core/core_opt_convex/",
            "text": "Model Solvers\n\u00b6\n\n\nModel solvers are implementations which either solve for the parameters/coefficients which determine the prediction of a model. Below is a list of all model solvers currently implemented, they are all sub-classes/subtraits of the top level optimization API. Refer to the \nwiki page\n on optimizers for more details on extending the API and writing your own optimizers.\n\n\nGradient Descent\n\u00b6\n\n\nThe bread and butter of any machine learning framework, the \nGradientDescent\n class in the \ndynaml\n.\noptimization\n package provides gradient based optimization primitives for solving optimization problems of the form.\n\n\n\n\n\n\\begin{equation}\n    f(w) :=\n    \\lambda\\, R(w) +\n    \\frac1n \\sum_{k=1}^n L(w;x_k,y_k)\n    \\label{eq:regPrimal}\n    \\ .\n\\end{equation}\n\n\n\n\n\\begin{equation}\n    f(w) :=\n    \\lambda\\, R(w) +\n    \\frac1n \\sum_{k=1}^n L(w;x_k,y_k)\n    \\label{eq:regPrimal}\n    \\ .\n\\end{equation}\n\n\n\n\n\nGradients\n\u00b6\n\n\n\n\n\n\n\n\nName\n\n\nClass\n\n\nEquation\n\n\n\n\n\n\n\n\n\n\nLogistic Gradient\n\n\nLogisticGradient\n\n\nL = \\frac1n \\sum_{k=1}^n \\log(1+\\exp( -y_k w^T x_k)), y_k \\in \\{-1, +1\\}\nL = \\frac1n \\sum_{k=1}^n \\log(1+\\exp( -y_k w^T x_k)), y_k \\in \\{-1, +1\\}\n\n\n\n\n\n\nLeast Squares Gradient\n\n\nLeastSquaresGradient\n\n\nL = \\frac1n \\sum_{k=1}^n \\|w^{T} \\cdot x_k - y_k\\|^2\nL = \\frac1n \\sum_{k=1}^n \\|w^{T} \\cdot x_k - y_k\\|^2\n\n\n\n\n\n\n\n\nUpdaters\n\u00b6\n\n\n\n\n\n\n\n\nName\n\n\nClass\n\n\nEquation\n\n\n\n\n\n\n\n\n\n\nL_1\nL_1\n Updater\n\n\nL1Updater\n\n\nR = \\|\\|w\\|\\|_{1}\nR = \\|\\|w\\|\\|_{1}\n\n\n\n\n\n\nL_2\nL_2\n Updater\n\n\nSquaredL2Updater\n\n\nR = \\frac{1}{2} \\|\\|w\\|\\|^2\nR = \\frac{1}{2} \\|\\|w\\|\\|^2\n\n\n\n\n\n\nBFGS Updater\n\n\nSimpleBFGSUpdater\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nnum_points\n \n=\n \ndata\n.\nlength\n\n\nval\n \ninitial_params\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \noptimizer\n \n=\n \nnew\n \nGradientDescent\n(\n\n    \nnew\n \nLogisticGradient\n,\n\n    \nnew\n \nSquaredL2Updater\n\n\n)\n\n\nval\n \nparams\n \n=\n \noptimizer\n.\nsetRegParam\n(\n0.002\n).\noptimize\n(\n\n  \nnum_points\n,\n \ndata\n,\n \ninitial_params\n)\n\n\n\n\n\n\n\nQuasi-Newton (BFGS)\n\u00b6\n\n\nThe \nBroydon-Fletcher-Goldfarb-Shanno\n (BFGS) is a Quasi-Newton based second order optimization method. To calculate an update to the parameters, it requires calculation of the inverse \nHessian\n \n\\mathit{H}^{-1}\n\\mathit{H}^{-1}\n as well as the gradient at each iteration.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \noptimizer\n \n=\n \nQuasiNewtonOptimizer\n(\n\n  \nnew\n \nLeastSquaresGradient\n,\n\n  \nnew\n \nSimpleBFGSUpdater\n)\n\n\n\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nnum_points\n \n=\n \ndata\n.\nlength\n\n\nval\n \ninitial_params\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\n\nval\n \nparams\n \n=\n \noptimizer\n.\nsetRegParam\n(\n0.002\n).\noptimize\n(\n\n  \nnum_points\n,\n \ndata\n,\n \ninitial_params\n)\n\n\n\n\n\n\n\nRegularized Least Squares\n\u00b6\n\n\nThis subroutine solves the regularized least squares optimization problem as shown below.\n\n\n\n\n\n\\begin{equation}\n    \\min_{w} \\ \\mathcal{J}_{P}(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2\n\\end{equation}\n\n\n\n\n\\begin{equation}\n    \\min_{w} \\ \\mathcal{J}_{P}(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2\n\\end{equation}\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \nnum_dim\n \n=\n \n...\n\n\nval\n \ndesignMatrix\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \nresponse\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\n\nval\n \noptimizer\n \n=\n \nnew\n \nRegularizedLSSolver\n()\n\n\n\n\nval\n \nx\n \n=\n \noptimizer\n.\nsetRegParam\n(\n0.05\n).\noptimize\n(\n\n  \ndesignMatrix\n.\nnrow\n,\n \n(\ndesignMatrix\n,\n \nresponse\n),\n\n  \nDenseVector\n.\nones\n[\nDouble\n](\nnum_dim\n))\n\n\n\n\n\n\n\nBack propagation with Momentum\n\u00b6\n\n\nThis is the most common learning methods for supervised training of feed forward neural networks, the edge weights are adjusted using the \ngeneralized delta rule\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nval\n \ndata\n:\n \nSeq\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]\n \n=\n \n_\n\n\n\n//Input, Hidden, Output\n\n\nval\n \nnum_units_by_layer\n \n=\n \nSeq\n(\n5\n,\n \n8\n,\n \n3\n)\n\n\nval\n \nacts\n \n=\n \nSeq\n(\nVectorSigmoid\n,\n \nVectorTansig\n)\n\n\nval\n \nbreezeStackFactory\n \n=\n \nNeuralStackFactory\n(\nnum_units_by_layer\n)(\nacts\n)\n\n\n\n//Random variable which samples layer weights\n\n\nval\n \nstackInitializer\n \n=\n \nGenericFFNeuralNet\n.\ngetWeightInitializer\n(\n\n  \nnum_units_by_layer\n\n\n)\n\n\n\nval\n \nopt_backprop\n \n=\n \nnew\n \nFFBackProp\n(\nbreezeStackFactory\n)\n\n\n\nval\n \nlearned_stack\n \n=\n \nopt_backprop\n.\noptimize\n(\n\n  \ndata\n.\nlength\n,\n \ndata\n,\n\n  \nstackInitializer\n.\ndraw\n)\n\n\n\n\n\n\n\n\n\nDeprecated back propagation API\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]\n \n=\n \n...\n\n\n\nval\n \ninitParam\n \n=\n \nFFNeuralGraph\n(\nnum_inputs\n \n=\n \ndata\n.\nhead\n.\n_1\n.\nlength\n,\n\n     \nnum_outputs\n \n=\n \ndata\n.\nhead\n.\n_2\n.\nlength\n,\n\n     \nhidden_layers\n \n=\n \n1\n,\n \nList\n(\n\"logsig\"\n,\n \n\"linear\"\n),\n\n     \nList\n(\n5\n))\n\n\n\nval\n \noptimizer\n \n=\n \nnew\n \nBackPropogation\n()\n\n     \n.\nsetNumIterations\n(\n100\n)\n\n     \n.\nsetStepSize\n(\n0.01\n)\n\n\n\nval\n \nnewparams\n \n=\n \noptimizer\n.\noptimize\n(\ndata\n.\nlength\n,\n \ndata\n,\n \ninitParam\n)\n\n\n\n\n\n\n\n\n\nConjugate Gradient\n\u00b6\n\n\nThe conjugate gradient method is used to solve linear systems of the form \nAx = b\nAx = b\n where \nA\nA\n is a symmetric positive definite matrix.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nval\n \nnum_dim\n \n=\n \n...\n\n\nval\n \nA\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\nval\n \nb\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \n...\n\n\n\n///Solves A.x = b\n\n\nval\n \nx\n \n=\n \nConjugateGradient\n.\nrunCG\n(\nA\n,\n \nb\n,\n\n    \nDenseVector\n.\nones\n[\nDouble\n](\nnum_dim\n),\n\n    \nepsilon\n \n=\n \n0.005\n,\n \nMAX_ITERATIONS\n \n=\n \n50\n)\n\n\n\n\n\n\n\nDual LSSVM Solver\n\u00b6\n\n\nThe LSSVM solver solves the linear program that results from the application of the \nKarush, Kuhn Tucker\n conditions on the LSSVM optimization problem.\n\n\n\n\n\n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\end{equation}\n\n\n\n\n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\end{equation}\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\n\nval\n \nkernelMatrix\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \n...\n\n\n\nval\n \ninitParam\n \n=\n  \nDenseVector\n.\nones\n[\nDouble\n](\nnum_points\n+\n1\n)\n\n\n\nval\n \noptimizer\n \n=\n \nnew\n \nLSSVMLinearSolver\n()\n\n\n\nval\n \nalpha\n \n=\n \noptimizer\n.\noptimize\n(\nnum_points\n,\n\n    \n(\nkernelMatrix\n,\n \nDenseVector\n(\ndata\n.\nmap\n(\n_\n.\n_2\n).\ntoArray\n)),\n\n    \ninitParam\n)\n\n\n\n\n\n\n\nCommittee Model Solver\n\u00b6\n\n\nThe committee model solver aims to find the optimum values of weights applied to the predictions of a set of base models. The weights are calculated as follows.\n\n\n\n\n\n\\alpha = \\frac{C^{-1} \\overrightarrow{1}}{\\overrightarrow{1}^T C^{-1} \\overrightarrow{1}}\n\n\n\n\n\\alpha = \\frac{C^{-1} \\overrightarrow{1}}{\\overrightarrow{1}^T C^{-1} \\overrightarrow{1}}\n\n\n\n\n\nWhere \nC\nC\n is the sample correlation matrix of errors for all combinations of the base models calculated on the training data.\n\n\n1\n2\n3\n4\n5\n6\n7\nval\n \noptimizer\n=\n \nnew\n \nCommitteeModelSolver\n()\n\n\n//Data Structure containing for each training point the following couple\n\n\n//(predictions from base models as a vector, actual target)\n\n\nval\n \npredictionsTargets\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n...\n\n\nval\n \nparams\n \n=\n \noptimizer\n.\noptimize\n(\nnum_points\n,\n\n    \npredictionsTargets\n,\n\n    \nDenseVector\n.\nones\n[\nDouble\n](\nnum_of_models\n))",
            "title": "Convex"
        },
        {
            "location": "/core/core_opt_convex/#model-solvers",
            "text": "Model solvers are implementations which either solve for the parameters/coefficients which determine the prediction of a model. Below is a list of all model solvers currently implemented, they are all sub-classes/subtraits of the top level optimization API. Refer to the  wiki page  on optimizers for more details on extending the API and writing your own optimizers.",
            "title": "Model Solvers"
        },
        {
            "location": "/core/core_opt_convex/#gradient-descent",
            "text": "The bread and butter of any machine learning framework, the  GradientDescent  class in the  dynaml . optimization  package provides gradient based optimization primitives for solving optimization problems of the form.   \n\\begin{equation}\n    f(w) :=\n    \\lambda\\, R(w) +\n    \\frac1n \\sum_{k=1}^n L(w;x_k,y_k)\n    \\label{eq:regPrimal}\n    \\ .\n\\end{equation}  \n\\begin{equation}\n    f(w) :=\n    \\lambda\\, R(w) +\n    \\frac1n \\sum_{k=1}^n L(w;x_k,y_k)\n    \\label{eq:regPrimal}\n    \\ .\n\\end{equation}",
            "title": "Gradient Descent"
        },
        {
            "location": "/core/core_opt_convex/#gradients",
            "text": "Name  Class  Equation      Logistic Gradient  LogisticGradient  L = \\frac1n \\sum_{k=1}^n \\log(1+\\exp( -y_k w^T x_k)), y_k \\in \\{-1, +1\\} L = \\frac1n \\sum_{k=1}^n \\log(1+\\exp( -y_k w^T x_k)), y_k \\in \\{-1, +1\\}    Least Squares Gradient  LeastSquaresGradient  L = \\frac1n \\sum_{k=1}^n \\|w^{T} \\cdot x_k - y_k\\|^2 L = \\frac1n \\sum_{k=1}^n \\|w^{T} \\cdot x_k - y_k\\|^2",
            "title": "Gradients"
        },
        {
            "location": "/core/core_opt_convex/#updaters",
            "text": "Name  Class  Equation      L_1 L_1  Updater  L1Updater  R = \\|\\|w\\|\\|_{1} R = \\|\\|w\\|\\|_{1}    L_2 L_2  Updater  SquaredL2Updater  R = \\frac{1}{2} \\|\\|w\\|\\|^2 R = \\frac{1}{2} \\|\\|w\\|\\|^2    BFGS Updater  SimpleBFGSUpdater      1\n2\n3\n4\n5\n6\n7\n8\n9 val   data :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   num_points   =   data . length  val   initial_params :   DenseVector [ Double ]   =   ...  val   optimizer   =   new   GradientDescent ( \n     new   LogisticGradient , \n     new   SquaredL2Updater  )  val   params   =   optimizer . setRegParam ( 0.002 ). optimize ( \n   num_points ,   data ,   initial_params )",
            "title": "Updaters"
        },
        {
            "location": "/core/core_opt_convex/#quasi-newton-bfgs",
            "text": "The  Broydon-Fletcher-Goldfarb-Shanno  (BFGS) is a Quasi-Newton based second order optimization method. To calculate an update to the parameters, it requires calculation of the inverse  Hessian   \\mathit{H}^{-1} \\mathit{H}^{-1}  as well as the gradient at each iteration.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   optimizer   =   QuasiNewtonOptimizer ( \n   new   LeastSquaresGradient , \n   new   SimpleBFGSUpdater )  val   data :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   num_points   =   data . length  val   initial_params :   DenseVector [ Double ]   =   ...  val   params   =   optimizer . setRegParam ( 0.002 ). optimize ( \n   num_points ,   data ,   initial_params )",
            "title": "Quasi-Newton (BFGS)"
        },
        {
            "location": "/core/core_opt_convex/#regularized-least-squares",
            "text": "This subroutine solves the regularized least squares optimization problem as shown below.   \n\\begin{equation}\n    \\min_{w} \\ \\mathcal{J}_{P}(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2\n\\end{equation}  \n\\begin{equation}\n    \\min_{w} \\ \\mathcal{J}_{P}(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2\n\\end{equation}    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   num_dim   =   ...  val   designMatrix :   DenseMatrix [ Double ]   =   ...  val   response :   DenseVector [ Double ]   =   ...  val   optimizer   =   new   RegularizedLSSolver ()  val   x   =   optimizer . setRegParam ( 0.05 ). optimize ( \n   designMatrix . nrow ,   ( designMatrix ,   response ), \n   DenseVector . ones [ Double ]( num_dim ))",
            "title": "Regularized Least Squares"
        },
        {
            "location": "/core/core_opt_convex/#back-propagation-with-momentum",
            "text": "This is the most common learning methods for supervised training of feed forward neural networks, the edge weights are adjusted using the  generalized delta rule .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 val   data :   Seq [( DenseVector [ Double ] ,  DenseVector [ Double ])]   =   _  //Input, Hidden, Output  val   num_units_by_layer   =   Seq ( 5 ,   8 ,   3 )  val   acts   =   Seq ( VectorSigmoid ,   VectorTansig )  val   breezeStackFactory   =   NeuralStackFactory ( num_units_by_layer )( acts )  //Random variable which samples layer weights  val   stackInitializer   =   GenericFFNeuralNet . getWeightInitializer ( \n   num_units_by_layer  )  val   opt_backprop   =   new   FFBackProp ( breezeStackFactory )  val   learned_stack   =   opt_backprop . optimize ( \n   data . length ,   data , \n   stackInitializer . draw )     Deprecated back propagation API   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 val   data :   Stream [( DenseVector [ Double ] ,  DenseVector [ Double ])]   =   ...  val   initParam   =   FFNeuralGraph ( num_inputs   =   data . head . _1 . length , \n      num_outputs   =   data . head . _2 . length , \n      hidden_layers   =   1 ,   List ( \"logsig\" ,   \"linear\" ), \n      List ( 5 ))  val   optimizer   =   new   BackPropogation () \n      . setNumIterations ( 100 ) \n      . setStepSize ( 0.01 )  val   newparams   =   optimizer . optimize ( data . length ,   data ,   initParam )",
            "title": "Back propagation with Momentum"
        },
        {
            "location": "/core/core_opt_convex/#conjugate-gradient",
            "text": "The conjugate gradient method is used to solve linear systems of the form  Ax = b Ax = b  where  A A  is a symmetric positive definite matrix.  1\n2\n3\n4\n5\n6\n7\n8 val   num_dim   =   ...  val   A :   DenseMatrix [ Double ]   =   ...  val   b :   DenseVector [ Double ]   =   ...  ///Solves A.x = b  val   x   =   ConjugateGradient . runCG ( A ,   b , \n     DenseVector . ones [ Double ]( num_dim ), \n     epsilon   =   0.005 ,   MAX_ITERATIONS   =   50 )",
            "title": "Conjugate Gradient"
        },
        {
            "location": "/core/core_opt_convex/#dual-lssvm-solver",
            "text": "The LSSVM solver solves the linear program that results from the application of the  Karush, Kuhn Tucker  conditions on the LSSVM optimization problem.   \n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\end{equation}  \n\\begin{equation}\n\\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\\end{equation}    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 val   data :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   kernelMatrix :   DenseMatrix [ Double ]   =   ...  val   initParam   =    DenseVector . ones [ Double ]( num_points + 1 )  val   optimizer   =   new   LSSVMLinearSolver ()  val   alpha   =   optimizer . optimize ( num_points , \n     ( kernelMatrix ,   DenseVector ( data . map ( _ . _2 ). toArray )), \n     initParam )",
            "title": "Dual LSSVM Solver"
        },
        {
            "location": "/core/core_opt_convex/#committee-model-solver",
            "text": "The committee model solver aims to find the optimum values of weights applied to the predictions of a set of base models. The weights are calculated as follows.   \n\\alpha = \\frac{C^{-1} \\overrightarrow{1}}{\\overrightarrow{1}^T C^{-1} \\overrightarrow{1}}  \n\\alpha = \\frac{C^{-1} \\overrightarrow{1}}{\\overrightarrow{1}^T C^{-1} \\overrightarrow{1}}   Where  C C  is the sample correlation matrix of errors for all combinations of the base models calculated on the training data.  1\n2\n3\n4\n5\n6\n7 val   optimizer =   new   CommitteeModelSolver ()  //Data Structure containing for each training point the following couple  //(predictions from base models as a vector, actual target)  val   predictionsTargets :   Stream [( DenseVector [ Double ] ,  Double )]   =   ...  val   params   =   optimizer . optimize ( num_points , \n     predictionsTargets , \n     DenseVector . ones [ Double ]( num_of_models ))",
            "title": "Committee Model Solver"
        },
        {
            "location": "/core/core_opt_global/",
            "text": "Model Selection Routines\n\u00b6\n\n\nThese routines are also known as \nglobal optimizers\n, paradigms/algorithms such as genetic algorithms, gibbs sampling, simulated annealing, evolutionary optimization fall under this category. They can be used in situations when the objective function in not \"smooth\".\n\n\nIn DynaML they are most prominently used in hyper-parameter optimization in kernel based learning methods. All \nglobal optimizers\n in DynaML extend the \nGlobalOptimizer\n trait, which implies that they provide an implementation for its \noptimize\n method.\n\n\nIn order to use a global optimization routine on an model, the model implementation in question must be extending the \nGloballyOptimizable\n trait in the \ndynaml.optimization\n package, this trait has only one method called \nenergy\n which is to be implemented by all sub-classes/traits.\n\n\nThe \nenergy\n method calculates the value of the global objective function for a particular configuration i.e. for particular values of model hyper-parameters. This objective function can be defined differently for each model class (marginal likelihood for Gaussian Processes, cross validation score for parametric models, etc).\n\n\nThe following model selection routines are available in DynaML so far.\n\n\nGrid Search\n\u00b6\n\n\nThe most elementary (naive) method of model selection is to evaluate its performance (value returned by \nenergy\n) on a fixed set of grid points which are initialized for the model hyper-parameters.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nval\n \nkernel\n \n=\n \n...\n\n\nval\n \nnoise\n \n=\n \n...\n\n\nval\n \ndata\n \n=\n \n...\n\n\nval\n \nmodel\n \n=\n \nnew\n \nGPRegression\n(\nkernel\n,\n \nnoise\n,\n \ndata\n)\n\n\n\nval\n \ngrid\n \n=\n \n5\n\n\nval\n \nstep\n \n=\n \n0.2\n\n\n\nval\n \ngs\n \n=\n \nnew\n \nGridSearch\n[\nmodel.\ntype\n](\nmodel\n)\n\n    \n.\nsetGridSize\n(\ngrid\n)\n\n    \n.\nsetStepSize\n(\nstep\n)\n\n    \n.\nsetLogScale\n(\nfalse\n)\n\n\n\nval\n \nstartConf\n \n=\n \nkernel\n.\nstate\n \n++\n \nnoise\n.\nstate\n\n\nval\n \n(\n_\n,\n \nconf\n)\n \n=\n \ngs\n.\noptimize\n(\nstartConf\n,\n \nopt\n)\n\n\n\nmodel\n.\nsetState\n(\nconf\n)\n\n\n\n\n\n\n\nCoupled Simulated Annealing\n\u00b6\n\n\nCoupled Simulated Annealing\n (CSA) is an iterative search procedure which evaluates model performance on a grid and in each iteration perturbs the grid points in a randomized manner. Each perturbed point is accepted using a certain acceptance probability which is a function of the performance on the whole grid.\n\n\nCoupled Simulated Annealing can be seen as an extension to the classical Simulated Annealing algorithm, since the acceptance probability and perturbation function are design choices, we can formulate a number of variants of CSA. Any CSA-like algorithm must have the following components.\n\n\n\n\nAn ensemble or grid of points \nx_i \\in \\Theta\nx_i \\in \\Theta\n.\n\n\nA perturbation distribution or function $P: x_i \\rightarrow y_i $.\n\n\nA coupling term \n\\gamma\n\\gamma\n for an ensemble.\n\n\nAn acceptance probability function \nA_{\\Theta}(\\gamma, x_i \\rightarrow y_i)\nA_{\\Theta}(\\gamma, x_i \\rightarrow y_i)\n.\n\n\nAn \nannealing schedule\n \nT_{k}^{ac}, k = 0, 1, \\cdots\nT_{k}^{ac}, k = 0, 1, \\cdots\n.\n\n\n\n\n\nThe \nCoupledSimulatedAnnealing\n class has a companion \nobject\n with the following available variants.\n\n\n\n\n\n\n\n\n\nVariant\n\n\nAcceptance Probability\n\n\nCoupling term \n\\gamma\n\\gamma\n\n\n\n\n\n\n\n\n\n\nSA\n:  Classical \nSimulated Annealing\n\n\n1/(1 + exp(\\frac{E(y) - E(x)}{T^{ac}_{k}}))\n1/(1 + exp(\\frac{E(y) - E(x)}{T^{ac}_{k}}))\n\n\n-\n\n\n\n\n\n\nMuSA\n: \nMulti-state Simulated Annealing\n: Direct generalization of \nSimulated Annealing\n\n\nexp(-E(y_i))/(exp(-E(y_i)) + \\gamma)\nexp(-E(y_i))/(exp(-E(y_i)) + \\gamma)\n\n\n\\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})}\n\\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})}\n\n\n\n\n\n\nBA\n:  \nBlind Acceptance\n CSA\n\n\n1 - exp(-E(x_i)/T_{k}^{ac})/\\gamma\n1 - exp(-E(x_i)/T_{k}^{ac})/\\gamma\n\n\n\\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})}\n\\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})}\n\n\n\n\n\n\nM\n:  Modified CSA\n\n\nexp(E(x_i)/T_{k}^{ac})/\\gamma\nexp(E(x_i)/T_{k}^{ac})/\\gamma\n\n\n\\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})}\n\\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})}\n\n\n\n\n\n\nMwVC\n:  Modified CSA with Variance Control: Employs an \nannealing schedule\n that controls the variance of the acceptance probabilities of states\n\n\nexp(E(x_i)/T_{k}^{ac})/\\gamma\nexp(E(x_i)/T_{k}^{ac})/\\gamma\n\n\n\\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})}\n\\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})}\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nval\n \nkernel\n \n=\n \n...\n\n\nval\n \nnoise\n \n=\n \n...\n\n\nval\n \ndata\n \n=\n \n...\n\n\nval\n \nmodel\n \n=\n \nnew\n \nGPRegression\n(\nkernel\n,\n \nnoise\n,\n \ndata\n)\n\n\n\n//The default variant of CSA is Mw-VC\n\n\nval\n \ngs\n \n=\n \nnew\n \nCoupledSimulatedAnnealing\n[\nmodel.\ntype\n](\nmodel\n)\n\n    \n.\nsetGridSize\n(\ngrid\n)\n\n    \n.\nsetStepSize\n(\nstep\n)\n\n    \n.\nsetLogScale\n(\nfalse\n)\n\n    \n.\nsetVariant\n(\nCoupledSimulatedAnnealing\n.\nMuSA\n)\n\n\n\nval\n \nstartConf\n \n=\n \nkernel\n.\nstate\n \n++\n \nnoise\n.\nstate\n\n\nval\n \n(\n_\n,\n \nconf\n)\n \n=\n \ngs\n.\noptimize\n(\nstartConf\n,\n \nopt\n)\n\n\n\nmodel\n.\nsetState\n(\nconf\n)\n\n\n\n\n\n\n\nGradient based Model Selection\n\u00b6\n\n\nGradient based model selection can be used if the model fitness function implemented in the \nenergy\n method has differentiability properties (e.g. using marginal likelihood in the case of stochastic process inference). The \nGloballyOptWithGrad\n trait is an extension of \nGlobalOptimizer\n and adds a method \ngradEnergy\n that should return the gradient of the fitness function in each hyper-parameter in the form of a \nMap\n[\nString\n, \nDouble\n]\n.\n\n\nMaximum Likelihood ML-II\n\u00b6\n\n\nIn the \nMaximum Likelihood\n (ML-II) algorithm (refer to \nRamussen & Williams\n for more details), we aim to maximize the log marginal likelihood by calculating its gradient with respect to the hyper-parameters \n\\theta_j\n\\theta_j\n in each iteration and performing \nsteepest ascent\n. The calculations are summarized below.\n\n\n\n\n\n\\begin{equation}\nlog \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = - \\frac{1}{2} \\mathbf{y}^T K^{-1} \\mathbf{y} - \\frac{1}{2} log |K| - \\frac{n}{2} log 2\\pi\n\\end{equation}\n\n\n\n\n\\begin{equation}\nlog \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = - \\frac{1}{2} \\mathbf{y}^T K^{-1} \\mathbf{y} - \\frac{1}{2} log |K| - \\frac{n}{2} log 2\\pi\n\\end{equation}\n\n\n\n\n\n\n\n\n\\begin{align}\n& \\frac{\\partial }{\\partial \\theta_j} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = \\frac{1}{2} tr ((\\mathbf{\\alpha} \\mathbf{\\alpha}^T - K^{-1}) \\frac{\\partial K}{\\partial \\theta_j}) \\\\\n& \\mathbf{\\alpha} = K^{-1} \\mathbf{y}\n\\end{align}\n\n\n\n\n\\begin{align}\n& \\frac{\\partial }{\\partial \\theta_j} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = \\frac{1}{2} tr ((\\mathbf{\\alpha} \\mathbf{\\alpha}^T - K^{-1}) \\frac{\\partial K}{\\partial \\theta_j}) \\\\\n& \\mathbf{\\alpha} = K^{-1} \\mathbf{y}\n\\end{align}\n\n\n\n\n\nThe \nGPMLOptimizer\n[\nI\n, \nT\n, \nM\n]\n class implements ML-II, by using the \ngradEnergy\n method implemented by the \nsystem\n:\n \nM\n member value (which refers to a model extending  \nGloballyOptWithGrad\n).\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nval\n \nkernel\n \n=\n \n...\n\n\nval\n \nnoise\n \n=\n \n...\n\n\nval\n \ndata\n \n=\n \n...\n\n\nval\n \nmodel\n \n=\n \nnew\n \nGPRegression\n(\nkernel\n,\n \nnoise\n,\n \ndata\n)\n\n\n\nval\n \nml\n \n=\n \nnew\n \nGPMLOptimizer\n[\nDenseVector\n[\nDouble\n]\n,\n    \nSeq\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n,\n    \nGPRegression\n](\nmodel\n)\n\n\n\nval\n \nstartConf\n \n=\n \nkernel\n.\nstate\n \n++\n \nnoise\n.\nstate\n\n\nval\n \n(\n_\n,\n \nconf\n)\n \n=\n \nml\n.\noptimize\n(\nstartConf\n,\n \nopt\n)\n\n\n\nmodel\n.\nsetState\n(\nconf\n)",
            "title": "Global"
        },
        {
            "location": "/core/core_opt_global/#model-selection-routines",
            "text": "These routines are also known as  global optimizers , paradigms/algorithms such as genetic algorithms, gibbs sampling, simulated annealing, evolutionary optimization fall under this category. They can be used in situations when the objective function in not \"smooth\".  In DynaML they are most prominently used in hyper-parameter optimization in kernel based learning methods. All  global optimizers  in DynaML extend the  GlobalOptimizer  trait, which implies that they provide an implementation for its  optimize  method.  In order to use a global optimization routine on an model, the model implementation in question must be extending the  GloballyOptimizable  trait in the  dynaml.optimization  package, this trait has only one method called  energy  which is to be implemented by all sub-classes/traits.  The  energy  method calculates the value of the global objective function for a particular configuration i.e. for particular values of model hyper-parameters. This objective function can be defined differently for each model class (marginal likelihood for Gaussian Processes, cross validation score for parametric models, etc).  The following model selection routines are available in DynaML so far.",
            "title": "Model Selection Routines"
        },
        {
            "location": "/core/core_opt_global/#grid-search",
            "text": "The most elementary (naive) method of model selection is to evaluate its performance (value returned by  energy ) on a fixed set of grid points which are initialized for the model hyper-parameters.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 val   kernel   =   ...  val   noise   =   ...  val   data   =   ...  val   model   =   new   GPRegression ( kernel ,   noise ,   data )  val   grid   =   5  val   step   =   0.2  val   gs   =   new   GridSearch [ model. type ]( model ) \n     . setGridSize ( grid ) \n     . setStepSize ( step ) \n     . setLogScale ( false )  val   startConf   =   kernel . state   ++   noise . state  val   ( _ ,   conf )   =   gs . optimize ( startConf ,   opt )  model . setState ( conf )",
            "title": "Grid Search"
        },
        {
            "location": "/core/core_opt_global/#coupled-simulated-annealing",
            "text": "Coupled Simulated Annealing  (CSA) is an iterative search procedure which evaluates model performance on a grid and in each iteration perturbs the grid points in a randomized manner. Each perturbed point is accepted using a certain acceptance probability which is a function of the performance on the whole grid.  Coupled Simulated Annealing can be seen as an extension to the classical Simulated Annealing algorithm, since the acceptance probability and perturbation function are design choices, we can formulate a number of variants of CSA. Any CSA-like algorithm must have the following components.   An ensemble or grid of points  x_i \\in \\Theta x_i \\in \\Theta .  A perturbation distribution or function $P: x_i \\rightarrow y_i $.  A coupling term  \\gamma \\gamma  for an ensemble.  An acceptance probability function  A_{\\Theta}(\\gamma, x_i \\rightarrow y_i) A_{\\Theta}(\\gamma, x_i \\rightarrow y_i) .  An  annealing schedule   T_{k}^{ac}, k = 0, 1, \\cdots T_{k}^{ac}, k = 0, 1, \\cdots .   \nThe  CoupledSimulatedAnnealing  class has a companion  object  with the following available variants.     Variant  Acceptance Probability  Coupling term  \\gamma \\gamma      SA :  Classical  Simulated Annealing  1/(1 + exp(\\frac{E(y) - E(x)}{T^{ac}_{k}})) 1/(1 + exp(\\frac{E(y) - E(x)}{T^{ac}_{k}}))  -    MuSA :  Multi-state Simulated Annealing : Direct generalization of  Simulated Annealing  exp(-E(y_i))/(exp(-E(y_i)) + \\gamma) exp(-E(y_i))/(exp(-E(y_i)) + \\gamma)  \\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})} \\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})}    BA :   Blind Acceptance  CSA  1 - exp(-E(x_i)/T_{k}^{ac})/\\gamma 1 - exp(-E(x_i)/T_{k}^{ac})/\\gamma  \\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})} \\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})}    M :  Modified CSA  exp(E(x_i)/T_{k}^{ac})/\\gamma exp(E(x_i)/T_{k}^{ac})/\\gamma  \\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})} \\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})}    MwVC :  Modified CSA with Variance Control: Employs an  annealing schedule  that controls the variance of the acceptance probabilities of states  exp(E(x_i)/T_{k}^{ac})/\\gamma exp(E(x_i)/T_{k}^{ac})/\\gamma  \\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})} \\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})}      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 val   kernel   =   ...  val   noise   =   ...  val   data   =   ...  val   model   =   new   GPRegression ( kernel ,   noise ,   data )  //The default variant of CSA is Mw-VC  val   gs   =   new   CoupledSimulatedAnnealing [ model. type ]( model ) \n     . setGridSize ( grid ) \n     . setStepSize ( step ) \n     . setLogScale ( false ) \n     . setVariant ( CoupledSimulatedAnnealing . MuSA )  val   startConf   =   kernel . state   ++   noise . state  val   ( _ ,   conf )   =   gs . optimize ( startConf ,   opt )  model . setState ( conf )",
            "title": "Coupled Simulated Annealing"
        },
        {
            "location": "/core/core_opt_global/#gradient-based-model-selection",
            "text": "Gradient based model selection can be used if the model fitness function implemented in the  energy  method has differentiability properties (e.g. using marginal likelihood in the case of stochastic process inference). The  GloballyOptWithGrad  trait is an extension of  GlobalOptimizer  and adds a method  gradEnergy  that should return the gradient of the fitness function in each hyper-parameter in the form of a  Map [ String ,  Double ] .",
            "title": "Gradient based Model Selection"
        },
        {
            "location": "/core/core_opt_global/#maximum-likelihood-ml-ii",
            "text": "In the  Maximum Likelihood  (ML-II) algorithm (refer to  Ramussen & Williams  for more details), we aim to maximize the log marginal likelihood by calculating its gradient with respect to the hyper-parameters  \\theta_j \\theta_j  in each iteration and performing  steepest ascent . The calculations are summarized below.   \n\\begin{equation}\nlog \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = - \\frac{1}{2} \\mathbf{y}^T K^{-1} \\mathbf{y} - \\frac{1}{2} log |K| - \\frac{n}{2} log 2\\pi\n\\end{equation}  \n\\begin{equation}\nlog \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = - \\frac{1}{2} \\mathbf{y}^T K^{-1} \\mathbf{y} - \\frac{1}{2} log |K| - \\frac{n}{2} log 2\\pi\n\\end{equation}    \n\\begin{align}\n& \\frac{\\partial }{\\partial \\theta_j} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = \\frac{1}{2} tr ((\\mathbf{\\alpha} \\mathbf{\\alpha}^T - K^{-1}) \\frac{\\partial K}{\\partial \\theta_j}) \\\\\n& \\mathbf{\\alpha} = K^{-1} \\mathbf{y}\n\\end{align}  \n\\begin{align}\n& \\frac{\\partial }{\\partial \\theta_j} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = \\frac{1}{2} tr ((\\mathbf{\\alpha} \\mathbf{\\alpha}^T - K^{-1}) \\frac{\\partial K}{\\partial \\theta_j}) \\\\\n& \\mathbf{\\alpha} = K^{-1} \\mathbf{y}\n\\end{align}   The  GPMLOptimizer [ I ,  T ,  M ]  class implements ML-II, by using the  gradEnergy  method implemented by the  system :   M  member value (which refers to a model extending   GloballyOptWithGrad ).   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 val   kernel   =   ...  val   noise   =   ...  val   data   =   ...  val   model   =   new   GPRegression ( kernel ,   noise ,   data )  val   ml   =   new   GPMLOptimizer [ DenseVector [ Double ] ,\n     Seq [( DenseVector [ Double ] ,  Double )] ,\n     GPRegression ]( model )  val   startConf   =   kernel . state   ++   noise . state  val   ( _ ,   conf )   =   ml . optimize ( startConf ,   opt )  model . setState ( conf )",
            "title": "Maximum Likelihood ML-II"
        },
        {
            "location": "/core/core_graphics/",
            "text": "Summary\n\n\nThe \ndynaml.graphics\n \npackage is a new addition to the API since the v1.5.3 release. It aims to provide more unified access point to producing\nvisualizations.\n\n\n\n\n3D Plots\n\u00b6\n\n\nSupport for 3d visualisations is provided by the \ndynaml.graphics.plot3d\n, under the hood the \nplot3d\n object calls \nthe Jzy3d library to produce interactive 3d plots.\n\n\nProducing 3d charts involves similar procedure for each chart kind, use \nplot3d.draw()\n to generate a chart object,\nand use \nplot3d.show()\n to display it in a GUI.\n\n\nSurface Plots\n\u00b6\n\n\nThe most common usage of \nplot3d\n is to visualise 3 dimensional surfaces, this can be done in two ways.\n\n\nFrom defined functions\n\u00b6\n\n\nIf the user can express the surface as a function of two arguments.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nimport\n \nio.github.mandar2812.dynaml.graphics._\n\n\n\nval\n \nmexican_hat\n \n=\n \n  \n(\nx\n:\n \nDouble\n,\n \ny\n:\n \nDouble\n)\n \n=>\n \n    \n(\n1.0\n/\nmath\n.\nPi\n)*(\n1.0\n \n-\n \n0.5\n*(\nx\n*\nx\n \n+\n \ny\n*\ny\n))*\nmath\n.\nexp\n(-\n0.5\n*(\nx\n*\nx\n \n+\n \ny\n*\ny\n))\n\n\n\n\nval\n \nmexican_hat_chart\n \n=\n \nplot3d\n.\ndraw\n(\nmexican_hat\n)\n\n\n\nplot3d\n.\nshow\n(\nmexican_hat_chart\n)\n\n\n\n\n\n\n\n\n\nFrom a set of points\n\u00b6\n\n\nIf the surface is not determined completely, but only sampled over a discrete set of points, \nit is still possible to visualise an approximation to the surface, using \nDelauney triangulation\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nimport\n \nio.github.mandar2812.dynaml.graphics._\n\n\nimport\n \nio.github.mandar2812.dynaml.probability._\n\n\n\n//A function generating the surface need not be known,\n\n\n//as long as one has access to the sampled points.\n\n\nval\n \nfunc\n \n=\n \n(\nx\n:\n \nDouble\n,\n \ny\n:\n \nDouble\n)\n \n=>\n \n  \nmath\n.\nsin\n(\nx\n*\ny\n \n+\n \ny\n*\nx\n)\n \n-\n \nmath\n.\ncos\n(\ny\n*\ny\n*\nx\n \n-\n \nx\n*\nx\n*\ny\n)\n \n\n//Sample the 2d plane using a gaussian distribution\n\n\nval\n \nrv2d\n \n=\n \nGaussianRV\n(\n0.0\n,\n \n2.0\n)\n \n:*\n \nGaussianRV\n(\n0.0\n,\n \n2.0\n)\n\n\n//Generate some random points and their z values\n\n\nval\n \npoints\n \n=\n \nrv2d\n.\niid\n(\n1000\n).\ndraw\n.\nmap\n(\nc\n \n=>\n \n(\nc\n,\n \nfunc\n(\nc\n.\n_1\n,\n \nc\n.\n_2\n)))\n\n\n\nval\n \nplot\n \n=\n \nplot3d\n.\ndraw\n(\npoints\n)\n\n\nplot3d\n.\nshow\n(\nplot\n)\n\n\n\n\n\n\n\n\n\nHistograms\n\u00b6\n\n\nThe \nplot3d\n object also allows the user to visualise 3d histograms from collections of\npoints on the 2d plane, expressed as a collection of \n(Double, Double)\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nimport\n \nio.github.mandar2812.dynaml.graphics._\n\n\nimport\n \nio.github.mandar2812.dynaml.probability._\n\n\nimport\n \nio.github.mandar2812.dynaml.probability.distributions._\n\n\n\n//Create two different random variables to \n\n\n//sample x and y coordinates.\n\n\nval\n \nrv_x\n \n=\n \nRandomVariable\n(\nnew\n \nSkewGaussian\n(\n2.0\n,\n \n0.0\n,\n \n1.0\n))\n \n\nval\n \nrv_y\n \n=\n \nRandomVariable\n(\nnew\n \nSkewGaussian\n(-\n4.0\n,\n \n0.0\n,\n \n0.5\n))\n \n\n//Sample a point on the 2d plane \n\n\nval\n \nrv\n  \n=\n \nrv_x\n \n:*\n \nrv_y\n \n\n\nval\n \nhistogram\n \n=\n \nplot3d\n.\ndraw\n(\nrv\n.\niid\n(\n2000\n).\ndraw\n,\n \n40\n)\n \n\nplot3d\n.\nshow\n(\nhistogram\n)",
            "title": "Graphics"
        },
        {
            "location": "/core/core_graphics/#3d-plots",
            "text": "Support for 3d visualisations is provided by the  dynaml.graphics.plot3d , under the hood the  plot3d  object calls \nthe Jzy3d library to produce interactive 3d plots.  Producing 3d charts involves similar procedure for each chart kind, use  plot3d.draw()  to generate a chart object,\nand use  plot3d.show()  to display it in a GUI.",
            "title": "3D Plots"
        },
        {
            "location": "/core/core_graphics/#surface-plots",
            "text": "The most common usage of  plot3d  is to visualise 3 dimensional surfaces, this can be done in two ways.",
            "title": "Surface Plots"
        },
        {
            "location": "/core/core_graphics/#from-defined-functions",
            "text": "If the user can express the surface as a function of two arguments.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 import   io.github.mandar2812.dynaml.graphics._  val   mexican_hat   =  \n   ( x :   Double ,   y :   Double )   =>  \n     ( 1.0 / math . Pi )*( 1.0   -   0.5 *( x * x   +   y * y ))* math . exp (- 0.5 *( x * x   +   y * y ))  val   mexican_hat_chart   =   plot3d . draw ( mexican_hat )  plot3d . show ( mexican_hat_chart )",
            "title": "From defined functions"
        },
        {
            "location": "/core/core_graphics/#from-a-set-of-points",
            "text": "If the surface is not determined completely, but only sampled over a discrete set of points, \nit is still possible to visualise an approximation to the surface, using  Delauney triangulation .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 import   io.github.mandar2812.dynaml.graphics._  import   io.github.mandar2812.dynaml.probability._  //A function generating the surface need not be known,  //as long as one has access to the sampled points.  val   func   =   ( x :   Double ,   y :   Double )   =>  \n   math . sin ( x * y   +   y * x )   -   math . cos ( y * y * x   -   x * x * y )   //Sample the 2d plane using a gaussian distribution  val   rv2d   =   GaussianRV ( 0.0 ,   2.0 )   :*   GaussianRV ( 0.0 ,   2.0 )  //Generate some random points and their z values  val   points   =   rv2d . iid ( 1000 ). draw . map ( c   =>   ( c ,   func ( c . _1 ,   c . _2 )))  val   plot   =   plot3d . draw ( points )  plot3d . show ( plot )",
            "title": "From a set of points"
        },
        {
            "location": "/core/core_graphics/#histograms",
            "text": "The  plot3d  object also allows the user to visualise 3d histograms from collections of\npoints on the 2d plane, expressed as a collection of  (Double, Double) .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 import   io.github.mandar2812.dynaml.graphics._  import   io.github.mandar2812.dynaml.probability._  import   io.github.mandar2812.dynaml.probability.distributions._  //Create two different random variables to   //sample x and y coordinates.  val   rv_x   =   RandomVariable ( new   SkewGaussian ( 2.0 ,   0.0 ,   1.0 ))   val   rv_y   =   RandomVariable ( new   SkewGaussian (- 4.0 ,   0.0 ,   0.5 ))   //Sample a point on the 2d plane   val   rv    =   rv_x   :*   rv_y   val   histogram   =   plot3d . draw ( rv . iid ( 2000 ). draw ,   40 )   plot3d . show ( histogram )",
            "title": "Histograms"
        },
        {
            "location": "/core/core_model_evaluation/",
            "text": "Model evaluation is the litmus test for knowing if your modeling effort is headed in the right direction and for comparing various alternative models (or hypothesis) attempting to explain a phenomenon. The \nevaluation\n package contains classes and traits to calculate performance metrics for DynaML models.\n\n\nClasses which implement model performance calculation can extend the \nMetrics\n[\nP\n]\n trait. The \nMetrics\n trait requires that its sub-classes implement three methods or behaviors.\n\n\n\n\nPrint out the performance metrics (whatever they may be) to the screen i.e. \nprint\n method.\n\n\nReturn the key performance indicators in the form of a breeze \nDenseVector\n[\nDouble\n]\n, i.e. the \nkpi\n()\n method.\n\n\n\n\nRegression Models\n\u00b6\n\n\nRegression models are generally evaluated on a few standard metrics such as \nmean square error\n, \nmean absolute error\n, \ncoefficient of determination\n (\nR^2\nR^2\n), etc. DynaML has implementations for single output and multi-output regression models.\n\n\nSingle Output\n\u00b6\n\n\nSmall Test Set\n\n\nThe \nRegressionMetrics\n class takes as input a scala list containing the predictions and actual outputs and calculates the following metrics.\n\n\n\n\nMean Absolute Error\n (mae)\n\n\nRoot Mean Square Error\n (rmse)\n\n\nCorrelation Coefficient\n (\n\\rho_{y \\hat{y}}\n\\rho_{y \\hat{y}}\n)\n\n\nCoefficient of Determination\n (\nR^2\nR^2\n)\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n//Predictions computed by any model.\n\n\nval\n \npredictionAndOutputs\n:\n \nList\n[(\nDouble\n, \nDouble\n)]\n \n=\n \n...\n\n\n\nval\n \nmetrics\n \n=\n \nnew\n \nRegressionMetrics\n(\npredictionAndOutputs\n,\n \npredictionAndOutputs\n.\nlength\n)\n\n\n\n//Print results on screen\n\n\nmetrics\n.\nprint\n\n\n\n\n\n\n\nLarge Test Set\n\n\nThe \nRegressionMetricsSpark\n class takes as input an \nApache Spark\n RDD\n containing the predictions and actual outputs and calculates the same metrics as above.\n\n\n1\n2\n3\n4\n5\n6\n7\n//Predictions computed by any model.\n\n\nval\n \npredictionAndOutputs\n:\n \nRDD\n[(\nDouble\n, \nDouble\n)]\n \n=\n \n...\n\n\n\nval\n \nmetrics\n \n=\n \nnew\n \nRegressionMetricsSpark\n(\npredictionAndOutputs\n,\n \npredictionAndOutputs\n.\nlength\n)\n\n\n\n//Print results on screen\n\n\nmetrics\n.\nprint\n\n\n\n\n\n\n\nMultiple Outputs\n\u00b6\n\n\nThe \nMultiRegressionMetrics\n class calculates regression performance for multi-output models.\n\n\n1\n2\n3\n4\n5\n6\n7\n//Predictions computed by any model.\n\n\nval\n \npredictionAndOutputs\n:\n \nList\n[(\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])]\n \n=\n \n...\n\n\n\nval\n \nmetrics\n \n=\n \nnew\n \nMultiRegressionMetrics\n(\npredictionAndOutputs\n,\n \npredictionAndOutputs\n.\nlength\n)\n\n\n\n//Print results on screen\n\n\nmetrics\n.\nprint\n\n\n\n\n\n\n\nClassification Models\n\u00b6\n\n\nCurrently (as of v1.4) there is only a binary classification implementation for calculating model performance.\n\n\nBinary Classification\n\u00b6\n\n\nSmall Test Sets\n\n\nThe \nBinaryClassificationMetrics\n class calculates the following performance indicators.\n\n\n\n\nClassification accuracy\n\n\nF-measure\n\n\nPrecision-Recall Curve (and area under it).\n\n\nReceiver Operating Characteristic (and area under it)\n\n\nMatthew's Correlation Coefficient\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nval\n \nscoresAndLabels\n:\n \nList\n[(\nDouble\n, \nDouble\n)]\n \n=\n \n...\n\n\n\n//Set logisticFlag = true in case outputs are produced via logistic regression\n\n\nval\n \nmetrics\n \n=\n \nnew\n \nBinaryClassificationMetrics\n(\n\n          \nscoresAndLabels\n,\n\n          \nscoresAndLabels\n.\nlength\n,\n\n          \nlogisticFlag\n \n=\n \ntrue\n)\n\n\n\nmetrics\n.\nprint\n\n\n\n\n\n\n\nLarge Test Sets\n\n\nThe \nBinaryClassificationMetricsSpark\n class takes as input an \nApache Spark\n RDD\n containing the predictions and actual labels and calculates the same metrics as above.",
            "title": "Performance Evaluation"
        },
        {
            "location": "/core/core_model_evaluation/#regression-models",
            "text": "Regression models are generally evaluated on a few standard metrics such as  mean square error ,  mean absolute error ,  coefficient of determination  ( R^2 R^2 ), etc. DynaML has implementations for single output and multi-output regression models.",
            "title": "Regression Models"
        },
        {
            "location": "/core/core_model_evaluation/#single-output",
            "text": "Small Test Set  The  RegressionMetrics  class takes as input a scala list containing the predictions and actual outputs and calculates the following metrics.   Mean Absolute Error  (mae)  Root Mean Square Error  (rmse)  Correlation Coefficient  ( \\rho_{y \\hat{y}} \\rho_{y \\hat{y}} )  Coefficient of Determination  ( R^2 R^2 )   1\n2\n3\n4\n5\n6\n7 //Predictions computed by any model.  val   predictionAndOutputs :   List [( Double ,  Double )]   =   ...  val   metrics   =   new   RegressionMetrics ( predictionAndOutputs ,   predictionAndOutputs . length )  //Print results on screen  metrics . print    Large Test Set  The  RegressionMetricsSpark  class takes as input an  Apache Spark  RDD  containing the predictions and actual outputs and calculates the same metrics as above.  1\n2\n3\n4\n5\n6\n7 //Predictions computed by any model.  val   predictionAndOutputs :   RDD [( Double ,  Double )]   =   ...  val   metrics   =   new   RegressionMetricsSpark ( predictionAndOutputs ,   predictionAndOutputs . length )  //Print results on screen  metrics . print",
            "title": "Single Output"
        },
        {
            "location": "/core/core_model_evaluation/#multiple-outputs",
            "text": "The  MultiRegressionMetrics  class calculates regression performance for multi-output models.  1\n2\n3\n4\n5\n6\n7 //Predictions computed by any model.  val   predictionAndOutputs :   List [( DenseVector [ Double ] ,  DenseVector [ Double ])]   =   ...  val   metrics   =   new   MultiRegressionMetrics ( predictionAndOutputs ,   predictionAndOutputs . length )  //Print results on screen  metrics . print",
            "title": "Multiple Outputs"
        },
        {
            "location": "/core/core_model_evaluation/#classification-models",
            "text": "Currently (as of v1.4) there is only a binary classification implementation for calculating model performance.",
            "title": "Classification Models"
        },
        {
            "location": "/core/core_model_evaluation/#binary-classification",
            "text": "Small Test Sets  The  BinaryClassificationMetrics  class calculates the following performance indicators.   Classification accuracy  F-measure  Precision-Recall Curve (and area under it).  Receiver Operating Characteristic (and area under it)  Matthew's Correlation Coefficient   1\n2\n3\n4\n5\n6\n7\n8\n9 val   scoresAndLabels :   List [( Double ,  Double )]   =   ...  //Set logisticFlag = true in case outputs are produced via logistic regression  val   metrics   =   new   BinaryClassificationMetrics ( \n           scoresAndLabels , \n           scoresAndLabels . length , \n           logisticFlag   =   true )  metrics . print    Large Test Sets  The  BinaryClassificationMetricsSpark  class takes as input an  Apache Spark  RDD  containing the predictions and actual labels and calculates the same metrics as above.",
            "title": "Binary Classification"
        },
        {
            "location": "/pipes/pipes/",
            "text": "Summary\n\n\nIn this section we attempt to give a simple yet effective introduction to the data pipes module of DynaML.\n\n\n\n\nMotivation\n\u00b6\n\n\nMachine Learning involves operations that can be thought of as being part of different stages.\n\n\n\n\n\n\nData pre-processing\n\n\nData \nmunging\n or pre-processing is one of the most time consuming activities in the analysis   and modeling cycle, yet very few libraries do justice to this need.\n\n\n\n\n\n\nModeling: train, validation & test :\n\n\nTraining and testing models on data is a cyclical process and in the interest of keeping things manageable, it is important to separate operations in stage 1 from stage 2 and 3.\n\n\n\n\n\n\nPost processing: produce and summarize results via reports, visualizations etc.\n\n\n\n\n\n\nWhat are Data Pipes?\n\u00b6\n\n\nAt their heart data pipes in DynaML are (wrapped) Scala functions. Every machine learning workflow can be thought of as a chain of functional transformations on data. These functional transformations are applied one after another (in fancy language \ncomposed\n) to yield a result which is then suitable for modeling/training.\n\n\nCreating a Pipe\n\u00b6\n\n\nAs we mentioned earlier a DynaML pipe is nothing but a thin wrapper around a scala function. Creating a new data pipe is very easy, you just create a scala function and give it to the \n`#!scala DataPipe()\n object.\n\n\n1\n2\n3\nval\n \nfunc\n \n=\n \n(\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n)\n\n\n\nval\n \ntPipe\n \n=\n \nDataPipe\n(\nfunc\n)\n\n\n\n\n\n\n\nStacking/Composing Data Pipes\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nval\n \npre_processing\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n))\n\n\nval\n \npost_processing\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nif\n(\nx\n \n<=\n \n0.2\n)\n \n\"Y\"\n \nelse\n \n\"N\"\n)\n\n\n\n//Compose the two pipes\n\n\n//The result will be \"Y\"\n\n\nval\n \ntPipe\n \n=\n \npre_processing\n \n>\n \npost_processing\n\n\n\ntPipe\n(\n15.5\n)\n\n\n\n\n\n\n\n\n\nTip\n\n\nIt is possible to create a pipe from any scala type to another, inclucing \nUnit\n. For example the statement \nval\n \np\n \n=\n \nDataPipe\n(()\n \n=>\n \nscala\n.\nRandom\n.\nnextGaussian\n())\n creates a pipe which when executed samples from a univariate gaussian distribution \nval\n \nsample\n \n=\n \np\n.\nrun\n()\n\n\n\n\n\n\nYou can compose or stack any number of pipes using the \n>\n character to create a composite data workflow. There is only one constraint when joining two pipes, that the destination type of the first pipe must be the same as the source type of the second pipe, in other words:\n\n\n\n\ndont put square pegs into round holes",
            "title": "DynaML Pipes"
        },
        {
            "location": "/pipes/pipes/#motivation",
            "text": "Machine Learning involves operations that can be thought of as being part of different stages.    Data pre-processing  Data  munging  or pre-processing is one of the most time consuming activities in the analysis   and modeling cycle, yet very few libraries do justice to this need.    Modeling: train, validation & test :  Training and testing models on data is a cyclical process and in the interest of keeping things manageable, it is important to separate operations in stage 1 from stage 2 and 3.    Post processing: produce and summarize results via reports, visualizations etc.",
            "title": "Motivation"
        },
        {
            "location": "/pipes/pipes/#what-are-data-pipes",
            "text": "At their heart data pipes in DynaML are (wrapped) Scala functions. Every machine learning workflow can be thought of as a chain of functional transformations on data. These functional transformations are applied one after another (in fancy language  composed ) to yield a result which is then suitable for modeling/training.",
            "title": "What are Data Pipes?"
        },
        {
            "location": "/pipes/pipes/#creating-a-pipe",
            "text": "As we mentioned earlier a DynaML pipe is nothing but a thin wrapper around a scala function. Creating a new data pipe is very easy, you just create a scala function and give it to the  `#!scala DataPipe()  object.  1\n2\n3 val   func   =   ( x :   Double )   =>   math . sin ( 2.0 * x )* math . exp (- 2.0 * x )  val   tPipe   =   DataPipe ( func )",
            "title": "Creating a Pipe"
        },
        {
            "location": "/pipes/pipes/#stackingcomposing-data-pipes",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 val   pre_processing   =   DataPipe (( x :   Double )   =>   math . sin ( 2.0 * x )* math . exp (- 2.0 * x ))  val   post_processing   =   DataPipe (( x :   Double )   =>   if ( x   <=   0.2 )   \"Y\"   else   \"N\" )  //Compose the two pipes  //The result will be \"Y\"  val   tPipe   =   pre_processing   >   post_processing  tPipe ( 15.5 )     Tip  It is possible to create a pipe from any scala type to another, inclucing  Unit . For example the statement  val   p   =   DataPipe (()   =>   scala . Random . nextGaussian ())  creates a pipe which when executed samples from a univariate gaussian distribution  val   sample   =   p . run ()    You can compose or stack any number of pipes using the  >  character to create a composite data workflow. There is only one constraint when joining two pipes, that the destination type of the first pipe must be the same as the source type of the second pipe, in other words:   dont put square pegs into round holes",
            "title": "Stacking/Composing Data Pipes"
        },
        {
            "location": "/pipes/pipes_api/",
            "text": "Base\n\u00b6\n\n\nAt the top of the pipes hierarchy is the base trait \nDataPipe[Source, Destination]\n which is a thin wrapper for a Scala function having the type \n(Source) => Destination\n. Along with that the base trait also defines how pipes are composed with each other to yield more complex workflows.\n\n\nPipes in Parallel\n\u00b6\n\n\nThe \nParallelPipe[Source1, Result1, Source2, Result2]\n trait models pipes which are attached to each other, from an implementation point of view these can be seen as data pipes taking input from \n(Source1, Source2)\n and yielding values from \n(Result1, Result2)\n. They can be created in two ways:\n\n\nBy supplying two pipes to the \nDataPipe()\n object.\n\n\n1\n2\n3\n4\n5\n6\nval\n \npipe1\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n))\n\n\nval\n \npipe2\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nif\n(\nx\n \n<=\n \n0.2\n)\n \n\"Y\"\n \nelse\n \n\"N\"\n)\n\n\n\nval\n \npipe3\n \n=\n \nDataPipe\n(\npipe1\n,\n \npipe2\n)\n\n\n//Returns (-0.013, \"N\")\n\n\npipe3\n((\n2.0\n,\n \n15.0\n))\n\n\n\n\n\n\n\nBy duplicating a single pipe using \nDynaMLPipe.duplicate\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n//Already imported in DynaML repl\n\n\n//but should be imported when using DynaML API\n\n\n//outside of its provided repl environment.\n\n\nimport\n \nio.github.mandar2812.dynaml.DynaMLPipe._\n\n\n\nval\n \npipe1\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n))\n\n\n\nval\n \npipe3\n \n=\n \nduplicate\n(\npipe1\n)\n\n\n//Returns (-0.013, -9E-14)\n\n\npipe3\n((\n2.0\n,\n \n15.0\n))\n\n\n\n\n\n\n\nDiverging Pipes\n\u00b6\n\n\nThe \nBifurcationPipe[Source, Result1, Result2]\n trait represents pipes which start from the same source and yield two result types, from an implementation point of view these can be seen as data pipes taking input from \nSource1\n and yielding values from \n(Result1, Result2)\n. They can be created in two ways:\n\n\nBy supplying a function of type \n(Source) => (Result1, Result2)\n to the \nDataPipe()\n object.\n\n\n1\n2\n3\nval\n \npipe1\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \n(\n1.0\n*\nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n),\n \nmath\n.\nexp\n(-\n2.0\n*\nx\n)))\n\n\n\npipe1\n(\n2.0\n)\n\n\n\n\n\n\n\nBy using the \nBifurcationPipe()\n object\n\n\n1\n2\n3\n4\n5\nval\n \npipe1\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n))\n\n\nval\n \npipe2\n \n=\n \nDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nif\n(\nx\n \n<=\n \n0.2\n)\n \n\"Y\"\n \nelse\n \n\"N\"\n)\n\n\n\nval\n \npipe3\n \n=\n \nBifurcationPipe\n(\npipe1\n,\n \npipe2\n)\n\n\npipe3\n(\n2.0\n)\n\n\n\n\n\n\n\nSide Effects\n\u00b6\n\n\nIn order to enable pipes which have side effects i.e. writing to disk, the \nSideEffectPipe[Source]\n trait is used. Conceptually it is a pipe taking as input a value from \nSource\n but has a return type of \nUnit\n.\n\n\nStream Processing\n\u00b6\n\n\nTo simplify writing pipes for scala streams, the \nStreamDataPipe[I, J, K]\n and its subclasses implement workflows on streams.  \n\n\nMap\n\u00b6\n\n\nMap every element of a stream.\n\n\n1\n2\n3\n4\nval\n \npipe1\n \n=\n \nStreamDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsin\n(\n2.0\n*\nx\n)*\nmath\n.\nexp\n(-\n2.0\n*\nx\n))\n\n\n\nval\n \nstr\n:\n \nStream\n[\nDouble\n]\n \n=\n \n(\n1\n \nto\n \n5\n).\nmap\n(\n_\n.\ntoDouble\n).\ntoStream\n\n\npipe1\n(\nstr\n)\n\n\n\n\n\n\n\nFilter\n\u00b6\n\n\nFilter certain elements of a stream.\n\n\n1\n2\n3\n4\nval\n \npipe1\n \n=\n \nStreamDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n \n<=\n \n2.5\n)\n\n\n\nval\n \nstr\n:\n \nStream\n[\nDouble\n]\n \n=\n \n(\n1\n \nto\n \n5\n).\nmap\n(\n_\n.\ntoDouble\n).\ntoStream\n\n\npipe1\n(\nstr\n)\n\n\n\n\n\n\n\nBifurcate stream\n\u00b6\n\n\n1\n2\n3\n4\nval\n \npipe1\n \n=\n \nStreamPartitionPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n \n<=\n \n2.5\n)\n\n\n\nval\n \nstr\n:\n \nStream\n[\nDouble\n]\n \n=\n \n(\n1\n \nto\n \n5\n).\nmap\n(\n_\n.\ntoDouble\n).\ntoStream\n\n\npipe1\n(\nstr\n)\n\n\n\n\n\n\n\nSide effect\n\u00b6\n\n\n1\n2\n3\n4\nval\n \npipe1\n \n=\n \nStreamDataPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nprintln\n(\n\"Number is: \"\n+\nx\n))\n\n\n\nval\n \nstr\n:\n \nStream\n[\nDouble\n]\n \n=\n \n(\n1\n \nto\n \n5\n).\nmap\n(\n_\n.\ntoDouble\n).\ntoStream\n\n\npipe1\n(\nstr\n)\n\n\n\n\n\n\n\n\n\nThe following API members were added in \nv1.4.1\n\n\nFlat Map\n\u00b6\n\n\nStreamFlatMapPipe\n carries out the scala flat-map operation on a stream.\n\n\n1\n2\n3\n4\nval\n \nmapFunc\n \n=\n \n(\nn\n:\n \nInt\n)\n \n=>\n \n(\n1\n \nto\n \nn\n).\nsliding\n(\n2\n).\ntoStream\n\n\nval\n \nstreamFMPipe\n \n=\n \nStreamFlatMapPipe\n(\nmapFunc\n)\n\n\n\nstreamFMPipe\n((\n1\n \nto\n \n20\n).\ntoStream\n)\n\n\n\n\n\n\n\n\n\nPipes on Spark RDDs\n\n\nIt is also possible to create pipes acting on Spark RDDs.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nval\n \nnum\n \n=\n \n20\n\n\nval\n \nsc\n:\n \nSparkContext\n \n=\n \n_\n\n\nval\n \nnumbers\n \n=\n \nsc\n.\nparallelize\n(\n1\n \nto\n \nnum\n)\n\n\nval\n \nconvPipe\n \n=\n \nRDDPipe\n((\nn\n:\n \nInt\n)\n \n=>\n \nn\n.\ntoDouble\n)\n\n\n\nval\n \nsqPipe\n \n=\n \nRDDPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n*\nx\n)\n\n\n\nval\n \nsqrtPipe\n \n=\n \nRDDPipe\n((\nx\n:\n \nDouble\n)\n \n=>\n \nmath\n.\nsqrt\n(\nx\n))\n\n\n\nval\n \nresultPipe\n \n=\n \nRDDPipe\n((\nr\n:\n \nRDD\n[\nDouble\n])\n \n=>\n \nr\n.\nreduce\n(\n_\n+\n_\n).\ntoInt\n)\n\n\n\nval\n \nnetPipeline\n \n=\n \nconvPipe\n \n>\n \nsqPipe\n \n>\n \nsqrtPipe\n \n>\n \nresultPipe\n\n\nnetPipeline\n(\nnumbers\n)\n\n\n\n\n\n\n\n\n\nAdvanced Pipes\n\u00b6\n\n\nApart from the basic capabilities offered by the \nDataPipe\n[\nSource\n, \nDestination\n]\n interface and its family, users can also work with\nmore complex workflow components some of which are shown below.\n\n\nThe advanced components of the pipes API enable two key extensions.\n\n\n\n\nData pipes which take more than one argument\n1\n.\n\n\nData pipes which take an argument and return a data pipe\n2\n\n\n\n\nData Pipe 2\n\u00b6\n\n\nDataPipe2\n[\nA\n, \nB\n, \nC\n]\n\n\narguments\n: 2 of type \nA\n and \nB\n respectively\n\n\nreturns\n: result of type \nC\n\n\n1\n2\nval\n \nf2\n:\n \n(\nA\n,\n \nB\n)\n \n=>\n \nC\n \n=\n \n_\n  \n\nval\n \npipe2\n \n=\n \nDataPipe2\n(\nf2\n)\n\n\n\n\n\n\n\nDataPipe 3\n\u00b6\n\n\nDataPipe3\n[\nA\n, \nB\n, \nC\n, \nD\n]\n\n\narguments\n: 3 of type \nA\n, \nB\n and \nC\n respectively\n\n\nreturns\n: result of type \nD\n\n\n1\n2\nval\n \nf3\n:\n \n(\nA\n,\n \nB\n,\n \nC\n)\n \n=>\n \nD\n \n=\n \n_\n  \n\nval\n \npipe3\n \n=\n \nDataPipe3\n(\nf3\n)\n\n\n\n\n\n\n\nDataPipe 4\n\u00b6\n\n\nDataPipe4\n[\nA\n, \nB\n, \nC\n, \nD\n, \nE\n]\n\n\narguments\n: 4 of type \nA\n, \nB\n, \nC\n and \nD\n respectively\n\n\nreturns\n: result of type \nE\n\n\n1\n2\nval\n \nf4\n:\n \n(\nA\n,\n \nB\n,\n \nC\n,\n \nD\n)\n \n=>\n \nE\n \n=\n \n_\n  \n\nval\n \npipe4\n \n=\n \nDataPipe4\n(\nf4\n)\n\n\n\n\n\n\n\nMeta Pipe\n\u00b6\n\n\nMetaPipe\n[\nA\n, \nB\n, \nC\n]\n\n\nTakes an argument returns a \nDataPipe\n\n\n1\n2\n3\n4\n5\n6\nval\n \nmpipe\n \n=\n \nMetaPipe\n(\n\n  \n(\nomega\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n\n  \n(\nx\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n \nmath\n.\nexp\n(-\nomega\n.\nt\n*\nx\n))\n\n\n\n//Returns a pipe which computes exp(-2pi/10 1Tx)\n\n\nval\n \nexpPipe\n \n=\n \nmpipe\n(\nDenseVector\n.\nfill\n[\nDouble\n](\n10\n)(\n2.0\n*\nmath\n.\nPi\n/\n10.0\n))\n\n\n\n\n\n\n\nMeta Pipe (2, 1)\n\u00b6\n\n\nMetaPipe21\n[\nA\n, \nB\n, \nC\n, \nD\n]\n\n\nTakes 2 arguments returns a \nDataPipe\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \npipe21\n \n=\n \nMetaPipe21\n(\n\n  \n(\nalpha\n:\n \nDouble\n,\n \nbeta\n:\n \nDouble\n)\n \n=>\n\n  \n(\nrv\n:\n \nContinuousRandomVariable\n[\nDouble\n])\n \n=>\n \n(\nrv\n*\nbeta\n)\n \n+\n \nalpha\n\n\n)\n\n\n\nval\n \nrandom_func\n \n=\n \npipe21\n(\n1.5\n,\n \n-\n0.5\n)\n\n\nval\n \nresult_rv\n \n=\n \nrandom_func\n(\nRandomVariable\n(\nGamma\n(\n1.5\n,\n \n2.5\n)))\n\n\n\n//Draw samples from resulting random variable\n\n\nresult_rv\n.\niid\n(\n500\n).\ndraw\n\n\n\n\n\n\n\nMeta Pipe (1, 2)\n\u00b6\n\n\nMetaPipe12\n[\nA\n, \nB\n, \nC\n, \nD\n]\n\n\nTakes an argument returns a \nDataPipe2\n\n\n\n\n>\n and \n>>\n operators on higher order pipes\n\n\nAlthough the \n>\n operator is defined on higher order pipes, it quickly becomes difficult\nto imagine what transformation it can be joined with. For example the \n>\n operator applied\nafter a \nMetaPipe\n[\nI\n, \nJ\n, \nK\n]\n instance would expect a \nDataPipe\n[\nDataPipe\n[\nJ\n,\nK\n]\n, \n_\n]\n\ninstance in order for the join to proceed.\n\n\nThe \n>>\n operator on the other hand has a different and easier purpose.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nval\n \nmpipe\n \n=\n \nMetaPipe\n(\n\n  \n(\nomega\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n\n  \n(\nx\n:\n \nDenseVector\n[\nDouble\n])\n \n=>\n \nmath\n.\nexp\n(-\nomega\n.\nt\n*\nx\n))\n\n\n\nval\n \nfurther_pipe\n \n=\n \nDataPipe\n((\ny\n:\n \nDouble\n)\n \n=>\n \ny\n*\ny\n \n+\n \n2.0\n)\n\n\n\n//Returns MetaPipe[DenseVector[Double], DenseVector[Double], Double]\n\n\n//Computes exp(2*omega.x) + 2.0\n\n\nval\n \nfinal_pipe\n \n=\n \nmpipe\n \n>>\n \nfurther_pipe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nso far DynaML has support till 4\u00a0\n\u21a9\n\n\n\n\n\n\nsimilar to curried functions in scala.\u00a0\n\u21a9",
            "title": "Pipes API"
        },
        {
            "location": "/pipes/pipes_api/#base",
            "text": "At the top of the pipes hierarchy is the base trait  DataPipe[Source, Destination]  which is a thin wrapper for a Scala function having the type  (Source) => Destination . Along with that the base trait also defines how pipes are composed with each other to yield more complex workflows.",
            "title": "Base"
        },
        {
            "location": "/pipes/pipes_api/#pipes-in-parallel",
            "text": "The  ParallelPipe[Source1, Result1, Source2, Result2]  trait models pipes which are attached to each other, from an implementation point of view these can be seen as data pipes taking input from  (Source1, Source2)  and yielding values from  (Result1, Result2) . They can be created in two ways:  By supplying two pipes to the  DataPipe()  object.  1\n2\n3\n4\n5\n6 val   pipe1   =   DataPipe (( x :   Double )   =>   math . sin ( 2.0 * x )* math . exp (- 2.0 * x ))  val   pipe2   =   DataPipe (( x :   Double )   =>   if ( x   <=   0.2 )   \"Y\"   else   \"N\" )  val   pipe3   =   DataPipe ( pipe1 ,   pipe2 )  //Returns (-0.013, \"N\")  pipe3 (( 2.0 ,   15.0 ))    By duplicating a single pipe using  DynaMLPipe.duplicate   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 //Already imported in DynaML repl  //but should be imported when using DynaML API  //outside of its provided repl environment.  import   io.github.mandar2812.dynaml.DynaMLPipe._  val   pipe1   =   DataPipe (( x :   Double )   =>   math . sin ( 2.0 * x )* math . exp (- 2.0 * x ))  val   pipe3   =   duplicate ( pipe1 )  //Returns (-0.013, -9E-14)  pipe3 (( 2.0 ,   15.0 ))",
            "title": "Pipes in Parallel"
        },
        {
            "location": "/pipes/pipes_api/#diverging-pipes",
            "text": "The  BifurcationPipe[Source, Result1, Result2]  trait represents pipes which start from the same source and yield two result types, from an implementation point of view these can be seen as data pipes taking input from  Source1  and yielding values from  (Result1, Result2) . They can be created in two ways:  By supplying a function of type  (Source) => (Result1, Result2)  to the  DataPipe()  object.  1\n2\n3 val   pipe1   =   DataPipe (( x :   Double )   =>   ( 1.0 * math . sin ( 2.0 * x )* math . exp (- 2.0 * x ),   math . exp (- 2.0 * x )))  pipe1 ( 2.0 )    By using the  BifurcationPipe()  object  1\n2\n3\n4\n5 val   pipe1   =   DataPipe (( x :   Double )   =>   math . sin ( 2.0 * x )* math . exp (- 2.0 * x ))  val   pipe2   =   DataPipe (( x :   Double )   =>   if ( x   <=   0.2 )   \"Y\"   else   \"N\" )  val   pipe3   =   BifurcationPipe ( pipe1 ,   pipe2 )  pipe3 ( 2.0 )",
            "title": "Diverging Pipes"
        },
        {
            "location": "/pipes/pipes_api/#side-effects",
            "text": "In order to enable pipes which have side effects i.e. writing to disk, the  SideEffectPipe[Source]  trait is used. Conceptually it is a pipe taking as input a value from  Source  but has a return type of  Unit .",
            "title": "Side Effects"
        },
        {
            "location": "/pipes/pipes_api/#stream-processing",
            "text": "To simplify writing pipes for scala streams, the  StreamDataPipe[I, J, K]  and its subclasses implement workflows on streams.",
            "title": "Stream Processing"
        },
        {
            "location": "/pipes/pipes_api/#map",
            "text": "Map every element of a stream.  1\n2\n3\n4 val   pipe1   =   StreamDataPipe (( x :   Double )   =>   math . sin ( 2.0 * x )* math . exp (- 2.0 * x ))  val   str :   Stream [ Double ]   =   ( 1   to   5 ). map ( _ . toDouble ). toStream  pipe1 ( str )",
            "title": "Map"
        },
        {
            "location": "/pipes/pipes_api/#filter",
            "text": "Filter certain elements of a stream.  1\n2\n3\n4 val   pipe1   =   StreamDataPipe (( x :   Double )   =>   x   <=   2.5 )  val   str :   Stream [ Double ]   =   ( 1   to   5 ). map ( _ . toDouble ). toStream  pipe1 ( str )",
            "title": "Filter"
        },
        {
            "location": "/pipes/pipes_api/#bifurcate-stream",
            "text": "1\n2\n3\n4 val   pipe1   =   StreamPartitionPipe (( x :   Double )   =>   x   <=   2.5 )  val   str :   Stream [ Double ]   =   ( 1   to   5 ). map ( _ . toDouble ). toStream  pipe1 ( str )",
            "title": "Bifurcate stream"
        },
        {
            "location": "/pipes/pipes_api/#side-effect",
            "text": "1\n2\n3\n4 val   pipe1   =   StreamDataPipe (( x :   Double )   =>   println ( \"Number is: \" + x ))  val   str :   Stream [ Double ]   =   ( 1   to   5 ). map ( _ . toDouble ). toStream  pipe1 ( str )     The following API members were added in  v1.4.1",
            "title": "Side effect"
        },
        {
            "location": "/pipes/pipes_api/#flat-map",
            "text": "StreamFlatMapPipe  carries out the scala flat-map operation on a stream.  1\n2\n3\n4 val   mapFunc   =   ( n :   Int )   =>   ( 1   to   n ). sliding ( 2 ). toStream  val   streamFMPipe   =   StreamFlatMapPipe ( mapFunc )  streamFMPipe (( 1   to   20 ). toStream )     Pipes on Spark RDDs  It is also possible to create pipes acting on Spark RDDs.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 val   num   =   20  val   sc :   SparkContext   =   _  val   numbers   =   sc . parallelize ( 1   to   num )  val   convPipe   =   RDDPipe (( n :   Int )   =>   n . toDouble )  val   sqPipe   =   RDDPipe (( x :   Double )   =>   x * x )  val   sqrtPipe   =   RDDPipe (( x :   Double )   =>   math . sqrt ( x ))  val   resultPipe   =   RDDPipe (( r :   RDD [ Double ])   =>   r . reduce ( _ + _ ). toInt )  val   netPipeline   =   convPipe   >   sqPipe   >   sqrtPipe   >   resultPipe  netPipeline ( numbers )",
            "title": "Flat Map"
        },
        {
            "location": "/pipes/pipes_api/#advanced-pipes",
            "text": "Apart from the basic capabilities offered by the  DataPipe [ Source ,  Destination ]  interface and its family, users can also work with\nmore complex workflow components some of which are shown below.  The advanced components of the pipes API enable two key extensions.   Data pipes which take more than one argument 1 .  Data pipes which take an argument and return a data pipe 2",
            "title": "Advanced Pipes"
        },
        {
            "location": "/pipes/pipes_api/#data-pipe-2",
            "text": "DataPipe2 [ A ,  B ,  C ]  arguments : 2 of type  A  and  B  respectively  returns : result of type  C  1\n2 val   f2 :   ( A ,   B )   =>   C   =   _    val   pipe2   =   DataPipe2 ( f2 )",
            "title": "Data Pipe 2"
        },
        {
            "location": "/pipes/pipes_api/#datapipe-3",
            "text": "DataPipe3 [ A ,  B ,  C ,  D ]  arguments : 3 of type  A ,  B  and  C  respectively  returns : result of type  D  1\n2 val   f3 :   ( A ,   B ,   C )   =>   D   =   _    val   pipe3   =   DataPipe3 ( f3 )",
            "title": "DataPipe 3"
        },
        {
            "location": "/pipes/pipes_api/#datapipe-4",
            "text": "DataPipe4 [ A ,  B ,  C ,  D ,  E ]  arguments : 4 of type  A ,  B ,  C  and  D  respectively  returns : result of type  E  1\n2 val   f4 :   ( A ,   B ,   C ,   D )   =>   E   =   _    val   pipe4   =   DataPipe4 ( f4 )",
            "title": "DataPipe 4"
        },
        {
            "location": "/pipes/pipes_api/#meta-pipe",
            "text": "MetaPipe [ A ,  B ,  C ]  Takes an argument returns a  DataPipe  1\n2\n3\n4\n5\n6 val   mpipe   =   MetaPipe ( \n   ( omega :   DenseVector [ Double ])   => \n   ( x :   DenseVector [ Double ])   =>   math . exp (- omega . t * x ))  //Returns a pipe which computes exp(-2pi/10 1Tx)  val   expPipe   =   mpipe ( DenseVector . fill [ Double ]( 10 )( 2.0 * math . Pi / 10.0 ))",
            "title": "Meta Pipe"
        },
        {
            "location": "/pipes/pipes_api/#meta-pipe-2-1",
            "text": "MetaPipe21 [ A ,  B ,  C ,  D ]  Takes 2 arguments returns a  DataPipe .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   pipe21   =   MetaPipe21 ( \n   ( alpha :   Double ,   beta :   Double )   => \n   ( rv :   ContinuousRandomVariable [ Double ])   =>   ( rv * beta )   +   alpha  )  val   random_func   =   pipe21 ( 1.5 ,   - 0.5 )  val   result_rv   =   random_func ( RandomVariable ( Gamma ( 1.5 ,   2.5 )))  //Draw samples from resulting random variable  result_rv . iid ( 500 ). draw",
            "title": "Meta Pipe (2, 1)"
        },
        {
            "location": "/pipes/pipes_api/#meta-pipe-1-2",
            "text": "MetaPipe12 [ A ,  B ,  C ,  D ]  Takes an argument returns a  DataPipe2   >  and  >>  operators on higher order pipes  Although the  >  operator is defined on higher order pipes, it quickly becomes difficult\nto imagine what transformation it can be joined with. For example the  >  operator applied\nafter a  MetaPipe [ I ,  J ,  K ]  instance would expect a  DataPipe [ DataPipe [ J , K ] ,  _ ] \ninstance in order for the join to proceed.  The  >>  operator on the other hand has a different and easier purpose.  1\n2\n3\n4\n5\n6\n7\n8\n9 val   mpipe   =   MetaPipe ( \n   ( omega :   DenseVector [ Double ])   => \n   ( x :   DenseVector [ Double ])   =>   math . exp (- omega . t * x ))  val   further_pipe   =   DataPipe (( y :   Double )   =>   y * y   +   2.0 )  //Returns MetaPipe[DenseVector[Double], DenseVector[Double], Double]  //Computes exp(2*omega.x) + 2.0  val   final_pipe   =   mpipe   >>   further_pipe         so far DynaML has support till 4\u00a0 \u21a9    similar to curried functions in scala.\u00a0 \u21a9",
            "title": "Meta Pipe (1, 2)"
        },
        {
            "location": "/pipes/pipes_scalers_encoders/",
            "text": "Summary\n\n\nThe pipes API provides a good foundation to construct data processing pipelines, in this section we show how it is extended for application to a specific application i.e. attribute scaling & transformation.  \n\n\n\n\nTransforming data attributes is an often repeated task, some examples include re-scaling values in a finite domain \n[min, max]\n[min, max]\n, gaussian centering, \nprincipal component analysis\n (PCA), \ndiscreet Haar wavelet\n (DWT) transform etc.\n\n\nThe pipes API contains traits for these tasks, they are abstract skeletons which can be extended by the user to create arbitrary feature re-scaling transformations.\n\n\nEncoders\n\u00b6\n\n\nEncoder\n[\nI\n, \nJ\n]\n are an extension of \nDataPipe\n[\nI\n, \nJ\n]\n class which has an extra value member \ni\n:\n \nDataPipe\n[\nJ\n, \nI\n]\n which represents the inverse transformation.\n\n\n\n\nNote\n\n\nEncoder\n[\nI\n, \nJ\n]\n implies a reversible, one to one transformation of the input. Mathematically this can be expressed as\n\n\n\n\n\n\\begin{align}\ng: \\mathcal{X} &\\rightarrow \\mathcal{Y} \\\\\nh: \\mathcal{Y} &\\rightarrow \\mathcal{X} \\\\\nh(g(x)) &= x \\ \\ \\ \\forall x \\in \\mathcal{X} \\\\\nh &\\equiv g^{-1}\n\\end{align}\n\n\n\n\n\\begin{align}\ng: \\mathcal{X} &\\rightarrow \\mathcal{Y} \\\\\nh: \\mathcal{Y} &\\rightarrow \\mathcal{X} \\\\\nh(g(x)) &= x \\ \\ \\ \\forall x \\in \\mathcal{X} \\\\\nh &\\equiv g^{-1}\n\\end{align}\n\n\n\n\n\n\n\nAn encoder can be created by calling the \napply\n method of the \nEncoder\n object.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n//Converts a point expressed in cartesian coordinates\n\n\n//into a point expressed in polar coordinates and vice versa.\n\n\nval\n \ncartesianToPolar\n \n=\n \nEncoder\n(\n\n  \n(\npointCart\n:\n \n(\nDouble\n,\n \nDouble\n))\n \n=>\n \n{\n\n    \nval\n \n(\nx\n,\ny\n)\n \n=\n \npointCart\n\n    \nval\n \nr\n \n=\n \nmath\n.\nsqrt\n(\nx\n*\nx\n \n+\n \ny\n*\ny\n)\n\n    \nif\n(\nr\n \n!=\n \n0.0\n)\n \n(\nr\n,\n \nmath\n.\narcsin\n(\ny\n/\nr\n))\n \nelse\n \n(\n0.0\n,\n \n0.0\n)\n\n  \n}),\n\n  \n(\npointPolar\n:\n \n(\nDouble\n,\n \nDouble\n))\n \n=>\n \n{\n\n    \nval\n \n(\nr\n,\n \ntheta\n)\n \n=\n \npointPolar\n\n    \n(\nr\n*\nmath\n.\ncos\n(\ntheta\n),\n \nr\n*\nmath\n.\nsin\n(\ntheta\n))\n\n  \n}\n\n\n)\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn the above example, we created a cartesian to polar coordinate converter by specifying the forward and reverse transformations as anonymous scala functions. But we could as well have passed the forward and reverse transforms as \nDataPipe\n instances.\n\n\n1\n2\n3\n4\nval\n \nforwardTransform\n:\n \nDataPipe\n[\nI\n, \nJ\n]\n \n=\n \n_\n\n\nval\n \nreverseTransform\n:\n \nDataPipe\n[\nJ\n, \nI\n]\n \n=\n \n_\n\n\n//Still works.\n\n\nval\n \nenc\n \n=\n \nEncoder\n(\nforwardTransform\n,\n \nreverseTransform\n)\n\n\n\n\n\n\n\n\n\nScalers\n\u00b6\n\n\nScaler\n[\nI\n]\n is an extension of the \nDataPipe\n[\nI\n, \nI\n]\n trait. Represents transformations of inputs which don't change their type.\n\n\n1\nval\n \nlinTr\n \n=\n \nScaler\n((\nx\n:\n \nDouble\n)\n \n=>\n \nx\n*\n5.0\n \n+\n \n-\n1.5\n)\n\n\n\n\n\n\n\nReversible Scalers\n\u00b6\n\n\nReversibleScaler\n[\nI\n]\n extends \nScaler\n[\nI\n]\n along with \nEncoder\n[\nI\n, \nJ\n]\n, a reversible re-scaling of inputs.\n\n\n\n\nThe \n>\n and \n*\n for scalers and encoders\n\n\nSince \nEncoder\n[\nS\n, \nD\n]\n, \nScaler\n[\nS\n]\n and \nReversibleScaler\n[\nS\n, \nD\n]\n are inherit the \nDataPipe\n trait, they can be composed with any data pipeline as usual, but there are special cases.\n\n\nIf an \nEncoder\n[\nI\n, \nJ\n]\n instance is composed with \nEncoder\n[\nJ\n, \nK\n]\n, the result is of type \nEncoder\n[\nI\n, \nK\n]\n and accordingly for \nScaler\n[\nI\n]\n and \nReversibleScaler\n[\nI\n]\n.\n\n\nThe \n*\n can be used to create cartesian products of encoders and scalers.\n\n\n1\n2\n3\nval\n \nenc1\n:\n \nEncoder\n[\nI\n, \nJ\n]\n \n=\n \n_\n\n\nval\n \nenc2\n:\n \nEncoder\n[\nK\n, \nL\n]\n \n=\n \n_\n\n\nval\n \nenc3\n:\n \nEncoder\n[(\nI\n, \nK\n)\n, \n(\nJ\n, \nL\n)]\n \n=\n \nenc1\n*\nenc2\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nCommon attribute transformations like gaussian centering, min-max scaling, etc are included in the \ndynaml\n.\nutils\n package, click \nhere\n to see their syntax.",
            "title": "Scalers & Encoders"
        },
        {
            "location": "/pipes/pipes_scalers_encoders/#encoders",
            "text": "Encoder [ I ,  J ]  are an extension of  DataPipe [ I ,  J ]  class which has an extra value member  i :   DataPipe [ J ,  I ]  which represents the inverse transformation.   Note  Encoder [ I ,  J ]  implies a reversible, one to one transformation of the input. Mathematically this can be expressed as   \n\\begin{align}\ng: \\mathcal{X} &\\rightarrow \\mathcal{Y} \\\\\nh: \\mathcal{Y} &\\rightarrow \\mathcal{X} \\\\\nh(g(x)) &= x \\ \\ \\ \\forall x \\in \\mathcal{X} \\\\\nh &\\equiv g^{-1}\n\\end{align}  \n\\begin{align}\ng: \\mathcal{X} &\\rightarrow \\mathcal{Y} \\\\\nh: \\mathcal{Y} &\\rightarrow \\mathcal{X} \\\\\nh(g(x)) &= x \\ \\ \\ \\forall x \\in \\mathcal{X} \\\\\nh &\\equiv g^{-1}\n\\end{align}    An encoder can be created by calling the  apply  method of the  Encoder  object.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 //Converts a point expressed in cartesian coordinates  //into a point expressed in polar coordinates and vice versa.  val   cartesianToPolar   =   Encoder ( \n   ( pointCart :   ( Double ,   Double ))   =>   { \n     val   ( x , y )   =   pointCart \n     val   r   =   math . sqrt ( x * x   +   y * y ) \n     if ( r   !=   0.0 )   ( r ,   math . arcsin ( y / r ))   else   ( 0.0 ,   0.0 ) \n   }), \n   ( pointPolar :   ( Double ,   Double ))   =>   { \n     val   ( r ,   theta )   =   pointPolar \n     ( r * math . cos ( theta ),   r * math . sin ( theta )) \n   }  )     Note  In the above example, we created a cartesian to polar coordinate converter by specifying the forward and reverse transformations as anonymous scala functions. But we could as well have passed the forward and reverse transforms as  DataPipe  instances.  1\n2\n3\n4 val   forwardTransform :   DataPipe [ I ,  J ]   =   _  val   reverseTransform :   DataPipe [ J ,  I ]   =   _  //Still works.  val   enc   =   Encoder ( forwardTransform ,   reverseTransform )",
            "title": "Encoders"
        },
        {
            "location": "/pipes/pipes_scalers_encoders/#scalers",
            "text": "Scaler [ I ]  is an extension of the  DataPipe [ I ,  I ]  trait. Represents transformations of inputs which don't change their type.  1 val   linTr   =   Scaler (( x :   Double )   =>   x * 5.0   +   - 1.5 )",
            "title": "Scalers"
        },
        {
            "location": "/pipes/pipes_scalers_encoders/#reversible-scalers",
            "text": "ReversibleScaler [ I ]  extends  Scaler [ I ]  along with  Encoder [ I ,  J ] , a reversible re-scaling of inputs.   The  >  and  *  for scalers and encoders  Since  Encoder [ S ,  D ] ,  Scaler [ S ]  and  ReversibleScaler [ S ,  D ]  are inherit the  DataPipe  trait, they can be composed with any data pipeline as usual, but there are special cases.  If an  Encoder [ I ,  J ]  instance is composed with  Encoder [ J ,  K ] , the result is of type  Encoder [ I ,  K ]  and accordingly for  Scaler [ I ]  and  ReversibleScaler [ I ] .  The  *  can be used to create cartesian products of encoders and scalers.  1\n2\n3 val   enc1 :   Encoder [ I ,  J ]   =   _  val   enc2 :   Encoder [ K ,  L ]   =   _  val   enc3 :   Encoder [( I ,  K ) ,  ( J ,  L )]   =   enc1 * enc2      Tip  Common attribute transformations like gaussian centering, min-max scaling, etc are included in the  dynaml . utils  package, click  here  to see their syntax.",
            "title": "Reversible Scalers"
        },
        {
            "location": "/pipes/model_pipes/",
            "text": "Summary\n\n\nModel pipes define pipelines which involve predictive models.\n\n\n\n\n\n\nNote\n\n\nThe classes described here exist in the \ndynaml.modelpipe\n package of the \ndynaml-core\n module. Although they are not strictly part of the pipes module, they are included here for clarity and continuity.  \n\n\n\n\nThe pipes module gives the user the ability to create workflows of arbitrary complexity. In order to enable end to end machine learning, we need pipelines which involve predictive models. These pipelines can be of two types.\n\n\n\n\n\n\nPipelines which take data as input and output a predictive model.\n\n\nIt is evident that the model creation itself is a common step in the data analysis workflow, therefore one needs library pipes which create machine learning models given the training data and other relevant inputs.\n\n\n\n\n\n\nPipelines which encapsulate predictive models and generate predictions for test data splits.\n\n\nOnce a model has been tuned/trained, it can be a part of a pipeline which generates predictions for previously unobserved data.\n\n\n\n\n\n\nModel Creation\n\u00b6\n\n\nAll pipelines which return predictive models as outputs extend the \nModelPipe\n trait.\n\n\nGeneralized Linear Model Pipe\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n//Pre-process data\n\n\nval\n \npre\n:\n \n(\nSource\n)\n \n=>\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n_\n\n\nval\n \nfeature_map\n:\n \n(\nDenseVector\n[\nDouble\n])\n \n=>\n \n(\nDenseVector\n[\nDouble\n])\n \n=\n \n_\n\n\n\nval\n \nglm_pipe\n \n=\n\n  \nGLMPipe\n[(\nDenseMatrix\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n])\n, \nSource\n](\n\n    \npre\n,\n \nmap\n,\n \ntask\n \n=\n \n\"regression\"\n,\n\n    \nmodelType\n \n=\n \n\"\"\n)\n\n\n\nval\n \ndataSource\n:\n \nSource\n \n=\n \n_\n\n\n\nval\n \nglm_model\n \n=\n \nglm_pipe\n(\ndataSource\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nSource\n, \nGeneralizedLinearModel\n[\nT\n]]\n\n\nResult\n: Takes as input a data of type \nSource\n and outputs a \nGeneralized Linear Model\n.\n\n\n\n\nGeneralized Least Squares Model Pipe\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n\n\nval\n \ngls_pipe2\n \n=\n \nGeneralizedLeastSquaresPipe2\n(\nkernel\n)\n\n\n\nval\n \nfeaturemap\n:\n \n(\nDenseVector\n[\nDouble\n])\n \n=>\n \n(\nDenseVector\n[\nDouble\n])\n \n=\n \n_\n\n\nval\n \ndata\n:\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n_\n\n\n\nval\n \ngls_model\n \n=\n \ngls_pipe2\n(\ndata\n,\n \nfeaturemap\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe2\n[\nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n, \nDataPipe\n[\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n]]\n, \nGeneralizedLeastSquaresModel\n]\n]\n\n\nResult\n: Takes as inputs data and a feature mapping and outputs a \nGeneralized Least Squares Model\n.\n\n\n\n\nGaussian Process Regression Model Pipe\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n//Pre-process data\n\n\nval\n \npre\n:\n \n(\nSource\n)\n \n=>\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n_\n\n\n//Declare kernel and noise\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\nval\n \nnoise\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n\nGPRegressionPipe\n(\n\n  \npre\n,\n \nkernel\n,\n \nnoise\n,\n\n  \norder\n:\n \nInt\n \n=\n \n0\n,\n \nex\n:\n \nInt\n \n=\n \n0\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nSource\n, \nM\n]\n\n\nResult\n: Takes as input data of type \nSource\n and outputs a \nGaussian Process\n regression\n model as the output.\n\n\n\n\nDual LS-SVM Model Pipe\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n//Pre-process data\n\n\nval\n \npre\n:\n \n(\nSource\n)\n \n=>\n \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n \n=\n \n_\n\n\n//Declare kernel\n\n\nval\n \nkernel\n:\n \nLocalScalarKernel\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n\nDLSSVMPipe\n(\npre\n,\n \nkernel\n,\n \ntask\n \n=\n \n\"regression\"\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nSource\n, \nDLSSVM\n]\n\n\nResult\n: Takes as input data of type \nSource\n and outputs a \nLS-SVM\n regression/classification model as the output.\n\n\n\n\nModel Prediction\n\u00b6\n\n\nPrediction pipelines encapsulate predictive models, the \nModelPredictionPipe\n class provides an expressive API for creating prediction pipelines.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n//Any model\n\n\nval\n \nmodel\n:\n \nModel\n[\nT\n, \nQ\n, \nR\n]\n \n=\n \n_\n\n\n\n//Data pre and post processing\n\n\nval\n \npreprocessing\n:\n \nDataPipe\n[\nP\n, \nQ\n]\n \n=\n \n_\n\n\nval\n \npostprocessing\n:\n \nDataPipe\n[\nR\n, \nS\n]\n \n=\n \n_\n\n\n\nval\n \nprediction_pipeline\n \n=\n \nModelPredictionPipe\n(\n\n  \npreprocessing\n,\n\n  \nmodel\n,\n\n  \npostprocessing\n)\n\n\n\n//In case no pre or post processing is done.\n\n\nval\n \nprediction_pipeline2\n \n=\n \nModelPredictionPipe\n(\nmodel\n)\n\n\n\n//Incase feature and target scaling is performed\n\n\n\nval\n \nfeatureScaling\n:\n \nReversibleScaler\n[\nQ\n]\n \n=\n \n_\n\n\nval\n \ntargetScaling\n:\n \nReversibleScaler\n[\nR\n]\n \n=\n \n_\n\n\n\nval\n \nprediction_pipeline3\n \n=\n \nModelPredictionPipe\n(\n\n  \nfeatureScaling\n,\n\n  \nmodel\n,\n\n  \ntargetScaling\n)",
            "title": "Model Pipes"
        },
        {
            "location": "/pipes/model_pipes/#model-creation",
            "text": "All pipelines which return predictive models as outputs extend the  ModelPipe  trait.",
            "title": "Model Creation"
        },
        {
            "location": "/pipes/model_pipes/#generalized-linear-model-pipe",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 //Pre-process data  val   pre :   ( Source )   =>   Stream [( DenseVector [ Double ] ,  Double )]   =   _  val   feature_map :   ( DenseVector [ Double ])   =>   ( DenseVector [ Double ])   =   _  val   glm_pipe   = \n   GLMPipe [( DenseMatrix [ Double ] ,  DenseVector [ Double ]) ,  Source ]( \n     pre ,   map ,   task   =   \"regression\" , \n     modelType   =   \"\" )  val   dataSource :   Source   =   _  val   glm_model   =   glm_pipe ( dataSource )     Type :  DataPipe [ Source ,  GeneralizedLinearModel [ T ]]  Result : Takes as input a data of type  Source  and outputs a  Generalized Linear Model .",
            "title": "Generalized Linear Model Pipe"
        },
        {
            "location": "/pipes/model_pipes/#generalized-least-squares-model-pipe",
            "text": "1\n2\n3\n4\n5\n6\n7 val   kernel :   LocalScalarKernel [ DenseVector [ Double ]]  val   gls_pipe2   =   GeneralizedLeastSquaresPipe2 ( kernel )  val   featuremap :   ( DenseVector [ Double ])   =>   ( DenseVector [ Double ])   =   _  val   data :   Stream [( DenseVector [ Double ] ,  Double )]   =   _  val   gls_model   =   gls_pipe2 ( data ,   featuremap )     Type :  DataPipe2 [ Stream [( DenseVector [ Double ] ,  Double )] ,  DataPipe [ DenseVector [ Double ] ,  DenseVector [ Double ]] ,  GeneralizedLeastSquaresModel ] ]  Result : Takes as inputs data and a feature mapping and outputs a  Generalized Least Squares Model .",
            "title": "Generalized Least Squares Model Pipe"
        },
        {
            "location": "/pipes/model_pipes/#gaussian-process-regression-model-pipe",
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 //Pre-process data  val   pre :   ( Source )   =>   Stream [( DenseVector [ Double ] ,  Double )]   =   _  //Declare kernel and noise  val   kernel :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  val   noise :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  GPRegressionPipe ( \n   pre ,   kernel ,   noise , \n   order :   Int   =   0 ,   ex :   Int   =   0 )     Type :  DataPipe [ Source ,  M ]  Result : Takes as input data of type  Source  and outputs a  Gaussian Process  regression  model as the output.",
            "title": "Gaussian Process Regression Model Pipe"
        },
        {
            "location": "/pipes/model_pipes/#dual-ls-svm-model-pipe",
            "text": "1\n2\n3\n4\n5\n6 //Pre-process data  val   pre :   ( Source )   =>   Stream [( DenseVector [ Double ] ,  Double )]   =   _  //Declare kernel  val   kernel :   LocalScalarKernel [ DenseVector [ Double ]]   =   _  DLSSVMPipe ( pre ,   kernel ,   task   =   \"regression\" )     Type :  DataPipe [ Source ,  DLSSVM ]  Result : Takes as input data of type  Source  and outputs a  LS-SVM  regression/classification model as the output.",
            "title": "Dual LS-SVM Model Pipe"
        },
        {
            "location": "/pipes/model_pipes/#model-prediction",
            "text": "Prediction pipelines encapsulate predictive models, the  ModelPredictionPipe  class provides an expressive API for creating prediction pipelines.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 //Any model  val   model :   Model [ T ,  Q ,  R ]   =   _  //Data pre and post processing  val   preprocessing :   DataPipe [ P ,  Q ]   =   _  val   postprocessing :   DataPipe [ R ,  S ]   =   _  val   prediction_pipeline   =   ModelPredictionPipe ( \n   preprocessing , \n   model , \n   postprocessing )  //In case no pre or post processing is done.  val   prediction_pipeline2   =   ModelPredictionPipe ( model )  //Incase feature and target scaling is performed  val   featureScaling :   ReversibleScaler [ Q ]   =   _  val   targetScaling :   ReversibleScaler [ R ]   =   _  val   prediction_pipeline3   =   ModelPredictionPipe ( \n   featureScaling , \n   model , \n   targetScaling )",
            "title": "Model Prediction"
        },
        {
            "location": "/pipes/file_processing/",
            "text": "Summary\n\n\nA List of data pipes useful for processing contents of data files.\n\n\n\n\nData Pre-processing\n\u00b6\n\n\nFile to Stream of Lines\n\u00b6\n\n\n1\nfileToStream\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nString\n, \nStream\n[\nString\n]]\n\n\nResult\n: Converts a text file (inputted as a file path string) into \nStream\n[\nString\n]\n   \n\n\n\n\nWrite Stream of Lines to File\n\u00b6\n\n\n1\nstreamToFile\n(\nfileName\n:\n \nString\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nUnit\n]\n\n\nResult\n: Writes a stream of lines to the file specified by \nfilePath\n\n\n\n\nDrop first line in Stream\n\u00b6\n\n\n1\ndropHead\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Drop the first element of a \nStream\n of \nString\n\n\n\n\nReplace Occurrences in of a String\n\u00b6\n\n\n1\nreplace\n(\noriginal\n,\n \nnewString\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Replace all occurrences of a regular expression or string in a \nStream\n of \nString\n with with a specified replacement string.\n\n\n\n\nReplace White Spaces\n\u00b6\n\n\n1\nreplaceWhiteSpaces\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Replace all white space characters in a stream of lines.\n\n\n\n\nRemove Trailing White Spaces\n\u00b6\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Trim white spaces from both sides of every line.\n\n\n\n\nRemove White Spaces\n\u00b6\n\n\n1\nreplaceWhiteSpaces\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Replace all white space characters in a stream of lines.\n\n\n\n\nRemove Missing Records\n\u00b6\n\n\n1\nremoveMissingLines\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Remove all lines/records which contain missing values\n\n\n\n\nCreate Train/Test splits\n\u00b6\n\n\n1\nsplitTrainingTest\n(\nnum_training\n,\n \nnum_test\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[(\nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n, \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)])\n, \n(\nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n, \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)])]\n\n\nResult\n: Extract a subset of the data into a \nTuple2\n which can be used as a training, test combo for model learning and evaluation.",
            "title": "String & File Processing"
        },
        {
            "location": "/pipes/file_processing/#data-pre-processing",
            "text": "",
            "title": "Data Pre-processing"
        },
        {
            "location": "/pipes/file_processing/#file-to-stream-of-lines",
            "text": "1 fileToStream     Type :  DataPipe [ String ,  Stream [ String ]]  Result : Converts a text file (inputted as a file path string) into  Stream [ String ]",
            "title": "File to Stream of Lines"
        },
        {
            "location": "/pipes/file_processing/#write-stream-of-lines-to-file",
            "text": "1 streamToFile ( fileName :   String )     Type :  DataPipe [ Stream [ String ] ,  Unit ]  Result : Writes a stream of lines to the file specified by  filePath",
            "title": "Write Stream of Lines to File"
        },
        {
            "location": "/pipes/file_processing/#drop-first-line-in-stream",
            "text": "1 dropHead     Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Drop the first element of a  Stream  of  String",
            "title": "Drop first line in Stream"
        },
        {
            "location": "/pipes/file_processing/#replace-occurrences-in-of-a-string",
            "text": "1 replace ( original ,   newString )     Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Replace all occurrences of a regular expression or string in a  Stream  of  String  with with a specified replacement string.",
            "title": "Replace Occurrences in of a String"
        },
        {
            "location": "/pipes/file_processing/#replace-white-spaces",
            "text": "1 replaceWhiteSpaces     Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Replace all white space characters in a stream of lines.",
            "title": "Replace White Spaces"
        },
        {
            "location": "/pipes/file_processing/#remove-trailing-white-spaces",
            "text": "Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Trim white spaces from both sides of every line.",
            "title": "Remove Trailing White Spaces"
        },
        {
            "location": "/pipes/file_processing/#remove-white-spaces",
            "text": "1 replaceWhiteSpaces     Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Replace all white space characters in a stream of lines.",
            "title": "Remove White Spaces"
        },
        {
            "location": "/pipes/file_processing/#remove-missing-records",
            "text": "1 removeMissingLines     Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Remove all lines/records which contain missing values",
            "title": "Remove Missing Records"
        },
        {
            "location": "/pipes/file_processing/#create-traintest-splits",
            "text": "1 splitTrainingTest ( num_training ,   num_test )     Type :  DataPipe [( Stream [( DenseVector [ Double ] ,  Double )] ,  Stream [( DenseVector [ Double ] ,  Double )]) ,  ( Stream [( DenseVector [ Double ] ,  Double )] ,  Stream [( DenseVector [ Double ] ,  Double )])]  Result : Extract a subset of the data into a  Tuple2  which can be used as a training, test combo for model learning and evaluation.",
            "title": "Create Train/Test splits"
        },
        {
            "location": "/pipes/feature_processing/",
            "text": "Feature Processing\n\u00b6\n\n\nExtract features and targets\n\u00b6\n\n\n1\nsplitFeaturesAndTargets\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]]\n\n\nResult\n: Take each line which is a comma separated string and extract all but the last element into a feature vector and leave the last element as the \"target\" value.\n\n\n\n\nExtract Specific Columns\n\u00b6\n\n\n1\nextractTrainingFeatures\n(\ncolumns\n,\n \nmissingVals\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[\nString\n]]\n\n\nResult\n: Extract a subset of columns from a stream of comma separated string also replace any missing value strings with the empty string.\n\n\nUsage\n: \nDynaMLPipe\n.\nextractTrainingFeatures\n(\nList\n(\n1\n,\n2\n,\n3\n),\n \nMap\n(\n1\n \n->\n \n\"N.A.\"\n,\n \n2\n \n->\n \n\"NA\"\n,\n \n3\n \n->\n \n\"na\"\n))\n\n\n\n\nGaussian Scaling of Data\n\u00b6\n\n\n1\ngaussianScaling\n\n\n\n\n\n\n\n\n\nResult\n:  Perform gaussian normalization of features & targets, on a data stream which is a of the form \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n.\n\n\n\n\nGaussian Scaling of Train/Test Splits\n\u00b6\n\n\n1\ngaussianScalingTrainTest\n\n\n\n\n\n\n\n\n\nResult\n:  Perform gaussian normalization of features & targets, on a data stream which is a \nTuple2\n of the form \n(\nStream\n(\ntraining\n \ndata\n),\n \nStream\n(\ntest\n \ndata\n))\n.\n\n\n\n\nMin-Max Scaling of Data\n\u00b6\n\n\n1\nminMaxScaling\n\n\n\n\n\n\n\n\n\nResult\n:  Perform 0-1 scaling of features & targets, on a data stream which is a of the form \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]\n.\n\n\n\n\nMin-Max Scaling of Train/Test Splits\n\u00b6\n\n\n1\nminMaxScalingTrainTest\n\n\n\n\n\n\n\n\n\nResult\n:  Perform 0-1 scaling of features & targets, on a data stream which is a \nTuple2\n of the form \n(\nStream\n(\ntraining_data\n),\n \nStream\n(\ntest_data\n))\n.",
            "title": "Feature Processing"
        },
        {
            "location": "/pipes/feature_processing/#feature-processing",
            "text": "",
            "title": "Feature Processing"
        },
        {
            "location": "/pipes/feature_processing/#extract-features-and-targets",
            "text": "1 splitFeaturesAndTargets     Type :  DataPipe [ Stream [ String ] ,  Stream [( DenseVector [ Double ] ,  Double )]]  Result : Take each line which is a comma separated string and extract all but the last element into a feature vector and leave the last element as the \"target\" value.",
            "title": "Extract features and targets"
        },
        {
            "location": "/pipes/feature_processing/#extract-specific-columns",
            "text": "1 extractTrainingFeatures ( columns ,   missingVals )     Type :  DataPipe [ Stream [ String ] ,  Stream [ String ]]  Result : Extract a subset of columns from a stream of comma separated string also replace any missing value strings with the empty string.  Usage :  DynaMLPipe . extractTrainingFeatures ( List ( 1 , 2 , 3 ),   Map ( 1   ->   \"N.A.\" ,   2   ->   \"NA\" ,   3   ->   \"na\" ))",
            "title": "Extract Specific Columns"
        },
        {
            "location": "/pipes/feature_processing/#gaussian-scaling-of-data",
            "text": "1 gaussianScaling     Result :  Perform gaussian normalization of features & targets, on a data stream which is a of the form  Stream [( DenseVector [ Double ] ,  Double )] .",
            "title": "Gaussian Scaling of Data"
        },
        {
            "location": "/pipes/feature_processing/#gaussian-scaling-of-traintest-splits",
            "text": "1 gaussianScalingTrainTest     Result :  Perform gaussian normalization of features & targets, on a data stream which is a  Tuple2  of the form  ( Stream ( training   data ),   Stream ( test   data )) .",
            "title": "Gaussian Scaling of Train/Test Splits"
        },
        {
            "location": "/pipes/feature_processing/#min-max-scaling-of-data",
            "text": "1 minMaxScaling     Result :  Perform 0-1 scaling of features & targets, on a data stream which is a of the form  Stream [( DenseVector [ Double ] ,  Double )] .",
            "title": "Min-Max Scaling of Data"
        },
        {
            "location": "/pipes/feature_processing/#min-max-scaling-of-traintest-splits",
            "text": "1 minMaxScalingTrainTest     Result :  Perform 0-1 scaling of features & targets, on a data stream which is a  Tuple2  of the form  ( Stream ( training_data ),   Stream ( test_data )) .",
            "title": "Min-Max Scaling of Train/Test Splits"
        },
        {
            "location": "/pipes/pipes_time_series/",
            "text": "Extract Data as Univariate Time Series\n\u00b6\n\n\n1\nextractTimeSeries\n(\nTfunc\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[(\nDouble\n, \nDouble\n)]]\n\n\nResult\n: This pipe assumes its input to be of the form \nYYYY,Day,Hour,Value\n. It takes as input a function (TFunc) which converts a \n(\nDouble\n,\n \nDouble\n,\n \nDouble\n)\n into a single \ntimestamp\n like value. The pipe processes its data source line by line and outputs a \nTuple2\n in the following format \n(Timestamp,Value)\n.\n\n\n\n\nExtract data as Multivariate Time Series\n\u00b6\n\n\n1\nextractTimeSeriesVec\n(\nTfunc\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[\nString\n]\n, \nStream\n[(\nDouble\n, \nDenseVector\n[\nDouble\n])]]\n\n\nResult\n: This pipe is similar to \nextractTimeSeries\n but for application in multivariate time series analysis such as nonlinear autoregressive models with exogenous inputs. The pipe processes its data source line by line and outputs a \n(\nDouble\n,\n \nDenseVector\n[\nDouble\n])\n in the following format \n(Timestamp,Values)\n.\n\n\n\n\nConstruct Time differenced Data\n\u00b6\n\n\n1\ndeltaOperation\n(\ndeltaT\n,\n \ntimelag\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[(\nDouble\n, \nDouble\n)]\n, \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]]\n\n\nResult\n: In order to generate features for auto-regressive models, one needs to construct sliding windows in time. This function takes two parameters \ndeltaT\n: the auto-regressive order and \ntimelag\n: the time lag after which the windowing is conducted. E.g Let \ndeltaT\n \n=\n \n2\n and \ntimelag\n \n=\n \n1\n This pipe will take stream data of the form \n(t, y(t))\n(t, y(t))\n and output a stream which looks like \n(t, [y(t-2), y(t-3)])\n(t, [y(t-2), y(t-3)])\n\n\n\n\nConstruct multivariate Time differenced Data\n\u00b6\n\n\n1\ndeltaOperationVec\n(\ndeltaT\n:\n \nInt\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nStream\n[(\nDouble\n, \nDouble\n)]\n, \nStream\n[(\nDenseVector\n[\nDouble\n]\n, \nDouble\n)]]\n\n\nResult\n: A variant of \ndeltaOperation\n for NARX models.\n\n\n\n\nHaar Discrete Wavelet Transform\n\u00b6\n\n\n1\nhaarWaveletFilter\n(\norder\n:\n \nInt\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe\n[\nDenseVector\n[\nDouble\n]\n, \nDenseVector\n[\nDouble\n]]\n\n\nResult\n: A Haar Discrete wavelet transform.",
            "title": "Time Series"
        },
        {
            "location": "/pipes/pipes_time_series/#extract-data-as-univariate-time-series",
            "text": "1 extractTimeSeries ( Tfunc )     Type :  DataPipe [ Stream [ String ] ,  Stream [( Double ,  Double )]]  Result : This pipe assumes its input to be of the form  YYYY,Day,Hour,Value . It takes as input a function (TFunc) which converts a  ( Double ,   Double ,   Double )  into a single  timestamp  like value. The pipe processes its data source line by line and outputs a  Tuple2  in the following format  (Timestamp,Value) .",
            "title": "Extract Data as Univariate Time Series"
        },
        {
            "location": "/pipes/pipes_time_series/#extract-data-as-multivariate-time-series",
            "text": "1 extractTimeSeriesVec ( Tfunc )     Type :  DataPipe [ Stream [ String ] ,  Stream [( Double ,  DenseVector [ Double ])]]  Result : This pipe is similar to  extractTimeSeries  but for application in multivariate time series analysis such as nonlinear autoregressive models with exogenous inputs. The pipe processes its data source line by line and outputs a  ( Double ,   DenseVector [ Double ])  in the following format  (Timestamp,Values) .",
            "title": "Extract data as Multivariate Time Series"
        },
        {
            "location": "/pipes/pipes_time_series/#construct-time-differenced-data",
            "text": "1 deltaOperation ( deltaT ,   timelag )     Type :  DataPipe [ Stream [( Double ,  Double )] ,  Stream [( DenseVector [ Double ] ,  Double )]]  Result : In order to generate features for auto-regressive models, one needs to construct sliding windows in time. This function takes two parameters  deltaT : the auto-regressive order and  timelag : the time lag after which the windowing is conducted. E.g Let  deltaT   =   2  and  timelag   =   1  This pipe will take stream data of the form  (t, y(t)) (t, y(t))  and output a stream which looks like  (t, [y(t-2), y(t-3)]) (t, [y(t-2), y(t-3)])",
            "title": "Construct Time differenced Data"
        },
        {
            "location": "/pipes/pipes_time_series/#construct-multivariate-time-differenced-data",
            "text": "1 deltaOperationVec ( deltaT :   Int )     Type :  DataPipe [ Stream [( Double ,  Double )] ,  Stream [( DenseVector [ Double ] ,  Double )]]  Result : A variant of  deltaOperation  for NARX models.",
            "title": "Construct multivariate Time differenced Data"
        },
        {
            "location": "/pipes/pipes_time_series/#haar-discrete-wavelet-transform",
            "text": "1 haarWaveletFilter ( order :   Int )     Type :  DataPipe [ DenseVector [ Double ] ,  DenseVector [ Double ]]  Result : A Haar Discrete wavelet transform.",
            "title": "Haar Discrete Wavelet Transform"
        },
        {
            "location": "/pipes/pipes_models/",
            "text": "Operations on Models\n\u00b6\n\n\nTrain a parametric model\n\u00b6\n\n\n1\n2\n3\n4\ntrainParametricModel\n[\n\n  \nG\n, \nT\n, \nQ\n, \nR\n,\n  \nS\n, \nM\n \n<:\n \nParameterizedLearner\n[\nG\n, \nT\n, \nQ\n, \nR\n, \nS\n]](\n\n  \nregParameter\n:\n \nDouble\n,\n \nstep\n:\n \nDouble\n,\n \nmaxIt\n:\n \nInt\n,\n \nmini\n:\n \nDouble\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe[M, M]\n\n\nResult\n: Takes as input a parametric model i.e. a subclass of \nParameterizedLearner[G, T, Q, R, S]\n, trains it and outputs the trained model.\n\n\n\n\nTune a model using global optimization\n\u00b6\n\n\n1\n2\n3\nmodelTuning\n[\nM\n \n<:\n \nGloballyOptWithGrad\n](\n\n  \nstartingState\n:\n \nMap\n[\nString\n, \nDouble\n],\n \nglobalOpt\n:\n \nString\n,\n\n  \ngrid\n:\n \nInt\n,\n \nstep\n:\n \nDouble\n)\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe[(S, S), (D, D)]\n\n\nResult\n: Takes as input a parametric model i.e. a subclass of \nGloballyOptimizableWithGrad\n, tunes it using a global optimization procedure \nglobalOpt\n and outputs the tuned model.",
            "title": "Workflows on Models"
        },
        {
            "location": "/pipes/pipes_models/#operations-on-models",
            "text": "",
            "title": "Operations on Models"
        },
        {
            "location": "/pipes/pipes_models/#train-a-parametric-model",
            "text": "1\n2\n3\n4 trainParametricModel [ \n   G ,  T ,  Q ,  R ,\n   S ,  M   <:   ParameterizedLearner [ G ,  T ,  Q ,  R ,  S ]]( \n   regParameter :   Double ,   step :   Double ,   maxIt :   Int ,   mini :   Double )     Type :  DataPipe[M, M]  Result : Takes as input a parametric model i.e. a subclass of  ParameterizedLearner[G, T, Q, R, S] , trains it and outputs the trained model.",
            "title": "Train a parametric model"
        },
        {
            "location": "/pipes/pipes_models/#tune-a-model-using-global-optimization",
            "text": "1\n2\n3 modelTuning [ M   <:   GloballyOptWithGrad ]( \n   startingState :   Map [ String ,  Double ],   globalOpt :   String , \n   grid :   Int ,   step :   Double )     Type :  DataPipe[(S, S), (D, D)]  Result : Takes as input a parametric model i.e. a subclass of  GloballyOptimizableWithGrad , tunes it using a global optimization procedure  globalOpt  and outputs the tuned model.",
            "title": "Tune a model using global optimization"
        },
        {
            "location": "/pipes/pipes_misc/",
            "text": "General\n\u00b6\n\n\nDuplicate a pipe\n\u00b6\n\n\n1\nduplicate\n[\nS\n, \nD\n](\npipe\n:\n \nDataPipe\n[\nS\n, \nD\n])\n\n\n\n\n\n\n\n\n\nType\n: \nDataPipe[(S, S), (D, D)]\n\n\nResult\n: Takes a base pipe and creates a parallel pipe by duplicating it.",
            "title": "Miscellaneuos"
        },
        {
            "location": "/pipes/pipes_misc/#general",
            "text": "",
            "title": "General"
        },
        {
            "location": "/pipes/pipes_misc/#duplicate-a-pipe",
            "text": "1 duplicate [ S ,  D ]( pipe :   DataPipe [ S ,  D ])     Type :  DataPipe[(S, S), (D, D)]  Result : Takes a base pipe and creates a parallel pipe by duplicating it.",
            "title": "Duplicate a pipe"
        },
        {
            "location": "/pipes/pipes_library/",
            "text": "DynaML Library Pipes\n\u00b6\n\n\nDynaML comes bundled with a set of data pipes which enable certain standard data processing tasks, they are defined in the \nDynaMLPipe\n object in the \nio.github.mandar2812.dynaml.pipes\n package and they can be invoked as \nDynaMLPipe.<pipe name>\n.\n\n\n\n\nExample\n\u00b6\n\n\nAs a simple motivating example consider the following hypothetical csv data file called \nsample.csv\n.\n\n\n1\n2\n3\na  b  c  NA  e f\nr  s  q  t  l   m\nu v w x z d\n\n\n\n\n\n\nLets say one wants to extract only the first, fourth and last columns of this file for further processing, also one is only interested in records which do not have missing values in any of the columns we want to extract. One can think of a data pipe as follows.\n\n\n\n\nReplace the erratic white space separators with a consistent separator character\n\n\nExtract a subset of the columns\n\n\nRemove the records with missing values \nNA\n\n\nWrite output to another file \nprocessedsample.csv\n with the comma character as separator\n\n\n\n\nWe can do this by 'composing' data flow pipes which achieve each of the sub tasks.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n//Import the workflow library.\n\n\nimport\n \nio.github.mandar2812.dynaml.DynaMLPipe._\n\n\n\nval\n \ncolumns\n \n=\n \nList\n(\n0\n,\n3\n,\n5\n)\n\n\nval\n \ndataPipe\n \n=\n\n  \nfileToStream\n \n>\n\n  \nreplaceWhiteSpaces\n \n>\n\n  \nextractTrainingFeatures\n(\n\n    \ncolumns\n,\n \nMap\n(\n0\n \n->\n \n\"NA\"\n,\n \n3\n \n->\n \n\"NA\"\n,\n \n5\n \n->\n \n\"NA\"\n)\n\n  \n)\n \n>\n\n  \nremoveMissingLines\n \n>\n\n  \nstreamToFile\n(\n\"processed_sample.csv\"\n)\n\n\n\nval\n \nresult\n \n=\n \ndataPipe\n(\n\"sample.csv\"\n)\n\n\n\n\n\n\n\nLets go over the code snippet piece by piece.\n\n\n\n\nFirst convert the text file to a Stream using \nfileToStream\n\n\nReplace white spaces in each line by using \nreplaceWhiteSpaces\n\n\nExtract the required columns by \nextractTrainingFeatures\n, be sure to supply it the column numbers (indexed from 0) and the missing value strings for each column to be extracted.\n\n\nRemove missing records \nremoveMissingLines\n\n\nWrite the resulting data stream to a file \nstreamToFile(\"processed_sample.csv\")",
            "title": "Pipes Example"
        },
        {
            "location": "/pipes/pipes_library/#dynaml-library-pipes",
            "text": "DynaML comes bundled with a set of data pipes which enable certain standard data processing tasks, they are defined in the  DynaMLPipe  object in the  io.github.mandar2812.dynaml.pipes  package and they can be invoked as  DynaMLPipe.<pipe name> .",
            "title": "DynaML Library Pipes"
        },
        {
            "location": "/pipes/pipes_library/#example",
            "text": "As a simple motivating example consider the following hypothetical csv data file called  sample.csv .  1\n2\n3 a  b  c  NA  e f\nr  s  q  t  l   m\nu v w x z d   Lets say one wants to extract only the first, fourth and last columns of this file for further processing, also one is only interested in records which do not have missing values in any of the columns we want to extract. One can think of a data pipe as follows.   Replace the erratic white space separators with a consistent separator character  Extract a subset of the columns  Remove the records with missing values  NA  Write output to another file  processedsample.csv  with the comma character as separator   We can do this by 'composing' data flow pipes which achieve each of the sub tasks.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 //Import the workflow library.  import   io.github.mandar2812.dynaml.DynaMLPipe._  val   columns   =   List ( 0 , 3 , 5 )  val   dataPipe   = \n   fileToStream   > \n   replaceWhiteSpaces   > \n   extractTrainingFeatures ( \n     columns ,   Map ( 0   ->   \"NA\" ,   3   ->   \"NA\" ,   5   ->   \"NA\" ) \n   )   > \n   removeMissingLines   > \n   streamToFile ( \"processed_sample.csv\" )  val   result   =   dataPipe ( \"sample.csv\" )    Lets go over the code snippet piece by piece.   First convert the text file to a Stream using  fileToStream  Replace white spaces in each line by using  replaceWhiteSpaces  Extract the required columns by  extractTrainingFeatures , be sure to supply it the column numbers (indexed from 0) and the missing value strings for each column to be extracted.  Remove missing records  removeMissingLines  Write the resulting data stream to a file  streamToFile(\"processed_sample.csv\")",
            "title": "Example"
        },
        {
            "location": "/repl-examples/p2_examples/",
            "text": "A good way to learn using DynaML's many features is to practice on some data science case studies. The \ndynaml-examples\n module contains a number of model training and testing experiments which are intended to serve as instructive material for getting comfortable with the DynaML API.\n\n\nThe example programs cover a number of categories.\n\n\n\n\nRegression\n\n\nClassification\n\n\nSystem Identification",
            "title": "Examples Module"
        },
        {
            "location": "/repl-examples/p2_boston_housing/",
            "text": "The \nHousing\n data set is a popular regression benchmarking data set hosted on the \nUCI Machine Learning Repository\n. It contains 506 records consisting of multivariate data attributes for various real estate zones and their housing price indices. The task is then to learn a regression model that can predict the price index or range.\n\n\nAttribute Information:\n\u00b6\n\n\n\n\nCRIM\n: per capita crime rate by town\n\n\nZN\n: proportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nINDUS\n: proportion of non-retail business acres per town\n\n\nCHAS\n: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n\nNOX\n: nitric oxides concentration (parts per 10 million)\n\n\nRM\n: average number of rooms per dwelling\n\n\nAGE\n: proportion of owner-occupied units built prior to 1940\n\n\nDIS\n: weighted distances to five Boston employment centres\n\n\nRAD\n: index of accessibility to radial highways\n\n\nTAX\n: full-value property-tax rate per $10,000\n\n\nPTRATIO\n: pupil-teacher ratio by town\n\n\nB\n: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n\n\nLSTAT\n: % lower status of the population\n\n\nMEDV\n: Median value of owner-occupied homes in $1000's\n\n\n\n\nModel\n\u00b6\n\n\nBelow is a GP model for predicting the \nMEDV\n\n\n\n\n\n    \\begin{align}\n        & MEDV(\\mathbf{u}) = f(\\mathbf{u}) + \\epsilon(\\mathbf{u}) \\\\\n        & f \\sim \\mathcal{GP}(m(\\mathbf{u}), K(\\mathbf{u},\\mathbf{v})) \\\\\n        & \\mathbb{E}[\\epsilon(\\mathbf{u}).\\epsilon(\\mathbf{v})] = K_{noise}(\\mathbf{u}, \\mathbf{v})\\\\\n    \\end{align}\n\n\n\n\n    \\begin{align}\n        & MEDV(\\mathbf{u}) = f(\\mathbf{u}) + \\epsilon(\\mathbf{u}) \\\\\n        & f \\sim \\mathcal{GP}(m(\\mathbf{u}), K(\\mathbf{u},\\mathbf{v})) \\\\\n        & \\mathbb{E}[\\epsilon(\\mathbf{u}).\\epsilon(\\mathbf{v})] = K_{noise}(\\mathbf{u}, \\mathbf{v})\\\\\n    \\end{align}\n\n\n\n\n\nSyntax\n\u00b6\n\n\nThe \nTestGPHousing()\n program can be run in the REPL, below is a description of each of its arguments.\n\n\nParameter | Type | Default value |Notes\n--------|-----------|-----------|------------|\nkernel | \nCovarianceFunction\n | - | The kernel function driving the GP model.\nnoise | \nCovarianceFunction\n | - | The additive noise that corrupts the values of the latent function.\ntrainFraction | \nDouble\n | 0.75 | Fraction of the data to be used for model training and hyper-parameter selection.\ncolumns | \nList\n[\nInt\n]\n | 13, 0,.., 12 | The columns to be selected for analysis (indexed from 0), first one is the target column.\ngrid| \nInt\n | 5 | The number of grid points for each hyper-parameter\n\nstep | \nDouble\n| 0.2| The space between grid points.\nglobalOpt | \nString\n | ML | The model selection procedure \n\"GS\"\n,\n \n\"CSA\"\n,\n or \n\"ML\"\n\nstepSize | \nDouble\n | 0.01 | Only relevant if \nglobalOpt\n \n=\n \n\"ML\"\n, determines step size of steepest ascent.\nmaxIt | \nInt\n | 300 | Maximum iterations for ML model selection procedure.\n\n\n1\n2\n3\n4\n5\nTestGPHousing\n(\n\n  \nkernel\n \n=\n \nnew\n \nFBMKernel\n(\n0.55\n)\n \n+\n \nnew\n \nLaplacianKernel\n(\n2.5\n),\n\n  \nnoise\n \n=\n \nnew\n \nRBFKernel\n(\n1.5\n),\n\n  \ngrid\n \n=\n \n5\n,\n \nstep\n \n=\n \n0.03\n,\n\n  \nglobalOpt\n \n=\n \n\"GS\"\n,\n \ntrainFraction\n \n=\n \n0.45\n)\n\n\n\n\n\n\n\n1\n2\n3\n16/03/03 20:45:41 INFO GridSearch: Optimum value of energy is: 278.1603309851301\nConfiguration: Map(hurst -> 0.4, beta -> 2.35, bandwidth -> 1.35)\n16/03/03 20:45:41 INFO SVMKernel$: Constructing kernel matrix.\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n16/03/03 20:45:42 INFO GPRegression: Generating error bars\n16/03/03 20:45:42 INFO RegressionMetrics: Regression Model Performance: MEDV\n16/03/03 20:45:42 INFO RegressionMetrics: ============================\n16/03/03 20:45:42 INFO RegressionMetrics: MAE: 5.800070254265218\n16/03/03 20:45:42 INFO RegressionMetrics: RMSE: 7.739266267762397\n16/03/03 20:45:42 INFO RegressionMetrics: RMSLE: 0.4150438478412412\n16/03/03 20:45:42 INFO RegressionMetrics: R^2: 0.3609909626630624\n16/03/03 20:45:42 INFO RegressionMetrics: Corr. Coefficient: 0.7633838930006132\n16/03/03 20:45:42 INFO RegressionMetrics: Model Yield: 0.7341944950376289\n16/03/03 20:45:42 INFO RegressionMetrics: Std Dev of Residuals: 6.287519509352036\n\n\n\n\n\n\nSource Code\n\u00b6\n\n\nBelow is the example program as a github gist, to view the original program in DynaML, click \nhere\n.",
            "title": "Boston Housing"
        },
        {
            "location": "/repl-examples/p2_boston_housing/#attribute-information",
            "text": "CRIM : per capita crime rate by town  ZN : proportion of residential land zoned for lots over 25,000 sq.ft.  INDUS : proportion of non-retail business acres per town  CHAS : Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)  NOX : nitric oxides concentration (parts per 10 million)  RM : average number of rooms per dwelling  AGE : proportion of owner-occupied units built prior to 1940  DIS : weighted distances to five Boston employment centres  RAD : index of accessibility to radial highways  TAX : full-value property-tax rate per $10,000  PTRATIO : pupil-teacher ratio by town  B : 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town  LSTAT : % lower status of the population  MEDV : Median value of owner-occupied homes in $1000's",
            "title": "Attribute Information:"
        },
        {
            "location": "/repl-examples/p2_boston_housing/#model",
            "text": "Below is a GP model for predicting the  MEDV   \n    \\begin{align}\n        & MEDV(\\mathbf{u}) = f(\\mathbf{u}) + \\epsilon(\\mathbf{u}) \\\\\n        & f \\sim \\mathcal{GP}(m(\\mathbf{u}), K(\\mathbf{u},\\mathbf{v})) \\\\\n        & \\mathbb{E}[\\epsilon(\\mathbf{u}).\\epsilon(\\mathbf{v})] = K_{noise}(\\mathbf{u}, \\mathbf{v})\\\\\n    \\end{align}  \n    \\begin{align}\n        & MEDV(\\mathbf{u}) = f(\\mathbf{u}) + \\epsilon(\\mathbf{u}) \\\\\n        & f \\sim \\mathcal{GP}(m(\\mathbf{u}), K(\\mathbf{u},\\mathbf{v})) \\\\\n        & \\mathbb{E}[\\epsilon(\\mathbf{u}).\\epsilon(\\mathbf{v})] = K_{noise}(\\mathbf{u}, \\mathbf{v})\\\\\n    \\end{align}",
            "title": "Model"
        },
        {
            "location": "/repl-examples/p2_boston_housing/#syntax",
            "text": "The  TestGPHousing()  program can be run in the REPL, below is a description of each of its arguments.  Parameter | Type | Default value |Notes\n--------|-----------|-----------|------------|\nkernel |  CovarianceFunction  | - | The kernel function driving the GP model.\nnoise |  CovarianceFunction  | - | The additive noise that corrupts the values of the latent function.\ntrainFraction |  Double  | 0.75 | Fraction of the data to be used for model training and hyper-parameter selection.\ncolumns |  List [ Int ]  | 13, 0,.., 12 | The columns to be selected for analysis (indexed from 0), first one is the target column.\ngrid|  Int  | 5 | The number of grid points for each hyper-parameter \nstep |  Double | 0.2| The space between grid points.\nglobalOpt |  String  | ML | The model selection procedure  \"GS\" ,   \"CSA\" ,  or  \"ML\" \nstepSize |  Double  | 0.01 | Only relevant if  globalOpt   =   \"ML\" , determines step size of steepest ascent.\nmaxIt |  Int  | 300 | Maximum iterations for ML model selection procedure.  1\n2\n3\n4\n5 TestGPHousing ( \n   kernel   =   new   FBMKernel ( 0.55 )   +   new   LaplacianKernel ( 2.5 ), \n   noise   =   new   RBFKernel ( 1.5 ), \n   grid   =   5 ,   step   =   0.03 , \n   globalOpt   =   \"GS\" ,   trainFraction   =   0.45 )    1\n2\n3 16/03/03 20:45:41 INFO GridSearch: Optimum value of energy is: 278.1603309851301\nConfiguration: Map(hurst -> 0.4, beta -> 2.35, bandwidth -> 1.35)\n16/03/03 20:45:41 INFO SVMKernel$: Constructing kernel matrix.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 16/03/03 20:45:42 INFO GPRegression: Generating error bars\n16/03/03 20:45:42 INFO RegressionMetrics: Regression Model Performance: MEDV\n16/03/03 20:45:42 INFO RegressionMetrics: ============================\n16/03/03 20:45:42 INFO RegressionMetrics: MAE: 5.800070254265218\n16/03/03 20:45:42 INFO RegressionMetrics: RMSE: 7.739266267762397\n16/03/03 20:45:42 INFO RegressionMetrics: RMSLE: 0.4150438478412412\n16/03/03 20:45:42 INFO RegressionMetrics: R^2: 0.3609909626630624\n16/03/03 20:45:42 INFO RegressionMetrics: Corr. Coefficient: 0.7633838930006132\n16/03/03 20:45:42 INFO RegressionMetrics: Model Yield: 0.7341944950376289\n16/03/03 20:45:42 INFO RegressionMetrics: Std Dev of Residuals: 6.287519509352036",
            "title": "Syntax"
        },
        {
            "location": "/repl-examples/p2_boston_housing/#source-code",
            "text": "Below is the example program as a github gist, to view the original program in DynaML, click  here .",
            "title": "Source Code"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/",
            "text": "The \nwine quality\n data set is a common example used to benchmark classification models. Here we use the \nDynaML\n scala machine learning environment to train classifiers to detect 'good' wine from 'bad' wine. A short listing of the data attributes/columns is given below. The UCI archive has two files in the wine quality data set namely \nwinequality-red.csv\n and \nwinequality-white.csv\n. We train two separate classification models, one for red wine and one for white.\n\n\n\n\nAttribute Information:\n\u00b6\n\n\nInputs:\n\u00b6\n\n\n\n\nfixed acidity\n\n\nvolatile acidity\n\n\ncitric acid\n\n\nresidual sugar\n\n\nchlorides\n\n\nfree sulfur dioxide\n\n\ntotal sulfur dioxide\n\n\ndensity\n\n\npH\n\n\nsulphates\n\n\nalcohol\n\n\n\n\nOutput (based on sensory data):\n\u00b6\n\n\n\n\nquality (score between 0 and 10)\n\n\n\n\nData Output Preprocessing\n\u00b6\n\n\nThe wine quality target variable can take integer values from \n0\n to \n10\n, first we convert this into a binary class variable by setting the quality to be 'good'(encoded by the value \n1\n) if the numerical value is greater than \n6\n and 'bad' (encoded by value \n0\n) otherwise.\n\n\nModel\n\u00b6\n\n\nBelow is a classification model for predicting the quality label \ny\ny\n.\n\n\nLogit\n\u00b6\n\n\n\n\n\n\\begin{align}\n  P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\sigma(w^T \\varphi(\\mathbf{x}) + b) \\\\\n  \\sigma(z) &= \\frac{1}{1 + exp(-z)}\n\\end{align}\n\n\n\n\n\\begin{align}\n  P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\sigma(w^T \\varphi(\\mathbf{x}) + b) \\\\\n  \\sigma(z) &= \\frac{1}{1 + exp(-z)}\n\\end{align}\n\n\n\n\n\nProbit\n\u00b6\n\n\nThe \nprobit regression\n model is an alternative to the \nlogit\n model it is represented as.\n\n\n\n\n\n\\begin{align}\n  P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\Phi(w^T \\varphi(\\mathbf{x}) + b) \\\\\n  \\Phi(z) &= \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2 \\pi}} exp(-\\frac{z^{2}}{2}) dz  \n\\end{align}\n\n\n\n\n\\begin{align}\n  P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\Phi(w^T \\varphi(\\mathbf{x}) + b) \\\\\n  \\Phi(z) &= \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2 \\pi}} exp(-\\frac{z^{2}}{2}) dz  \n\\end{align}\n\n\n\n\n\nSyntax\n\u00b6\n\n\nThe \nTestLogisticWineQuality\n program in the \nexamples\n package trains and tests logit and probit models on the wine quality data.\n\n\nParameter | Type | Default value |Notes\n--------|-----------|-----------|------------|\ntraining | \nInt\n | 100 | Number of training samples\ntest | \nInt\n | 1000 | Number of test samples\ncolumns | \nList\n[\nInt\n]\n | 11, 0, ... , 10 | The columns to be selected for analysis (indexed from 0), first one is the target column.\nstepSize | \nDouble\n | 0.01 | Step size chosen for \nGradientDescent\n\nmaxIt | \nInt\n | 30 | Maximum number of iterations for gradient descent update.\nmini | \nDouble\n | 1.0 | Fraction of training samples to sample for each batch update.\nregularization | \nDouble\n | 0.5 | Regularization parameter.\nwineType | \nString\n | red | The type of wine: red or white\nmodelType | \nString\n | logistic | The type of model: logistic or probit\n\n\nRed Wine\n\u00b6\n\n\n1\n2\n3\n4\nTestLogisticWineQuality\n(\nstepSize\n \n=\n \n0.2\n,\n \nmaxIt\n \n=\n \n120\n,\n\n\nmini\n \n=\n \n1.0\n,\n \ntraining\n \n=\n \n800\n,\n\n\ntest\n \n=\n \n800\n,\n \nregularization\n \n=\n \n0.2\n,\n\n\nwineType\n \n=\n \n\"red\"\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Classification Model Performance\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: ============================\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Accuracy: 0.8475\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Area under ROC: 0.7968417788802267\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Maximum F Measure: 0.7493563745371187\n\n\n\n\n\n\n\n\n\n\nWhite Wine\n\u00b6\n\n\n1\n2\n3\n4\nTestLogisticWineQuality\n(\nstepSize\n \n=\n \n0.26\n,\n \nmaxIt\n \n=\n \n300\n,\n\n\nmini\n \n=\n \n1.0\n,\n \ntraining\n \n=\n \n3800\n,\n\n\ntest\n \n=\n \n1000\n,\n \nregularization\n \n=\n \n0.0\n,\n\n\nwineType\n \n=\n \n\"white\"\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Classification Model Performance\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: ============================\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Accuracy: 0.829\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Area under ROC: 0.7184782682020251\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Maximum F Measure: 0.7182203962483446\n\n\n\n\n\n\n\n\n\n\nSource Code\n\u00b6",
            "title": "Wine Quality"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#attribute-information",
            "text": "",
            "title": "Attribute Information:"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#inputs",
            "text": "fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  density  pH  sulphates  alcohol",
            "title": "Inputs:"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#output-based-on-sensory-data",
            "text": "quality (score between 0 and 10)",
            "title": "Output (based on sensory data):"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#data-output-preprocessing",
            "text": "The wine quality target variable can take integer values from  0  to  10 , first we convert this into a binary class variable by setting the quality to be 'good'(encoded by the value  1 ) if the numerical value is greater than  6  and 'bad' (encoded by value  0 ) otherwise.",
            "title": "Data Output Preprocessing"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#model",
            "text": "Below is a classification model for predicting the quality label  y y .",
            "title": "Model"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#logit",
            "text": "\\begin{align}\n  P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\sigma(w^T \\varphi(\\mathbf{x}) + b) \\\\\n  \\sigma(z) &= \\frac{1}{1 + exp(-z)}\n\\end{align}  \n\\begin{align}\n  P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\sigma(w^T \\varphi(\\mathbf{x}) + b) \\\\\n  \\sigma(z) &= \\frac{1}{1 + exp(-z)}\n\\end{align}",
            "title": "Logit"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#probit",
            "text": "The  probit regression  model is an alternative to the  logit  model it is represented as.   \n\\begin{align}\n  P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\Phi(w^T \\varphi(\\mathbf{x}) + b) \\\\\n  \\Phi(z) &= \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2 \\pi}} exp(-\\frac{z^{2}}{2}) dz  \n\\end{align}  \n\\begin{align}\n  P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\Phi(w^T \\varphi(\\mathbf{x}) + b) \\\\\n  \\Phi(z) &= \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2 \\pi}} exp(-\\frac{z^{2}}{2}) dz  \n\\end{align}",
            "title": "Probit"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#syntax",
            "text": "The  TestLogisticWineQuality  program in the  examples  package trains and tests logit and probit models on the wine quality data.  Parameter | Type | Default value |Notes\n--------|-----------|-----------|------------|\ntraining |  Int  | 100 | Number of training samples\ntest |  Int  | 1000 | Number of test samples\ncolumns |  List [ Int ]  | 11, 0, ... , 10 | The columns to be selected for analysis (indexed from 0), first one is the target column.\nstepSize |  Double  | 0.01 | Step size chosen for  GradientDescent \nmaxIt |  Int  | 30 | Maximum number of iterations for gradient descent update.\nmini |  Double  | 1.0 | Fraction of training samples to sample for each batch update.\nregularization |  Double  | 0.5 | Regularization parameter.\nwineType |  String  | red | The type of wine: red or white\nmodelType |  String  | logistic | The type of model: logistic or probit",
            "title": "Syntax"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#red-wine",
            "text": "1\n2\n3\n4 TestLogisticWineQuality ( stepSize   =   0.2 ,   maxIt   =   120 ,  mini   =   1.0 ,   training   =   800 ,  test   =   800 ,   regularization   =   0.2 ,  wineType   =   \"red\" )    1\n2\n3\n4\n5 16/04/01 15:21:57 INFO BinaryClassificationMetrics: Classification Model Performance\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: ============================\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Accuracy: 0.8475\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Area under ROC: 0.7968417788802267\n16/04/01 15:21:57 INFO BinaryClassificationMetrics: Maximum F Measure: 0.7493563745371187",
            "title": "Red Wine"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#white-wine",
            "text": "1\n2\n3\n4 TestLogisticWineQuality ( stepSize   =   0.26 ,   maxIt   =   300 ,  mini   =   1.0 ,   training   =   3800 ,  test   =   1000 ,   regularization   =   0.0 ,  wineType   =   \"white\" )    1\n2\n3\n4\n5 16/04/01 15:27:17 INFO BinaryClassificationMetrics: Classification Model Performance\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: ============================\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Accuracy: 0.829\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Area under ROC: 0.7184782682020251\n16/04/01 15:27:17 INFO BinaryClassificationMetrics: Maximum F Measure: 0.7182203962483446",
            "title": "White Wine"
        },
        {
            "location": "/repl-examples/p2_winequality_logistic/#source-code",
            "text": "",
            "title": "Source Code"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/",
            "text": "System identification\n refers to the process of learning a predictive model for a given dynamic system i.e. a system whose dynamics evolve with time. The most important aspect of these models is their structure, specifically the following are the common dynamic system models for discretely sampled time dependent systems.\n\n\nDaISy: System Identification Database\n\u00b6\n\n\nDaISy\n is a database of (artificial and real world) dynamic systems maintained by the \nSTADIUS\n research group at KU Leuven. We will work with the power plant data set listed on the \nDaISy\n home page in this post. Using \nDynaML\n, which comes preloaded with the power plant data, we will train \nLSSVM\n models to predict the various output indicators of the power plant in question.\n\n\nSystem Identification Models\n\u00b6\n\n\nBelow is a quick and dirty description of \nnon-linear auto-regressive\n (NARX) models which are popular in the system identification research community and among practitioners.\n\n\nNonlinear AutoRegresive (NAR)\n\u00b6\n\n\nSignal \ny(t)\ny(t)\n modeled as a function of its previous \np\np\n values\n\n\n\n\n\n    \\begin{align}\n    y(t) = f(y(t-1), y(t-2), \\cdots, y(t-p)) + \\epsilon(t)\n    \\end{align}\n\n\n\n\n    \\begin{align}\n    y(t) = f(y(t-1), y(t-2), \\cdots, y(t-p)) + \\epsilon(t)\n    \\end{align}\n\n\n\n\n\nNonlinear AutoRegressive with eXogenous inputs (NARX)\n\u00b6\n\n\nSignal \ny(t)\ny(t)\n modeled as a function of the previous \np\np\n values of itself and the \nm\nm\n exogenous inputs \nu_{1}, \\cdots u_{m}\nu_{1}, \\cdots u_{m}\n\n\n\n\n\n    \\begin{align}\n    \\begin{split}\n        y(t) = & f(y(t-1), y(t-2), \\cdots, y(t-p), \\\\\n        & u_{1}(t-1), u_{1}(t-2), \\cdots, u_{1}(t-p),\\\\\n        & \\cdots, \\\\\n        & u_{m}(t-1), u_{m}(t-2), \\cdots, u_{m}(t-p)) \\\\\n        & + \\epsilon(t)\n    \\end{split}\n    \\end{align}\n\n\n\n\n    \\begin{align}\n    \\begin{split}\n        y(t) = & f(y(t-1), y(t-2), \\cdots, y(t-p), \\\\\n        & u_{1}(t-1), u_{1}(t-2), \\cdots, u_{1}(t-p),\\\\\n        & \\cdots, \\\\\n        & u_{m}(t-1), u_{m}(t-2), \\cdots, u_{m}(t-p)) \\\\\n        & + \\epsilon(t)\n    \\end{split}\n    \\end{align}\n\n\n\n\n\n\n\nPont-sur-Sambre Power Plant Data\n\u00b6\n\n\n\n\nYou can obtain the metadata from this \nlink\n, it is also summarized below.\n\n\nData Attributes\n\u00b6\n\n\nInstances\n: 200\n\n\nInputs\n:\n\n\n\n\nGas flow\n\n\nTurbine valves opening\n\n\nSuper heater spray flow\n\n\nGas dampers\n\n\nAir flow\n\n\n\n\nOutputs\n:\n6. Steam pressure\n7. Main stem temperature\n8. Reheat steam temperature\n\n\nSystem Model\n\u00b6\n\n\nAn \nLS-SVM\n \nNARX\n of autoregressive order \np = 2\np = 2\n is chosen to model the plant output data. An LS-SVM model builds a predictor of the following form.\n\n\n\n\n\n    \\begin{align*}\n    y(x) = \\sum_{k = 1}^{N}\\alpha_k K(\\mathbf{x}, \\mathbf{x_k}) + b\n    \\end{align*}\n\n\n\n\n    \\begin{align*}\n    y(x) = \\sum_{k = 1}^{N}\\alpha_k K(\\mathbf{x}, \\mathbf{x_k}) + b\n    \\end{align*}\n\n\n\n\n\nWhich is the result of solving the following linear system.\n\n\n\n\n\n    \\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\n\n\n\n    \\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]\n\n\n\n\n\nHere the matrix \nK\nK\n is constructed from the training data using a kernel function \nK(\\mathbf{x}, \\mathbf{y})\nK(\\mathbf{x}, \\mathbf{y})\n.\n\n\nChoice of Kernel Function\n\u00b6\n\n\nFor this problem we choose a polynomial kernel.\n\n\n\n\n\n    \\begin{align*}\n        K(\\mathbf{x},\\mathbf{y}) = K_{poly}(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^{T}.\\mathbf{y} + \\alpha)^{d}\n    \\end{align*}\n\n\n\n\n    \\begin{align*}\n        K(\\mathbf{x},\\mathbf{y}) = K_{poly}(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^{T}.\\mathbf{y} + \\alpha)^{d}\n    \\end{align*}\n\n\n\n\n\nSyntax\n\u00b6\n\n\nThe \nDaisyPowerPlant\n program can be used to train and test LS-SVM models on the Pont Sur-Sambre power plant data.\n\n\nParameter | Type | Default value |Notes\n--------|-----------|-----------|------------|\nkernel | \nCovarianceFunction\n | - | The kernel function driving the LS-SVM model.\ndeltaT | \nInt\n | 2 | Order of auto-regressive model i.e. number of steps in the past to look for input features.\ntimelag | \nInt\n | 0 | The number of steps in the past to start using inputs.\nnum_training | \nInt\n | 150 | Number of training data instances.\ncolumn| \nInt\n | 7 | The column number of the output variable (indexed from 0).\n\nopt | \nMap\n[\nString\n, \nDouble\n]\n| - | Extra options for model selection routine.\n\n\nSteam Pressure\n\u00b6\n\n\n1\n2\n3\n4\n5\nDynaML\n>\nDaisyPowerPlant\n(\nnew\n \nPolynomialKernel\n(\n2\n,\n \n0.5\n),\n\n\nopt\n \n=\n \nMap\n(\n\"regularization\"\n \n->\n \n\"2.5\"\n,\n \n\"globalOpt\"\n \n->\n \n\"GS\"\n,\n\n\n\"grid\"\n \n->\n \n\"4\"\n,\n \n\"step\"\n \n->\n \n\"0.1\"\n),\n\n\nnum_training\n \n=\n \n100\n,\n \ndeltaT\n \n=\n \n2\n,\n\n\ncolumn\n \n=\n \n6\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: Regression Model Performance: steam pressure\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: \n============================\n\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: MAE: \n82\n.12740530161123\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: RMSE: \n104\n.39251587470388\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: RMSLE: \n0\n.9660077848586197\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: R^2: \n0\n.8395534877128238\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: Corr. Coefficient: \n0\n.9311734118932473\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: Model Yield: \n0\n.6288000962818303\n\n16\n/03/04 \n17\n:13:43 INFO RegressionMetrics: Std Dev of Residuals: \n87\n.82754320038951\n\n\n\n\n\n\n\n\n\n\nReheat Steam Temperature\n\u00b6\n\n\n1\n2\n3\n4\nDaisyPowerPlant\n(\nnew\n \nPolynomialKernel\n(\n2\n,\n \n1.5\n),\n\n\nopt\n \n=\n \nMap\n(\n\"regularization\"\n \n->\n \n\"2.5\"\n,\n \n\"globalOpt\"\n \n->\n \n\"GS\"\n,\n\n\n\"grid\"\n \n->\n \n\"4\"\n,\n \n\"step\"\n \n->\n \n\"0.1\"\n),\n \nnum_training\n \n=\n \n150\n,\n\n\ndeltaT\n \n=\n \n1\n,\n \ncolumn\n \n=\n \n8\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: Regression Model Performance: reheat steam temperature\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: \n============================\n\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: MAE: \n124\n.60921194767073\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: RMSE: \n137\n.33314302068544\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: RMSLE: \n0\n.5275727128626408\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: R^2: \n0\n.8247581957573777\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: Corr. Coefficient: \n0\n.9744133881055823\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: Model Yield: \n0\n.7871288689840381\n\n16\n/03/04 \n16\n:50:42 INFO RegressionMetrics: Std Dev of Residuals: \n111\n.86852905896446\n\n\n\n\n\n\n\n\n\n\n\n\nSource Code\n\u00b6\n\n\nBelow is the example program as a github gist, to view the original program in DynaML, click \nhere\n.",
            "title": "Pont-sur-Sambre Power Plant"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#daisy-system-identification-database",
            "text": "DaISy  is a database of (artificial and real world) dynamic systems maintained by the  STADIUS  research group at KU Leuven. We will work with the power plant data set listed on the  DaISy  home page in this post. Using  DynaML , which comes preloaded with the power plant data, we will train  LSSVM  models to predict the various output indicators of the power plant in question.",
            "title": "DaISy: System Identification Database"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#system-identification-models",
            "text": "Below is a quick and dirty description of  non-linear auto-regressive  (NARX) models which are popular in the system identification research community and among practitioners.",
            "title": "System Identification Models"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#nonlinear-autoregresive-nar",
            "text": "Signal  y(t) y(t)  modeled as a function of its previous  p p  values   \n    \\begin{align}\n    y(t) = f(y(t-1), y(t-2), \\cdots, y(t-p)) + \\epsilon(t)\n    \\end{align}  \n    \\begin{align}\n    y(t) = f(y(t-1), y(t-2), \\cdots, y(t-p)) + \\epsilon(t)\n    \\end{align}",
            "title": "Nonlinear AutoRegresive (NAR)"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#nonlinear-autoregressive-with-exogenous-inputs-narx",
            "text": "Signal  y(t) y(t)  modeled as a function of the previous  p p  values of itself and the  m m  exogenous inputs  u_{1}, \\cdots u_{m} u_{1}, \\cdots u_{m}   \n    \\begin{align}\n    \\begin{split}\n        y(t) = & f(y(t-1), y(t-2), \\cdots, y(t-p), \\\\\n        & u_{1}(t-1), u_{1}(t-2), \\cdots, u_{1}(t-p),\\\\\n        & \\cdots, \\\\\n        & u_{m}(t-1), u_{m}(t-2), \\cdots, u_{m}(t-p)) \\\\\n        & + \\epsilon(t)\n    \\end{split}\n    \\end{align}  \n    \\begin{align}\n    \\begin{split}\n        y(t) = & f(y(t-1), y(t-2), \\cdots, y(t-p), \\\\\n        & u_{1}(t-1), u_{1}(t-2), \\cdots, u_{1}(t-p),\\\\\n        & \\cdots, \\\\\n        & u_{m}(t-1), u_{m}(t-2), \\cdots, u_{m}(t-p)) \\\\\n        & + \\epsilon(t)\n    \\end{split}\n    \\end{align}",
            "title": "Nonlinear AutoRegressive with eXogenous inputs (NARX)"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#pont-sur-sambre-power-plant-data",
            "text": "You can obtain the metadata from this  link , it is also summarized below.",
            "title": "Pont-sur-Sambre Power Plant Data"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#data-attributes",
            "text": "Instances : 200  Inputs :   Gas flow  Turbine valves opening  Super heater spray flow  Gas dampers  Air flow   Outputs :\n6. Steam pressure\n7. Main stem temperature\n8. Reheat steam temperature",
            "title": "Data Attributes"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#system-model",
            "text": "An  LS-SVM   NARX  of autoregressive order  p = 2 p = 2  is chosen to model the plant output data. An LS-SVM model builds a predictor of the following form.   \n    \\begin{align*}\n    y(x) = \\sum_{k = 1}^{N}\\alpha_k K(\\mathbf{x}, \\mathbf{x_k}) + b\n    \\end{align*}  \n    \\begin{align*}\n    y(x) = \\sum_{k = 1}^{N}\\alpha_k K(\\mathbf{x}, \\mathbf{x_k}) + b\n    \\end{align*}   Which is the result of solving the following linear system.   \n    \\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]  \n    \\left[\\begin{array}{c|c}\n   0  & 1^\\intercal_v   \\\\ \\hline\n   1_v & K + \\gamma^{-1} \\mathit{I}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n   b    \\\\ \\hline\n   \\alpha  \n\\end{array}\\right] = \\left[\\begin{array}{c}\n   0    \\\\ \\hline\n   y  \n\\end{array}\\right]   Here the matrix  K K  is constructed from the training data using a kernel function  K(\\mathbf{x}, \\mathbf{y}) K(\\mathbf{x}, \\mathbf{y}) .",
            "title": "System Model"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#choice-of-kernel-function",
            "text": "For this problem we choose a polynomial kernel.   \n    \\begin{align*}\n        K(\\mathbf{x},\\mathbf{y}) = K_{poly}(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^{T}.\\mathbf{y} + \\alpha)^{d}\n    \\end{align*}  \n    \\begin{align*}\n        K(\\mathbf{x},\\mathbf{y}) = K_{poly}(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^{T}.\\mathbf{y} + \\alpha)^{d}\n    \\end{align*}",
            "title": "Choice of Kernel Function"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#syntax",
            "text": "The  DaisyPowerPlant  program can be used to train and test LS-SVM models on the Pont Sur-Sambre power plant data.  Parameter | Type | Default value |Notes\n--------|-----------|-----------|------------|\nkernel |  CovarianceFunction  | - | The kernel function driving the LS-SVM model.\ndeltaT |  Int  | 2 | Order of auto-regressive model i.e. number of steps in the past to look for input features.\ntimelag |  Int  | 0 | The number of steps in the past to start using inputs.\nnum_training |  Int  | 150 | Number of training data instances.\ncolumn|  Int  | 7 | The column number of the output variable (indexed from 0). \nopt |  Map [ String ,  Double ] | - | Extra options for model selection routine.",
            "title": "Syntax"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#steam-pressure",
            "text": "1\n2\n3\n4\n5 DynaML > DaisyPowerPlant ( new   PolynomialKernel ( 2 ,   0.5 ),  opt   =   Map ( \"regularization\"   ->   \"2.5\" ,   \"globalOpt\"   ->   \"GS\" ,  \"grid\"   ->   \"4\" ,   \"step\"   ->   \"0.1\" ),  num_training   =   100 ,   deltaT   =   2 ,  column   =   6 )    1\n2\n3\n4\n5\n6\n7\n8\n9 16 /03/04  17 :13:43 INFO RegressionMetrics: Regression Model Performance: steam pressure 16 /03/04  17 :13:43 INFO RegressionMetrics:  ============================  16 /03/04  17 :13:43 INFO RegressionMetrics: MAE:  82 .12740530161123 16 /03/04  17 :13:43 INFO RegressionMetrics: RMSE:  104 .39251587470388 16 /03/04  17 :13:43 INFO RegressionMetrics: RMSLE:  0 .9660077848586197 16 /03/04  17 :13:43 INFO RegressionMetrics: R^2:  0 .8395534877128238 16 /03/04  17 :13:43 INFO RegressionMetrics: Corr. Coefficient:  0 .9311734118932473 16 /03/04  17 :13:43 INFO RegressionMetrics: Model Yield:  0 .6288000962818303 16 /03/04  17 :13:43 INFO RegressionMetrics: Std Dev of Residuals:  87 .82754320038951",
            "title": "Steam Pressure"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#reheat-steam-temperature",
            "text": "1\n2\n3\n4 DaisyPowerPlant ( new   PolynomialKernel ( 2 ,   1.5 ),  opt   =   Map ( \"regularization\"   ->   \"2.5\" ,   \"globalOpt\"   ->   \"GS\" ,  \"grid\"   ->   \"4\" ,   \"step\"   ->   \"0.1\" ),   num_training   =   150 ,  deltaT   =   1 ,   column   =   8 )    1\n2\n3\n4\n5\n6\n7\n8\n9 16 /03/04  16 :50:42 INFO RegressionMetrics: Regression Model Performance: reheat steam temperature 16 /03/04  16 :50:42 INFO RegressionMetrics:  ============================  16 /03/04  16 :50:42 INFO RegressionMetrics: MAE:  124 .60921194767073 16 /03/04  16 :50:42 INFO RegressionMetrics: RMSE:  137 .33314302068544 16 /03/04  16 :50:42 INFO RegressionMetrics: RMSLE:  0 .5275727128626408 16 /03/04  16 :50:42 INFO RegressionMetrics: R^2:  0 .8247581957573777 16 /03/04  16 :50:42 INFO RegressionMetrics: Corr. Coefficient:  0 .9744133881055823 16 /03/04  16 :50:42 INFO RegressionMetrics: Model Yield:  0 .7871288689840381 16 /03/04  16 :50:42 INFO RegressionMetrics: Std Dev of Residuals:  111 .86852905896446",
            "title": "Reheat Steam Temperature"
        },
        {
            "location": "/repl-examples/p2_lssvm_powerplant/#source-code",
            "text": "Below is the example program as a github gist, to view the original program in DynaML, click  here .",
            "title": "Source Code"
        },
        {
            "location": "/utils/utils_data_transforms/",
            "text": "Summary\n\n\nSome attribute transformations are included in the DynaML distribution, here we show how to use them. All of them inherit \nReversibleScaler\n[\nI\n]\n trait. They are contained in the \ndynml.utils\n package.\n\n\n\n\nGaussian Centering\n\u00b6\n\n\nGaussian scaling/centering involves calculating the sample mean and variance of data and applying a gaussian standardization operations using the calculated statistics.\n\n\nIt has different implementations in slightly varying contexts.\n\n\nUnivariate\n\u00b6\n\n\nUnivariate gaussian scaling involves\n\n\n\n\n\n\\begin{align}\nx &\\in \\mathbb{R} \\\\\n\\mu &\\in \\mathbb{R} \\\\\n\\sigma &\\in \\mathbb{R} \\\\\n\\bar{x} &= \\frac{x-\\mu}{\\sigma}\n\\end{align}\n\n\n\n\n\\begin{align}\nx &\\in \\mathbb{R} \\\\\n\\mu &\\in \\mathbb{R} \\\\\n\\sigma &\\in \\mathbb{R} \\\\\n\\bar{x} &= \\frac{x-\\mu}{\\sigma}\n\\end{align}\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \nmean\n \n=\n \n-\n1.5\n\n\nval\n \nsigma\n \n=\n \n2.5\n\n\n\nval\n \nugs\n \n=\n \nUnivariateGaussianScaler\n(\nmean\n,\n \nsigma\n)\n\n\n\nval\n \nx\n \n=\n \n3.0\n\n\n\nval\n \nxs\n \n=\n \nugs\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \nugs\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\nMultivariate\n\u00b6\n\n\nThe data attributes form components of a vector, in this case we can assume each component is independent and calculate the diagonal variance or compute all the component covariances in the form of a symmetric matrix.\n\n\n\n\n\n\\begin{align}\nx &\\in \\mathbb{R}^n \\\\\n\\mu &\\in \\mathbb{R}^n \\\\\n\\Sigma &\\in \\mathbb{R}^{n \\times n}\\\\\nL L^\\intercal &= \\Sigma \\\\\n\\bar{x} &= L^{-1} (x - \\mu)\n\\end{align}\n\n\n\n\n\\begin{align}\nx &\\in \\mathbb{R}^n \\\\\n\\mu &\\in \\mathbb{R}^n \\\\\n\\Sigma &\\in \\mathbb{R}^{n \\times n}\\\\\nL L^\\intercal &= \\Sigma \\\\\n\\bar{x} &= L^{-1} (x - \\mu)\n\\end{align}\n\n\n\n\n\nDiagonal\n\u00b6\n\n\nIn this case the sample covariance matrix calculated from the data is diagonal and neglecting the correlations between the attributes.\n\n\n\n\n\n\\Sigma = \\begin{pmatrix}\n\\sigma^{2}_1 & \\cdots & 0\\\\\n \\vdots & \\ddots  & \\vdots\\\\\n 0 & \\cdots & \\sigma^{2}_n  \n\\end{pmatrix}\n\n\n\n\n\\Sigma = \\begin{pmatrix}\n\\sigma^{2}_1 & \\cdots & 0\\\\\n \\vdots & \\ddots  & \\vdots\\\\\n 0 & \\cdots & \\sigma^{2}_n  \n\\end{pmatrix}\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \nmean\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(-\n1.5\n,\n \n1.5\n,\n \n0.25\n)\n\n\nval\n \nsigma\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.5\n,\n \n2.5\n,\n \n1.0\n)\n\n\n\nval\n \ngs\n \n=\n \nGaussianScaler\n(\nmean\n,\n \nsigma\n)\n\n\n\nval\n \nx\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.2\n,\n \n-\n3.5\n,\n \n-\n1.5\n)\n\n\n\nval\n \nxs\n \n=\n \ngs\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \ngs\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\nFull Matrix\n\u00b6\n\n\nWhen the sample covariance matrix is calculated taking into account correlations between data attributes.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nval\n \nmean\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(-\n1.5\n,\n \n1.5\n,\n \n0.25\n)\n\n\nval\n \nsigma\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \nDenseMatrix\n(\n\n  \n(\n2.5\n,\n \n0.5\n,\n \n0.25\n),\n\n  \n(\n0.5\n,\n \n3.5\n,\n \n1.2\n),\n\n  \n(\n0.25\n,\n \n1.2\n,\n \n2.25\n)\n\n\n)\n\n\n\nval\n \nmv_gs\n \n=\n \nMVGaussianScaler\n(\nmean\n,\n \nsigma\n)\n\n\n\nval\n \nx\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.2\n,\n \n-\n3.5\n,\n \n-\n1.5\n)\n\n\n\nval\n \nxs\n \n=\n \nmv_gs\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \nmv_gs\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\nMean Centering\n\u00b6\n\n\nUnivariate\n\u00b6\n\n\n\n\n\n\\begin{align}\nx &\\in \\mathbb{R} \\\\\n\\mu &\\in \\mathbb{R} \\\\\n\\bar{x} &= x-\\mu\n\\end{align}\n\n\n\n\n\\begin{align}\nx &\\in \\mathbb{R} \\\\\n\\mu &\\in \\mathbb{R} \\\\\n\\bar{x} &= x-\\mu\n\\end{align}\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nval\n \nc\n \n=\n \n-\n1.5\n\n\n\nval\n \nums\n \n=\n \nUnivariateMeanScaler\n(\nc\n)\n\n\n\nval\n \nx\n \n=\n \n3.0\n\n\n\nval\n \nxs\n \n=\n \nums\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \nums\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\nMultivariate\n\u00b6\n\n\n\n\n\n\\begin{align}\nx &\\in \\mathbb{R}^n \\\\\n\\mu &\\in \\mathbb{R}^n \\\\\n\\bar{x} &= x - \\mu\n\\end{align}\n\n\n\n\n\\begin{align}\nx &\\in \\mathbb{R}^n \\\\\n\\mu &\\in \\mathbb{R}^n \\\\\n\\bar{x} &= x - \\mu\n\\end{align}\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nval\n \nmean\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(-\n1.5\n,\n \n1.5\n,\n \n0.25\n)\n\n\n\nval\n \nmms\n \n=\n \nMeanScaler\n(\nmean\n)\n\n\n\nval\n \nx\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.2\n,\n \n-\n3.5\n,\n \n-\n1.5\n)\n\n\n\nval\n \nxs\n \n=\n \nmms\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \nmms\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\nMin-Max Scaling\n\u00b6\n\n\nMin-max scaling is also known as \n0,1\n0,1\n scaling because attributes are scaled down to the domain \n[0, 1]\n[0, 1]\n. This is done by calculating the minimum and maximum of attribute values.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nval\n \nmin\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(-\n1.5\n,\n \n1.5\n,\n \n0.25\n)\n\n\nval\n \nmax\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.5\n,\n \n2.5\n,\n \n1.0\n)\n\n\n\nval\n \nmin_max_scaler\n \n=\n \nMinMaxScaler\n(\nmin\n,\n \nmax\n)\n\n\n\nval\n \nx\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.2\n,\n \n-\n3.5\n,\n \n-\n1.5\n)\n\n\n\nval\n \nxs\n \n=\n \nmin_max_scaler\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \nmin_max_scaler\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\nPrincipal Component Analysis\n\u00b6\n\n\nPrincipal component analysis\n consists of projecting data onto the eigenvectors of its sample covariance matrix.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nval\n \nmean\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(-\n1.5\n,\n \n1.5\n,\n \n0.25\n)\n\n\nval\n \nsigma\n:\n \nDenseMatrix\n[\nDouble\n]\n \n=\n \nDenseMatrix\n(\n\n  \n(\n2.5\n,\n \n0.5\n,\n \n0.25\n),\n\n  \n(\n0.5\n,\n \n3.5\n,\n \n1.2\n),\n\n  \n(\n0.25\n,\n \n1.2\n,\n \n2.25\n)\n\n\n)\n\n\n\nval\n \npca\n \n=\n \nPCAScaler\n(\nmean\n,\n \nsigma\n)\n\n\n\nval\n \nx\n:\n \nDenseVector\n[\nDouble\n]\n \n=\n \nDenseVector\n(\n0.2\n,\n \n-\n3.5\n,\n \n-\n1.5\n)\n\n\n\nval\n \nxs\n \n=\n \npca\n(\nx\n)\n\n\n\nval\n \nxhat\n \n=\n \npca\n.\ni\n(\nxs\n)\n\n\n\n\n\n\n\n\n\nSlicing scalers\n\n\nIt is possible to \nslice\n the scalers shown above if they act on vectors. For example.\n\n1\n2\n3\n4\n//Slice on subset of columns\n\n\nval\n \ngs_sub\n:\n \nGaussianScaler\n \n=\n \ngs\n(\n0\n \nto\n \n1\n)\n\n\n//Slice on a single column\n\n\nval\n \ngs_last\n:\n \nUnivariateGaussianScaler\n \n=\n \ngs\n(\n2\n)",
            "title": "Data Transforms"
        },
        {
            "location": "/utils/utils_data_transforms/#gaussian-centering",
            "text": "Gaussian scaling/centering involves calculating the sample mean and variance of data and applying a gaussian standardization operations using the calculated statistics.  It has different implementations in slightly varying contexts.",
            "title": "Gaussian Centering"
        },
        {
            "location": "/utils/utils_data_transforms/#univariate",
            "text": "Univariate gaussian scaling involves   \n\\begin{align}\nx &\\in \\mathbb{R} \\\\\n\\mu &\\in \\mathbb{R} \\\\\n\\sigma &\\in \\mathbb{R} \\\\\n\\bar{x} &= \\frac{x-\\mu}{\\sigma}\n\\end{align}  \n\\begin{align}\nx &\\in \\mathbb{R} \\\\\n\\mu &\\in \\mathbb{R} \\\\\n\\sigma &\\in \\mathbb{R} \\\\\n\\bar{x} &= \\frac{x-\\mu}{\\sigma}\n\\end{align}    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   mean   =   - 1.5  val   sigma   =   2.5  val   ugs   =   UnivariateGaussianScaler ( mean ,   sigma )  val   x   =   3.0  val   xs   =   ugs ( x )  val   xhat   =   ugs . i ( xs )",
            "title": "Univariate"
        },
        {
            "location": "/utils/utils_data_transforms/#multivariate",
            "text": "The data attributes form components of a vector, in this case we can assume each component is independent and calculate the diagonal variance or compute all the component covariances in the form of a symmetric matrix.   \n\\begin{align}\nx &\\in \\mathbb{R}^n \\\\\n\\mu &\\in \\mathbb{R}^n \\\\\n\\Sigma &\\in \\mathbb{R}^{n \\times n}\\\\\nL L^\\intercal &= \\Sigma \\\\\n\\bar{x} &= L^{-1} (x - \\mu)\n\\end{align}  \n\\begin{align}\nx &\\in \\mathbb{R}^n \\\\\n\\mu &\\in \\mathbb{R}^n \\\\\n\\Sigma &\\in \\mathbb{R}^{n \\times n}\\\\\nL L^\\intercal &= \\Sigma \\\\\n\\bar{x} &= L^{-1} (x - \\mu)\n\\end{align}",
            "title": "Multivariate"
        },
        {
            "location": "/utils/utils_data_transforms/#diagonal",
            "text": "In this case the sample covariance matrix calculated from the data is diagonal and neglecting the correlations between the attributes.   \n\\Sigma = \\begin{pmatrix}\n\\sigma^{2}_1 & \\cdots & 0\\\\\n \\vdots & \\ddots  & \\vdots\\\\\n 0 & \\cdots & \\sigma^{2}_n  \n\\end{pmatrix}  \n\\Sigma = \\begin{pmatrix}\n\\sigma^{2}_1 & \\cdots & 0\\\\\n \\vdots & \\ddots  & \\vdots\\\\\n 0 & \\cdots & \\sigma^{2}_n  \n\\end{pmatrix}    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   mean :   DenseVector [ Double ]   =   DenseVector (- 1.5 ,   1.5 ,   0.25 )  val   sigma :   DenseVector [ Double ]   =   DenseVector ( 0.5 ,   2.5 ,   1.0 )  val   gs   =   GaussianScaler ( mean ,   sigma )  val   x :   DenseVector [ Double ]   =   DenseVector ( 0.2 ,   - 3.5 ,   - 1.5 )  val   xs   =   gs ( x )  val   xhat   =   gs . i ( xs )",
            "title": "Diagonal"
        },
        {
            "location": "/utils/utils_data_transforms/#full-matrix",
            "text": "When the sample covariance matrix is calculated taking into account correlations between data attributes.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 val   mean :   DenseVector [ Double ]   =   DenseVector (- 1.5 ,   1.5 ,   0.25 )  val   sigma :   DenseMatrix [ Double ]   =   DenseMatrix ( \n   ( 2.5 ,   0.5 ,   0.25 ), \n   ( 0.5 ,   3.5 ,   1.2 ), \n   ( 0.25 ,   1.2 ,   2.25 )  )  val   mv_gs   =   MVGaussianScaler ( mean ,   sigma )  val   x :   DenseVector [ Double ]   =   DenseVector ( 0.2 ,   - 3.5 ,   - 1.5 )  val   xs   =   mv_gs ( x )  val   xhat   =   mv_gs . i ( xs )",
            "title": "Full Matrix"
        },
        {
            "location": "/utils/utils_data_transforms/#mean-centering",
            "text": "",
            "title": "Mean Centering"
        },
        {
            "location": "/utils/utils_data_transforms/#univariate_1",
            "text": "\\begin{align}\nx &\\in \\mathbb{R} \\\\\n\\mu &\\in \\mathbb{R} \\\\\n\\bar{x} &= x-\\mu\n\\end{align}  \n\\begin{align}\nx &\\in \\mathbb{R} \\\\\n\\mu &\\in \\mathbb{R} \\\\\n\\bar{x} &= x-\\mu\n\\end{align}   1\n2\n3\n4\n5\n6\n7\n8\n9 val   c   =   - 1.5  val   ums   =   UnivariateMeanScaler ( c )  val   x   =   3.0  val   xs   =   ums ( x )  val   xhat   =   ums . i ( xs )",
            "title": "Univariate"
        },
        {
            "location": "/utils/utils_data_transforms/#multivariate_1",
            "text": "\\begin{align}\nx &\\in \\mathbb{R}^n \\\\\n\\mu &\\in \\mathbb{R}^n \\\\\n\\bar{x} &= x - \\mu\n\\end{align}  \n\\begin{align}\nx &\\in \\mathbb{R}^n \\\\\n\\mu &\\in \\mathbb{R}^n \\\\\n\\bar{x} &= x - \\mu\n\\end{align}   1\n2\n3\n4\n5\n6\n7\n8\n9 val   mean :   DenseVector [ Double ]   =   DenseVector (- 1.5 ,   1.5 ,   0.25 )  val   mms   =   MeanScaler ( mean )  val   x :   DenseVector [ Double ]   =   DenseVector ( 0.2 ,   - 3.5 ,   - 1.5 )  val   xs   =   mms ( x )  val   xhat   =   mms . i ( xs )",
            "title": "Multivariate"
        },
        {
            "location": "/utils/utils_data_transforms/#min-max-scaling",
            "text": "Min-max scaling is also known as  0,1 0,1  scaling because attributes are scaled down to the domain  [0, 1] [0, 1] . This is done by calculating the minimum and maximum of attribute values.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 val   min :   DenseVector [ Double ]   =   DenseVector (- 1.5 ,   1.5 ,   0.25 )  val   max :   DenseVector [ Double ]   =   DenseVector ( 0.5 ,   2.5 ,   1.0 )  val   min_max_scaler   =   MinMaxScaler ( min ,   max )  val   x :   DenseVector [ Double ]   =   DenseVector ( 0.2 ,   - 3.5 ,   - 1.5 )  val   xs   =   min_max_scaler ( x )  val   xhat   =   min_max_scaler . i ( xs )",
            "title": "Min-Max Scaling"
        },
        {
            "location": "/utils/utils_data_transforms/#principal-component-analysis",
            "text": "Principal component analysis  consists of projecting data onto the eigenvectors of its sample covariance matrix.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 val   mean :   DenseVector [ Double ]   =   DenseVector (- 1.5 ,   1.5 ,   0.25 )  val   sigma :   DenseMatrix [ Double ]   =   DenseMatrix ( \n   ( 2.5 ,   0.5 ,   0.25 ), \n   ( 0.5 ,   3.5 ,   1.2 ), \n   ( 0.25 ,   1.2 ,   2.25 )  )  val   pca   =   PCAScaler ( mean ,   sigma )  val   x :   DenseVector [ Double ]   =   DenseVector ( 0.2 ,   - 3.5 ,   - 1.5 )  val   xs   =   pca ( x )  val   xhat   =   pca . i ( xs )     Slicing scalers  It is possible to  slice  the scalers shown above if they act on vectors. For example. 1\n2\n3\n4 //Slice on subset of columns  val   gs_sub :   GaussianScaler   =   gs ( 0   to   1 )  //Slice on a single column  val   gs_last :   UnivariateGaussianScaler   =   gs ( 2 )",
            "title": "Principal Component Analysis"
        },
        {
            "location": "/utils/package/",
            "text": "Summary\n\n\nThe \nutils\n object contains some useful helper functions which are used by a number of API components of DynaML.\n\n\n\n\nString/File Processing\n\u00b6\n\n\nLoad File into a Stream\n\u00b6\n\n\n1\nval\n \ncontent\n \n=\n \nutils\n.\ntextFileToStream\n(\n\"data.csv\"\n)\n\n\n\n\n\n\n\nString Replace\n\u00b6\n\n\nReplace all occurrences of a string (or regular expression) in a target string\n\n\n1\nval\n \nnew_str\n \n=\n \nutils\n.\nreplace\n(\nfind\n \n=\n \n\",\"\n)(\nreplace\n \n=\n \n\"|\"\n)(\ninput\n \n=\n \n\"1,2,3,4\"\n)\n\n\n\n\n\n\n\nURL download\n\u00b6\n\n\nDownload the content of a url to a specified location on disk.\n\n\n1\nutils\n.\ndownloadURL\n(\n\"www.google.com\"\n,\n \n\"google_home_page.html\"\n)\n\n\n\n\n\n\n\nWrite to File\n\u00b6\n\n\n1\n2\nval\n \ncontent\n:\n \nStream\n[\nString\n]\n \n=\n \n_\n\n\nutils\n.\nwriteToFile\n(\n\"foo.csv\"\n)(\ncontent\n)\n\n\n\n\n\n\n\nNumerics\n\u00b6\n\n\nlog1p\n\u00b6\n\n\nCalculates \nlog_{e}(1+x)\nlog_{e}(1+x)\n.\n\n\n1\nval\n \nl\n \n=\n \nutils\n.\nlog1pExp\n(\n0.02\n)\n\n\n\n\n\n\n\nHaar DWT Matrix\n\u00b6\n\n\nConstructs the Haar \ndiscrete wavelet transform\n matrix for orders which are powers of two.\n\n\n1\nval\n \ndwt_mat\n \n=\n \nutils\n.\nhaarMatrix\n(\nmath\n.\npow\n(\n2\n,\n \n3\n).\ntoInt\n)\n\n\n\n\n\n\n\nHermite Polynomials\n\u00b6\n\n\nThe \nHermite polynomials\n are an important class of orthogonal polynomials used in numerical analysis. There are two definitions of the \nHermite\n polynomials i.e. the probabilist and physicist definitions, which are equivalent up-to a scale factor. The the \nutils\n object, the probabilist polynomials are calculated.\n\n\n1\n2\n3\n4\n5\n//Calculate the 3rd order Hermite polynomial\n\n\n\nval\n \nh3\n \n=\n \n(\nx\n:\n \nDouble\n)\n \n=>\n \nutils\n.\nhermite\n(\n3\n,\n \nx\n)\n\n\n\nh3\n(\n2.5\n)\n\n\n\n\n\n\n\nChebyshev Polynomials\n\u00b6\n\n\nChebyshev polynomials\n are another important class of orthogonal polynomials used in numerical analysis. There are two types, the \nfirst kind\n and \nsecond kind\n.\n\n\n1\n2\n3\n4\n5\n//Calculate the Chebyshev polynomial of second kind order 3\n\n\n\nval\n \nc23\n \n=\n \n(\nx\n:\n \nDouble\n)\n \n=>\n \nutils\n.\nchebyshev\n(\n3\n,\n \nx\n,\n \nkind\n \n=\n \n2\n)\n\n\n\nc23\n(\n2.5\n)\n\n\n\n\n\n\n\nQuick Select\n\u00b6\n\n\nThe quick select aims to find the \nk^{th}\nk^{th}\n smallest element of a list of numbers.\n\n\n1\nval\n \nsecond\n \n=\n \nutils\n.\nquickselect\n(\nList\n(\n3\n,\n2\n,\n4\n,\n5\n,\n1\n,\n6\n),\n \n2\n)\n\n\n\n\n\n\n\nMedian\n\u00b6\n\n\n1\nval\n \nsecond\n \n=\n \nutils\n.\nmedian\n(\nList\n(\n3\n,\n2\n,\n4\n,\n5\n,\n1\n,\n6\n))\n\n\n\n\n\n\n\nSample Statistics\n\u00b6\n\n\nCalculate the mean and variance (or covariance), minimum, maximum of a list of \nDenseVector\n[\nDouble\n]\n instances.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nval\n \ndata\n:\n \nList\n[\nDenseVector\n[\nDouble\n]]\n \n=\n \n_\n\n\n\nval\n \n(\nmu\n,\n \nvard\n)\n:\n \n(\nDenseVector\n[\nDouble\n],\n \nDenseVector\n[\nDouble\n])\n \n=\n\n  \nutils\n.\ngetStats\n(\ndata\n)\n\n\n\nval\n \n(\nmean\n,\n \ncov\n)\n:\n \n(\nDenseVector\n[\nDouble\n],\n \nDenseMatrix\n[\nDouble\n])\n \n=\n\n  \nutils\n.\ngetStatsMult\n(\ndata\n)\n\n\n\nval\n \n(\nmin\n,\n \nmax\n)\n \n=\n \nutils\n.\ngetMinMax\n(\ndata\n)",
            "title": "`utils` object"
        },
        {
            "location": "/utils/package/#stringfile-processing",
            "text": "",
            "title": "String/File Processing"
        },
        {
            "location": "/utils/package/#load-file-into-a-stream",
            "text": "1 val   content   =   utils . textFileToStream ( \"data.csv\" )",
            "title": "Load File into a Stream"
        },
        {
            "location": "/utils/package/#string-replace",
            "text": "Replace all occurrences of a string (or regular expression) in a target string  1 val   new_str   =   utils . replace ( find   =   \",\" )( replace   =   \"|\" )( input   =   \"1,2,3,4\" )",
            "title": "String Replace"
        },
        {
            "location": "/utils/package/#url-download",
            "text": "Download the content of a url to a specified location on disk.  1 utils . downloadURL ( \"www.google.com\" ,   \"google_home_page.html\" )",
            "title": "URL download"
        },
        {
            "location": "/utils/package/#write-to-file",
            "text": "1\n2 val   content :   Stream [ String ]   =   _  utils . writeToFile ( \"foo.csv\" )( content )",
            "title": "Write to File"
        },
        {
            "location": "/utils/package/#numerics",
            "text": "",
            "title": "Numerics"
        },
        {
            "location": "/utils/package/#log1p",
            "text": "Calculates  log_{e}(1+x) log_{e}(1+x) .  1 val   l   =   utils . log1pExp ( 0.02 )",
            "title": "log1p"
        },
        {
            "location": "/utils/package/#haar-dwt-matrix",
            "text": "Constructs the Haar  discrete wavelet transform  matrix for orders which are powers of two.  1 val   dwt_mat   =   utils . haarMatrix ( math . pow ( 2 ,   3 ). toInt )",
            "title": "Haar DWT Matrix"
        },
        {
            "location": "/utils/package/#hermite-polynomials",
            "text": "The  Hermite polynomials  are an important class of orthogonal polynomials used in numerical analysis. There are two definitions of the  Hermite  polynomials i.e. the probabilist and physicist definitions, which are equivalent up-to a scale factor. The the  utils  object, the probabilist polynomials are calculated.  1\n2\n3\n4\n5 //Calculate the 3rd order Hermite polynomial  val   h3   =   ( x :   Double )   =>   utils . hermite ( 3 ,   x )  h3 ( 2.5 )",
            "title": "Hermite Polynomials"
        },
        {
            "location": "/utils/package/#chebyshev-polynomials",
            "text": "Chebyshev polynomials  are another important class of orthogonal polynomials used in numerical analysis. There are two types, the  first kind  and  second kind .  1\n2\n3\n4\n5 //Calculate the Chebyshev polynomial of second kind order 3  val   c23   =   ( x :   Double )   =>   utils . chebyshev ( 3 ,   x ,   kind   =   2 )  c23 ( 2.5 )",
            "title": "Chebyshev Polynomials"
        },
        {
            "location": "/utils/package/#quick-select",
            "text": "The quick select aims to find the  k^{th} k^{th}  smallest element of a list of numbers.  1 val   second   =   utils . quickselect ( List ( 3 , 2 , 4 , 5 , 1 , 6 ),   2 )",
            "title": "Quick Select"
        },
        {
            "location": "/utils/package/#median",
            "text": "1 val   second   =   utils . median ( List ( 3 , 2 , 4 , 5 , 1 , 6 ))",
            "title": "Median"
        },
        {
            "location": "/utils/package/#sample-statistics",
            "text": "Calculate the mean and variance (or covariance), minimum, maximum of a list of  DenseVector [ Double ]  instances.  1\n2\n3\n4\n5\n6\n7\n8\n9 val   data :   List [ DenseVector [ Double ]]   =   _  val   ( mu ,   vard ) :   ( DenseVector [ Double ],   DenseVector [ Double ])   = \n   utils . getStats ( data )  val   ( mean ,   cov ) :   ( DenseVector [ Double ],   DenseMatrix [ Double ])   = \n   utils . getStatsMult ( data )  val   ( min ,   max )   =   utils . getMinMax ( data )",
            "title": "Sample Statistics"
        },
        {
            "location": "/license/",
            "text": "Copyright 2015 Mandar Chandorkar (\nmandar2812@gmail.com\n)\n\n\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n\nApache License\n\n\n\n\n\n\nWarning\n\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n\n\n\n\nDependency Licenses\n\u00b6\n\n\nThe various components of DynaML which are enabled using dependencies are\nsubject to their license requirements. The following table lists\nall the component licences used by DynaML's dependencies.\n\n\n\n\n\n\n\n\nCategory\n\n\nLicense\n\n\nDependency\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\ncom.chuusai # shapeless_2.11 # 2.3.3\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\ncom.quantifind # sumac_2.11 # 0.3.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\ncom.quantifind # wisp_2.11 # 0.0.4\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\ncom.trueaccord.lenses # lenses_2.11 # 0.4.12\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\ncom.trueaccord.scalapb # scalapb-runtime_2.11 # 0.6.2\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\ncom.twitter # chill-java # 0.8.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\ncom.twitter # chill_2.11 # 0.8.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\ncom.univocity # univocity-parsers # 2.2.1\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\nio.spray # spray-json_2.11 # 1.3.3\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\njoda-time # joda-time # 2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\nnet.sf.opencsv # opencsv # 2.3\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\norg.objenesis # objenesis # 2.6\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\norg.roaringbitmap # RoaringBitmap # 0.5.11\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\norg.scalaj # scalaj-http_2.11 # 2.3.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\norg.scalanlp # breeze-macros_2.11 # 0.13.2\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\norg.scalanlp # breeze-natives_2.11 # 0.13.2\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\norg.scalanlp # breeze_2.11 # 0.13.2\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2\n\n\norg.typelevel # macro-compat_2.11 # 1.1.1\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0\n\n\nio.circe # circe-core_2.11 # 0.9.1\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0\n\n\nio.circe # circe-generic_2.11 # 0.9.1\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0\n\n\nio.circe # circe-jawn_2.11 # 0.9.1\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0\n\n\nio.circe # circe-numbers_2.11 # 0.9.1\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0\n\n\nio.circe # circe-parser_2.11 # 0.9.1\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0\n\n\nio.get-coursier # coursier-cache_2.11 # 1.0.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0\n\n\nio.get-coursier # coursier_2.11 # 1.0.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0\n\n\nnet.java.dev.jets3t # jets3t # 0.9.3\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\ncom.typesafe.scala-logging # scala-logging_2.11 # 3.9.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.spark # spark-catalyst_2.11 # 2.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.spark # spark-core_2.11 # 2.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.spark # spark-graphx_2.11 # 2.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.spark # spark-launcher_2.11 # 2.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.spark # spark-mllib-local_2.11 # 2.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.spark # spark-mllib_2.11 # 2.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.spark # spark-network-common_2.11 # 2.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.spark # spark-network-shuffle_2.11 # 2.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.spark # spark-sketch_2.11 # 2.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.spark # spark-sql_2.11 # 2.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.spark # spark-streaming_2.11 # 2.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.spark # spark-tags_2.11 # 2.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.spark # spark-unsafe_2.11 # 2.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache 2.0 License\n\n\norg.apache.sshd # sshd-core # 1.2.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache License\n\n\ncommons-httpclient # commons-httpclient # 3.1\n\n\n\n\n\n\n\n\nApache\n\n\nApache License\n\n\ninfo.folone # poi-scala_2.11 # 0.18\n\n\n\n\n\n\n\n\nApache\n\n\nApache License\n\n\norg.apache.httpcomponents # httpmime # 4.2.5\n\n\n\n\n\n\n\n\nApache\n\n\nApache License 2.0\n\n\norg.renjin.cran # utf8 # 1.1.3-b8\n\n\n\n\n\n\n\n\nApache\n\n\nApache License 2.0\n\n\ncom.ning # compress-lzf # 1.0.3\n\n\n\n\n\n\n\n\nApache\n\n\nApache License 2.0\n\n\nio.dropwizard.metrics # metrics-core # 3.1.2\n\n\n\n\n\n\n\n\nApache\n\n\nApache License 2.0\n\n\nio.dropwizard.metrics # metrics-graphite # 3.1.2\n\n\n\n\n\n\n\n\nApache\n\n\nApache License 2.0\n\n\nio.dropwizard.metrics # metrics-json # 3.1.2\n\n\n\n\n\n\n\n\nApache\n\n\nApache License 2.0\n\n\nio.dropwizard.metrics # metrics-jvm # 3.1.2\n\n\n\n\n\n\n\n\nApache\n\n\nApache License 2.0\n\n\norg.platanios # tensorflow-api_2.11 # 0.2.4\n\n\n\n\n\n\n\n\nApache\n\n\nApache License 2.0\n\n\norg.platanios # tensorflow-data_2.11 # 0.2.4\n\n\n\n\n\n\n\n\nApache\n\n\nApache License 2.0\n\n\norg.platanios # tensorflow-jni_2.11 # 0.2.4\n\n\n\n\n\n\n\n\nApache\n\n\nApache License 2.0\n\n\norg.platanios # tensorflow_2.11 # 0.2.4\n\n\n\n\n\n\n\n\nApache\n\n\nApache License 2.0\n\n\norg.renjin.cran # RColorBrewer # 1.1-2-b321\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\ncom.clearspring.analytics # stream # 2.7.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\ncom.github.javaparser # javaparser-core # 3.2.5\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\ncom.github.tototoshi # scala-csv_2.11 # 1.1.2\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\ncom.jamesmurty.utils # java-xmlbuilder # 1.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\ncom.typesafe # config # 1.3.1\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\ncom.typesafe # ssl-config-core_2.11 # 0.2.1\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\ncom.typesafe.akka # akka-actor_2.11 # 2.5.3\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\ncom.typesafe.akka # akka-stream-testkit_2.11 # 2.4.19\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\ncom.typesafe.akka # akka-stream_2.11 # 2.5.3\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\ncom.typesafe.akka # akka-testkit_2.11 # 2.5.3\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\ncommons-codec # commons-codec # 1.10\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\ncommons-collections # commons-collections # 3.2.2\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\ncommons-io # commons-io # 2.6\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\nio.netty # netty # 3.9.9.Final\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\nio.netty # netty-all # 4.0.43.Final\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\norg.apache.commons # commons-compress # 1.15\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\norg.apache.commons # commons-crypto # 1.0.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\norg.apache.commons # commons-lang3 # 3.5\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\norg.apache.httpcomponents # httpclient # 4.5.1\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\norg.apache.httpcomponents # httpcore # 4.4.3\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\norg.apache.pdfbox # fontbox # 2.0.9\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\norg.apache.pdfbox # pdfbox # 2.0.9\n\n\n\n\n\n\n\n\nApache\n\n\nApache License, Version 2.0\n\n\norg.codehaus.jettison # jettison # 1.3.3\n\n\n\n\n\n\n\n\nApache\n\n\nApache Software License - Version 2.0\n\n\norg.eclipse.jetty # jetty-continuation # 8.1.13.v20130916\n\n\n\n\n\n\n\n\nApache\n\n\nApache Software License - Version 2.0\n\n\norg.eclipse.jetty # jetty-http # 8.1.13.v20130916\n\n\n\n\n\n\n\n\nApache\n\n\nApache Software License - Version 2.0\n\n\norg.eclipse.jetty # jetty-io # 8.1.13.v20130916\n\n\n\n\n\n\n\n\nApache\n\n\nApache Software License - Version 2.0\n\n\norg.eclipse.jetty # jetty-security # 8.1.13.v20130916\n\n\n\n\n\n\n\n\nApache\n\n\nApache Software License - Version 2.0\n\n\norg.eclipse.jetty # jetty-server # 8.1.13.v20130916\n\n\n\n\n\n\n\n\nApache\n\n\nApache Software License - Version 2.0\n\n\norg.eclipse.jetty # jetty-servlet # 8.1.13.v20130916\n\n\n\n\n\n\n\n\nApache\n\n\nApache Software License - Version 2.0\n\n\norg.eclipse.jetty # jetty-util # 8.1.13.v20130916\n\n\n\n\n\n\n\n\nApache\n\n\nApache Software License - Version 2.0\n\n\norg.eclipse.jetty # jetty-webapp # 8.1.13.v20130916\n\n\n\n\n\n\n\n\nApache\n\n\nApache Software License - Version 2.0\n\n\norg.eclipse.jetty # jetty-xml # 8.1.13.v20130916\n\n\n\n\n\n\n\n\nApache\n\n\nApache Software License - Version 2.0\n\n\norg.eclipse.jetty.orbit # javax.servlet # 3.0.0.v201112011016\n\n\n\n\n\n\n\n\nApache\n\n\nApache Software License - Version 2.0\n\n\norg.mortbay.jetty # jetty-util # 6.1.26\n\n\n\n\n\n\n\n\nApache\n\n\nApache-2.0\n\n\ncom.typesafe.akka # akka-http-core_2.11 # 10.0.9\n\n\n\n\n\n\n\n\nApache\n\n\nApache-2.0\n\n\ncom.typesafe.akka # akka-http-spray-json_2.11 # 10.0.9\n\n\n\n\n\n\n\n\nApache\n\n\nApache-2.0\n\n\ncom.typesafe.akka # akka-http-testkit_2.11 # 10.0.9\n\n\n\n\n\n\n\n\nApache\n\n\nApache-2.0\n\n\ncom.typesafe.akka # akka-http_2.11 # 10.0.9\n\n\n\n\n\n\n\n\nApache\n\n\nApache-2.0\n\n\ncom.typesafe.akka # akka-parsing_2.11 # 10.0.9\n\n\n\n\n\n\n\n\nApache\n\n\nApache-2.0\n\n\norg.json4s # json4s-ast_2.11 # 3.3.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache-2.0\n\n\norg.json4s # json4s-core_2.11 # 3.3.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache-2.0\n\n\norg.json4s # json4s-native_2.11 # 3.3.0\n\n\n\n\n\n\n\n\nApache\n\n\nApache-2.0\n\n\norg.json4s # json4s-scalap_2.11 # 3.3.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache License, Version 2.0\n\n\ncom.github.ghik # silencer-lib_2.11 # 0.6\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache License, Version 2.0\n\n\norg.spark-project.spark # unused # 1.0.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\nar.com.hjg # pngj # 2.1.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.carrotsearch # hppc # 0.6.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.drewnoakes # metadata-extractor # 2.8.1\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.fasterxml.jackson.core # jackson-annotations # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.fasterxml.jackson.core # jackson-core # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.fasterxml.jackson.core # jackson-databind # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.fasterxml.jackson.module # jackson-module-paranamer # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.fasterxml.jackson.module # jackson-module-scala_2.11 # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.google.code.findbugs # jsr305 # 1.3.9\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.google.code.gson # gson # 2.2.4\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.google.guava # guava # 14.0.1\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.google.inject # guice # 3.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.sksamuel.scrimage # scrimage-core_2.11 # 2.1.8\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.sksamuel.scrimage # scrimage-filters_2.11 # 2.1.8\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncom.sksamuel.scrimage # scrimage-io-extra_2.11 # 2.1.8\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncommons-beanutils # commons-beanutils-core # 1.8.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncommons-cli # commons-cli # 1.2\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncommons-configuration # commons-configuration # 1.6\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncommons-digester # commons-digester # 1.8\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncommons-lang # commons-lang # 2.6\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncommons-logging # commons-logging # 1.2\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\ncommons-net # commons-net # 2.2\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\njavax.inject # javax.inject # 1\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\njavax.validation # validation-api # 1.1.0.Final\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\nlog4j # log4j # 1.2.17\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\nmx4j # mx4j # 3.0.2\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\nnet.jpountz.lz4 # lz4 # 1.3.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.ant # ant # 1.8.3\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.ant # ant-launcher # 1.8.3\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.avro # avro # 1.7.7\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.avro # avro-ipc # 1.7.7\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.avro # avro-mapred # 1.7.7\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.commons # commons-math # 2.2\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.commons # commons-math3 # 3.4.1\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.commons # commons-vfs2 # 2.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.curator # curator-client # 2.6.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.curator # curator-framework # 2.6.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.curator # curator-recipes # 2.6.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.directory.api # api-asn1-api # 1.0.0-M20\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.directory.api # api-util # 1.0.0-M20\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.directory.server # apacheds-i18n # 2.0.0-M15\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.directory.server # apacheds-kerberos-codec # 2.0.0-M15\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-annotations # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-auth # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-client # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-common # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-hdfs # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-mapreduce-client-app # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-mapreduce-client-common # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-mapreduce-client-core # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-mapreduce-client-jobclient # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-mapreduce-client-shuffle # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-yarn-api # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-yarn-client # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-yarn-common # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-yarn-server-common # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.hadoop # hadoop-yarn-server-nodemanager # 2.6.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.ivy # ivy # 2.4.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.maven.scm # maven-scm-api # 1.4\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.maven.scm # maven-scm-provider-svn-commons # 1.4\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.maven.scm # maven-scm-provider-svnexe # 1.4\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.parquet # parquet-column # 1.8.2\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.parquet # parquet-common # 1.8.2\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.parquet # parquet-encoding # 1.8.2\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.parquet # parquet-format # 2.3.1\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.parquet # parquet-hadoop # 1.8.2\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.parquet # parquet-jackson # 1.8.2\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.poi # poi # 3.14\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.poi # poi-ooxml # 3.14\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.poi # poi-ooxml-schemas # 3.14\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.apache.xmlbeans # xmlbeans # 2.6.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.codehaus.groovy # groovy # 1.8.9\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.codehaus.jackson # jackson-core-asl # 1.9.13\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.codehaus.jackson # jackson-jaxrs # 1.9.13\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.codehaus.jackson # jackson-mapper-asl # 1.9.13\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.codehaus.jackson # jackson-xc # 1.9.13\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.codehaus.plexus # plexus-utils # 1.5.6\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.fusesource.jansi # jansi # 1.5\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.htrace # htrace-core # 3.0.4\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.la4j # la4j # 0.6.0\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.renjin # renjin-asm # 5.0.4b\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.renjin # renjin-guava # 17.0b\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.scala-graph # graph-core_2.11 # 1.11.3\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.smurn # jply # 0.2.1\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.sonatype.sisu.inject # cglib # 2.2.1-v20090111\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.tensorflow # proto # 1.9.0-rc1\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\norg.xerial.snappy # snappy-java # 1.1.2.6\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\nstax # stax-api # 1.0.1\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\nxerces # xercesImpl # 2.9.1\n\n\n\n\n\n\n\n\nApache\n\n\nThe Apache Software License, Version 2.0\n\n\nxml-apis # xml-apis # 1.3.04\n\n\n\n\n\n\n\n\nApache\n\n\nthe Apache License, ASL Version 2.0\n\n\norg.scalactic # scalactic_2.11 # 3.0.4\n\n\n\n\n\n\n\n\nBSD\n\n\n3-Clause BSD License\n\n\ncom.google.protobuf # protobuf-java # 3.5.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\nasm # asm # 3.2\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\nasm # asm-analysis # 3.2\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\nasm # asm-commons # 3.2\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\nasm # asm-tree # 3.2\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\nasm # asm-util # 3.2\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\ncom.miglayout # miglayout # 3.7.4\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\ncom.thoughtworks.paranamer # paranamer # 2.8\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\njline # jline # 0.9.94\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\nnet.sourceforge.jmatio # jmatio # 1.0\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalafx # scalafx_2.11 # 8.0.92-R10\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalameta # common_2.11 # 2.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalameta # dialects_2.11 # 2.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalameta # inputs_2.11 # 2.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalameta # io_2.11 # 2.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalameta # langmeta_2.11 # 2.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalameta # parsers_2.11 # 2.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalameta # quasiquotes_2.11 # 2.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalameta # scalameta_2.11 # 2.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalameta # semanticdb_2.11 # 2.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalameta # tokenizers_2.11 # 2.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalameta # tokens_2.11 # 2.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalameta # transversers_2.11 # 2.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD\n\n\norg.scalameta # trees_2.11 # 2.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # all # 1.1.2\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # core # 1.1.2\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # native_ref-java # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # native_system-java # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # netlib-native_ref-linux-armhf # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # netlib-native_ref-linux-i686 # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # netlib-native_ref-linux-x86_64 # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # netlib-native_ref-osx-x86_64 # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # netlib-native_ref-win-i686 # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # netlib-native_ref-win-x86_64 # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # netlib-native_system-linux-armhf # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # netlib-native_system-linux-i686 # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # netlib-native_system-linux-x86_64 # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # netlib-native_system-osx-x86_64 # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # netlib-native_system-win-i686 # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3 Clause\n\n\ncom.github.fommil.netlib # netlib-native_system-win-x86_64 # 1.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-Clause\n\n\ncom.tinkerpop # frames # 2.5.0\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-Clause\n\n\ncom.tinkerpop # pipes # 2.6.0\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-Clause\n\n\ncom.tinkerpop.blueprints # blueprints-core # 2.6.0\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-Clause\n\n\ncom.tinkerpop.gremlin # gremlin-groovy # 2.5.0\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-Clause\n\n\ncom.tinkerpop.gremlin # gremlin-java # 2.6.0\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-Clause\n\n\norg.scala-lang # scala-compiler # 2.11.8\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-Clause\n\n\norg.scala-lang # scala-library # 2.11.8\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-Clause\n\n\norg.scala-lang # scala-reflect # 2.11.8\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-Clause\n\n\norg.webjars.bower # vega # 2.6.5\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-Clause\n\n\norg.webjars.bower # vega-lite # 1.2.0\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-Clause License\n\n\norg.jpmml # pmml-model # 1.2.15\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-Clause License\n\n\norg.jpmml # pmml-schema # 1.2.15\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-clause\n\n\norg.scala-lang.modules # scala-java8-compat_2.11 # 0.7.0\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-clause\n\n\norg.scala-lang.modules # scala-parser-combinators_2.11 # 1.0.4\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-clause\n\n\norg.scala-lang.modules # scala-swing_2.11 # 1.0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD 3-clause\n\n\norg.scala-lang.modules # scala-xml_2.11 # 1.0.6\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD License\n\n\nantlr # antlr # 2.7.7\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD License\n\n\ncom.github.virtuald # curvesapi # 1.03\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD style\n\n\ncom.thoughtworks.xstream # xstream # 1.4.7\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD-2 License\n\n\norg.jogamp.gluegen # gluegen-rt # 2.3.2\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD-2 License\n\n\norg.jogamp.gluegen # gluegen-rt-main # 2.3.2\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD-2 License\n\n\norg.jogamp.jogl # jogl-all # 2.3.2\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD-2 License\n\n\norg.jogamp.jogl # jogl-all-main # 2.3.2\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD-3-Clause\n\n\norg.webjars.bower # d3 # 3.5.17\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD-like\n\n\norg.scala-lang # jline # 2.11.0-M3\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD-like\n\n\norg.scala-lang # scala-pickling_2.11 # 0.9.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD-style\n\n\norg.scalaforge # scalax # 0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD-style\n\n\norg.scalaz # scalaz-concurrent_2.11 # 7.2.16\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD-style\n\n\norg.scalaz # scalaz-core_2.11 # 7.2.16\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD-style\n\n\norg.scalaz # scalaz-effect_2.11 # 7.2.16\n\n\n\n\n\n\n\n\nBSD\n\n\nBSD_3_clause + file LICENSE\n\n\norg.renjin.cran # colorspace # 1.3-2-b41\n\n\n\n\n\n\n\n\nBSD\n\n\nNew BSD License\n\n\ncom.diffplug.matsim # matfilerw # 3.0.0\n\n\n\n\n\n\n\n\nBSD\n\n\nNew BSD License\n\n\ncom.esotericsoftware # kryo-shaded # 3.0.3\n\n\n\n\n\n\n\n\nBSD\n\n\nNew BSD License\n\n\ncom.esotericsoftware # minlog # 1.3.0\n\n\n\n\n\n\n\n\nBSD\n\n\nNew BSD License\n\n\norg.codehaus.janino # commons-compiler # 3.0.0\n\n\n\n\n\n\n\n\nBSD\n\n\nNew BSD License\n\n\norg.codehaus.janino # janino # 3.0.0\n\n\n\n\n\n\n\n\nBSD\n\n\nNew BSD License\n\n\norg.hamcrest # hamcrest-core # 1.3\n\n\n\n\n\n\n\n\nBSD\n\n\nThe (New) BSD License\n\n\norg.jzy3d # jzy3d-api # 1.0.2\n\n\n\n\n\n\n\n\nBSD\n\n\nThe (New) BSD License\n\n\norg.jzy3d # jzy3d-jdt-core # 1.0.2\n\n\n\n\n\n\n\n\nBSD\n\n\nThe BSD 3-Clause License\n\n\norg.fusesource.leveldbjni # leveldbjni-all # 1.8\n\n\n\n\n\n\n\n\nBSD\n\n\nThe BSD License\n\n\ncom.adobe.xmp # xmpcore # 5.1.2\n\n\n\n\n\n\n\n\nBSD\n\n\nThe BSD License\n\n\nnet.sourceforge.f2j # arpack_combined_all # 0.1\n\n\n\n\n\n\n\n\nBSD\n\n\nThe BSD License\n\n\norg.antlr # antlr4-runtime # 4.5.3\n\n\n\n\n\n\n\n\nBSD\n\n\nThe BSD License\n\n\norg.jline # jline # 3.6.2\n\n\n\n\n\n\n\n\nBSD\n\n\nThe BSD License\n\n\norg.jline # jline-terminal # 3.6.2\n\n\n\n\n\n\n\n\nBSD\n\n\nThe BSD License\n\n\norg.jline # jline-terminal-jna # 3.6.2\n\n\n\n\n\n\n\n\nBSD\n\n\nThe BSD License\n\n\nxmlenc # xmlenc # 0.52\n\n\n\n\n\n\n\n\nBSD\n\n\nThe New BSD License\n\n\nnet.sf.py4j # py4j # 0.10.4\n\n\n\n\n\n\n\n\nCC0\n\n\nCC0\n\n\norg.reactivestreams # reactive-streams # 1.0.0\n\n\n\n\n\n\n\n\nCDDL\n\n\nCOMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0\n\n\njavax.activation # activation # 1.1.1\n\n\n\n\n\n\n\n\nCDDL\n\n\nCOMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0\n\n\njavax.xml.stream # stax-api # 1.0-2\n\n\n\n\n\n\n\n\nGPL\n\n\nCDDL v1.1 / GPL v2 dual license\n\n\ncom.sun.codemodel # codemodel # 2.6\n\n\n\n\n\n\n\n\nGPL\n\n\nCDDL+GPL License\n\n\norg.glassfish.jersey.bundles.repackaged # jersey-guava # 2.22.2\n\n\n\n\n\n\n\n\nGPL\n\n\nCDDL+GPL License\n\n\norg.glassfish.jersey.containers # jersey-container-servlet # 2.22.2\n\n\n\n\n\n\n\n\nGPL\n\n\nCDDL+GPL License\n\n\norg.glassfish.jersey.containers # jersey-container-servlet-core # 2.22.2\n\n\n\n\n\n\n\n\nGPL\n\n\nCDDL+GPL License\n\n\norg.glassfish.jersey.core # jersey-client # 2.22.2\n\n\n\n\n\n\n\n\nGPL\n\n\nCDDL+GPL License\n\n\norg.glassfish.jersey.core # jersey-common # 2.22.2\n\n\n\n\n\n\n\n\nGPL\n\n\nCDDL+GPL License\n\n\norg.glassfish.jersey.core # jersey-server # 2.22.2\n\n\n\n\n\n\n\n\nGPL\n\n\nCDDL+GPL License\n\n\norg.glassfish.jersey.media # jersey-media-jaxb # 2.22.2\n\n\n\n\n\n\n\n\nGPL\n\n\nGNU General Public License (GPL)\n\n\norg.jfree # jfreesvg # 3.3\n\n\n\n\n\n\n\n\nGPL\n\n\nGNU General Public License 3\n\n\norg.openml # apiconnector # 1.0.11\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL (>= 2)\n\n\norg.renjin.cran # MatrixModels # 0.4-1-b237\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL (>= 2)\n\n\norg.renjin.cran # SparseM # 1.77-b20\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL (>= 2)\n\n\norg.renjin.cran # digest # 0.6.15-b10\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL (>= 2)\n\n\norg.renjin.cran # lattice # 0.20-35-b66\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL (>= 2)\n\n\norg.renjin.cran # locfit # 1.5-9.1-b345\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL (>= 2)\n\n\norg.renjin.cran # quantreg # 5.35-b5\n\n\n\n\n\n\n\n\nGPL\n\n\n[GPL (>= 2)\n\n\nfile LICENCE](null)\n\n\norg.renjin.cran # Matrix # 1.2-14-b2\n\n\n\n\n\n\nGPL\n\n\nGPL (>= 3)\n\n\norg.renjin.cran # abc # 2.1-b294\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL (>= 3)\n\n\norg.renjin.cran # abc.data # 1.0-b272\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL-2\n\n\norg.renjin.cran # dichromat # 2.0-0-b332\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL-2\n\n\norg.renjin.cran # gtable # 0.2.0-b96\n\n\n\n\n\n\n\n\nGPL\n\n\n[GPL-2\n\n\nGPL-3](null)\n\n\norg.renjin.cran # MASS # 7.3-49-b11\n\n\n\n\n\n\nGPL\n\n\n[GPL-2\n\n\nGPL-3](null)\n\n\norg.renjin.cran # nnet # 7.3-12-b88\n\n\n\n\n\n\nGPL\n\n\n[GPL-2\n\n\nfile LICENSE](null)\n\n\norg.renjin.cran # ggplot2 # 2.2.1-b112\n\n\n\n\n\n\nGPL\n\n\n[GPL-2\n\n\nfile LICENSE](null)\n\n\norg.renjin.cran # stringr # 1.3.0-b7\n\n\n\n\n\n\nGPL\n\n\nGPL-3\n\n\norg.renjin.cran # assertthat # 0.2.0-b42\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL-3\n\n\norg.renjin.cran # lazyeval # 0.2.1-b19\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL-3\n\n\norg.renjin.cran # pillar # 1.2.2-b2\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL-3\n\n\norg.renjin.cran # rlang # 0.2.0-b39\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL2 w/ CPE\n\n\njavax.ws.rs # javax.ws.rs-api # 2.0-m10\n\n\n\n\n\n\n\n\nGPL\n\n\nGPL2 w/ CPE\n\n\njavax.xml.bind # jaxb-api # 2.2.2\n\n\n\n\n\n\n\n\nGPL\n\n\nGPLv2+CE\n\n\njavax.mail # mail # 1.4.7\n\n\n\n\n\n\n\n\nGPL with Classpath Extension\n\n\nCDDL + GPLv2 with classpath exception\n\n\njavax.annotation # javax.annotation-api # 1.2\n\n\n\n\n\n\n\n\nGPL with Classpath Extension\n\n\nCDDL + GPLv2 with classpath exception\n\n\njavax.servlet # javax.servlet-api # 3.1.0\n\n\n\n\n\n\n\n\nGPL with Classpath Extension\n\n\nCDDL + GPLv2 with classpath exception\n\n\norg.glassfish.hk2 # hk2-api # 2.4.0-b34\n\n\n\n\n\n\n\n\nGPL with Classpath Extension\n\n\nCDDL + GPLv2 with classpath exception\n\n\norg.glassfish.hk2 # hk2-locator # 2.4.0-b34\n\n\n\n\n\n\n\n\nGPL with Classpath Extension\n\n\nCDDL + GPLv2 with classpath exception\n\n\norg.glassfish.hk2 # hk2-utils # 2.4.0-b34\n\n\n\n\n\n\n\n\nGPL with Classpath Extension\n\n\nCDDL + GPLv2 with classpath exception\n\n\norg.glassfish.hk2 # osgi-resource-locator # 1.0.1\n\n\n\n\n\n\n\n\nGPL with Classpath Extension\n\n\nCDDL + GPLv2 with classpath exception\n\n\norg.glassfish.hk2.external # aopalliance-repackaged # 2.4.0-b34\n\n\n\n\n\n\n\n\nGPL with Classpath Extension\n\n\nCDDL + GPLv2 with classpath exception\n\n\norg.glassfish.hk2.external # javax.inject # 2.4.0-b34\n\n\n\n\n\n\n\n\nLGPL\n\n\nGNU Lesser General Public License\n\n\nch.qos.logback # logback-classic # 1.2.3\n\n\n\n\n\n\n\n\nLGPL\n\n\nGNU Lesser General Public License\n\n\nch.qos.logback # logback-core # 1.2.3\n\n\n\n\n\n\n\n\nLGPL\n\n\nGNU Lesser General Public License 2.1\n\n\nnet.sf.trove4j # trove4j # 3.0.3\n\n\n\n\n\n\n\n\nLGPL\n\n\nGNU Lesser General Public License v3.0\n\n\ncom.github.vagmcs # optimus_2.11 # 2.0.0\n\n\n\n\n\n\n\n\nLGPL\n\n\nLGPL\n\n\ncom.github.fommil # jniloader # 1.1\n\n\n\n\n\n\n\n\nLGPL\n\n\nLGPL, version 2.1\n\n\nnet.java.dev.jna # jna # 4.2.2\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\nco.theasi # plotly_2.11 # 0.1\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\ncom.github.julien-truffaut # monocle-core_2.11 # 1.1.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\ncom.github.julien-truffaut # monocle-macro_2.11 # 1.1.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\ncom.lihaoyi # fansi_2.11 # 0.2.4\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\ncom.lihaoyi # pprint_2.11 # 0.5.2\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\ncom.lihaoyi # sourcecode_2.11 # 0.1.4\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\ncom.lihaoyi # ujson-jvm-2.11.11_2.11 # 0.6.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\ncom.lihaoyi # upickle_2.11 # 0.6.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\nnet.databinder # unfiltered-filter_2.11 # 0.8.3\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\nnet.databinder # unfiltered-jetty_2.11 # 0.8.3\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\nnet.databinder # unfiltered-util_2.11 # 0.8.3\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\nnet.databinder # unfiltered_2.11 # 0.8.3\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\norg.spire-math # jawn-parser_2.11 # 0.11.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\norg.typelevel # algebra_2.11 # 0.7.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\norg.typelevel # cats-core_2.11 # 1.0.1\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\norg.typelevel # cats-kernel_2.11 # 1.0.1\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\norg.typelevel # cats-macros_2.11 # 1.0.1\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\norg.typelevel # machinist_2.11 # 0.6.2\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\norg.typelevel # spire-macros_2.11 # 0.14.1\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT\n\n\norg.typelevel # spire_2.11 # 0.14.1\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT + file LICENSE\n\n\norg.renjin.cran # R6 # 2.2.2-b44\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT + file LICENSE\n\n\norg.renjin.cran # cli # 1.0.0-b25\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT + file LICENSE\n\n\norg.renjin.cran # crayon # 1.3.4-b16\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT + file LICENSE\n\n\norg.renjin.cran # glue # 1.2.0-b22\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT + file LICENSE\n\n\norg.renjin.cran # magrittr # 1.5-b334\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT + file LICENSE\n\n\norg.renjin.cran # munsell # 0.4.3-b96\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT + file LICENSE\n\n\norg.renjin.cran # plyr # 1.8.4-b82\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT + file LICENSE\n\n\norg.renjin.cran # reshape2 # 1.4.3-b16\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT + file LICENSE\n\n\norg.renjin.cran # scales # 0.5.0-b32\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT + file LICENSE\n\n\norg.renjin.cran # tibble # 1.4.2-b7\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT + file LICENSE\n\n\norg.renjin.cran # viridisLite # 0.3.0-b7\n\n\n\n\n\n\n\n\nMIT\n\n\n[MIT + file LICENSE\n\n\nUnlimited](null)\n\n\norg.renjin.cran # labeling # 0.3-b313\n\n\n\n\n\n\nMIT\n\n\nMIT License\n\n\ncom.github.scopt # scopt_2.11 # 3.5.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT License\n\n\nnet.razorvine # pyrolite # 4.13\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT License\n\n\norg.slf4j # jcl-over-slf4j # 1.7.16\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT License\n\n\norg.slf4j # jul-to-slf4j # 1.7.16\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT License\n\n\norg.slf4j # slf4j-api # 1.7.25\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT License\n\n\norg.vegas-viz # vegas-macros_2.11 # 0.3.11\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT License\n\n\norg.vegas-viz # vegas_2.11 # 0.3.11\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT license\n\n\ncom.lihaoyi # ammonite-compiler_2.11.8 # 1.1.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT license\n\n\ncom.lihaoyi # ammonite-ops_2.11 # 1.1.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT license\n\n\ncom.lihaoyi # ammonite-repl_2.11.8 # 1.1.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT license\n\n\ncom.lihaoyi # ammonite-runtime_2.11 # 1.1.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT license\n\n\ncom.lihaoyi # ammonite-sshd_2.11.8 # 1.1.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT license\n\n\ncom.lihaoyi # ammonite-terminal_2.11 # 1.1.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT license\n\n\ncom.lihaoyi # ammonite-util_2.11 # 1.1.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT license\n\n\ncom.lihaoyi # ammonite_2.11.8 # 1.1.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT license\n\n\ncom.lihaoyi # fastparse-utils_2.11 # 1.0.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT license\n\n\ncom.lihaoyi # fastparse_2.11 # 1.0.0\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT license\n\n\ncom.lihaoyi # geny_2.11 # 0.1.2\n\n\n\n\n\n\n\n\nMIT\n\n\nMIT license\n\n\ncom.lihaoyi # scalaparse_2.11 # 1.0.0\n\n\n\n\n\n\n\n\nMozilla\n\n\nMPL\n\n\ncom.github.rwl # jtransforms # 2.4.0\n\n\n\n\n\n\n\n\nMozilla\n\n\nMPL 1.1\n\n\norg.javassist # javassist # 3.21.0-GA\n\n\n\n\n\n\n\n\nPublic Domain\n\n\nPublic Domain\n\n\naopalliance # aopalliance # 1.0\n\n\n\n\n\n\n\n\nPublic Domain\n\n\nPublic Domain\n\n\ngov.nist.math # scimark # 2.0\n\n\n\n\n\n\n\n\nPublic Domain\n\n\nPublic Domain\n\n\nxmlpull # xmlpull # 1.1.3.1\n\n\n\n\n\n\n\n\nPublic Domain\n\n\nPublic Domain\n\n\nxpp3 # xpp3_min # 1.1.4c\n\n\n\n\n\n\n\n\nPublic Domain\n\n\nPublic domain\n\n\nnet.iharder # base64 # 2.3.8\n\n\n\n\n\n\n\n\nunrecognized\n\n\nASL\n\n\norg.json4s # json4s-jackson_2.11 # 3.2.11\n\n\n\n\n\n\n\n\nunrecognized\n\n\nBouncy Castle Licence\n\n\norg.bouncycastle # bcprov-jdk15on # 1.56\n\n\n\n\n\n\n\n\nunrecognized\n\n\nEclipse Public License 1.0\n\n\njunit # junit # 4.12\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Lesser General Public Licence\n\n\ncom.github.wookietreiber # scala-chart_2.11 # 0.4.2\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Lesser General Public Licence\n\n\norg.jfree # jcommon # 1.0.21\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Lesser General Public Licence\n\n\norg.jfree # jfreechart # 1.0.17\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # compiler # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # datasets # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # gcc-runtime # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # grDevices # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # graphics # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # grid # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # methods # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # parallel # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # renjin-appl # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # renjin-blas # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # renjin-core # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # renjin-gnur-runtime # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # renjin-lapack # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # renjin-math-common # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # renjin-nmath # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # renjin-script-engine # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # splines # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # stats # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # tcltk # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # tools # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nGNU Public License, v2\n\n\norg.renjin # utils # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nPart of R 2.14.2\n\n\norg.renjin # stats4 # 0.9.2643\n\n\n\n\n\n\n\n\nunrecognized\n\n\nThe JSON License\n\n\norg.json # json # 20140107\n\n\n\n\n\n\n\n\nunrecognized\n\n\nUnicode/ICU License\n\n\ncom.ibm.icu # icu4j # 59.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nUnknown License\n\n\norg.apache.xbean # xbean-asm5-shaded # 4.4\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.common # common-image # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.common # common-io # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.common # common-lang # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-bmp # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-core # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-icns # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-iff # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-jpeg # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-metadata # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-pcx # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-pdf # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-pict # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-pnm # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-psd # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-sgi # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-tga # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-thumbsdb # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncom.twelvemonkeys.imageio # imageio-tiff # 3.2.1\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\ncommons-beanutils # commons-beanutils # 1.7.0\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\norg.apache.zookeeper # zookeeper # 3.4.6\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\norg.renjin # libstdcxx # 4.7.4-b18\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\norg.renjin.cran # Rcpp # 0.12.13-renjin-15\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\norg.renjin.cran # stringi # 1.1.6-renjin-b22\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\noro # oro # 2.0.8\n\n\n\n\n\n\n\n\nunrecognized\n\n\nnone specified\n\n\nregexp # regexp # 1.3",
            "title": "License"
        },
        {
            "location": "/license/#dependency-licenses",
            "text": "The various components of DynaML which are enabled using dependencies are\nsubject to their license requirements. The following table lists\nall the component licences used by DynaML's dependencies.     Category  License  Dependency  Notes      Apache  Apache 2  com.chuusai # shapeless_2.11 # 2.3.3     Apache  Apache 2  com.quantifind # sumac_2.11 # 0.3.0     Apache  Apache 2  com.quantifind # wisp_2.11 # 0.0.4     Apache  Apache 2  com.trueaccord.lenses # lenses_2.11 # 0.4.12     Apache  Apache 2  com.trueaccord.scalapb # scalapb-runtime_2.11 # 0.6.2     Apache  Apache 2  com.twitter # chill-java # 0.8.0     Apache  Apache 2  com.twitter # chill_2.11 # 0.8.0     Apache  Apache 2  com.univocity # univocity-parsers # 2.2.1     Apache  Apache 2  io.spray # spray-json_2.11 # 1.3.3     Apache  Apache 2  joda-time # joda-time # 2.0     Apache  Apache 2  net.sf.opencsv # opencsv # 2.3     Apache  Apache 2  org.objenesis # objenesis # 2.6     Apache  Apache 2  org.roaringbitmap # RoaringBitmap # 0.5.11     Apache  Apache 2  org.scalaj # scalaj-http_2.11 # 2.3.0     Apache  Apache 2  org.scalanlp # breeze-macros_2.11 # 0.13.2     Apache  Apache 2  org.scalanlp # breeze-natives_2.11 # 0.13.2     Apache  Apache 2  org.scalanlp # breeze_2.11 # 0.13.2     Apache  Apache 2  org.typelevel # macro-compat_2.11 # 1.1.1     Apache  Apache 2.0  io.circe # circe-core_2.11 # 0.9.1     Apache  Apache 2.0  io.circe # circe-generic_2.11 # 0.9.1     Apache  Apache 2.0  io.circe # circe-jawn_2.11 # 0.9.1     Apache  Apache 2.0  io.circe # circe-numbers_2.11 # 0.9.1     Apache  Apache 2.0  io.circe # circe-parser_2.11 # 0.9.1     Apache  Apache 2.0  io.get-coursier # coursier-cache_2.11 # 1.0.0     Apache  Apache 2.0  io.get-coursier # coursier_2.11 # 1.0.0     Apache  Apache 2.0  net.java.dev.jets3t # jets3t # 0.9.3     Apache  Apache 2.0 License  com.typesafe.scala-logging # scala-logging_2.11 # 3.9.0     Apache  Apache 2.0 License  org.apache.spark # spark-catalyst_2.11 # 2.2.0     Apache  Apache 2.0 License  org.apache.spark # spark-core_2.11 # 2.2.0     Apache  Apache 2.0 License  org.apache.spark # spark-graphx_2.11 # 2.2.0     Apache  Apache 2.0 License  org.apache.spark # spark-launcher_2.11 # 2.2.0     Apache  Apache 2.0 License  org.apache.spark # spark-mllib-local_2.11 # 2.2.0     Apache  Apache 2.0 License  org.apache.spark # spark-mllib_2.11 # 2.2.0     Apache  Apache 2.0 License  org.apache.spark # spark-network-common_2.11 # 2.2.0     Apache  Apache 2.0 License  org.apache.spark # spark-network-shuffle_2.11 # 2.2.0     Apache  Apache 2.0 License  org.apache.spark # spark-sketch_2.11 # 2.2.0     Apache  Apache 2.0 License  org.apache.spark # spark-sql_2.11 # 2.2.0     Apache  Apache 2.0 License  org.apache.spark # spark-streaming_2.11 # 2.2.0     Apache  Apache 2.0 License  org.apache.spark # spark-tags_2.11 # 2.2.0     Apache  Apache 2.0 License  org.apache.spark # spark-unsafe_2.11 # 2.2.0     Apache  Apache 2.0 License  org.apache.sshd # sshd-core # 1.2.0     Apache  Apache License  commons-httpclient # commons-httpclient # 3.1     Apache  Apache License  info.folone # poi-scala_2.11 # 0.18     Apache  Apache License  org.apache.httpcomponents # httpmime # 4.2.5     Apache  Apache License 2.0  org.renjin.cran # utf8 # 1.1.3-b8     Apache  Apache License 2.0  com.ning # compress-lzf # 1.0.3     Apache  Apache License 2.0  io.dropwizard.metrics # metrics-core # 3.1.2     Apache  Apache License 2.0  io.dropwizard.metrics # metrics-graphite # 3.1.2     Apache  Apache License 2.0  io.dropwizard.metrics # metrics-json # 3.1.2     Apache  Apache License 2.0  io.dropwizard.metrics # metrics-jvm # 3.1.2     Apache  Apache License 2.0  org.platanios # tensorflow-api_2.11 # 0.2.4     Apache  Apache License 2.0  org.platanios # tensorflow-data_2.11 # 0.2.4     Apache  Apache License 2.0  org.platanios # tensorflow-jni_2.11 # 0.2.4     Apache  Apache License 2.0  org.platanios # tensorflow_2.11 # 0.2.4     Apache  Apache License 2.0  org.renjin.cran # RColorBrewer # 1.1-2-b321     Apache  Apache License, Version 2.0  com.clearspring.analytics # stream # 2.7.0     Apache  Apache License, Version 2.0  com.github.javaparser # javaparser-core # 3.2.5     Apache  Apache License, Version 2.0  com.github.tototoshi # scala-csv_2.11 # 1.1.2     Apache  Apache License, Version 2.0  com.jamesmurty.utils # java-xmlbuilder # 1.0     Apache  Apache License, Version 2.0  com.typesafe # config # 1.3.1     Apache  Apache License, Version 2.0  com.typesafe # ssl-config-core_2.11 # 0.2.1     Apache  Apache License, Version 2.0  com.typesafe.akka # akka-actor_2.11 # 2.5.3     Apache  Apache License, Version 2.0  com.typesafe.akka # akka-stream-testkit_2.11 # 2.4.19     Apache  Apache License, Version 2.0  com.typesafe.akka # akka-stream_2.11 # 2.5.3     Apache  Apache License, Version 2.0  com.typesafe.akka # akka-testkit_2.11 # 2.5.3     Apache  Apache License, Version 2.0  commons-codec # commons-codec # 1.10     Apache  Apache License, Version 2.0  commons-collections # commons-collections # 3.2.2     Apache  Apache License, Version 2.0  commons-io # commons-io # 2.6     Apache  Apache License, Version 2.0  io.netty # netty # 3.9.9.Final     Apache  Apache License, Version 2.0  io.netty # netty-all # 4.0.43.Final     Apache  Apache License, Version 2.0  org.apache.commons # commons-compress # 1.15     Apache  Apache License, Version 2.0  org.apache.commons # commons-crypto # 1.0.0     Apache  Apache License, Version 2.0  org.apache.commons # commons-lang3 # 3.5     Apache  Apache License, Version 2.0  org.apache.httpcomponents # httpclient # 4.5.1     Apache  Apache License, Version 2.0  org.apache.httpcomponents # httpcore # 4.4.3     Apache  Apache License, Version 2.0  org.apache.pdfbox # fontbox # 2.0.9     Apache  Apache License, Version 2.0  org.apache.pdfbox # pdfbox # 2.0.9     Apache  Apache License, Version 2.0  org.codehaus.jettison # jettison # 1.3.3     Apache  Apache Software License - Version 2.0  org.eclipse.jetty # jetty-continuation # 8.1.13.v20130916     Apache  Apache Software License - Version 2.0  org.eclipse.jetty # jetty-http # 8.1.13.v20130916     Apache  Apache Software License - Version 2.0  org.eclipse.jetty # jetty-io # 8.1.13.v20130916     Apache  Apache Software License - Version 2.0  org.eclipse.jetty # jetty-security # 8.1.13.v20130916     Apache  Apache Software License - Version 2.0  org.eclipse.jetty # jetty-server # 8.1.13.v20130916     Apache  Apache Software License - Version 2.0  org.eclipse.jetty # jetty-servlet # 8.1.13.v20130916     Apache  Apache Software License - Version 2.0  org.eclipse.jetty # jetty-util # 8.1.13.v20130916     Apache  Apache Software License - Version 2.0  org.eclipse.jetty # jetty-webapp # 8.1.13.v20130916     Apache  Apache Software License - Version 2.0  org.eclipse.jetty # jetty-xml # 8.1.13.v20130916     Apache  Apache Software License - Version 2.0  org.eclipse.jetty.orbit # javax.servlet # 3.0.0.v201112011016     Apache  Apache Software License - Version 2.0  org.mortbay.jetty # jetty-util # 6.1.26     Apache  Apache-2.0  com.typesafe.akka # akka-http-core_2.11 # 10.0.9     Apache  Apache-2.0  com.typesafe.akka # akka-http-spray-json_2.11 # 10.0.9     Apache  Apache-2.0  com.typesafe.akka # akka-http-testkit_2.11 # 10.0.9     Apache  Apache-2.0  com.typesafe.akka # akka-http_2.11 # 10.0.9     Apache  Apache-2.0  com.typesafe.akka # akka-parsing_2.11 # 10.0.9     Apache  Apache-2.0  org.json4s # json4s-ast_2.11 # 3.3.0     Apache  Apache-2.0  org.json4s # json4s-core_2.11 # 3.3.0     Apache  Apache-2.0  org.json4s # json4s-native_2.11 # 3.3.0     Apache  Apache-2.0  org.json4s # json4s-scalap_2.11 # 3.3.0     Apache  The Apache License, Version 2.0  com.github.ghik # silencer-lib_2.11 # 0.6     Apache  The Apache License, Version 2.0  org.spark-project.spark # unused # 1.0.0     Apache  The Apache Software License, Version 2.0  ar.com.hjg # pngj # 2.1.0     Apache  The Apache Software License, Version 2.0  com.carrotsearch # hppc # 0.6.0     Apache  The Apache Software License, Version 2.0  com.drewnoakes # metadata-extractor # 2.8.1     Apache  The Apache Software License, Version 2.0  com.fasterxml.jackson.core # jackson-annotations # 2.6.5     Apache  The Apache Software License, Version 2.0  com.fasterxml.jackson.core # jackson-core # 2.6.5     Apache  The Apache Software License, Version 2.0  com.fasterxml.jackson.core # jackson-databind # 2.6.5     Apache  The Apache Software License, Version 2.0  com.fasterxml.jackson.module # jackson-module-paranamer # 2.6.5     Apache  The Apache Software License, Version 2.0  com.fasterxml.jackson.module # jackson-module-scala_2.11 # 2.6.5     Apache  The Apache Software License, Version 2.0  com.google.code.findbugs # jsr305 # 1.3.9     Apache  The Apache Software License, Version 2.0  com.google.code.gson # gson # 2.2.4     Apache  The Apache Software License, Version 2.0  com.google.guava # guava # 14.0.1     Apache  The Apache Software License, Version 2.0  com.google.inject # guice # 3.0     Apache  The Apache Software License, Version 2.0  com.sksamuel.scrimage # scrimage-core_2.11 # 2.1.8     Apache  The Apache Software License, Version 2.0  com.sksamuel.scrimage # scrimage-filters_2.11 # 2.1.8     Apache  The Apache Software License, Version 2.0  com.sksamuel.scrimage # scrimage-io-extra_2.11 # 2.1.8     Apache  The Apache Software License, Version 2.0  commons-beanutils # commons-beanutils-core # 1.8.0     Apache  The Apache Software License, Version 2.0  commons-cli # commons-cli # 1.2     Apache  The Apache Software License, Version 2.0  commons-configuration # commons-configuration # 1.6     Apache  The Apache Software License, Version 2.0  commons-digester # commons-digester # 1.8     Apache  The Apache Software License, Version 2.0  commons-lang # commons-lang # 2.6     Apache  The Apache Software License, Version 2.0  commons-logging # commons-logging # 1.2     Apache  The Apache Software License, Version 2.0  commons-net # commons-net # 2.2     Apache  The Apache Software License, Version 2.0  javax.inject # javax.inject # 1     Apache  The Apache Software License, Version 2.0  javax.validation # validation-api # 1.1.0.Final     Apache  The Apache Software License, Version 2.0  log4j # log4j # 1.2.17     Apache  The Apache Software License, Version 2.0  mx4j # mx4j # 3.0.2     Apache  The Apache Software License, Version 2.0  net.jpountz.lz4 # lz4 # 1.3.0     Apache  The Apache Software License, Version 2.0  org.apache.ant # ant # 1.8.3     Apache  The Apache Software License, Version 2.0  org.apache.ant # ant-launcher # 1.8.3     Apache  The Apache Software License, Version 2.0  org.apache.avro # avro # 1.7.7     Apache  The Apache Software License, Version 2.0  org.apache.avro # avro-ipc # 1.7.7     Apache  The Apache Software License, Version 2.0  org.apache.avro # avro-mapred # 1.7.7     Apache  The Apache Software License, Version 2.0  org.apache.commons # commons-math # 2.2     Apache  The Apache Software License, Version 2.0  org.apache.commons # commons-math3 # 3.4.1     Apache  The Apache Software License, Version 2.0  org.apache.commons # commons-vfs2 # 2.0     Apache  The Apache Software License, Version 2.0  org.apache.curator # curator-client # 2.6.0     Apache  The Apache Software License, Version 2.0  org.apache.curator # curator-framework # 2.6.0     Apache  The Apache Software License, Version 2.0  org.apache.curator # curator-recipes # 2.6.0     Apache  The Apache Software License, Version 2.0  org.apache.directory.api # api-asn1-api # 1.0.0-M20     Apache  The Apache Software License, Version 2.0  org.apache.directory.api # api-util # 1.0.0-M20     Apache  The Apache Software License, Version 2.0  org.apache.directory.server # apacheds-i18n # 2.0.0-M15     Apache  The Apache Software License, Version 2.0  org.apache.directory.server # apacheds-kerberos-codec # 2.0.0-M15     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-annotations # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-auth # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-client # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-common # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-hdfs # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-mapreduce-client-app # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-mapreduce-client-common # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-mapreduce-client-core # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-mapreduce-client-jobclient # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-mapreduce-client-shuffle # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-yarn-api # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-yarn-client # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-yarn-common # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-yarn-server-common # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.hadoop # hadoop-yarn-server-nodemanager # 2.6.5     Apache  The Apache Software License, Version 2.0  org.apache.ivy # ivy # 2.4.0     Apache  The Apache Software License, Version 2.0  org.apache.maven.scm # maven-scm-api # 1.4     Apache  The Apache Software License, Version 2.0  org.apache.maven.scm # maven-scm-provider-svn-commons # 1.4     Apache  The Apache Software License, Version 2.0  org.apache.maven.scm # maven-scm-provider-svnexe # 1.4     Apache  The Apache Software License, Version 2.0  org.apache.parquet # parquet-column # 1.8.2     Apache  The Apache Software License, Version 2.0  org.apache.parquet # parquet-common # 1.8.2     Apache  The Apache Software License, Version 2.0  org.apache.parquet # parquet-encoding # 1.8.2     Apache  The Apache Software License, Version 2.0  org.apache.parquet # parquet-format # 2.3.1     Apache  The Apache Software License, Version 2.0  org.apache.parquet # parquet-hadoop # 1.8.2     Apache  The Apache Software License, Version 2.0  org.apache.parquet # parquet-jackson # 1.8.2     Apache  The Apache Software License, Version 2.0  org.apache.poi # poi # 3.14     Apache  The Apache Software License, Version 2.0  org.apache.poi # poi-ooxml # 3.14     Apache  The Apache Software License, Version 2.0  org.apache.poi # poi-ooxml-schemas # 3.14     Apache  The Apache Software License, Version 2.0  org.apache.xmlbeans # xmlbeans # 2.6.0     Apache  The Apache Software License, Version 2.0  org.codehaus.groovy # groovy # 1.8.9     Apache  The Apache Software License, Version 2.0  org.codehaus.jackson # jackson-core-asl # 1.9.13     Apache  The Apache Software License, Version 2.0  org.codehaus.jackson # jackson-jaxrs # 1.9.13     Apache  The Apache Software License, Version 2.0  org.codehaus.jackson # jackson-mapper-asl # 1.9.13     Apache  The Apache Software License, Version 2.0  org.codehaus.jackson # jackson-xc # 1.9.13     Apache  The Apache Software License, Version 2.0  org.codehaus.plexus # plexus-utils # 1.5.6     Apache  The Apache Software License, Version 2.0  org.fusesource.jansi # jansi # 1.5     Apache  The Apache Software License, Version 2.0  org.htrace # htrace-core # 3.0.4     Apache  The Apache Software License, Version 2.0  org.la4j # la4j # 0.6.0     Apache  The Apache Software License, Version 2.0  org.renjin # renjin-asm # 5.0.4b     Apache  The Apache Software License, Version 2.0  org.renjin # renjin-guava # 17.0b     Apache  The Apache Software License, Version 2.0  org.scala-graph # graph-core_2.11 # 1.11.3     Apache  The Apache Software License, Version 2.0  org.smurn # jply # 0.2.1     Apache  The Apache Software License, Version 2.0  org.sonatype.sisu.inject # cglib # 2.2.1-v20090111     Apache  The Apache Software License, Version 2.0  org.tensorflow # proto # 1.9.0-rc1     Apache  The Apache Software License, Version 2.0  org.xerial.snappy # snappy-java # 1.1.2.6     Apache  The Apache Software License, Version 2.0  stax # stax-api # 1.0.1     Apache  The Apache Software License, Version 2.0  xerces # xercesImpl # 2.9.1     Apache  The Apache Software License, Version 2.0  xml-apis # xml-apis # 1.3.04     Apache  the Apache License, ASL Version 2.0  org.scalactic # scalactic_2.11 # 3.0.4     BSD  3-Clause BSD License  com.google.protobuf # protobuf-java # 3.5.1     BSD  BSD  asm # asm # 3.2     BSD  BSD  asm # asm-analysis # 3.2     BSD  BSD  asm # asm-commons # 3.2     BSD  BSD  asm # asm-tree # 3.2     BSD  BSD  asm # asm-util # 3.2     BSD  BSD  com.miglayout # miglayout # 3.7.4     BSD  BSD  com.thoughtworks.paranamer # paranamer # 2.8     BSD  BSD  jline # jline # 0.9.94     BSD  BSD  net.sourceforge.jmatio # jmatio # 1.0     BSD  BSD  org.scalafx # scalafx_2.11 # 8.0.92-R10     BSD  BSD  org.scalameta # common_2.11 # 2.0.1     BSD  BSD  org.scalameta # dialects_2.11 # 2.0.1     BSD  BSD  org.scalameta # inputs_2.11 # 2.0.1     BSD  BSD  org.scalameta # io_2.11 # 2.0.1     BSD  BSD  org.scalameta # langmeta_2.11 # 2.0.1     BSD  BSD  org.scalameta # parsers_2.11 # 2.0.1     BSD  BSD  org.scalameta # quasiquotes_2.11 # 2.0.1     BSD  BSD  org.scalameta # scalameta_2.11 # 2.0.1     BSD  BSD  org.scalameta # semanticdb_2.11 # 2.0.1     BSD  BSD  org.scalameta # tokenizers_2.11 # 2.0.1     BSD  BSD  org.scalameta # tokens_2.11 # 2.0.1     BSD  BSD  org.scalameta # transversers_2.11 # 2.0.1     BSD  BSD  org.scalameta # trees_2.11 # 2.0.1     BSD  BSD 3 Clause  com.github.fommil.netlib # all # 1.1.2     BSD  BSD 3 Clause  com.github.fommil.netlib # core # 1.1.2     BSD  BSD 3 Clause  com.github.fommil.netlib # native_ref-java # 1.1     BSD  BSD 3 Clause  com.github.fommil.netlib # native_system-java # 1.1     BSD  BSD 3 Clause  com.github.fommil.netlib # netlib-native_ref-linux-armhf # 1.1     BSD  BSD 3 Clause  com.github.fommil.netlib # netlib-native_ref-linux-i686 # 1.1     BSD  BSD 3 Clause  com.github.fommil.netlib # netlib-native_ref-linux-x86_64 # 1.1     BSD  BSD 3 Clause  com.github.fommil.netlib # netlib-native_ref-osx-x86_64 # 1.1     BSD  BSD 3 Clause  com.github.fommil.netlib # netlib-native_ref-win-i686 # 1.1     BSD  BSD 3 Clause  com.github.fommil.netlib # netlib-native_ref-win-x86_64 # 1.1     BSD  BSD 3 Clause  com.github.fommil.netlib # netlib-native_system-linux-armhf # 1.1     BSD  BSD 3 Clause  com.github.fommil.netlib # netlib-native_system-linux-i686 # 1.1     BSD  BSD 3 Clause  com.github.fommil.netlib # netlib-native_system-linux-x86_64 # 1.1     BSD  BSD 3 Clause  com.github.fommil.netlib # netlib-native_system-osx-x86_64 # 1.1     BSD  BSD 3 Clause  com.github.fommil.netlib # netlib-native_system-win-i686 # 1.1     BSD  BSD 3 Clause  com.github.fommil.netlib # netlib-native_system-win-x86_64 # 1.1     BSD  BSD 3-Clause  com.tinkerpop # frames # 2.5.0     BSD  BSD 3-Clause  com.tinkerpop # pipes # 2.6.0     BSD  BSD 3-Clause  com.tinkerpop.blueprints # blueprints-core # 2.6.0     BSD  BSD 3-Clause  com.tinkerpop.gremlin # gremlin-groovy # 2.5.0     BSD  BSD 3-Clause  com.tinkerpop.gremlin # gremlin-java # 2.6.0     BSD  BSD 3-Clause  org.scala-lang # scala-compiler # 2.11.8     BSD  BSD 3-Clause  org.scala-lang # scala-library # 2.11.8     BSD  BSD 3-Clause  org.scala-lang # scala-reflect # 2.11.8     BSD  BSD 3-Clause  org.webjars.bower # vega # 2.6.5     BSD  BSD 3-Clause  org.webjars.bower # vega-lite # 1.2.0     BSD  BSD 3-Clause License  org.jpmml # pmml-model # 1.2.15     BSD  BSD 3-Clause License  org.jpmml # pmml-schema # 1.2.15     BSD  BSD 3-clause  org.scala-lang.modules # scala-java8-compat_2.11 # 0.7.0     BSD  BSD 3-clause  org.scala-lang.modules # scala-parser-combinators_2.11 # 1.0.4     BSD  BSD 3-clause  org.scala-lang.modules # scala-swing_2.11 # 1.0.1     BSD  BSD 3-clause  org.scala-lang.modules # scala-xml_2.11 # 1.0.6     BSD  BSD License  antlr # antlr # 2.7.7     BSD  BSD License  com.github.virtuald # curvesapi # 1.03     BSD  BSD style  com.thoughtworks.xstream # xstream # 1.4.7     BSD  BSD-2 License  org.jogamp.gluegen # gluegen-rt # 2.3.2     BSD  BSD-2 License  org.jogamp.gluegen # gluegen-rt-main # 2.3.2     BSD  BSD-2 License  org.jogamp.jogl # jogl-all # 2.3.2     BSD  BSD-2 License  org.jogamp.jogl # jogl-all-main # 2.3.2     BSD  BSD-3-Clause  org.webjars.bower # d3 # 3.5.17     BSD  BSD-like  org.scala-lang # jline # 2.11.0-M3     BSD  BSD-like  org.scala-lang # scala-pickling_2.11 # 0.9.1     BSD  BSD-style  org.scalaforge # scalax # 0.1     BSD  BSD-style  org.scalaz # scalaz-concurrent_2.11 # 7.2.16     BSD  BSD-style  org.scalaz # scalaz-core_2.11 # 7.2.16     BSD  BSD-style  org.scalaz # scalaz-effect_2.11 # 7.2.16     BSD  BSD_3_clause + file LICENSE  org.renjin.cran # colorspace # 1.3-2-b41     BSD  New BSD License  com.diffplug.matsim # matfilerw # 3.0.0     BSD  New BSD License  com.esotericsoftware # kryo-shaded # 3.0.3     BSD  New BSD License  com.esotericsoftware # minlog # 1.3.0     BSD  New BSD License  org.codehaus.janino # commons-compiler # 3.0.0     BSD  New BSD License  org.codehaus.janino # janino # 3.0.0     BSD  New BSD License  org.hamcrest # hamcrest-core # 1.3     BSD  The (New) BSD License  org.jzy3d # jzy3d-api # 1.0.2     BSD  The (New) BSD License  org.jzy3d # jzy3d-jdt-core # 1.0.2     BSD  The BSD 3-Clause License  org.fusesource.leveldbjni # leveldbjni-all # 1.8     BSD  The BSD License  com.adobe.xmp # xmpcore # 5.1.2     BSD  The BSD License  net.sourceforge.f2j # arpack_combined_all # 0.1     BSD  The BSD License  org.antlr # antlr4-runtime # 4.5.3     BSD  The BSD License  org.jline # jline # 3.6.2     BSD  The BSD License  org.jline # jline-terminal # 3.6.2     BSD  The BSD License  org.jline # jline-terminal-jna # 3.6.2     BSD  The BSD License  xmlenc # xmlenc # 0.52     BSD  The New BSD License  net.sf.py4j # py4j # 0.10.4     CC0  CC0  org.reactivestreams # reactive-streams # 1.0.0     CDDL  COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0  javax.activation # activation # 1.1.1     CDDL  COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0  javax.xml.stream # stax-api # 1.0-2     GPL  CDDL v1.1 / GPL v2 dual license  com.sun.codemodel # codemodel # 2.6     GPL  CDDL+GPL License  org.glassfish.jersey.bundles.repackaged # jersey-guava # 2.22.2     GPL  CDDL+GPL License  org.glassfish.jersey.containers # jersey-container-servlet # 2.22.2     GPL  CDDL+GPL License  org.glassfish.jersey.containers # jersey-container-servlet-core # 2.22.2     GPL  CDDL+GPL License  org.glassfish.jersey.core # jersey-client # 2.22.2     GPL  CDDL+GPL License  org.glassfish.jersey.core # jersey-common # 2.22.2     GPL  CDDL+GPL License  org.glassfish.jersey.core # jersey-server # 2.22.2     GPL  CDDL+GPL License  org.glassfish.jersey.media # jersey-media-jaxb # 2.22.2     GPL  GNU General Public License (GPL)  org.jfree # jfreesvg # 3.3     GPL  GNU General Public License 3  org.openml # apiconnector # 1.0.11     GPL  GPL (>= 2)  org.renjin.cran # MatrixModels # 0.4-1-b237     GPL  GPL (>= 2)  org.renjin.cran # SparseM # 1.77-b20     GPL  GPL (>= 2)  org.renjin.cran # digest # 0.6.15-b10     GPL  GPL (>= 2)  org.renjin.cran # lattice # 0.20-35-b66     GPL  GPL (>= 2)  org.renjin.cran # locfit # 1.5-9.1-b345     GPL  GPL (>= 2)  org.renjin.cran # quantreg # 5.35-b5     GPL  [GPL (>= 2)  file LICENCE](null)  org.renjin.cran # Matrix # 1.2-14-b2    GPL  GPL (>= 3)  org.renjin.cran # abc # 2.1-b294     GPL  GPL (>= 3)  org.renjin.cran # abc.data # 1.0-b272     GPL  GPL-2  org.renjin.cran # dichromat # 2.0-0-b332     GPL  GPL-2  org.renjin.cran # gtable # 0.2.0-b96     GPL  [GPL-2  GPL-3](null)  org.renjin.cran # MASS # 7.3-49-b11    GPL  [GPL-2  GPL-3](null)  org.renjin.cran # nnet # 7.3-12-b88    GPL  [GPL-2  file LICENSE](null)  org.renjin.cran # ggplot2 # 2.2.1-b112    GPL  [GPL-2  file LICENSE](null)  org.renjin.cran # stringr # 1.3.0-b7    GPL  GPL-3  org.renjin.cran # assertthat # 0.2.0-b42     GPL  GPL-3  org.renjin.cran # lazyeval # 0.2.1-b19     GPL  GPL-3  org.renjin.cran # pillar # 1.2.2-b2     GPL  GPL-3  org.renjin.cran # rlang # 0.2.0-b39     GPL  GPL2 w/ CPE  javax.ws.rs # javax.ws.rs-api # 2.0-m10     GPL  GPL2 w/ CPE  javax.xml.bind # jaxb-api # 2.2.2     GPL  GPLv2+CE  javax.mail # mail # 1.4.7     GPL with Classpath Extension  CDDL + GPLv2 with classpath exception  javax.annotation # javax.annotation-api # 1.2     GPL with Classpath Extension  CDDL + GPLv2 with classpath exception  javax.servlet # javax.servlet-api # 3.1.0     GPL with Classpath Extension  CDDL + GPLv2 with classpath exception  org.glassfish.hk2 # hk2-api # 2.4.0-b34     GPL with Classpath Extension  CDDL + GPLv2 with classpath exception  org.glassfish.hk2 # hk2-locator # 2.4.0-b34     GPL with Classpath Extension  CDDL + GPLv2 with classpath exception  org.glassfish.hk2 # hk2-utils # 2.4.0-b34     GPL with Classpath Extension  CDDL + GPLv2 with classpath exception  org.glassfish.hk2 # osgi-resource-locator # 1.0.1     GPL with Classpath Extension  CDDL + GPLv2 with classpath exception  org.glassfish.hk2.external # aopalliance-repackaged # 2.4.0-b34     GPL with Classpath Extension  CDDL + GPLv2 with classpath exception  org.glassfish.hk2.external # javax.inject # 2.4.0-b34     LGPL  GNU Lesser General Public License  ch.qos.logback # logback-classic # 1.2.3     LGPL  GNU Lesser General Public License  ch.qos.logback # logback-core # 1.2.3     LGPL  GNU Lesser General Public License 2.1  net.sf.trove4j # trove4j # 3.0.3     LGPL  GNU Lesser General Public License v3.0  com.github.vagmcs # optimus_2.11 # 2.0.0     LGPL  LGPL  com.github.fommil # jniloader # 1.1     LGPL  LGPL, version 2.1  net.java.dev.jna # jna # 4.2.2     MIT  MIT  co.theasi # plotly_2.11 # 0.1     MIT  MIT  com.github.julien-truffaut # monocle-core_2.11 # 1.1.0     MIT  MIT  com.github.julien-truffaut # monocle-macro_2.11 # 1.1.0     MIT  MIT  com.lihaoyi # fansi_2.11 # 0.2.4     MIT  MIT  com.lihaoyi # pprint_2.11 # 0.5.2     MIT  MIT  com.lihaoyi # sourcecode_2.11 # 0.1.4     MIT  MIT  com.lihaoyi # ujson-jvm-2.11.11_2.11 # 0.6.0     MIT  MIT  com.lihaoyi # upickle_2.11 # 0.6.0     MIT  MIT  net.databinder # unfiltered-filter_2.11 # 0.8.3     MIT  MIT  net.databinder # unfiltered-jetty_2.11 # 0.8.3     MIT  MIT  net.databinder # unfiltered-util_2.11 # 0.8.3     MIT  MIT  net.databinder # unfiltered_2.11 # 0.8.3     MIT  MIT  org.spire-math # jawn-parser_2.11 # 0.11.0     MIT  MIT  org.typelevel # algebra_2.11 # 0.7.0     MIT  MIT  org.typelevel # cats-core_2.11 # 1.0.1     MIT  MIT  org.typelevel # cats-kernel_2.11 # 1.0.1     MIT  MIT  org.typelevel # cats-macros_2.11 # 1.0.1     MIT  MIT  org.typelevel # machinist_2.11 # 0.6.2     MIT  MIT  org.typelevel # spire-macros_2.11 # 0.14.1     MIT  MIT  org.typelevel # spire_2.11 # 0.14.1     MIT  MIT + file LICENSE  org.renjin.cran # R6 # 2.2.2-b44     MIT  MIT + file LICENSE  org.renjin.cran # cli # 1.0.0-b25     MIT  MIT + file LICENSE  org.renjin.cran # crayon # 1.3.4-b16     MIT  MIT + file LICENSE  org.renjin.cran # glue # 1.2.0-b22     MIT  MIT + file LICENSE  org.renjin.cran # magrittr # 1.5-b334     MIT  MIT + file LICENSE  org.renjin.cran # munsell # 0.4.3-b96     MIT  MIT + file LICENSE  org.renjin.cran # plyr # 1.8.4-b82     MIT  MIT + file LICENSE  org.renjin.cran # reshape2 # 1.4.3-b16     MIT  MIT + file LICENSE  org.renjin.cran # scales # 0.5.0-b32     MIT  MIT + file LICENSE  org.renjin.cran # tibble # 1.4.2-b7     MIT  MIT + file LICENSE  org.renjin.cran # viridisLite # 0.3.0-b7     MIT  [MIT + file LICENSE  Unlimited](null)  org.renjin.cran # labeling # 0.3-b313    MIT  MIT License  com.github.scopt # scopt_2.11 # 3.5.0     MIT  MIT License  net.razorvine # pyrolite # 4.13     MIT  MIT License  org.slf4j # jcl-over-slf4j # 1.7.16     MIT  MIT License  org.slf4j # jul-to-slf4j # 1.7.16     MIT  MIT License  org.slf4j # slf4j-api # 1.7.25     MIT  MIT License  org.vegas-viz # vegas-macros_2.11 # 0.3.11     MIT  MIT License  org.vegas-viz # vegas_2.11 # 0.3.11     MIT  MIT license  com.lihaoyi # ammonite-compiler_2.11.8 # 1.1.0     MIT  MIT license  com.lihaoyi # ammonite-ops_2.11 # 1.1.0     MIT  MIT license  com.lihaoyi # ammonite-repl_2.11.8 # 1.1.0     MIT  MIT license  com.lihaoyi # ammonite-runtime_2.11 # 1.1.0     MIT  MIT license  com.lihaoyi # ammonite-sshd_2.11.8 # 1.1.0     MIT  MIT license  com.lihaoyi # ammonite-terminal_2.11 # 1.1.0     MIT  MIT license  com.lihaoyi # ammonite-util_2.11 # 1.1.0     MIT  MIT license  com.lihaoyi # ammonite_2.11.8 # 1.1.0     MIT  MIT license  com.lihaoyi # fastparse-utils_2.11 # 1.0.0     MIT  MIT license  com.lihaoyi # fastparse_2.11 # 1.0.0     MIT  MIT license  com.lihaoyi # geny_2.11 # 0.1.2     MIT  MIT license  com.lihaoyi # scalaparse_2.11 # 1.0.0     Mozilla  MPL  com.github.rwl # jtransforms # 2.4.0     Mozilla  MPL 1.1  org.javassist # javassist # 3.21.0-GA     Public Domain  Public Domain  aopalliance # aopalliance # 1.0     Public Domain  Public Domain  gov.nist.math # scimark # 2.0     Public Domain  Public Domain  xmlpull # xmlpull # 1.1.3.1     Public Domain  Public Domain  xpp3 # xpp3_min # 1.1.4c     Public Domain  Public domain  net.iharder # base64 # 2.3.8     unrecognized  ASL  org.json4s # json4s-jackson_2.11 # 3.2.11     unrecognized  Bouncy Castle Licence  org.bouncycastle # bcprov-jdk15on # 1.56     unrecognized  Eclipse Public License 1.0  junit # junit # 4.12     unrecognized  GNU Lesser General Public Licence  com.github.wookietreiber # scala-chart_2.11 # 0.4.2     unrecognized  GNU Lesser General Public Licence  org.jfree # jcommon # 1.0.21     unrecognized  GNU Lesser General Public Licence  org.jfree # jfreechart # 1.0.17     unrecognized  GNU Public License, v2  org.renjin # compiler # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # datasets # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # gcc-runtime # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # grDevices # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # graphics # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # grid # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # methods # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # parallel # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # renjin-appl # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # renjin-blas # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # renjin-core # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # renjin-gnur-runtime # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # renjin-lapack # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # renjin-math-common # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # renjin-nmath # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # renjin-script-engine # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # splines # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # stats # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # tcltk # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # tools # 0.9.2643     unrecognized  GNU Public License, v2  org.renjin # utils # 0.9.2643     unrecognized  Part of R 2.14.2  org.renjin # stats4 # 0.9.2643     unrecognized  The JSON License  org.json # json # 20140107     unrecognized  Unicode/ICU License  com.ibm.icu # icu4j # 59.1     unrecognized  Unknown License  org.apache.xbean # xbean-asm5-shaded # 4.4     unrecognized  none specified  com.twelvemonkeys.common # common-image # 3.2.1     unrecognized  none specified  com.twelvemonkeys.common # common-io # 3.2.1     unrecognized  none specified  com.twelvemonkeys.common # common-lang # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-bmp # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-core # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-icns # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-iff # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-jpeg # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-metadata # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-pcx # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-pdf # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-pict # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-pnm # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-psd # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-sgi # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-tga # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-thumbsdb # 3.2.1     unrecognized  none specified  com.twelvemonkeys.imageio # imageio-tiff # 3.2.1     unrecognized  none specified  commons-beanutils # commons-beanutils # 1.7.0     unrecognized  none specified  org.apache.zookeeper # zookeeper # 3.4.6     unrecognized  none specified  org.renjin # libstdcxx # 4.7.4-b18     unrecognized  none specified  org.renjin.cran # Rcpp # 0.12.13-renjin-15     unrecognized  none specified  org.renjin.cran # stringi # 1.1.6-renjin-b22     unrecognized  none specified  oro # oro # 2.0.8     unrecognized  none specified  regexp # regexp # 1.3",
            "title": "Dependency Licenses"
        },
        {
            "location": "/about/",
            "text": "Affliations\n\u00b6\n\n\nDynaML is developed and maintained by \nTranscendent AI Labs\n\n\nDynaML is proud to be a part of the \nMozilla Science\n Collaborate \nplatform\n\n\nContributors\n\u00b6\n\n\n\n\n\n\nMandar Chandorkar\n\n\n\n\n\n\nAmit Kumar Jaiswal\n\n\n\n\n\n\nSisir Kopakka\n\n\n\n\n\n\nHave questions or suggestions? Feel free to \nopen an issue on GitHub\n or \nask me on Twitter\n.\n\n\nAttributions\n\u00b6\n\n\nLogo\n\u00b6\n\n\nBusiness graphic by \nroundicons\n from \nFlaticon\n is licensed under \nCC BY 3.0\n.",
            "title": "About Us"
        },
        {
            "location": "/about/#affliations",
            "text": "DynaML is developed and maintained by  Transcendent AI Labs  DynaML is proud to be a part of the  Mozilla Science  Collaborate  platform",
            "title": "Affliations"
        },
        {
            "location": "/about/#contributors",
            "text": "Mandar Chandorkar    Amit Kumar Jaiswal    Sisir Kopakka    Have questions or suggestions? Feel free to  open an issue on GitHub  or  ask me on Twitter .",
            "title": "Contributors"
        },
        {
            "location": "/about/#attributions",
            "text": "",
            "title": "Attributions"
        },
        {
            "location": "/about/#logo",
            "text": "Business graphic by  roundicons  from  Flaticon  is licensed under  CC BY 3.0 .",
            "title": "Logo"
        },
        {
            "location": "/blog/2016-12-11-dynaml-new-features/",
            "text": "Summarizes some of the pet projects being tackled in DynaML\n\n\n\n\nThe past year has seen DynaML grow by leaps and bounds, this post hopes to give you an update about what has been achieved\nand a taste for what is to come.\n\n\nCompleted Features\n\u00b6\n\n\nA short tour of the enhancements which were completed.\n\n\nJanuary to June\n\u00b6\n\n\n\n\nReleased \nv1.3.x\n series with the following new additions\n\n\n\n\nModels\n\n\n\n\nRegularized Least Squares\n\n\nLogistic and Probit Regression\n\n\nFeed Forward Neural Nets\n\n\nGaussian Process (GP) classification and NARX based models\n\n\nLeast Squares Support Vector Machines (LSSVM) for classification and regression\n\n\nMeta model API, committee models\n\n\n\n\nOptimization Primitives\n\n\n\n\nRegularized Least Squares Solvers\n\n\nGradient Descent\n\n\nCommittee model solvers\n\n\nLinear Solvers for LSSVM\n\n\nLaplace approximation for GPs\n\n\n\n\nMiscellaneous\n\n\n\n\nData Pipes API\n\n\n\n\nMigration to scala version 2.11.8\n\n\n\n\n\n\nStarted work on release \n1.4.x\n series with initial progress\n\n\n\n\n\n\nImprovements\n\n\n\n\nMigrated from Maven to Sbt.\n\n\nSet \nAmmonite\n as default REPL.\n\n\n\n\nJune to December\n\u00b6\n\n\n\n\nReleased \nv1.4\n with the following features.\n\n\n\n\nModels\n\n\nThe following inference models have been added.\n\n\n\n\nLSSVM committees.\n\n\nMulti-output, multi-task \nGaussian Process\n models as reviewed in \nLawrence et. al\n.\n\n\nStudent T Processes\n: single and multi output inspired from \nShah, Ghahramani et. al\n\n\nPerformance improvement to computation of \nmarginal likelihood\n and \nposterior predictive distribution\n in Gaussian Process models.\n\n\nPosterior predictive distribution outputted by the \nAbstractGPRegression\n base class is now changed to \nMultGaussianRV\n which is added to the \ndynaml.probability\n package.\n\n\n\n\nKernels\n\n\n\n\n\n\nAdded \nStationaryKernel\n and \nLocallyStationaryKernel\n classes in the kernel APIs, converted \nRBFKernel\n, \nCauchyKernel\n, \nRationalQuadraticKernel\n & \nLaplacianKernel\n to subclasses of \nStationaryKernel\n\n\n\n\n\n\nAdded \nMLPKernel\n which implements the \nmaximum likelihood perceptron\n kernel as shown \nhere\n.\n\n\n\n\n\n\nAdded \nco-regionalization kernels\n which are used in \nLawrence et. al\n to formulate kernels for vector valued functions. In this category the following co-regionalization kernels were implemented.\n\n\n\n\nCoRegRBFKernel\n\n\nCoRegCauchyKernel\n\n\nCoRegLaplaceKernel\n\n\n\n\nCoRegDiracKernel\n\n\n\n\n\n\nImproved performance when calculating kernel matrices for composite kernels.\n\n\n\n\n\n\nAdded \n:*\n operator to kernels so that one can create separable kernels used in \nco-regionalization models\n.\n\n\n\n\n\n\nOptimization\n\n\n\n\nImproved performance of \nCoupledSimulatedAnnealing\n, enabled use of 4 variants of \nCoupled Simulated Annealing\n, adding the ability to set annealing schedule using so called \nvariance control\n scheme as outlined in \nde-Souza, Suykens et. al\n.\n\n\n\n\nPipes\n\n\n\n\n\n\nAdded \nScaler\n and \nReversibleScaler\n traits to represent transformations which input and output into the same domain set, these traits are extensions of \nDataPipe\n.\n\n\n\n\n\n\nAdded \nDiscrete Wavelet Transform\n based on the \nHaar\n wavelet.\n\n\n\n\n\n\n\n\n\n\nStarted work on \nv1.4.1\n with the following progress\n\n\n\n\n\n\nLinear Algebra API\n\n\n\n\n\n\nPartitioned Matrices/Vectors and the following operations\n\n\n\n\nAddition, Subtraction\n\n\nMatrix, vector multiplication\n\n\nLU, Cholesky\n\n\nA\\y, A\\Y\n\n\n\n\n\n\n\n\nProbability API\n\n\n\n\nAdded API end points for representing Measurable Functions of random variables.\n\n\n\n\nModel Evaluation\n\n\n\n\nAdded Matthews Correlation Coefficient calculation to \nBinaryClassificationMetrics\n via the \nmatthewsCCByThreshold\n method  \n\n\n\n\nData Pipes API\n\n\n\n\nAdded \nEncoder[S,D]\n traits which are reversible data pipes representing an encoding between types \nS\n and \nD\n.\n\n\n\n\nMiscellaneous\n\n\n\n\nUpdated \nammonite\n version to \n0.8.1\n\n\nAdded support for compiling basic R code with \nrenjin\n. Run R code in the following manner:\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nval\n \ntoRDF\n \n=\n \ncsvToRDF\n(\n\"dfWine\"\n,\n \n';'\n)\n\n\nval\n \nwine_quality_red\n \n=\n \ntoRDF\n(\n\"data/winequality-red.csv\"\n)\n\n\n//Descriptive statistics\n\n\nval\n \ncommands\n:\n \nString\n \n=\n \n\"\"\"\n\n\nprint(summary(dfWine))\n\n\nprint(\"\\n\")\n\n\nprint(str(dfWine))\n\n\n\"\"\"\n\n\nr\n(\ncommands\n)\n\n\n//Build Linear Model\n\n\nval\n \nmodelGLM\n \n=\n \nrdfToGLM\n(\n\"model\"\n,\n \n\"quality\"\n,\n \nArray\n(\n\"fixed.acidity\"\n,\n \n\"citric.acid\"\n,\n \n\"chlorides\"\n))\n\n\nmodelGLM\n(\n\"dfWine\"\n)\n\n\n//Print goodness of fit\n\n\nr\n(\n\"print(summary(model))\"\n)\n\n\n\n\n\n\n\nOngoing Work\n\u00b6\n\n\nSome projects being worked on right now are.\n\n\n\n\nBayesian optimization using Gaussian Process models.\n\n\nImplementation of Neural Networks using the \nakka\n actor API.\n\n\nImplementation of kernels which can be decomposed on data dimensions \nk((x_1, x_2), (y_1, y_2)) = k_1(x_1, y_1) + k_2(x_2, y_2)\nk((x_1, x_2), (y_1, y_2)) = k_1(x_1, y_1) + k_2(x_2, y_2)",
            "title": "State of DynaML 2016"
        },
        {
            "location": "/blog/2016-12-11-dynaml-new-features/#completed-features",
            "text": "A short tour of the enhancements which were completed.",
            "title": "Completed Features"
        },
        {
            "location": "/blog/2016-12-11-dynaml-new-features/#january-to-june",
            "text": "Released  v1.3.x  series with the following new additions   Models   Regularized Least Squares  Logistic and Probit Regression  Feed Forward Neural Nets  Gaussian Process (GP) classification and NARX based models  Least Squares Support Vector Machines (LSSVM) for classification and regression  Meta model API, committee models   Optimization Primitives   Regularized Least Squares Solvers  Gradient Descent  Committee model solvers  Linear Solvers for LSSVM  Laplace approximation for GPs   Miscellaneous   Data Pipes API   Migration to scala version 2.11.8    Started work on release  1.4.x  series with initial progress    Improvements   Migrated from Maven to Sbt.  Set  Ammonite  as default REPL.",
            "title": "January to June"
        },
        {
            "location": "/blog/2016-12-11-dynaml-new-features/#june-to-december",
            "text": "Released  v1.4  with the following features.   Models  The following inference models have been added.   LSSVM committees.  Multi-output, multi-task  Gaussian Process  models as reviewed in  Lawrence et. al .  Student T Processes : single and multi output inspired from  Shah, Ghahramani et. al  Performance improvement to computation of  marginal likelihood  and  posterior predictive distribution  in Gaussian Process models.  Posterior predictive distribution outputted by the  AbstractGPRegression  base class is now changed to  MultGaussianRV  which is added to the  dynaml.probability  package.   Kernels    Added  StationaryKernel  and  LocallyStationaryKernel  classes in the kernel APIs, converted  RBFKernel ,  CauchyKernel ,  RationalQuadraticKernel  &  LaplacianKernel  to subclasses of  StationaryKernel    Added  MLPKernel  which implements the  maximum likelihood perceptron  kernel as shown  here .    Added  co-regionalization kernels  which are used in  Lawrence et. al  to formulate kernels for vector valued functions. In this category the following co-regionalization kernels were implemented.   CoRegRBFKernel  CoRegCauchyKernel  CoRegLaplaceKernel   CoRegDiracKernel    Improved performance when calculating kernel matrices for composite kernels.    Added  :*  operator to kernels so that one can create separable kernels used in  co-regionalization models .    Optimization   Improved performance of  CoupledSimulatedAnnealing , enabled use of 4 variants of  Coupled Simulated Annealing , adding the ability to set annealing schedule using so called  variance control  scheme as outlined in  de-Souza, Suykens et. al .   Pipes    Added  Scaler  and  ReversibleScaler  traits to represent transformations which input and output into the same domain set, these traits are extensions of  DataPipe .    Added  Discrete Wavelet Transform  based on the  Haar  wavelet.      Started work on  v1.4.1  with the following progress    Linear Algebra API    Partitioned Matrices/Vectors and the following operations   Addition, Subtraction  Matrix, vector multiplication  LU, Cholesky  A\\y, A\\Y     Probability API   Added API end points for representing Measurable Functions of random variables.   Model Evaluation   Added Matthews Correlation Coefficient calculation to  BinaryClassificationMetrics  via the  matthewsCCByThreshold  method     Data Pipes API   Added  Encoder[S,D]  traits which are reversible data pipes representing an encoding between types  S  and  D .   Miscellaneous   Updated  ammonite  version to  0.8.1  Added support for compiling basic R code with  renjin . Run R code in the following manner:    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 val   toRDF   =   csvToRDF ( \"dfWine\" ,   ';' )  val   wine_quality_red   =   toRDF ( \"data/winequality-red.csv\" )  //Descriptive statistics  val   commands :   String   =   \"\"\"  print(summary(dfWine))  print(\"\\n\")  print(str(dfWine))  \"\"\"  r ( commands )  //Build Linear Model  val   modelGLM   =   rdfToGLM ( \"model\" ,   \"quality\" ,   Array ( \"fixed.acidity\" ,   \"citric.acid\" ,   \"chlorides\" ))  modelGLM ( \"dfWine\" )  //Print goodness of fit  r ( \"print(summary(model))\" )",
            "title": "June to December"
        },
        {
            "location": "/blog/2016-12-11-dynaml-new-features/#ongoing-work",
            "text": "Some projects being worked on right now are.   Bayesian optimization using Gaussian Process models.  Implementation of Neural Networks using the  akka  actor API.  Implementation of kernels which can be decomposed on data dimensions  k((x_1, x_2), (y_1, y_2)) = k_1(x_1, y_1) + k_2(x_2, y_2) k((x_1, x_2), (y_1, y_2)) = k_1(x_1, y_1) + k_2(x_2, y_2)",
            "title": "Ongoing Work"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/",
            "text": "Summary\n\n\nSome resources for people interested in contributing code or starting ML research/applications\n\n\n\n\nDynaML aims to make a versatile and powerful data analysis and machine learning toolkit available as a shell and runtime environment. As is the case with any tool, relevant background and knowledge is crucial in order to yield its power. This post is intended to be a starting point for online resources which are relevant to DynaML.\n\n\nMachine Learning\n\u00b6\n\n\nMachine Learning\n refers to the ability to make predictions and decisions from data. It is a field that has evolved from the study of \npattern recognition\n and \ncomputational learning theory\n in artificial intelligence.\n\n\nMachine Learning theory and applications lie at the intersection of a number of concepts in the domains of Mathematics, Physics and Computer Science. It is no surprise that machine learning is a rich and deep domain with much intellectual and practical rewards to offer to the persistent and observant student.\n\n\nThe following is a non-exhaustive list of educational resources for learning ML.\n\n\nOnline Courses\n\u00b6\n\n\n\n\nAndrew Ng's famous \ncourse\n on \nCoursera\n.\n\n\nIntro to Machine Learning\n at \nUdacity\n\n\nMachine Learning\n: MIT Open Course Ware\n\n\n\n\nVideos/Youtube\n\u00b6\n\n\n\n\nMachine Learning Playlist\n by \nmathematicalmonk\n\n\nMachine Learning Course\n by \ncaltech\n\n\n\n\nBooks\n\u00b6\n\n\n\n\nBayesian Reasoning and Machine Learning\n by David Barber\n\n\nMachine Learning: A Probabilistic Perspective\n by Kevin P. Murphy\n\n\nPattern Recognition and Machine Learning\n by Christopher Bishop\n\n\nUnderstanding Machine Learning: From Theory to Algorithms\n by Shai Shalev-Shwartz and Shai Ben-David\n\n\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction\n by Trevor Hastie, Robert Tibshirani and Jerome Friedman\n\n\n\n\nForums\n\u00b6\n\n\n\n\nHow do I learn machine learning\n on \nQuora\n\n\n\n\nBlogs\n\u00b6\n\n\n\n\nDeepmind Blog\n\n\nCortana Blog\n\n\nShakir Mohammad's Blog\n\n\nDarren Wilkinson's research blog\n\n\nJohn's Langford's Blog\n.\n\n\nKyle Kastner's Blog\n\n\nSander Dieleman's Blog\n\n\n\n\nProgramming Environment: Scala\n\u00b6\n\n\nScala\n is the implementation language for DynaML, it is a hybrid language which gives the user the ability to leverage functional and object oriented programming styles. Scala code compiles to Java \nbyte code\n giving Scala complete interoperability with Java, i.e. you can use Java libraries and classes in Scala code.\n\n\nThe \nJava Virtual Machine\n which executes \nbyte code\n is the run time for the complete Java ecosystem. This enables Scala, Java, Groovy and Clojure programs to run on a common platform, which is a boon for Machine Learning applications as we can leverage all the libraries in the Java ecosystem.\n\n\nLearning Scala can be a significant investment as the language has a large number of features which require varying levels of skill and practice to master. Some resources for learning Scala are given below.\n\n\nCourses\n\u00b6\n\n\n\n\nFunctional Programming Principles with Scala\n by Martin Odersky.\n\n\nFunctional Program Design in Scala\n by Martin Odersky.\n\n\n\n\nVideos/Youtube\n\u00b6\n\n\n\n\nScala tutorials playlist\n\n\n\n\nBlogs\n\u00b6\n\n\n\n\nHaoyi's Programming Blog\n\n\nScala News\n\n\nTypelevel Blog\n\n\nCodacy Blog",
            "title": "Resources for Beginners"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#machine-learning",
            "text": "Machine Learning  refers to the ability to make predictions and decisions from data. It is a field that has evolved from the study of  pattern recognition  and  computational learning theory  in artificial intelligence.  Machine Learning theory and applications lie at the intersection of a number of concepts in the domains of Mathematics, Physics and Computer Science. It is no surprise that machine learning is a rich and deep domain with much intellectual and practical rewards to offer to the persistent and observant student.  The following is a non-exhaustive list of educational resources for learning ML.",
            "title": "Machine Learning"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#online-courses",
            "text": "Andrew Ng's famous  course  on  Coursera .  Intro to Machine Learning  at  Udacity  Machine Learning : MIT Open Course Ware",
            "title": "Online Courses"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#videosyoutube",
            "text": "Machine Learning Playlist  by  mathematicalmonk  Machine Learning Course  by  caltech",
            "title": "Videos/Youtube"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#books",
            "text": "Bayesian Reasoning and Machine Learning  by David Barber  Machine Learning: A Probabilistic Perspective  by Kevin P. Murphy  Pattern Recognition and Machine Learning  by Christopher Bishop  Understanding Machine Learning: From Theory to Algorithms  by Shai Shalev-Shwartz and Shai Ben-David  The Elements of Statistical Learning: Data Mining, Inference, and Prediction  by Trevor Hastie, Robert Tibshirani and Jerome Friedman",
            "title": "Books"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#forums",
            "text": "How do I learn machine learning  on  Quora",
            "title": "Forums"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#blogs",
            "text": "Deepmind Blog  Cortana Blog  Shakir Mohammad's Blog  Darren Wilkinson's research blog  John's Langford's Blog .  Kyle Kastner's Blog  Sander Dieleman's Blog",
            "title": "Blogs"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#programming-environment-scala",
            "text": "Scala  is the implementation language for DynaML, it is a hybrid language which gives the user the ability to leverage functional and object oriented programming styles. Scala code compiles to Java  byte code  giving Scala complete interoperability with Java, i.e. you can use Java libraries and classes in Scala code.  The  Java Virtual Machine  which executes  byte code  is the run time for the complete Java ecosystem. This enables Scala, Java, Groovy and Clojure programs to run on a common platform, which is a boon for Machine Learning applications as we can leverage all the libraries in the Java ecosystem.  Learning Scala can be a significant investment as the language has a large number of features which require varying levels of skill and practice to master. Some resources for learning Scala are given below.",
            "title": "Programming Environment: Scala"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#courses",
            "text": "Functional Programming Principles with Scala  by Martin Odersky.  Functional Program Design in Scala  by Martin Odersky.",
            "title": "Courses"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#videosyoutube_1",
            "text": "Scala tutorials playlist",
            "title": "Videos/Youtube"
        },
        {
            "location": "/blog/2016-12-08-getting-started-ml/#blogs_1",
            "text": "Haoyi's Programming Blog  Scala News  Typelevel Blog  Codacy Blog",
            "title": "Blogs"
        },
        {
            "location": "/Contributing/",
            "text": "First off, thank you for considering contributing to DynaML.\n\n\nWhat to Contribute\n\u00b6\n\n\n\n\nDynaML is an open source project and we love to receive contributions from our community \u2014 you! \nThere are many ways to contribute, from writing tutorials or blog posts, improving the documentation, \nsubmitting bug reports and feature requests or writing code which can be incorporated into DynaML itself.\n\n\n\n\nHow to Contribute\n\u00b6\n\n\nBugs & Issues & Features\n\u00b6\n\n\nBe sure to go through the \nuser guide\n as it is the \none stop shop for tutorials and documentation.\n\n\nBefore you think of submitting an \nissue\n, visit\nthe \nDynaML chat room\n\non gitter and drop us a word about your problem.\n\n\nWhen submitting a new issue for a feature addition be sure to.\n\n\n\n\nGive a short motivation for the new feature, model, etc.\n\n\nGive references to research papers where relevant.\n\n\nSuggest (if possible) which classes/traits of the API can be used/extended for the feature\n\n\nBe prepared to discuss at depth on chat as well as issue forums on the need, execution and logistics of implementing the new feature\n\n\nStrive to provide relevant details to make collaboration easier.",
            "title": "Contributing"
        },
        {
            "location": "/Contributing/#what-to-contribute",
            "text": "DynaML is an open source project and we love to receive contributions from our community \u2014 you! \nThere are many ways to contribute, from writing tutorials or blog posts, improving the documentation, \nsubmitting bug reports and feature requests or writing code which can be incorporated into DynaML itself.",
            "title": "What to Contribute"
        },
        {
            "location": "/Contributing/#how-to-contribute",
            "text": "",
            "title": "How to Contribute"
        },
        {
            "location": "/Contributing/#bugs-issues-features",
            "text": "Be sure to go through the  user guide  as it is the \none stop shop for tutorials and documentation.  Before you think of submitting an  issue , visit\nthe  DynaML chat room \non gitter and drop us a word about your problem.  When submitting a new issue for a feature addition be sure to.   Give a short motivation for the new feature, model, etc.  Give references to research papers where relevant.  Suggest (if possible) which classes/traits of the API can be used/extended for the feature  Be prepared to discuss at depth on chat as well as issue forums on the need, execution and logistics of implementing the new feature  Strive to provide relevant details to make collaboration easier.",
            "title": "Bugs &amp; Issues &amp; Features"
        },
        {
            "location": "/code_of_conduct/",
            "text": "Contributor Covenant Code of Conduct\n\u00b6\n\n\nOur Pledge\n\u00b6\n\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of experience,\nnationality, personal appearance, race, religion, or sexual identity and\norientation.\n\n\nOur Standards\n\u00b6\n\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n\n\n\nUsing welcoming and inclusive language\n\n\nBeing respectful of differing viewpoints and experiences\n\n\nGracefully accepting constructive criticism\n\n\nFocusing on what is best for the community\n\n\nShowing empathy towards other community members\n\n\n\n\nExamples of unacceptable behavior by participants include:\n\n\n\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\n\n\nTrolling, derogatory comments, and personal or political attacks\n\n\nPublic or private harassment\n\n\nPublishing others' private information, such as a physical or electronic address, without explicit permission\n\n\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nWeapons Policy\n\u00b6\n\n\nNo weapons will be allowed at DynaML events, community spaces, or in other spaces covered by the scope of this \nCode of Conduct. Weapons include but are not limited to guns, explosives (including fireworks), and large knives \nsuch as those used for hunting or display, as well as any other item used for the purpose of causing injury \nor harm to others. Anyone seen in possession of one of these items will be asked to leave immediately, \nand will only be allowed to return without the weapon. Community members are further expected to comply \nwith all state and local laws on this matter.\n\n\nOur Responsibilities\n\u00b6\n\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n\nScope\n\u00b6\n\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n\nEnforcement\n\u00b6\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team \nhere\n. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n\nAttribution\n\u00b6\n\n\nThis Code of Conduct is adapted from the \nContributor Covenant\n, version 1.4,\navailable at \nhttp://contributor-covenant.org/version/\u00bc",
            "title": "Code of Conduct"
        },
        {
            "location": "/code_of_conduct/#contributor-covenant-code-of-conduct",
            "text": "",
            "title": "Contributor Covenant Code of Conduct"
        },
        {
            "location": "/code_of_conduct/#our-pledge",
            "text": "In the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of experience,\nnationality, personal appearance, race, religion, or sexual identity and\norientation.",
            "title": "Our Pledge"
        },
        {
            "location": "/code_of_conduct/#our-standards",
            "text": "Examples of behavior that contributes to creating a positive environment\ninclude:   Using welcoming and inclusive language  Being respectful of differing viewpoints and experiences  Gracefully accepting constructive criticism  Focusing on what is best for the community  Showing empathy towards other community members   Examples of unacceptable behavior by participants include:   The use of sexualized language or imagery and unwelcome sexual attention or advances  Trolling, derogatory comments, and personal or political attacks  Public or private harassment  Publishing others' private information, such as a physical or electronic address, without explicit permission  Other conduct which could reasonably be considered inappropriate in a professional setting",
            "title": "Our Standards"
        },
        {
            "location": "/code_of_conduct/#weapons-policy",
            "text": "No weapons will be allowed at DynaML events, community spaces, or in other spaces covered by the scope of this \nCode of Conduct. Weapons include but are not limited to guns, explosives (including fireworks), and large knives \nsuch as those used for hunting or display, as well as any other item used for the purpose of causing injury \nor harm to others. Anyone seen in possession of one of these items will be asked to leave immediately, \nand will only be allowed to return without the weapon. Community members are further expected to comply \nwith all state and local laws on this matter.",
            "title": "Weapons Policy"
        },
        {
            "location": "/code_of_conduct/#our-responsibilities",
            "text": "Project maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.  Project maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.",
            "title": "Our Responsibilities"
        },
        {
            "location": "/code_of_conduct/#scope",
            "text": "This Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.",
            "title": "Scope"
        },
        {
            "location": "/code_of_conduct/#enforcement",
            "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team  here . All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.  Project maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.",
            "title": "Enforcement"
        },
        {
            "location": "/code_of_conduct/#attribution",
            "text": "This Code of Conduct is adapted from the  Contributor Covenant , version 1.4,\navailable at  http://contributor-covenant.org/version/\u00bc",
            "title": "Attribution"
        }
    ]
}