{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DynaML: ML + JVM + Scala \u00b6 DynaML is a Scala & JVM Machine Learning toolbox for research, education & industry. Motivation \u00b6 Interactive. Don't want to create Maven/sbt project skeletons every time you want to try out ideas? Create and execute scala worksheets in the DynaML shell. DynaML comes packaged with a customized version of the Ammonite REPL, with auto-complete , file operations and scripting capabilities. End to End. Create complex pre-processing pipelines with the data pipes API, train models ( deep nets , gaussian processes , linear models and more), optimize over hyper-parameters , evaluate model predictions and visualise results. Enterprise Friendly. Take advantage of the JVM and Scala ecosystem, use Apache Spark to write scalable data analysis jobs, Tensorflow for deep learning, all in the same toolbox. Getting Started \u00b6 Platform Compatibility \u00b6 Currently, only *nix and OSX platforms are supported. DynaML is compatible with Scala 2.11 Installation \u00b6 Easiest way to install DynaML is cloning & compiling from the github repository. Please take a look at the installation instructions, to make sure that you have the pre-requisites and to configure your installation. CIFAR in 100 lines \u00b6 Below is a sample script where we train a neural network of stacked Inception cells on the CIFAR-10 image classification task. import ammonite.ops._ import io.github.mandar2812.dynaml.pipes.DataPipe import io.github.mandar2812.dynaml.tensorflow.data.AbstractDataSet import io.github.mandar2812.dynaml.tensorflow. { dtflearn , dtfutils } import io.github.mandar2812.dynaml.tensorflow.implicits._ import org.platanios.tensorflow.api._ import org.platanios.tensorflow.api.learn.layers.Activation import org.platanios.tensorflow.data.image.CIFARLoader import java.nio.file.Paths val tempdir = home / \"tmp\" val dataSet = CIFARLoader . load ( Paths . get ( tempdir . toString ()), CIFARLoader . CIFAR_10 ) val tf_dataset = AbstractDataSet ( dataSet . trainImages , dataSet . trainLabels , dataSet . trainLabels . shape ( 0 ), dataSet . testImages , dataSet . testLabels , dataSet . testLabels . shape ( 0 )) val trainData = tf_dataset . training_data . repeat () . shuffle ( 10000 ) . batch ( 128 ) . prefetch ( 10 ) println ( \"Building the model.\" ) val input = tf . learn . Input ( UINT8 , Shape ( - 1 , dataSet . trainImages . shape ( 1 ), dataSet . trainImages . shape ( 2 ), dataSet . trainImages . shape ( 3 )) ) val trainInput = tf . learn . Input ( UINT8 , Shape (- 1 )) val relu_act = DataPipe [ String , Activation ]( tf . learn . ReLU ( _ )) val architecture = tf . learn . Cast ( \"Input/Cast\" , FLOAT32 ) >> dtflearn . inception_unit ( channels = 3 , Seq . fill ( 4 )( 10 ), relu_act )( layer_index = 1 ) >> dtflearn . inception_unit ( channels = 40 , Seq . fill ( 4 )( 5 ), relu_act )( layer_index = 2 ) >> tf . learn . Flatten ( \"Layer_3/Flatten\" ) >> dtflearn . feedforward ( 256 )( id = 4 ) >> tf . learn . ReLU ( \"Layer_4/ReLU\" , 0.1f ) >> dtflearn . feedforward ( 10 )( id = 5 ) val trainingInputLayer = tf . learn . Cast ( \"TrainInput/Cast\" , INT64 ) val loss = tf . learn . SparseSoftmaxCrossEntropy ( \"Loss/CrossEntropy\" ) >> tf . learn . Mean ( \"Loss/Mean\" ) >> tf . learn . ScalarSummary ( \"Loss/Summary\" , \"Loss\" ) val optimizer = tf . train . Adam ( 0.1 ) val summariesDir = Paths . get (( tempdir / \"cifar_summaries\" ). toString ()) val ( model , estimator ) = dtflearn . build_tf_model ( architecture , input , trainInput , trainingInputLayer , loss , optimizer , summariesDir , dtflearn . max_iter_stop ( 500 ), 100 , 100 , 100 )( trainData , true ) def accuracy ( predictions : Tensor , labels : Tensor ) : Float = predictions . argmax ( 1 ) . cast ( UINT8 ) . equal ( labels ) . cast ( FLOAT32 ) . mean () . scalar . asInstanceOf [ Float ] val ( trainingPreds , testPreds ) : ( Option [ Tensor ], Option [ Tensor ]) = dtfutils . predict_data [ Tensor , Output , DataType , Shape , Output , Tensor , Output , DataType , Shape , Output , Tensor , Tensor ]( estimator , data = tf_dataset , pred_flags = ( true , true ), buff_size = 20000 ) val ( trainAccuracy , testAccuracy ) = ( accuracy ( trainingPreds . get , dataSet . trainLabels ), accuracy ( testPreds . get , dataSet . testLabels )) print ( \"Train accuracy = \" ) pprint . pprintln ( trainAccuracy ) print ( \"Test accuracy = \" ) pprint . pprintln ( testAccuracy ) Support & Community \u00b6 Gitter Contributing Code of Conduct","title":"DynaML"},{"location":"#dynaml-ml-jvm-scala","text":"DynaML is a Scala & JVM Machine Learning toolbox for research, education & industry.","title":"DynaML: ML + JVM + Scala"},{"location":"#motivation","text":"Interactive. Don't want to create Maven/sbt project skeletons every time you want to try out ideas? Create and execute scala worksheets in the DynaML shell. DynaML comes packaged with a customized version of the Ammonite REPL, with auto-complete , file operations and scripting capabilities. End to End. Create complex pre-processing pipelines with the data pipes API, train models ( deep nets , gaussian processes , linear models and more), optimize over hyper-parameters , evaluate model predictions and visualise results. Enterprise Friendly. Take advantage of the JVM and Scala ecosystem, use Apache Spark to write scalable data analysis jobs, Tensorflow for deep learning, all in the same toolbox.","title":"Motivation"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#platform-compatibility","text":"Currently, only *nix and OSX platforms are supported. DynaML is compatible with Scala 2.11","title":"Platform Compatibility"},{"location":"#installation","text":"Easiest way to install DynaML is cloning & compiling from the github repository. Please take a look at the installation instructions, to make sure that you have the pre-requisites and to configure your installation.","title":"Installation"},{"location":"#cifar-in-100-lines","text":"Below is a sample script where we train a neural network of stacked Inception cells on the CIFAR-10 image classification task. import ammonite.ops._ import io.github.mandar2812.dynaml.pipes.DataPipe import io.github.mandar2812.dynaml.tensorflow.data.AbstractDataSet import io.github.mandar2812.dynaml.tensorflow. { dtflearn , dtfutils } import io.github.mandar2812.dynaml.tensorflow.implicits._ import org.platanios.tensorflow.api._ import org.platanios.tensorflow.api.learn.layers.Activation import org.platanios.tensorflow.data.image.CIFARLoader import java.nio.file.Paths val tempdir = home / \"tmp\" val dataSet = CIFARLoader . load ( Paths . get ( tempdir . toString ()), CIFARLoader . CIFAR_10 ) val tf_dataset = AbstractDataSet ( dataSet . trainImages , dataSet . trainLabels , dataSet . trainLabels . shape ( 0 ), dataSet . testImages , dataSet . testLabels , dataSet . testLabels . shape ( 0 )) val trainData = tf_dataset . training_data . repeat () . shuffle ( 10000 ) . batch ( 128 ) . prefetch ( 10 ) println ( \"Building the model.\" ) val input = tf . learn . Input ( UINT8 , Shape ( - 1 , dataSet . trainImages . shape ( 1 ), dataSet . trainImages . shape ( 2 ), dataSet . trainImages . shape ( 3 )) ) val trainInput = tf . learn . Input ( UINT8 , Shape (- 1 )) val relu_act = DataPipe [ String , Activation ]( tf . learn . ReLU ( _ )) val architecture = tf . learn . Cast ( \"Input/Cast\" , FLOAT32 ) >> dtflearn . inception_unit ( channels = 3 , Seq . fill ( 4 )( 10 ), relu_act )( layer_index = 1 ) >> dtflearn . inception_unit ( channels = 40 , Seq . fill ( 4 )( 5 ), relu_act )( layer_index = 2 ) >> tf . learn . Flatten ( \"Layer_3/Flatten\" ) >> dtflearn . feedforward ( 256 )( id = 4 ) >> tf . learn . ReLU ( \"Layer_4/ReLU\" , 0.1f ) >> dtflearn . feedforward ( 10 )( id = 5 ) val trainingInputLayer = tf . learn . Cast ( \"TrainInput/Cast\" , INT64 ) val loss = tf . learn . SparseSoftmaxCrossEntropy ( \"Loss/CrossEntropy\" ) >> tf . learn . Mean ( \"Loss/Mean\" ) >> tf . learn . ScalarSummary ( \"Loss/Summary\" , \"Loss\" ) val optimizer = tf . train . Adam ( 0.1 ) val summariesDir = Paths . get (( tempdir / \"cifar_summaries\" ). toString ()) val ( model , estimator ) = dtflearn . build_tf_model ( architecture , input , trainInput , trainingInputLayer , loss , optimizer , summariesDir , dtflearn . max_iter_stop ( 500 ), 100 , 100 , 100 )( trainData , true ) def accuracy ( predictions : Tensor , labels : Tensor ) : Float = predictions . argmax ( 1 ) . cast ( UINT8 ) . equal ( labels ) . cast ( FLOAT32 ) . mean () . scalar . asInstanceOf [ Float ] val ( trainingPreds , testPreds ) : ( Option [ Tensor ], Option [ Tensor ]) = dtfutils . predict_data [ Tensor , Output , DataType , Shape , Output , Tensor , Output , DataType , Shape , Output , Tensor , Tensor ]( estimator , data = tf_dataset , pred_flags = ( true , true ), buff_size = 20000 ) val ( trainAccuracy , testAccuracy ) = ( accuracy ( trainingPreds . get , dataSet . trainLabels ), accuracy ( testPreds . get , dataSet . testLabels )) print ( \"Train accuracy = \" ) pprint . pprintln ( trainAccuracy ) print ( \"Test accuracy = \" ) pprint . pprintln ( testAccuracy )","title":"CIFAR in 100 lines"},{"location":"#support-community","text":"Gitter Contributing Code of Conduct","title":"Support &amp; Community"},{"location":"Contributing/","text":"First off, thank you for considering contributing to DynaML. What to Contribute \u00b6 DynaML is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into DynaML itself. How to Contribute \u00b6 Bugs & Issues & Features \u00b6 Be sure to go through the user guide as it is the one stop shop for tutorials and documentation. Before you think of submitting an issue , visit the DynaML chat room on gitter and drop us a word about your problem. When submitting a new issue for a feature addition be sure to. Give a short motivation for the new feature, model, etc. Give references to research papers where relevant. Suggest (if possible) which classes/traits of the API can be used/extended for the feature Be prepared to discuss at depth on chat as well as issue forums on the need, execution and logistics of implementing the new feature Strive to provide relevant details to make collaboration easier.","title":"Contributing"},{"location":"Contributing/#what-to-contribute","text":"DynaML is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into DynaML itself.","title":"What to Contribute"},{"location":"Contributing/#how-to-contribute","text":"","title":"How to Contribute"},{"location":"Contributing/#bugs-issues-features","text":"Be sure to go through the user guide as it is the one stop shop for tutorials and documentation. Before you think of submitting an issue , visit the DynaML chat room on gitter and drop us a word about your problem. When submitting a new issue for a feature addition be sure to. Give a short motivation for the new feature, model, etc. Give references to research papers where relevant. Suggest (if possible) which classes/traits of the API can be used/extended for the feature Be prepared to discuss at depth on chat as well as issue forums on the need, execution and logistics of implementing the new feature Strive to provide relevant details to make collaboration easier.","title":"Bugs &amp; Issues &amp; Features"},{"location":"about/","text":"Affliations \u00b6 DynaML is developed and maintained by Transcendent AI Labs DynaML is proud to be a part of the Mozilla Science Collaborate platform Contributors \u00b6 Mandar Chandorkar Amit Kumar Jaiswal Sisir Kopakka Have questions or suggestions? Feel free to open an issue on GitHub or ask me on Twitter . Attributions \u00b6 Logo \u00b6 Business graphic by roundicons from Flaticon is licensed under CC BY 3.0 .","title":"About Us"},{"location":"about/#affliations","text":"DynaML is developed and maintained by Transcendent AI Labs DynaML is proud to be a part of the Mozilla Science Collaborate platform","title":"Affliations"},{"location":"about/#contributors","text":"Mandar Chandorkar Amit Kumar Jaiswal Sisir Kopakka Have questions or suggestions? Feel free to open an issue on GitHub or ask me on Twitter .","title":"Contributors"},{"location":"about/#attributions","text":"","title":"Attributions"},{"location":"about/#logo","text":"Business graphic by roundicons from Flaticon is licensed under CC BY 3.0 .","title":"Logo"},{"location":"code_of_conduct/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \u00b6 Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Weapons Policy \u00b6 No weapons will be allowed at DynaML events, community spaces, or in other spaces covered by the scope of this Code of Conduct. Weapons include but are not limited to guns, explosives (including fireworks), and large knives such as those used for hunting or display, as well as any other item used for the purpose of causing injury or harm to others. Anyone seen in possession of one of these items will be asked to leave immediately, and will only be allowed to return without the weapon. Community members are further expected to comply with all state and local laws on this matter. Our Responsibilities \u00b6 Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \u00b6 This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team here . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/\u00bc","title":"Code of Conduct"},{"location":"code_of_conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code_of_conduct/#weapons-policy","text":"No weapons will be allowed at DynaML events, community spaces, or in other spaces covered by the scope of this Code of Conduct. Weapons include but are not limited to guns, explosives (including fireworks), and large knives such as those used for hunting or display, as well as any other item used for the purpose of causing injury or harm to others. Anyone seen in possession of one of these items will be asked to leave immediately, and will only be allowed to return without the weapon. Community members are further expected to comply with all state and local laws on this matter.","title":"Weapons Policy"},{"location":"code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code_of_conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team here . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/\u00bc","title":"Attribution"},{"location":"license/","text":"Copyright 2015 Mandar Chandorkar ( mandar2812@gmail.com ) Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at Apache License Warning Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Dependency Licenses \u00b6 The various components of DynaML which are enabled using dependencies are subject to their license requirements. The following table lists all the component licences used by DynaML's dependencies. Category License Dependency Notes Apache Apache 2 com.chuusai # shapeless_2.11 # 2.3.3 Apache Apache 2 com.quantifind # sumac_2.11 # 0.3.0 Apache Apache 2 com.quantifind # wisp_2.11 # 0.0.4 Apache Apache 2 com.trueaccord.lenses # lenses_2.11 # 0.4.12 Apache Apache 2 com.trueaccord.scalapb # scalapb-runtime_2.11 # 0.6.2 Apache Apache 2 com.twitter # chill-java # 0.8.0 Apache Apache 2 com.twitter # chill_2.11 # 0.8.0 Apache Apache 2 com.univocity # univocity-parsers # 2.2.1 Apache Apache 2 io.spray # spray-json_2.11 # 1.3.3 Apache Apache 2 joda-time # joda-time # 2.0 Apache Apache 2 net.sf.opencsv # opencsv # 2.3 Apache Apache 2 org.objenesis # objenesis # 2.6 Apache Apache 2 org.roaringbitmap # RoaringBitmap # 0.5.11 Apache Apache 2 org.scalaj # scalaj-http_2.11 # 2.3.0 Apache Apache 2 org.scalanlp # breeze-macros_2.11 # 0.13.2 Apache Apache 2 org.scalanlp # breeze-natives_2.11 # 0.13.2 Apache Apache 2 org.scalanlp # breeze_2.11 # 0.13.2 Apache Apache 2 org.typelevel # macro-compat_2.11 # 1.1.1 Apache Apache 2.0 io.circe # circe-core_2.11 # 0.9.1 Apache Apache 2.0 io.circe # circe-generic_2.11 # 0.9.1 Apache Apache 2.0 io.circe # circe-jawn_2.11 # 0.9.1 Apache Apache 2.0 io.circe # circe-numbers_2.11 # 0.9.1 Apache Apache 2.0 io.circe # circe-parser_2.11 # 0.9.1 Apache Apache 2.0 io.get-coursier # coursier-cache_2.11 # 1.0.0 Apache Apache 2.0 io.get-coursier # coursier_2.11 # 1.0.0 Apache Apache 2.0 net.java.dev.jets3t # jets3t # 0.9.3 Apache Apache 2.0 License com.typesafe.scala-logging # scala-logging_2.11 # 3.9.0 Apache Apache 2.0 License org.apache.spark # spark-catalyst_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-core_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-graphx_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-launcher_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-mllib-local_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-mllib_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-network-common_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-network-shuffle_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-sketch_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-sql_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-streaming_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-tags_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-unsafe_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.sshd # sshd-core # 1.2.0 Apache Apache License commons-httpclient # commons-httpclient # 3.1 Apache Apache License info.folone # poi-scala_2.11 # 0.18 Apache Apache License org.apache.httpcomponents # httpmime # 4.2.5 Apache Apache License 2.0 org.renjin.cran # utf8 # 1.1.3-b8 Apache Apache License 2.0 com.ning # compress-lzf # 1.0.3 Apache Apache License 2.0 io.dropwizard.metrics # metrics-core # 3.1.2 Apache Apache License 2.0 io.dropwizard.metrics # metrics-graphite # 3.1.2 Apache Apache License 2.0 io.dropwizard.metrics # metrics-json # 3.1.2 Apache Apache License 2.0 io.dropwizard.metrics # metrics-jvm # 3.1.2 Apache Apache License 2.0 org.platanios # tensorflow-api_2.11 # 0.2.4 Apache Apache License 2.0 org.platanios # tensorflow-data_2.11 # 0.2.4 Apache Apache License 2.0 org.platanios # tensorflow-jni_2.11 # 0.2.4 Apache Apache License 2.0 org.platanios # tensorflow_2.11 # 0.2.4 Apache Apache License 2.0 org.renjin.cran # RColorBrewer # 1.1-2-b321 Apache Apache License, Version 2.0 com.clearspring.analytics # stream # 2.7.0 Apache Apache License, Version 2.0 com.github.javaparser # javaparser-core # 3.2.5 Apache Apache License, Version 2.0 com.github.tototoshi # scala-csv_2.11 # 1.1.2 Apache Apache License, Version 2.0 com.jamesmurty.utils # java-xmlbuilder # 1.0 Apache Apache License, Version 2.0 com.typesafe # config # 1.3.1 Apache Apache License, Version 2.0 com.typesafe # ssl-config-core_2.11 # 0.2.1 Apache Apache License, Version 2.0 com.typesafe.akka # akka-actor_2.11 # 2.5.3 Apache Apache License, Version 2.0 com.typesafe.akka # akka-stream-testkit_2.11 # 2.4.19 Apache Apache License, Version 2.0 com.typesafe.akka # akka-stream_2.11 # 2.5.3 Apache Apache License, Version 2.0 com.typesafe.akka # akka-testkit_2.11 # 2.5.3 Apache Apache License, Version 2.0 commons-codec # commons-codec # 1.10 Apache Apache License, Version 2.0 commons-collections # commons-collections # 3.2.2 Apache Apache License, Version 2.0 commons-io # commons-io # 2.6 Apache Apache License, Version 2.0 io.netty # netty # 3.9.9.Final Apache Apache License, Version 2.0 io.netty # netty-all # 4.0.43.Final Apache Apache License, Version 2.0 org.apache.commons # commons-compress # 1.15 Apache Apache License, Version 2.0 org.apache.commons # commons-crypto # 1.0.0 Apache Apache License, Version 2.0 org.apache.commons # commons-lang3 # 3.5 Apache Apache License, Version 2.0 org.apache.httpcomponents # httpclient # 4.5.1 Apache Apache License, Version 2.0 org.apache.httpcomponents # httpcore # 4.4.3 Apache Apache License, Version 2.0 org.apache.pdfbox # fontbox # 2.0.9 Apache Apache License, Version 2.0 org.apache.pdfbox # pdfbox # 2.0.9 Apache Apache License, Version 2.0 org.codehaus.jettison # jettison # 1.3.3 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-continuation # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-http # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-io # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-security # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-server # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-servlet # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-util # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-webapp # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-xml # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty.orbit # javax.servlet # 3.0.0.v201112011016 Apache Apache Software License - Version 2.0 org.mortbay.jetty # jetty-util # 6.1.26 Apache Apache-2.0 com.typesafe.akka # akka-http-core_2.11 # 10.0.9 Apache Apache-2.0 com.typesafe.akka # akka-http-spray-json_2.11 # 10.0.9 Apache Apache-2.0 com.typesafe.akka # akka-http-testkit_2.11 # 10.0.9 Apache Apache-2.0 com.typesafe.akka # akka-http_2.11 # 10.0.9 Apache Apache-2.0 com.typesafe.akka # akka-parsing_2.11 # 10.0.9 Apache Apache-2.0 org.json4s # json4s-ast_2.11 # 3.3.0 Apache Apache-2.0 org.json4s # json4s-core_2.11 # 3.3.0 Apache Apache-2.0 org.json4s # json4s-native_2.11 # 3.3.0 Apache Apache-2.0 org.json4s # json4s-scalap_2.11 # 3.3.0 Apache The Apache License, Version 2.0 com.github.ghik # silencer-lib_2.11 # 0.6 Apache The Apache License, Version 2.0 org.spark-project.spark # unused # 1.0.0 Apache The Apache Software License, Version 2.0 ar.com.hjg # pngj # 2.1.0 Apache The Apache Software License, Version 2.0 com.carrotsearch # hppc # 0.6.0 Apache The Apache Software License, Version 2.0 com.drewnoakes # metadata-extractor # 2.8.1 Apache The Apache Software License, Version 2.0 com.fasterxml.jackson.core # jackson-annotations # 2.6.5 Apache The Apache Software License, Version 2.0 com.fasterxml.jackson.core # jackson-core # 2.6.5 Apache The Apache Software License, Version 2.0 com.fasterxml.jackson.core # jackson-databind # 2.6.5 Apache The Apache Software License, Version 2.0 com.fasterxml.jackson.module # jackson-module-paranamer # 2.6.5 Apache The Apache Software License, Version 2.0 com.fasterxml.jackson.module # jackson-module-scala_2.11 # 2.6.5 Apache The Apache Software License, Version 2.0 com.google.code.findbugs # jsr305 # 1.3.9 Apache The Apache Software License, Version 2.0 com.google.code.gson # gson # 2.2.4 Apache The Apache Software License, Version 2.0 com.google.guava # guava # 14.0.1 Apache The Apache Software License, Version 2.0 com.google.inject # guice # 3.0 Apache The Apache Software License, Version 2.0 com.sksamuel.scrimage # scrimage-core_2.11 # 2.1.8 Apache The Apache Software License, Version 2.0 com.sksamuel.scrimage # scrimage-filters_2.11 # 2.1.8 Apache The Apache Software License, Version 2.0 com.sksamuel.scrimage # scrimage-io-extra_2.11 # 2.1.8 Apache The Apache Software License, Version 2.0 commons-beanutils # commons-beanutils-core # 1.8.0 Apache The Apache Software License, Version 2.0 commons-cli # commons-cli # 1.2 Apache The Apache Software License, Version 2.0 commons-configuration # commons-configuration # 1.6 Apache The Apache Software License, Version 2.0 commons-digester # commons-digester # 1.8 Apache The Apache Software License, Version 2.0 commons-lang # commons-lang # 2.6 Apache The Apache Software License, Version 2.0 commons-logging # commons-logging # 1.2 Apache The Apache Software License, Version 2.0 commons-net # commons-net # 2.2 Apache The Apache Software License, Version 2.0 javax.inject # javax.inject # 1 Apache The Apache Software License, Version 2.0 javax.validation # validation-api # 1.1.0.Final Apache The Apache Software License, Version 2.0 log4j # log4j # 1.2.17 Apache The Apache Software License, Version 2.0 mx4j # mx4j # 3.0.2 Apache The Apache Software License, Version 2.0 net.jpountz.lz4 # lz4 # 1.3.0 Apache The Apache Software License, Version 2.0 org.apache.ant # ant # 1.8.3 Apache The Apache Software License, Version 2.0 org.apache.ant # ant-launcher # 1.8.3 Apache The Apache Software License, Version 2.0 org.apache.avro # avro # 1.7.7 Apache The Apache Software License, Version 2.0 org.apache.avro # avro-ipc # 1.7.7 Apache The Apache Software License, Version 2.0 org.apache.avro # avro-mapred # 1.7.7 Apache The Apache Software License, Version 2.0 org.apache.commons # commons-math # 2.2 Apache The Apache Software License, Version 2.0 org.apache.commons # commons-math3 # 3.4.1 Apache The Apache Software License, Version 2.0 org.apache.commons # commons-vfs2 # 2.0 Apache The Apache Software License, Version 2.0 org.apache.curator # curator-client # 2.6.0 Apache The Apache Software License, Version 2.0 org.apache.curator # curator-framework # 2.6.0 Apache The Apache Software License, Version 2.0 org.apache.curator # curator-recipes # 2.6.0 Apache The Apache Software License, Version 2.0 org.apache.directory.api # api-asn1-api # 1.0.0-M20 Apache The Apache Software License, Version 2.0 org.apache.directory.api # api-util # 1.0.0-M20 Apache The Apache Software License, Version 2.0 org.apache.directory.server # apacheds-i18n # 2.0.0-M15 Apache The Apache Software License, Version 2.0 org.apache.directory.server # apacheds-kerberos-codec # 2.0.0-M15 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-annotations # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-auth # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-client # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-common # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-hdfs # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-mapreduce-client-app # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-mapreduce-client-common # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-mapreduce-client-core # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-mapreduce-client-jobclient # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-mapreduce-client-shuffle # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-yarn-api # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-yarn-client # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-yarn-common # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-yarn-server-common # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-yarn-server-nodemanager # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.ivy # ivy # 2.4.0 Apache The Apache Software License, Version 2.0 org.apache.maven.scm # maven-scm-api # 1.4 Apache The Apache Software License, Version 2.0 org.apache.maven.scm # maven-scm-provider-svn-commons # 1.4 Apache The Apache Software License, Version 2.0 org.apache.maven.scm # maven-scm-provider-svnexe # 1.4 Apache The Apache Software License, Version 2.0 org.apache.parquet # parquet-column # 1.8.2 Apache The Apache Software License, Version 2.0 org.apache.parquet # parquet-common # 1.8.2 Apache The Apache Software License, Version 2.0 org.apache.parquet # parquet-encoding # 1.8.2 Apache The Apache Software License, Version 2.0 org.apache.parquet # parquet-format # 2.3.1 Apache The Apache Software License, Version 2.0 org.apache.parquet # parquet-hadoop # 1.8.2 Apache The Apache Software License, Version 2.0 org.apache.parquet # parquet-jackson # 1.8.2 Apache The Apache Software License, Version 2.0 org.apache.poi # poi # 3.14 Apache The Apache Software License, Version 2.0 org.apache.poi # poi-ooxml # 3.14 Apache The Apache Software License, Version 2.0 org.apache.poi # poi-ooxml-schemas # 3.14 Apache The Apache Software License, Version 2.0 org.apache.xmlbeans # xmlbeans # 2.6.0 Apache The Apache Software License, Version 2.0 org.codehaus.groovy # groovy # 1.8.9 Apache The Apache Software License, Version 2.0 org.codehaus.jackson # jackson-core-asl # 1.9.13 Apache The Apache Software License, Version 2.0 org.codehaus.jackson # jackson-jaxrs # 1.9.13 Apache The Apache Software License, Version 2.0 org.codehaus.jackson # jackson-mapper-asl # 1.9.13 Apache The Apache Software License, Version 2.0 org.codehaus.jackson # jackson-xc # 1.9.13 Apache The Apache Software License, Version 2.0 org.codehaus.plexus # plexus-utils # 1.5.6 Apache The Apache Software License, Version 2.0 org.fusesource.jansi # jansi # 1.5 Apache The Apache Software License, Version 2.0 org.htrace # htrace-core # 3.0.4 Apache The Apache Software License, Version 2.0 org.la4j # la4j # 0.6.0 Apache The Apache Software License, Version 2.0 org.renjin # renjin-asm # 5.0.4b Apache The Apache Software License, Version 2.0 org.renjin # renjin-guava # 17.0b Apache The Apache Software License, Version 2.0 org.scala-graph # graph-core_2.11 # 1.11.3 Apache The Apache Software License, Version 2.0 org.smurn # jply # 0.2.1 Apache The Apache Software License, Version 2.0 org.sonatype.sisu.inject # cglib # 2.2.1-v20090111 Apache The Apache Software License, Version 2.0 org.tensorflow # proto # 1.9.0-rc1 Apache The Apache Software License, Version 2.0 org.xerial.snappy # snappy-java # 1.1.2.6 Apache The Apache Software License, Version 2.0 stax # stax-api # 1.0.1 Apache The Apache Software License, Version 2.0 xerces # xercesImpl # 2.9.1 Apache The Apache Software License, Version 2.0 xml-apis # xml-apis # 1.3.04 Apache the Apache License, ASL Version 2.0 org.scalactic # scalactic_2.11 # 3.0.4 BSD 3-Clause BSD License com.google.protobuf # protobuf-java # 3.5.1 BSD BSD asm # asm # 3.2 BSD BSD asm # asm-analysis # 3.2 BSD BSD asm # asm-commons # 3.2 BSD BSD asm # asm-tree # 3.2 BSD BSD asm # asm-util # 3.2 BSD BSD com.miglayout # miglayout # 3.7.4 BSD BSD com.thoughtworks.paranamer # paranamer # 2.8 BSD BSD jline # jline # 0.9.94 BSD BSD net.sourceforge.jmatio # jmatio # 1.0 BSD BSD org.scalafx # scalafx_2.11 # 8.0.92-R10 BSD BSD org.scalameta # common_2.11 # 2.0.1 BSD BSD org.scalameta # dialects_2.11 # 2.0.1 BSD BSD org.scalameta # inputs_2.11 # 2.0.1 BSD BSD org.scalameta # io_2.11 # 2.0.1 BSD BSD org.scalameta # langmeta_2.11 # 2.0.1 BSD BSD org.scalameta # parsers_2.11 # 2.0.1 BSD BSD org.scalameta # quasiquotes_2.11 # 2.0.1 BSD BSD org.scalameta # scalameta_2.11 # 2.0.1 BSD BSD org.scalameta # semanticdb_2.11 # 2.0.1 BSD BSD org.scalameta # tokenizers_2.11 # 2.0.1 BSD BSD org.scalameta # tokens_2.11 # 2.0.1 BSD BSD org.scalameta # transversers_2.11 # 2.0.1 BSD BSD org.scalameta # trees_2.11 # 2.0.1 BSD BSD 3 Clause com.github.fommil.netlib # all # 1.1.2 BSD BSD 3 Clause com.github.fommil.netlib # core # 1.1.2 BSD BSD 3 Clause com.github.fommil.netlib # native_ref-java # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # native_system-java # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_ref-linux-armhf # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_ref-linux-i686 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_ref-linux-x86_64 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_ref-osx-x86_64 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_ref-win-i686 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_ref-win-x86_64 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_system-linux-armhf # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_system-linux-i686 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_system-linux-x86_64 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_system-osx-x86_64 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_system-win-i686 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_system-win-x86_64 # 1.1 BSD BSD 3-Clause com.tinkerpop # frames # 2.5.0 BSD BSD 3-Clause com.tinkerpop # pipes # 2.6.0 BSD BSD 3-Clause com.tinkerpop.blueprints # blueprints-core # 2.6.0 BSD BSD 3-Clause com.tinkerpop.gremlin # gremlin-groovy # 2.5.0 BSD BSD 3-Clause com.tinkerpop.gremlin # gremlin-java # 2.6.0 BSD BSD 3-Clause org.scala-lang # scala-compiler # 2.11.8 BSD BSD 3-Clause org.scala-lang # scala-library # 2.11.8 BSD BSD 3-Clause org.scala-lang # scala-reflect # 2.11.8 BSD BSD 3-Clause org.webjars.bower # vega # 2.6.5 BSD BSD 3-Clause org.webjars.bower # vega-lite # 1.2.0 BSD BSD 3-Clause License org.jpmml # pmml-model # 1.2.15 BSD BSD 3-Clause License org.jpmml # pmml-schema # 1.2.15 BSD BSD 3-clause org.scala-lang.modules # scala-java8-compat_2.11 # 0.7.0 BSD BSD 3-clause org.scala-lang.modules # scala-parser-combinators_2.11 # 1.0.4 BSD BSD 3-clause org.scala-lang.modules # scala-swing_2.11 # 1.0.1 BSD BSD 3-clause org.scala-lang.modules # scala-xml_2.11 # 1.0.6 BSD BSD License antlr # antlr # 2.7.7 BSD BSD License com.github.virtuald # curvesapi # 1.03 BSD BSD style com.thoughtworks.xstream # xstream # 1.4.7 BSD BSD-2 License org.jogamp.gluegen # gluegen-rt # 2.3.2 BSD BSD-2 License org.jogamp.gluegen # gluegen-rt-main # 2.3.2 BSD BSD-2 License org.jogamp.jogl # jogl-all # 2.3.2 BSD BSD-2 License org.jogamp.jogl # jogl-all-main # 2.3.2 BSD BSD-3-Clause org.webjars.bower # d3 # 3.5.17 BSD BSD-like org.scala-lang # jline # 2.11.0-M3 BSD BSD-like org.scala-lang # scala-pickling_2.11 # 0.9.1 BSD BSD-style org.scalaforge # scalax # 0.1 BSD BSD-style org.scalaz # scalaz-concurrent_2.11 # 7.2.16 BSD BSD-style org.scalaz # scalaz-core_2.11 # 7.2.16 BSD BSD-style org.scalaz # scalaz-effect_2.11 # 7.2.16 BSD BSD_3_clause + file LICENSE org.renjin.cran # colorspace # 1.3-2-b41 BSD New BSD License com.diffplug.matsim # matfilerw # 3.0.0 BSD New BSD License com.esotericsoftware # kryo-shaded # 3.0.3 BSD New BSD License com.esotericsoftware # minlog # 1.3.0 BSD New BSD License org.codehaus.janino # commons-compiler # 3.0.0 BSD New BSD License org.codehaus.janino # janino # 3.0.0 BSD New BSD License org.hamcrest # hamcrest-core # 1.3 BSD The (New) BSD License org.jzy3d # jzy3d-api # 1.0.2 BSD The (New) BSD License org.jzy3d # jzy3d-jdt-core # 1.0.2 BSD The BSD 3-Clause License org.fusesource.leveldbjni # leveldbjni-all # 1.8 BSD The BSD License com.adobe.xmp # xmpcore # 5.1.2 BSD The BSD License net.sourceforge.f2j # arpack_combined_all # 0.1 BSD The BSD License org.antlr # antlr4-runtime # 4.5.3 BSD The BSD License org.jline # jline # 3.6.2 BSD The BSD License org.jline # jline-terminal # 3.6.2 BSD The BSD License org.jline # jline-terminal-jna # 3.6.2 BSD The BSD License xmlenc # xmlenc # 0.52 BSD The New BSD License net.sf.py4j # py4j # 0.10.4 CC0 CC0 org.reactivestreams # reactive-streams # 1.0.0 CDDL COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0 javax.activation # activation # 1.1.1 CDDL COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0 javax.xml.stream # stax-api # 1.0-2 GPL CDDL v1.1 / GPL v2 dual license com.sun.codemodel # codemodel # 2.6 GPL CDDL+GPL License org.glassfish.jersey.bundles.repackaged # jersey-guava # 2.22.2 GPL CDDL+GPL License org.glassfish.jersey.containers # jersey-container-servlet # 2.22.2 GPL CDDL+GPL License org.glassfish.jersey.containers # jersey-container-servlet-core # 2.22.2 GPL CDDL+GPL License org.glassfish.jersey.core # jersey-client # 2.22.2 GPL CDDL+GPL License org.glassfish.jersey.core # jersey-common # 2.22.2 GPL CDDL+GPL License org.glassfish.jersey.core # jersey-server # 2.22.2 GPL CDDL+GPL License org.glassfish.jersey.media # jersey-media-jaxb # 2.22.2 GPL GNU General Public License (GPL) org.jfree # jfreesvg # 3.3 GPL GNU General Public License 3 org.openml # apiconnector # 1.0.11 GPL GPL (>= 2) org.renjin.cran # MatrixModels # 0.4-1-b237 GPL GPL (>= 2) org.renjin.cran # SparseM # 1.77-b20 GPL GPL (>= 2) org.renjin.cran # digest # 0.6.15-b10 GPL GPL (>= 2) org.renjin.cran # lattice # 0.20-35-b66 GPL GPL (>= 2) org.renjin.cran # locfit # 1.5-9.1-b345 GPL GPL (>= 2) org.renjin.cran # quantreg # 5.35-b5 GPL [GPL (>= 2) file LICENCE](null) org.renjin.cran # Matrix # 1.2-14-b2 GPL GPL (>= 3) org.renjin.cran # abc # 2.1-b294 GPL GPL (>= 3) org.renjin.cran # abc.data # 1.0-b272 GPL GPL-2 org.renjin.cran # dichromat # 2.0-0-b332 GPL GPL-2 org.renjin.cran # gtable # 0.2.0-b96 GPL [GPL-2 GPL-3](null) org.renjin.cran # MASS # 7.3-49-b11 GPL [GPL-2 GPL-3](null) org.renjin.cran # nnet # 7.3-12-b88 GPL [GPL-2 file LICENSE](null) org.renjin.cran # ggplot2 # 2.2.1-b112 GPL [GPL-2 file LICENSE](null) org.renjin.cran # stringr # 1.3.0-b7 GPL GPL-3 org.renjin.cran # assertthat # 0.2.0-b42 GPL GPL-3 org.renjin.cran # lazyeval # 0.2.1-b19 GPL GPL-3 org.renjin.cran # pillar # 1.2.2-b2 GPL GPL-3 org.renjin.cran # rlang # 0.2.0-b39 GPL GPL2 w/ CPE javax.ws.rs # javax.ws.rs-api # 2.0-m10 GPL GPL2 w/ CPE javax.xml.bind # jaxb-api # 2.2.2 GPL GPLv2+CE javax.mail # mail # 1.4.7 GPL with Classpath Extension CDDL + GPLv2 with classpath exception javax.annotation # javax.annotation-api # 1.2 GPL with Classpath Extension CDDL + GPLv2 with classpath exception javax.servlet # javax.servlet-api # 3.1.0 GPL with Classpath Extension CDDL + GPLv2 with classpath exception org.glassfish.hk2 # hk2-api # 2.4.0-b34 GPL with Classpath Extension CDDL + GPLv2 with classpath exception org.glassfish.hk2 # hk2-locator # 2.4.0-b34 GPL with Classpath Extension CDDL + GPLv2 with classpath exception org.glassfish.hk2 # hk2-utils # 2.4.0-b34 GPL with Classpath Extension CDDL + GPLv2 with classpath exception org.glassfish.hk2 # osgi-resource-locator # 1.0.1 GPL with Classpath Extension CDDL + GPLv2 with classpath exception org.glassfish.hk2.external # aopalliance-repackaged # 2.4.0-b34 GPL with Classpath Extension CDDL + GPLv2 with classpath exception org.glassfish.hk2.external # javax.inject # 2.4.0-b34 LGPL GNU Lesser General Public License ch.qos.logback # logback-classic # 1.2.3 LGPL GNU Lesser General Public License ch.qos.logback # logback-core # 1.2.3 LGPL GNU Lesser General Public License 2.1 net.sf.trove4j # trove4j # 3.0.3 LGPL GNU Lesser General Public License v3.0 com.github.vagmcs # optimus_2.11 # 2.0.0 LGPL LGPL com.github.fommil # jniloader # 1.1 LGPL LGPL, version 2.1 net.java.dev.jna # jna # 4.2.2 MIT MIT co.theasi # plotly_2.11 # 0.1 MIT MIT com.github.julien-truffaut # monocle-core_2.11 # 1.1.0 MIT MIT com.github.julien-truffaut # monocle-macro_2.11 # 1.1.0 MIT MIT com.lihaoyi # fansi_2.11 # 0.2.4 MIT MIT com.lihaoyi # pprint_2.11 # 0.5.2 MIT MIT com.lihaoyi # sourcecode_2.11 # 0.1.4 MIT MIT com.lihaoyi # ujson-jvm-2.11.11_2.11 # 0.6.0 MIT MIT com.lihaoyi # upickle_2.11 # 0.6.0 MIT MIT net.databinder # unfiltered-filter_2.11 # 0.8.3 MIT MIT net.databinder # unfiltered-jetty_2.11 # 0.8.3 MIT MIT net.databinder # unfiltered-util_2.11 # 0.8.3 MIT MIT net.databinder # unfiltered_2.11 # 0.8.3 MIT MIT org.spire-math # jawn-parser_2.11 # 0.11.0 MIT MIT org.typelevel # algebra_2.11 # 0.7.0 MIT MIT org.typelevel # cats-core_2.11 # 1.0.1 MIT MIT org.typelevel # cats-kernel_2.11 # 1.0.1 MIT MIT org.typelevel # cats-macros_2.11 # 1.0.1 MIT MIT org.typelevel # machinist_2.11 # 0.6.2 MIT MIT org.typelevel # spire-macros_2.11 # 0.14.1 MIT MIT org.typelevel # spire_2.11 # 0.14.1 MIT MIT + file LICENSE org.renjin.cran # R6 # 2.2.2-b44 MIT MIT + file LICENSE org.renjin.cran # cli # 1.0.0-b25 MIT MIT + file LICENSE org.renjin.cran # crayon # 1.3.4-b16 MIT MIT + file LICENSE org.renjin.cran # glue # 1.2.0-b22 MIT MIT + file LICENSE org.renjin.cran # magrittr # 1.5-b334 MIT MIT + file LICENSE org.renjin.cran # munsell # 0.4.3-b96 MIT MIT + file LICENSE org.renjin.cran # plyr # 1.8.4-b82 MIT MIT + file LICENSE org.renjin.cran # reshape2 # 1.4.3-b16 MIT MIT + file LICENSE org.renjin.cran # scales # 0.5.0-b32 MIT MIT + file LICENSE org.renjin.cran # tibble # 1.4.2-b7 MIT MIT + file LICENSE org.renjin.cran # viridisLite # 0.3.0-b7 MIT [MIT + file LICENSE Unlimited](null) org.renjin.cran # labeling # 0.3-b313 MIT MIT License com.github.scopt # scopt_2.11 # 3.5.0 MIT MIT License net.razorvine # pyrolite # 4.13 MIT MIT License org.slf4j # jcl-over-slf4j # 1.7.16 MIT MIT License org.slf4j # jul-to-slf4j # 1.7.16 MIT MIT License org.slf4j # slf4j-api # 1.7.25 MIT MIT License org.vegas-viz # vegas-macros_2.11 # 0.3.11 MIT MIT License org.vegas-viz # vegas_2.11 # 0.3.11 MIT MIT license com.lihaoyi # ammonite-compiler_2.11.8 # 1.1.0 MIT MIT license com.lihaoyi # ammonite-ops_2.11 # 1.1.0 MIT MIT license com.lihaoyi # ammonite-repl_2.11.8 # 1.1.0 MIT MIT license com.lihaoyi # ammonite-runtime_2.11 # 1.1.0 MIT MIT license com.lihaoyi # ammonite-sshd_2.11.8 # 1.1.0 MIT MIT license com.lihaoyi # ammonite-terminal_2.11 # 1.1.0 MIT MIT license com.lihaoyi # ammonite-util_2.11 # 1.1.0 MIT MIT license com.lihaoyi # ammonite_2.11.8 # 1.1.0 MIT MIT license com.lihaoyi # fastparse-utils_2.11 # 1.0.0 MIT MIT license com.lihaoyi # fastparse_2.11 # 1.0.0 MIT MIT license com.lihaoyi # geny_2.11 # 0.1.2 MIT MIT license com.lihaoyi # scalaparse_2.11 # 1.0.0 Mozilla MPL com.github.rwl # jtransforms # 2.4.0 Mozilla MPL 1.1 org.javassist # javassist # 3.21.0-GA Public Domain Public Domain aopalliance # aopalliance # 1.0 Public Domain Public Domain gov.nist.math # scimark # 2.0 Public Domain Public Domain xmlpull # xmlpull # 1.1.3.1 Public Domain Public Domain xpp3 # xpp3_min # 1.1.4c Public Domain Public domain net.iharder # base64 # 2.3.8 unrecognized ASL org.json4s # json4s-jackson_2.11 # 3.2.11 unrecognized Bouncy Castle Licence org.bouncycastle # bcprov-jdk15on # 1.56 unrecognized Eclipse Public License 1.0 junit # junit # 4.12 unrecognized GNU Lesser General Public Licence com.github.wookietreiber # scala-chart_2.11 # 0.4.2 unrecognized GNU Lesser General Public Licence org.jfree # jcommon # 1.0.21 unrecognized GNU Lesser General Public Licence org.jfree # jfreechart # 1.0.17 unrecognized GNU Public License, v2 org.renjin # compiler # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # datasets # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # gcc-runtime # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # grDevices # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # graphics # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # grid # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # methods # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # parallel # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-appl # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-blas # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-core # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-gnur-runtime # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-lapack # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-math-common # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-nmath # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-script-engine # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # splines # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # stats # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # tcltk # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # tools # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # utils # 0.9.2643 unrecognized Part of R 2.14.2 org.renjin # stats4 # 0.9.2643 unrecognized The JSON License org.json # json # 20140107 unrecognized Unicode/ICU License com.ibm.icu # icu4j # 59.1 unrecognized Unknown License org.apache.xbean # xbean-asm5-shaded # 4.4 unrecognized none specified com.twelvemonkeys.common # common-image # 3.2.1 unrecognized none specified com.twelvemonkeys.common # common-io # 3.2.1 unrecognized none specified com.twelvemonkeys.common # common-lang # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-bmp # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-core # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-icns # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-iff # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-jpeg # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-metadata # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-pcx # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-pdf # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-pict # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-pnm # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-psd # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-sgi # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-tga # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-thumbsdb # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-tiff # 3.2.1 unrecognized none specified commons-beanutils # commons-beanutils # 1.7.0 unrecognized none specified org.apache.zookeeper # zookeeper # 3.4.6 unrecognized none specified org.renjin # libstdcxx # 4.7.4-b18 unrecognized none specified org.renjin.cran # Rcpp # 0.12.13-renjin-15 unrecognized none specified org.renjin.cran # stringi # 1.1.6-renjin-b22 unrecognized none specified oro # oro # 2.0.8 unrecognized none specified regexp # regexp # 1.3","title":"License"},{"location":"license/#dependency-licenses","text":"The various components of DynaML which are enabled using dependencies are subject to their license requirements. The following table lists all the component licences used by DynaML's dependencies. Category License Dependency Notes Apache Apache 2 com.chuusai # shapeless_2.11 # 2.3.3 Apache Apache 2 com.quantifind # sumac_2.11 # 0.3.0 Apache Apache 2 com.quantifind # wisp_2.11 # 0.0.4 Apache Apache 2 com.trueaccord.lenses # lenses_2.11 # 0.4.12 Apache Apache 2 com.trueaccord.scalapb # scalapb-runtime_2.11 # 0.6.2 Apache Apache 2 com.twitter # chill-java # 0.8.0 Apache Apache 2 com.twitter # chill_2.11 # 0.8.0 Apache Apache 2 com.univocity # univocity-parsers # 2.2.1 Apache Apache 2 io.spray # spray-json_2.11 # 1.3.3 Apache Apache 2 joda-time # joda-time # 2.0 Apache Apache 2 net.sf.opencsv # opencsv # 2.3 Apache Apache 2 org.objenesis # objenesis # 2.6 Apache Apache 2 org.roaringbitmap # RoaringBitmap # 0.5.11 Apache Apache 2 org.scalaj # scalaj-http_2.11 # 2.3.0 Apache Apache 2 org.scalanlp # breeze-macros_2.11 # 0.13.2 Apache Apache 2 org.scalanlp # breeze-natives_2.11 # 0.13.2 Apache Apache 2 org.scalanlp # breeze_2.11 # 0.13.2 Apache Apache 2 org.typelevel # macro-compat_2.11 # 1.1.1 Apache Apache 2.0 io.circe # circe-core_2.11 # 0.9.1 Apache Apache 2.0 io.circe # circe-generic_2.11 # 0.9.1 Apache Apache 2.0 io.circe # circe-jawn_2.11 # 0.9.1 Apache Apache 2.0 io.circe # circe-numbers_2.11 # 0.9.1 Apache Apache 2.0 io.circe # circe-parser_2.11 # 0.9.1 Apache Apache 2.0 io.get-coursier # coursier-cache_2.11 # 1.0.0 Apache Apache 2.0 io.get-coursier # coursier_2.11 # 1.0.0 Apache Apache 2.0 net.java.dev.jets3t # jets3t # 0.9.3 Apache Apache 2.0 License com.typesafe.scala-logging # scala-logging_2.11 # 3.9.0 Apache Apache 2.0 License org.apache.spark # spark-catalyst_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-core_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-graphx_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-launcher_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-mllib-local_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-mllib_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-network-common_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-network-shuffle_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-sketch_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-sql_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-streaming_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-tags_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.spark # spark-unsafe_2.11 # 2.2.0 Apache Apache 2.0 License org.apache.sshd # sshd-core # 1.2.0 Apache Apache License commons-httpclient # commons-httpclient # 3.1 Apache Apache License info.folone # poi-scala_2.11 # 0.18 Apache Apache License org.apache.httpcomponents # httpmime # 4.2.5 Apache Apache License 2.0 org.renjin.cran # utf8 # 1.1.3-b8 Apache Apache License 2.0 com.ning # compress-lzf # 1.0.3 Apache Apache License 2.0 io.dropwizard.metrics # metrics-core # 3.1.2 Apache Apache License 2.0 io.dropwizard.metrics # metrics-graphite # 3.1.2 Apache Apache License 2.0 io.dropwizard.metrics # metrics-json # 3.1.2 Apache Apache License 2.0 io.dropwizard.metrics # metrics-jvm # 3.1.2 Apache Apache License 2.0 org.platanios # tensorflow-api_2.11 # 0.2.4 Apache Apache License 2.0 org.platanios # tensorflow-data_2.11 # 0.2.4 Apache Apache License 2.0 org.platanios # tensorflow-jni_2.11 # 0.2.4 Apache Apache License 2.0 org.platanios # tensorflow_2.11 # 0.2.4 Apache Apache License 2.0 org.renjin.cran # RColorBrewer # 1.1-2-b321 Apache Apache License, Version 2.0 com.clearspring.analytics # stream # 2.7.0 Apache Apache License, Version 2.0 com.github.javaparser # javaparser-core # 3.2.5 Apache Apache License, Version 2.0 com.github.tototoshi # scala-csv_2.11 # 1.1.2 Apache Apache License, Version 2.0 com.jamesmurty.utils # java-xmlbuilder # 1.0 Apache Apache License, Version 2.0 com.typesafe # config # 1.3.1 Apache Apache License, Version 2.0 com.typesafe # ssl-config-core_2.11 # 0.2.1 Apache Apache License, Version 2.0 com.typesafe.akka # akka-actor_2.11 # 2.5.3 Apache Apache License, Version 2.0 com.typesafe.akka # akka-stream-testkit_2.11 # 2.4.19 Apache Apache License, Version 2.0 com.typesafe.akka # akka-stream_2.11 # 2.5.3 Apache Apache License, Version 2.0 com.typesafe.akka # akka-testkit_2.11 # 2.5.3 Apache Apache License, Version 2.0 commons-codec # commons-codec # 1.10 Apache Apache License, Version 2.0 commons-collections # commons-collections # 3.2.2 Apache Apache License, Version 2.0 commons-io # commons-io # 2.6 Apache Apache License, Version 2.0 io.netty # netty # 3.9.9.Final Apache Apache License, Version 2.0 io.netty # netty-all # 4.0.43.Final Apache Apache License, Version 2.0 org.apache.commons # commons-compress # 1.15 Apache Apache License, Version 2.0 org.apache.commons # commons-crypto # 1.0.0 Apache Apache License, Version 2.0 org.apache.commons # commons-lang3 # 3.5 Apache Apache License, Version 2.0 org.apache.httpcomponents # httpclient # 4.5.1 Apache Apache License, Version 2.0 org.apache.httpcomponents # httpcore # 4.4.3 Apache Apache License, Version 2.0 org.apache.pdfbox # fontbox # 2.0.9 Apache Apache License, Version 2.0 org.apache.pdfbox # pdfbox # 2.0.9 Apache Apache License, Version 2.0 org.codehaus.jettison # jettison # 1.3.3 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-continuation # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-http # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-io # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-security # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-server # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-servlet # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-util # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-webapp # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty # jetty-xml # 8.1.13.v20130916 Apache Apache Software License - Version 2.0 org.eclipse.jetty.orbit # javax.servlet # 3.0.0.v201112011016 Apache Apache Software License - Version 2.0 org.mortbay.jetty # jetty-util # 6.1.26 Apache Apache-2.0 com.typesafe.akka # akka-http-core_2.11 # 10.0.9 Apache Apache-2.0 com.typesafe.akka # akka-http-spray-json_2.11 # 10.0.9 Apache Apache-2.0 com.typesafe.akka # akka-http-testkit_2.11 # 10.0.9 Apache Apache-2.0 com.typesafe.akka # akka-http_2.11 # 10.0.9 Apache Apache-2.0 com.typesafe.akka # akka-parsing_2.11 # 10.0.9 Apache Apache-2.0 org.json4s # json4s-ast_2.11 # 3.3.0 Apache Apache-2.0 org.json4s # json4s-core_2.11 # 3.3.0 Apache Apache-2.0 org.json4s # json4s-native_2.11 # 3.3.0 Apache Apache-2.0 org.json4s # json4s-scalap_2.11 # 3.3.0 Apache The Apache License, Version 2.0 com.github.ghik # silencer-lib_2.11 # 0.6 Apache The Apache License, Version 2.0 org.spark-project.spark # unused # 1.0.0 Apache The Apache Software License, Version 2.0 ar.com.hjg # pngj # 2.1.0 Apache The Apache Software License, Version 2.0 com.carrotsearch # hppc # 0.6.0 Apache The Apache Software License, Version 2.0 com.drewnoakes # metadata-extractor # 2.8.1 Apache The Apache Software License, Version 2.0 com.fasterxml.jackson.core # jackson-annotations # 2.6.5 Apache The Apache Software License, Version 2.0 com.fasterxml.jackson.core # jackson-core # 2.6.5 Apache The Apache Software License, Version 2.0 com.fasterxml.jackson.core # jackson-databind # 2.6.5 Apache The Apache Software License, Version 2.0 com.fasterxml.jackson.module # jackson-module-paranamer # 2.6.5 Apache The Apache Software License, Version 2.0 com.fasterxml.jackson.module # jackson-module-scala_2.11 # 2.6.5 Apache The Apache Software License, Version 2.0 com.google.code.findbugs # jsr305 # 1.3.9 Apache The Apache Software License, Version 2.0 com.google.code.gson # gson # 2.2.4 Apache The Apache Software License, Version 2.0 com.google.guava # guava # 14.0.1 Apache The Apache Software License, Version 2.0 com.google.inject # guice # 3.0 Apache The Apache Software License, Version 2.0 com.sksamuel.scrimage # scrimage-core_2.11 # 2.1.8 Apache The Apache Software License, Version 2.0 com.sksamuel.scrimage # scrimage-filters_2.11 # 2.1.8 Apache The Apache Software License, Version 2.0 com.sksamuel.scrimage # scrimage-io-extra_2.11 # 2.1.8 Apache The Apache Software License, Version 2.0 commons-beanutils # commons-beanutils-core # 1.8.0 Apache The Apache Software License, Version 2.0 commons-cli # commons-cli # 1.2 Apache The Apache Software License, Version 2.0 commons-configuration # commons-configuration # 1.6 Apache The Apache Software License, Version 2.0 commons-digester # commons-digester # 1.8 Apache The Apache Software License, Version 2.0 commons-lang # commons-lang # 2.6 Apache The Apache Software License, Version 2.0 commons-logging # commons-logging # 1.2 Apache The Apache Software License, Version 2.0 commons-net # commons-net # 2.2 Apache The Apache Software License, Version 2.0 javax.inject # javax.inject # 1 Apache The Apache Software License, Version 2.0 javax.validation # validation-api # 1.1.0.Final Apache The Apache Software License, Version 2.0 log4j # log4j # 1.2.17 Apache The Apache Software License, Version 2.0 mx4j # mx4j # 3.0.2 Apache The Apache Software License, Version 2.0 net.jpountz.lz4 # lz4 # 1.3.0 Apache The Apache Software License, Version 2.0 org.apache.ant # ant # 1.8.3 Apache The Apache Software License, Version 2.0 org.apache.ant # ant-launcher # 1.8.3 Apache The Apache Software License, Version 2.0 org.apache.avro # avro # 1.7.7 Apache The Apache Software License, Version 2.0 org.apache.avro # avro-ipc # 1.7.7 Apache The Apache Software License, Version 2.0 org.apache.avro # avro-mapred # 1.7.7 Apache The Apache Software License, Version 2.0 org.apache.commons # commons-math # 2.2 Apache The Apache Software License, Version 2.0 org.apache.commons # commons-math3 # 3.4.1 Apache The Apache Software License, Version 2.0 org.apache.commons # commons-vfs2 # 2.0 Apache The Apache Software License, Version 2.0 org.apache.curator # curator-client # 2.6.0 Apache The Apache Software License, Version 2.0 org.apache.curator # curator-framework # 2.6.0 Apache The Apache Software License, Version 2.0 org.apache.curator # curator-recipes # 2.6.0 Apache The Apache Software License, Version 2.0 org.apache.directory.api # api-asn1-api # 1.0.0-M20 Apache The Apache Software License, Version 2.0 org.apache.directory.api # api-util # 1.0.0-M20 Apache The Apache Software License, Version 2.0 org.apache.directory.server # apacheds-i18n # 2.0.0-M15 Apache The Apache Software License, Version 2.0 org.apache.directory.server # apacheds-kerberos-codec # 2.0.0-M15 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-annotations # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-auth # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-client # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-common # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-hdfs # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-mapreduce-client-app # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-mapreduce-client-common # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-mapreduce-client-core # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-mapreduce-client-jobclient # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-mapreduce-client-shuffle # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-yarn-api # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-yarn-client # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-yarn-common # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-yarn-server-common # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.hadoop # hadoop-yarn-server-nodemanager # 2.6.5 Apache The Apache Software License, Version 2.0 org.apache.ivy # ivy # 2.4.0 Apache The Apache Software License, Version 2.0 org.apache.maven.scm # maven-scm-api # 1.4 Apache The Apache Software License, Version 2.0 org.apache.maven.scm # maven-scm-provider-svn-commons # 1.4 Apache The Apache Software License, Version 2.0 org.apache.maven.scm # maven-scm-provider-svnexe # 1.4 Apache The Apache Software License, Version 2.0 org.apache.parquet # parquet-column # 1.8.2 Apache The Apache Software License, Version 2.0 org.apache.parquet # parquet-common # 1.8.2 Apache The Apache Software License, Version 2.0 org.apache.parquet # parquet-encoding # 1.8.2 Apache The Apache Software License, Version 2.0 org.apache.parquet # parquet-format # 2.3.1 Apache The Apache Software License, Version 2.0 org.apache.parquet # parquet-hadoop # 1.8.2 Apache The Apache Software License, Version 2.0 org.apache.parquet # parquet-jackson # 1.8.2 Apache The Apache Software License, Version 2.0 org.apache.poi # poi # 3.14 Apache The Apache Software License, Version 2.0 org.apache.poi # poi-ooxml # 3.14 Apache The Apache Software License, Version 2.0 org.apache.poi # poi-ooxml-schemas # 3.14 Apache The Apache Software License, Version 2.0 org.apache.xmlbeans # xmlbeans # 2.6.0 Apache The Apache Software License, Version 2.0 org.codehaus.groovy # groovy # 1.8.9 Apache The Apache Software License, Version 2.0 org.codehaus.jackson # jackson-core-asl # 1.9.13 Apache The Apache Software License, Version 2.0 org.codehaus.jackson # jackson-jaxrs # 1.9.13 Apache The Apache Software License, Version 2.0 org.codehaus.jackson # jackson-mapper-asl # 1.9.13 Apache The Apache Software License, Version 2.0 org.codehaus.jackson # jackson-xc # 1.9.13 Apache The Apache Software License, Version 2.0 org.codehaus.plexus # plexus-utils # 1.5.6 Apache The Apache Software License, Version 2.0 org.fusesource.jansi # jansi # 1.5 Apache The Apache Software License, Version 2.0 org.htrace # htrace-core # 3.0.4 Apache The Apache Software License, Version 2.0 org.la4j # la4j # 0.6.0 Apache The Apache Software License, Version 2.0 org.renjin # renjin-asm # 5.0.4b Apache The Apache Software License, Version 2.0 org.renjin # renjin-guava # 17.0b Apache The Apache Software License, Version 2.0 org.scala-graph # graph-core_2.11 # 1.11.3 Apache The Apache Software License, Version 2.0 org.smurn # jply # 0.2.1 Apache The Apache Software License, Version 2.0 org.sonatype.sisu.inject # cglib # 2.2.1-v20090111 Apache The Apache Software License, Version 2.0 org.tensorflow # proto # 1.9.0-rc1 Apache The Apache Software License, Version 2.0 org.xerial.snappy # snappy-java # 1.1.2.6 Apache The Apache Software License, Version 2.0 stax # stax-api # 1.0.1 Apache The Apache Software License, Version 2.0 xerces # xercesImpl # 2.9.1 Apache The Apache Software License, Version 2.0 xml-apis # xml-apis # 1.3.04 Apache the Apache License, ASL Version 2.0 org.scalactic # scalactic_2.11 # 3.0.4 BSD 3-Clause BSD License com.google.protobuf # protobuf-java # 3.5.1 BSD BSD asm # asm # 3.2 BSD BSD asm # asm-analysis # 3.2 BSD BSD asm # asm-commons # 3.2 BSD BSD asm # asm-tree # 3.2 BSD BSD asm # asm-util # 3.2 BSD BSD com.miglayout # miglayout # 3.7.4 BSD BSD com.thoughtworks.paranamer # paranamer # 2.8 BSD BSD jline # jline # 0.9.94 BSD BSD net.sourceforge.jmatio # jmatio # 1.0 BSD BSD org.scalafx # scalafx_2.11 # 8.0.92-R10 BSD BSD org.scalameta # common_2.11 # 2.0.1 BSD BSD org.scalameta # dialects_2.11 # 2.0.1 BSD BSD org.scalameta # inputs_2.11 # 2.0.1 BSD BSD org.scalameta # io_2.11 # 2.0.1 BSD BSD org.scalameta # langmeta_2.11 # 2.0.1 BSD BSD org.scalameta # parsers_2.11 # 2.0.1 BSD BSD org.scalameta # quasiquotes_2.11 # 2.0.1 BSD BSD org.scalameta # scalameta_2.11 # 2.0.1 BSD BSD org.scalameta # semanticdb_2.11 # 2.0.1 BSD BSD org.scalameta # tokenizers_2.11 # 2.0.1 BSD BSD org.scalameta # tokens_2.11 # 2.0.1 BSD BSD org.scalameta # transversers_2.11 # 2.0.1 BSD BSD org.scalameta # trees_2.11 # 2.0.1 BSD BSD 3 Clause com.github.fommil.netlib # all # 1.1.2 BSD BSD 3 Clause com.github.fommil.netlib # core # 1.1.2 BSD BSD 3 Clause com.github.fommil.netlib # native_ref-java # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # native_system-java # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_ref-linux-armhf # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_ref-linux-i686 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_ref-linux-x86_64 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_ref-osx-x86_64 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_ref-win-i686 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_ref-win-x86_64 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_system-linux-armhf # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_system-linux-i686 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_system-linux-x86_64 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_system-osx-x86_64 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_system-win-i686 # 1.1 BSD BSD 3 Clause com.github.fommil.netlib # netlib-native_system-win-x86_64 # 1.1 BSD BSD 3-Clause com.tinkerpop # frames # 2.5.0 BSD BSD 3-Clause com.tinkerpop # pipes # 2.6.0 BSD BSD 3-Clause com.tinkerpop.blueprints # blueprints-core # 2.6.0 BSD BSD 3-Clause com.tinkerpop.gremlin # gremlin-groovy # 2.5.0 BSD BSD 3-Clause com.tinkerpop.gremlin # gremlin-java # 2.6.0 BSD BSD 3-Clause org.scala-lang # scala-compiler # 2.11.8 BSD BSD 3-Clause org.scala-lang # scala-library # 2.11.8 BSD BSD 3-Clause org.scala-lang # scala-reflect # 2.11.8 BSD BSD 3-Clause org.webjars.bower # vega # 2.6.5 BSD BSD 3-Clause org.webjars.bower # vega-lite # 1.2.0 BSD BSD 3-Clause License org.jpmml # pmml-model # 1.2.15 BSD BSD 3-Clause License org.jpmml # pmml-schema # 1.2.15 BSD BSD 3-clause org.scala-lang.modules # scala-java8-compat_2.11 # 0.7.0 BSD BSD 3-clause org.scala-lang.modules # scala-parser-combinators_2.11 # 1.0.4 BSD BSD 3-clause org.scala-lang.modules # scala-swing_2.11 # 1.0.1 BSD BSD 3-clause org.scala-lang.modules # scala-xml_2.11 # 1.0.6 BSD BSD License antlr # antlr # 2.7.7 BSD BSD License com.github.virtuald # curvesapi # 1.03 BSD BSD style com.thoughtworks.xstream # xstream # 1.4.7 BSD BSD-2 License org.jogamp.gluegen # gluegen-rt # 2.3.2 BSD BSD-2 License org.jogamp.gluegen # gluegen-rt-main # 2.3.2 BSD BSD-2 License org.jogamp.jogl # jogl-all # 2.3.2 BSD BSD-2 License org.jogamp.jogl # jogl-all-main # 2.3.2 BSD BSD-3-Clause org.webjars.bower # d3 # 3.5.17 BSD BSD-like org.scala-lang # jline # 2.11.0-M3 BSD BSD-like org.scala-lang # scala-pickling_2.11 # 0.9.1 BSD BSD-style org.scalaforge # scalax # 0.1 BSD BSD-style org.scalaz # scalaz-concurrent_2.11 # 7.2.16 BSD BSD-style org.scalaz # scalaz-core_2.11 # 7.2.16 BSD BSD-style org.scalaz # scalaz-effect_2.11 # 7.2.16 BSD BSD_3_clause + file LICENSE org.renjin.cran # colorspace # 1.3-2-b41 BSD New BSD License com.diffplug.matsim # matfilerw # 3.0.0 BSD New BSD License com.esotericsoftware # kryo-shaded # 3.0.3 BSD New BSD License com.esotericsoftware # minlog # 1.3.0 BSD New BSD License org.codehaus.janino # commons-compiler # 3.0.0 BSD New BSD License org.codehaus.janino # janino # 3.0.0 BSD New BSD License org.hamcrest # hamcrest-core # 1.3 BSD The (New) BSD License org.jzy3d # jzy3d-api # 1.0.2 BSD The (New) BSD License org.jzy3d # jzy3d-jdt-core # 1.0.2 BSD The BSD 3-Clause License org.fusesource.leveldbjni # leveldbjni-all # 1.8 BSD The BSD License com.adobe.xmp # xmpcore # 5.1.2 BSD The BSD License net.sourceforge.f2j # arpack_combined_all # 0.1 BSD The BSD License org.antlr # antlr4-runtime # 4.5.3 BSD The BSD License org.jline # jline # 3.6.2 BSD The BSD License org.jline # jline-terminal # 3.6.2 BSD The BSD License org.jline # jline-terminal-jna # 3.6.2 BSD The BSD License xmlenc # xmlenc # 0.52 BSD The New BSD License net.sf.py4j # py4j # 0.10.4 CC0 CC0 org.reactivestreams # reactive-streams # 1.0.0 CDDL COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0 javax.activation # activation # 1.1.1 CDDL COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0 javax.xml.stream # stax-api # 1.0-2 GPL CDDL v1.1 / GPL v2 dual license com.sun.codemodel # codemodel # 2.6 GPL CDDL+GPL License org.glassfish.jersey.bundles.repackaged # jersey-guava # 2.22.2 GPL CDDL+GPL License org.glassfish.jersey.containers # jersey-container-servlet # 2.22.2 GPL CDDL+GPL License org.glassfish.jersey.containers # jersey-container-servlet-core # 2.22.2 GPL CDDL+GPL License org.glassfish.jersey.core # jersey-client # 2.22.2 GPL CDDL+GPL License org.glassfish.jersey.core # jersey-common # 2.22.2 GPL CDDL+GPL License org.glassfish.jersey.core # jersey-server # 2.22.2 GPL CDDL+GPL License org.glassfish.jersey.media # jersey-media-jaxb # 2.22.2 GPL GNU General Public License (GPL) org.jfree # jfreesvg # 3.3 GPL GNU General Public License 3 org.openml # apiconnector # 1.0.11 GPL GPL (>= 2) org.renjin.cran # MatrixModels # 0.4-1-b237 GPL GPL (>= 2) org.renjin.cran # SparseM # 1.77-b20 GPL GPL (>= 2) org.renjin.cran # digest # 0.6.15-b10 GPL GPL (>= 2) org.renjin.cran # lattice # 0.20-35-b66 GPL GPL (>= 2) org.renjin.cran # locfit # 1.5-9.1-b345 GPL GPL (>= 2) org.renjin.cran # quantreg # 5.35-b5 GPL [GPL (>= 2) file LICENCE](null) org.renjin.cran # Matrix # 1.2-14-b2 GPL GPL (>= 3) org.renjin.cran # abc # 2.1-b294 GPL GPL (>= 3) org.renjin.cran # abc.data # 1.0-b272 GPL GPL-2 org.renjin.cran # dichromat # 2.0-0-b332 GPL GPL-2 org.renjin.cran # gtable # 0.2.0-b96 GPL [GPL-2 GPL-3](null) org.renjin.cran # MASS # 7.3-49-b11 GPL [GPL-2 GPL-3](null) org.renjin.cran # nnet # 7.3-12-b88 GPL [GPL-2 file LICENSE](null) org.renjin.cran # ggplot2 # 2.2.1-b112 GPL [GPL-2 file LICENSE](null) org.renjin.cran # stringr # 1.3.0-b7 GPL GPL-3 org.renjin.cran # assertthat # 0.2.0-b42 GPL GPL-3 org.renjin.cran # lazyeval # 0.2.1-b19 GPL GPL-3 org.renjin.cran # pillar # 1.2.2-b2 GPL GPL-3 org.renjin.cran # rlang # 0.2.0-b39 GPL GPL2 w/ CPE javax.ws.rs # javax.ws.rs-api # 2.0-m10 GPL GPL2 w/ CPE javax.xml.bind # jaxb-api # 2.2.2 GPL GPLv2+CE javax.mail # mail # 1.4.7 GPL with Classpath Extension CDDL + GPLv2 with classpath exception javax.annotation # javax.annotation-api # 1.2 GPL with Classpath Extension CDDL + GPLv2 with classpath exception javax.servlet # javax.servlet-api # 3.1.0 GPL with Classpath Extension CDDL + GPLv2 with classpath exception org.glassfish.hk2 # hk2-api # 2.4.0-b34 GPL with Classpath Extension CDDL + GPLv2 with classpath exception org.glassfish.hk2 # hk2-locator # 2.4.0-b34 GPL with Classpath Extension CDDL + GPLv2 with classpath exception org.glassfish.hk2 # hk2-utils # 2.4.0-b34 GPL with Classpath Extension CDDL + GPLv2 with classpath exception org.glassfish.hk2 # osgi-resource-locator # 1.0.1 GPL with Classpath Extension CDDL + GPLv2 with classpath exception org.glassfish.hk2.external # aopalliance-repackaged # 2.4.0-b34 GPL with Classpath Extension CDDL + GPLv2 with classpath exception org.glassfish.hk2.external # javax.inject # 2.4.0-b34 LGPL GNU Lesser General Public License ch.qos.logback # logback-classic # 1.2.3 LGPL GNU Lesser General Public License ch.qos.logback # logback-core # 1.2.3 LGPL GNU Lesser General Public License 2.1 net.sf.trove4j # trove4j # 3.0.3 LGPL GNU Lesser General Public License v3.0 com.github.vagmcs # optimus_2.11 # 2.0.0 LGPL LGPL com.github.fommil # jniloader # 1.1 LGPL LGPL, version 2.1 net.java.dev.jna # jna # 4.2.2 MIT MIT co.theasi # plotly_2.11 # 0.1 MIT MIT com.github.julien-truffaut # monocle-core_2.11 # 1.1.0 MIT MIT com.github.julien-truffaut # monocle-macro_2.11 # 1.1.0 MIT MIT com.lihaoyi # fansi_2.11 # 0.2.4 MIT MIT com.lihaoyi # pprint_2.11 # 0.5.2 MIT MIT com.lihaoyi # sourcecode_2.11 # 0.1.4 MIT MIT com.lihaoyi # ujson-jvm-2.11.11_2.11 # 0.6.0 MIT MIT com.lihaoyi # upickle_2.11 # 0.6.0 MIT MIT net.databinder # unfiltered-filter_2.11 # 0.8.3 MIT MIT net.databinder # unfiltered-jetty_2.11 # 0.8.3 MIT MIT net.databinder # unfiltered-util_2.11 # 0.8.3 MIT MIT net.databinder # unfiltered_2.11 # 0.8.3 MIT MIT org.spire-math # jawn-parser_2.11 # 0.11.0 MIT MIT org.typelevel # algebra_2.11 # 0.7.0 MIT MIT org.typelevel # cats-core_2.11 # 1.0.1 MIT MIT org.typelevel # cats-kernel_2.11 # 1.0.1 MIT MIT org.typelevel # cats-macros_2.11 # 1.0.1 MIT MIT org.typelevel # machinist_2.11 # 0.6.2 MIT MIT org.typelevel # spire-macros_2.11 # 0.14.1 MIT MIT org.typelevel # spire_2.11 # 0.14.1 MIT MIT + file LICENSE org.renjin.cran # R6 # 2.2.2-b44 MIT MIT + file LICENSE org.renjin.cran # cli # 1.0.0-b25 MIT MIT + file LICENSE org.renjin.cran # crayon # 1.3.4-b16 MIT MIT + file LICENSE org.renjin.cran # glue # 1.2.0-b22 MIT MIT + file LICENSE org.renjin.cran # magrittr # 1.5-b334 MIT MIT + file LICENSE org.renjin.cran # munsell # 0.4.3-b96 MIT MIT + file LICENSE org.renjin.cran # plyr # 1.8.4-b82 MIT MIT + file LICENSE org.renjin.cran # reshape2 # 1.4.3-b16 MIT MIT + file LICENSE org.renjin.cran # scales # 0.5.0-b32 MIT MIT + file LICENSE org.renjin.cran # tibble # 1.4.2-b7 MIT MIT + file LICENSE org.renjin.cran # viridisLite # 0.3.0-b7 MIT [MIT + file LICENSE Unlimited](null) org.renjin.cran # labeling # 0.3-b313 MIT MIT License com.github.scopt # scopt_2.11 # 3.5.0 MIT MIT License net.razorvine # pyrolite # 4.13 MIT MIT License org.slf4j # jcl-over-slf4j # 1.7.16 MIT MIT License org.slf4j # jul-to-slf4j # 1.7.16 MIT MIT License org.slf4j # slf4j-api # 1.7.25 MIT MIT License org.vegas-viz # vegas-macros_2.11 # 0.3.11 MIT MIT License org.vegas-viz # vegas_2.11 # 0.3.11 MIT MIT license com.lihaoyi # ammonite-compiler_2.11.8 # 1.1.0 MIT MIT license com.lihaoyi # ammonite-ops_2.11 # 1.1.0 MIT MIT license com.lihaoyi # ammonite-repl_2.11.8 # 1.1.0 MIT MIT license com.lihaoyi # ammonite-runtime_2.11 # 1.1.0 MIT MIT license com.lihaoyi # ammonite-sshd_2.11.8 # 1.1.0 MIT MIT license com.lihaoyi # ammonite-terminal_2.11 # 1.1.0 MIT MIT license com.lihaoyi # ammonite-util_2.11 # 1.1.0 MIT MIT license com.lihaoyi # ammonite_2.11.8 # 1.1.0 MIT MIT license com.lihaoyi # fastparse-utils_2.11 # 1.0.0 MIT MIT license com.lihaoyi # fastparse_2.11 # 1.0.0 MIT MIT license com.lihaoyi # geny_2.11 # 0.1.2 MIT MIT license com.lihaoyi # scalaparse_2.11 # 1.0.0 Mozilla MPL com.github.rwl # jtransforms # 2.4.0 Mozilla MPL 1.1 org.javassist # javassist # 3.21.0-GA Public Domain Public Domain aopalliance # aopalliance # 1.0 Public Domain Public Domain gov.nist.math # scimark # 2.0 Public Domain Public Domain xmlpull # xmlpull # 1.1.3.1 Public Domain Public Domain xpp3 # xpp3_min # 1.1.4c Public Domain Public domain net.iharder # base64 # 2.3.8 unrecognized ASL org.json4s # json4s-jackson_2.11 # 3.2.11 unrecognized Bouncy Castle Licence org.bouncycastle # bcprov-jdk15on # 1.56 unrecognized Eclipse Public License 1.0 junit # junit # 4.12 unrecognized GNU Lesser General Public Licence com.github.wookietreiber # scala-chart_2.11 # 0.4.2 unrecognized GNU Lesser General Public Licence org.jfree # jcommon # 1.0.21 unrecognized GNU Lesser General Public Licence org.jfree # jfreechart # 1.0.17 unrecognized GNU Public License, v2 org.renjin # compiler # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # datasets # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # gcc-runtime # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # grDevices # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # graphics # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # grid # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # methods # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # parallel # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-appl # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-blas # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-core # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-gnur-runtime # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-lapack # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-math-common # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-nmath # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # renjin-script-engine # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # splines # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # stats # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # tcltk # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # tools # 0.9.2643 unrecognized GNU Public License, v2 org.renjin # utils # 0.9.2643 unrecognized Part of R 2.14.2 org.renjin # stats4 # 0.9.2643 unrecognized The JSON License org.json # json # 20140107 unrecognized Unicode/ICU License com.ibm.icu # icu4j # 59.1 unrecognized Unknown License org.apache.xbean # xbean-asm5-shaded # 4.4 unrecognized none specified com.twelvemonkeys.common # common-image # 3.2.1 unrecognized none specified com.twelvemonkeys.common # common-io # 3.2.1 unrecognized none specified com.twelvemonkeys.common # common-lang # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-bmp # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-core # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-icns # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-iff # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-jpeg # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-metadata # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-pcx # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-pdf # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-pict # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-pnm # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-psd # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-sgi # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-tga # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-thumbsdb # 3.2.1 unrecognized none specified com.twelvemonkeys.imageio # imageio-tiff # 3.2.1 unrecognized none specified commons-beanutils # commons-beanutils # 1.7.0 unrecognized none specified org.apache.zookeeper # zookeeper # 3.4.6 unrecognized none specified org.renjin # libstdcxx # 4.7.4-b18 unrecognized none specified org.renjin.cran # Rcpp # 0.12.13-renjin-15 unrecognized none specified org.renjin.cran # stringi # 1.1.6-renjin-b22 unrecognized none specified oro # oro # 2.0.8 unrecognized none specified regexp # regexp # 1.3","title":"Dependency Licenses"},{"location":"structure/","text":"Motivation \u00b6 DynaML was born out of the need to have a performant, extensible and easy to use Machine Learning research environment. Scala was a natural choice for these requirements due to its sprawling data science ecosystem (i.e. Apache Spark ), its functional object-oriented duality and its interoperability with the Java Virtual Machine. The DynaML distribution is divided into four principal modules. Modules \u00b6 Core \u00b6 The heart of the DynaML distribution is the dynaml-core module. The core api consists of : Model implementations Parametric Models Stochastic Process Models Optimization solvers Probability distributions/random variables Kernel functions for Non parametric models Data workflows & Pipes \u00b6 The dynaml-pipes module provides an API for creating modular data processing workflows. The pipes module aims to separate model pre-processing tasks such as cleaning data files, replacing missing or corrupt records, applying transformations on data etc. Ability to create arbitrary workflows from scala functions and join them Feature transformations such as wavelet transform, gaussian scaling, auto-encoders etc DynaML REPL \u00b6 The read evaluate print loop (REPL) gives the user the ability to experiment with the data pre-processing and model building process in a mix and match fashion. The DynaML shell is based on the Ammonite project which is an augmented Scala REPL, all the features of the Ammonite REPL are a part of the DynaML REPL. DynaML Examples \u00b6 The module dynaml-examples contains programs which build regression and classification models on various data sets. These examples serve as case studies as well as instructional material to show the capabilities of DynaML in a hands on manner. Click here to get started with the examples. Libraries Used \u00b6 DynaML leverages a number of open source projects and builds on their useful features. Breeze for linear algebra operations with vectors, matrices etc. Gremlin for building graphs in Neural network based models. Spire for creating algebraic entities like Fields, Groups etc. Ammonite for the shell environment. DynaML uses the newly minted Wisp plotting library to generate aesthetic charts of common model validation metrics. There is also support for the JZY3D scientific plotting library.","title":"Structure"},{"location":"structure/#motivation","text":"DynaML was born out of the need to have a performant, extensible and easy to use Machine Learning research environment. Scala was a natural choice for these requirements due to its sprawling data science ecosystem (i.e. Apache Spark ), its functional object-oriented duality and its interoperability with the Java Virtual Machine. The DynaML distribution is divided into four principal modules.","title":"Motivation"},{"location":"structure/#modules","text":"","title":"Modules"},{"location":"structure/#core","text":"The heart of the DynaML distribution is the dynaml-core module. The core api consists of : Model implementations Parametric Models Stochastic Process Models Optimization solvers Probability distributions/random variables Kernel functions for Non parametric models","title":"Core"},{"location":"structure/#data-workflows-pipes","text":"The dynaml-pipes module provides an API for creating modular data processing workflows. The pipes module aims to separate model pre-processing tasks such as cleaning data files, replacing missing or corrupt records, applying transformations on data etc. Ability to create arbitrary workflows from scala functions and join them Feature transformations such as wavelet transform, gaussian scaling, auto-encoders etc","title":"Data workflows &amp; Pipes"},{"location":"structure/#dynaml-repl","text":"The read evaluate print loop (REPL) gives the user the ability to experiment with the data pre-processing and model building process in a mix and match fashion. The DynaML shell is based on the Ammonite project which is an augmented Scala REPL, all the features of the Ammonite REPL are a part of the DynaML REPL.","title":"DynaML REPL"},{"location":"structure/#dynaml-examples","text":"The module dynaml-examples contains programs which build regression and classification models on various data sets. These examples serve as case studies as well as instructional material to show the capabilities of DynaML in a hands on manner. Click here to get started with the examples.","title":"DynaML Examples"},{"location":"structure/#libraries-used","text":"DynaML leverages a number of open source projects and builds on their useful features. Breeze for linear algebra operations with vectors, matrices etc. Gremlin for building graphs in Neural network based models. Spire for creating algebraic entities like Fields, Groups etc. Ammonite for the shell environment. DynaML uses the newly minted Wisp plotting library to generate aesthetic charts of common model validation metrics. There is also support for the JZY3D scientific plotting library.","title":"Libraries Used"},{"location":"supported_features/","text":"Summary \"If you're not sure whether DynaML fits your requirements, this list provides a semi-comprehensive overview of available features.\" Models \u00b6 Model Family Supported Notes Generalized Linear Models Yes Supports regularized least squares based models for regression as well as logistic and probit models for classification. Generalized Least Squares Models Yes - Least Squares Support Vector Machines Yes Contains implementation of dual LS-SVM applied to classification and regression. Gaussian Processes Yes Supports gaussian process inference models for regression and binary classification; the binary classification GP implementation uses the Laplace approximation for posterior mode computation. For regression problems, there are also multi-output and multi-task GP implementations. Student T Processes Yes Supports student T process inference models for regression, there are also multi-output and multi-task STP implementations. Multi-output Matrix T Process Yes _ Skew Gaussian Processes Yes Supports extended skew gaussian process inference models for regression. Feed forward Neural Networks Yes Can build and learn feedforward neural nets of various sizes. Committee/Meta Models Yes Supports creation of gating networks or committee models. Optimizers & Solvers \u00b6 Parametric Solvers \u00b6 Solver Supported Notes Regularized Least Squares Yes Solves the Tikhonov Regularization problem exactly (not suitable for large data sets) Gradient Descent Yes Stochastic and batch gradient descent is implemented. Quasi-Newton BFGS Yes Second order convex optimization (using Hessian). Conjugate Gradient Yes Supports solving of linear systems of type A.x = b A.x = b where A A is a symmetric positive definite matrix. Committee Model Solver Yes Solves any committee based model to calculate member model coefficients or confidences. Back-propagation Yes Least squares based back-propagation with momentum and regularization. Global Optimization Solvers \u00b6 Solver Supported Notes Grid Search Yes Simple search over a grid of configurations. Coupled Simulated Annealing Yes Supports vanilla (simulated annealing) along with variants of CSA such as CSA with variance (temperature) control. ML-II Yes Gradient based optimization of log marginal likelihood in Gaussian Process regression models.","title":"Supported Features"},{"location":"supported_features/#models","text":"Model Family Supported Notes Generalized Linear Models Yes Supports regularized least squares based models for regression as well as logistic and probit models for classification. Generalized Least Squares Models Yes - Least Squares Support Vector Machines Yes Contains implementation of dual LS-SVM applied to classification and regression. Gaussian Processes Yes Supports gaussian process inference models for regression and binary classification; the binary classification GP implementation uses the Laplace approximation for posterior mode computation. For regression problems, there are also multi-output and multi-task GP implementations. Student T Processes Yes Supports student T process inference models for regression, there are also multi-output and multi-task STP implementations. Multi-output Matrix T Process Yes _ Skew Gaussian Processes Yes Supports extended skew gaussian process inference models for regression. Feed forward Neural Networks Yes Can build and learn feedforward neural nets of various sizes. Committee/Meta Models Yes Supports creation of gating networks or committee models.","title":"Models"},{"location":"supported_features/#optimizers-solvers","text":"","title":"Optimizers &amp; Solvers"},{"location":"supported_features/#parametric-solvers","text":"Solver Supported Notes Regularized Least Squares Yes Solves the Tikhonov Regularization problem exactly (not suitable for large data sets) Gradient Descent Yes Stochastic and batch gradient descent is implemented. Quasi-Newton BFGS Yes Second order convex optimization (using Hessian). Conjugate Gradient Yes Supports solving of linear systems of type A.x = b A.x = b where A A is a symmetric positive definite matrix. Committee Model Solver Yes Solves any committee based model to calculate member model coefficients or confidences. Back-propagation Yes Least squares based back-propagation with momentum and regularization.","title":"Parametric Solvers"},{"location":"supported_features/#global-optimization-solvers","text":"Solver Supported Notes Grid Search Yes Simple search over a grid of configurations. Coupled Simulated Annealing Yes Supports vanilla (simulated annealing) along with variants of CSA such as CSA with variance (temperature) control. ML-II Yes Gradient based optimization of log marginal likelihood in Gaussian Process regression models.","title":"Global Optimization Solvers"},{"location":"blog/2016-12-08-getting-started-ml/","text":"Summary Some resources for people interested in contributing code or starting ML research/applications DynaML aims to make a versatile and powerful data analysis and machine learning toolkit available as a shell and runtime environment. As is the case with any tool, relevant background and knowledge is crucial in order to yield its power. This post is intended to be a starting point for online resources which are relevant to DynaML. Machine Learning \u00b6 Machine Learning refers to the ability to make predictions and decisions from data. It is a field that has evolved from the study of pattern recognition and computational learning theory in artificial intelligence. Machine Learning theory and applications lie at the intersection of a number of concepts in the domains of Mathematics, Physics and Computer Science. It is no surprise that machine learning is a rich and deep domain with much intellectual and practical rewards to offer to the persistent and observant student. The following is a non-exhaustive list of educational resources for learning ML. Online Courses \u00b6 Andrew Ng's famous course on Coursera . Intro to Machine Learning at Udacity Machine Learning : MIT Open Course Ware Videos/Youtube \u00b6 Machine Learning Playlist by mathematicalmonk Machine Learning Course by caltech Books \u00b6 Bayesian Reasoning and Machine Learning by David Barber Machine Learning: A Probabilistic Perspective by Kevin P. Murphy Pattern Recognition and Machine Learning by Christopher Bishop Understanding Machine Learning: From Theory to Algorithms by Shai Shalev-Shwartz and Shai Ben-David The Elements of Statistical Learning: Data Mining, Inference, and Prediction by Trevor Hastie, Robert Tibshirani and Jerome Friedman Forums \u00b6 How do I learn machine learning on Quora Blogs \u00b6 Deepmind Blog Cortana Blog Shakir Mohammad's Blog Darren Wilkinson's research blog John's Langford's Blog . Kyle Kastner's Blog Sander Dieleman's Blog Programming Environment: Scala \u00b6 Scala is the implementation language for DynaML, it is a hybrid language which gives the user the ability to leverage functional and object oriented programming styles. Scala code compiles to Java byte code giving Scala complete interoperability with Java, i.e. you can use Java libraries and classes in Scala code. The Java Virtual Machine which executes byte code is the run time for the complete Java ecosystem. This enables Scala, Java, Groovy and Clojure programs to run on a common platform, which is a boon for Machine Learning applications as we can leverage all the libraries in the Java ecosystem. Learning Scala can be a significant investment as the language has a large number of features which require varying levels of skill and practice to master. Some resources for learning Scala are given below. Courses \u00b6 Functional Programming Principles with Scala by Martin Odersky. Functional Program Design in Scala by Martin Odersky. Videos/Youtube \u00b6 Scala tutorials playlist Blogs \u00b6 Haoyi's Programming Blog Scala News Typelevel Blog Codacy Blog","title":"Resources for Beginners"},{"location":"blog/2016-12-08-getting-started-ml/#machine-learning","text":"Machine Learning refers to the ability to make predictions and decisions from data. It is a field that has evolved from the study of pattern recognition and computational learning theory in artificial intelligence. Machine Learning theory and applications lie at the intersection of a number of concepts in the domains of Mathematics, Physics and Computer Science. It is no surprise that machine learning is a rich and deep domain with much intellectual and practical rewards to offer to the persistent and observant student. The following is a non-exhaustive list of educational resources for learning ML.","title":"Machine Learning"},{"location":"blog/2016-12-08-getting-started-ml/#online-courses","text":"Andrew Ng's famous course on Coursera . Intro to Machine Learning at Udacity Machine Learning : MIT Open Course Ware","title":"Online Courses"},{"location":"blog/2016-12-08-getting-started-ml/#videosyoutube","text":"Machine Learning Playlist by mathematicalmonk Machine Learning Course by caltech","title":"Videos/Youtube"},{"location":"blog/2016-12-08-getting-started-ml/#books","text":"Bayesian Reasoning and Machine Learning by David Barber Machine Learning: A Probabilistic Perspective by Kevin P. Murphy Pattern Recognition and Machine Learning by Christopher Bishop Understanding Machine Learning: From Theory to Algorithms by Shai Shalev-Shwartz and Shai Ben-David The Elements of Statistical Learning: Data Mining, Inference, and Prediction by Trevor Hastie, Robert Tibshirani and Jerome Friedman","title":"Books"},{"location":"blog/2016-12-08-getting-started-ml/#forums","text":"How do I learn machine learning on Quora","title":"Forums"},{"location":"blog/2016-12-08-getting-started-ml/#blogs","text":"Deepmind Blog Cortana Blog Shakir Mohammad's Blog Darren Wilkinson's research blog John's Langford's Blog . Kyle Kastner's Blog Sander Dieleman's Blog","title":"Blogs"},{"location":"blog/2016-12-08-getting-started-ml/#programming-environment-scala","text":"Scala is the implementation language for DynaML, it is a hybrid language which gives the user the ability to leverage functional and object oriented programming styles. Scala code compiles to Java byte code giving Scala complete interoperability with Java, i.e. you can use Java libraries and classes in Scala code. The Java Virtual Machine which executes byte code is the run time for the complete Java ecosystem. This enables Scala, Java, Groovy and Clojure programs to run on a common platform, which is a boon for Machine Learning applications as we can leverage all the libraries in the Java ecosystem. Learning Scala can be a significant investment as the language has a large number of features which require varying levels of skill and practice to master. Some resources for learning Scala are given below.","title":"Programming Environment: Scala"},{"location":"blog/2016-12-08-getting-started-ml/#courses","text":"Functional Programming Principles with Scala by Martin Odersky. Functional Program Design in Scala by Martin Odersky.","title":"Courses"},{"location":"blog/2016-12-08-getting-started-ml/#videosyoutube_1","text":"Scala tutorials playlist","title":"Videos/Youtube"},{"location":"blog/2016-12-08-getting-started-ml/#blogs_1","text":"Haoyi's Programming Blog Scala News Typelevel Blog Codacy Blog","title":"Blogs"},{"location":"blog/2016-12-11-dynaml-new-features/","text":"Summarizes some of the pet projects being tackled in DynaML The past year has seen DynaML grow by leaps and bounds, this post hopes to give you an update about what has been achieved and a taste for what is to come. Completed Features \u00b6 A short tour of the enhancements which were completed. January to June \u00b6 Released v1.3.x series with the following new additions Models Regularized Least Squares Logistic and Probit Regression Feed Forward Neural Nets Gaussian Process (GP) classification and NARX based models Least Squares Support Vector Machines (LSSVM) for classification and regression Meta model API, committee models Optimization Primitives Regularized Least Squares Solvers Gradient Descent Committee model solvers Linear Solvers for LSSVM Laplace approximation for GPs Miscellaneous Data Pipes API Migration to scala version 2.11.8 Started work on release 1.4.x series with initial progress Improvements Migrated from Maven to Sbt. Set Ammonite as default REPL. June to December \u00b6 Released v1.4 with the following features. Models The following inference models have been added. LSSVM committees. Multi-output, multi-task Gaussian Process models as reviewed in Lawrence et. al . Student T Processes : single and multi output inspired from Shah, Ghahramani et. al Performance improvement to computation of marginal likelihood and posterior predictive distribution in Gaussian Process models. Posterior predictive distribution outputted by the AbstractGPRegression base class is now changed to MultGaussianRV which is added to the dynaml.probability package. Kernels Added StationaryKernel and LocallyStationaryKernel classes in the kernel APIs, converted RBFKernel , CauchyKernel , RationalQuadraticKernel & LaplacianKernel to subclasses of StationaryKernel Added MLPKernel which implements the maximum likelihood perceptron kernel as shown here . Added co-regionalization kernels which are used in Lawrence et. al to formulate kernels for vector valued functions. In this category the following co-regionalization kernels were implemented. CoRegRBFKernel CoRegCauchyKernel CoRegLaplaceKernel CoRegDiracKernel Improved performance when calculating kernel matrices for composite kernels. Added :* operator to kernels so that one can create separable kernels used in co-regionalization models . Optimization Improved performance of CoupledSimulatedAnnealing , enabled use of 4 variants of Coupled Simulated Annealing , adding the ability to set annealing schedule using so called variance control scheme as outlined in de-Souza, Suykens et. al . Pipes Added Scaler and ReversibleScaler traits to represent transformations which input and output into the same domain set, these traits are extensions of DataPipe . Added Discrete Wavelet Transform based on the Haar wavelet. Started work on v1.4.1 with the following progress Linear Algebra API Partitioned Matrices/Vectors and the following operations Addition, Subtraction Matrix, vector multiplication LU, Cholesky A\\y, A\\Y Probability API Added API end points for representing Measurable Functions of random variables. Model Evaluation Added Matthews Correlation Coefficient calculation to BinaryClassificationMetrics via the matthewsCCByThreshold method Data Pipes API Added Encoder[S,D] traits which are reversible data pipes representing an encoding between types S and D . Miscellaneous Updated ammonite version to 0.8.1 Added support for compiling basic R code with renjin . Run R code in the following manner: val toRDF = csvToRDF ( \"dfWine\" , ';' ) val wine_quality_red = toRDF ( \"data/winequality-red.csv\" ) //Descriptive statistics val commands : String = \"\"\" print(summary(dfWine)) print(\"\\n\") print(str(dfWine)) \"\"\" r ( commands ) //Build Linear Model val modelGLM = rdfToGLM ( \"model\" , \"quality\" , Array ( \"fixed.acidity\" , \"citric.acid\" , \"chlorides\" )) modelGLM ( \"dfWine\" ) //Print goodness of fit r ( \"print(summary(model))\" ) Ongoing Work \u00b6 Some projects being worked on right now are. Bayesian optimization using Gaussian Process models. Implementation of Neural Networks using the akka actor API. Implementation of kernels which can be decomposed on data dimensions k((x_1, x_2), (y_1, y_2)) = k_1(x_1, y_1) + k_2(x_2, y_2) k((x_1, x_2), (y_1, y_2)) = k_1(x_1, y_1) + k_2(x_2, y_2)","title":"State of DynaML 2016"},{"location":"blog/2016-12-11-dynaml-new-features/#completed-features","text":"A short tour of the enhancements which were completed.","title":"Completed Features"},{"location":"blog/2016-12-11-dynaml-new-features/#january-to-june","text":"Released v1.3.x series with the following new additions Models Regularized Least Squares Logistic and Probit Regression Feed Forward Neural Nets Gaussian Process (GP) classification and NARX based models Least Squares Support Vector Machines (LSSVM) for classification and regression Meta model API, committee models Optimization Primitives Regularized Least Squares Solvers Gradient Descent Committee model solvers Linear Solvers for LSSVM Laplace approximation for GPs Miscellaneous Data Pipes API Migration to scala version 2.11.8 Started work on release 1.4.x series with initial progress Improvements Migrated from Maven to Sbt. Set Ammonite as default REPL.","title":"January to June"},{"location":"blog/2016-12-11-dynaml-new-features/#june-to-december","text":"Released v1.4 with the following features. Models The following inference models have been added. LSSVM committees. Multi-output, multi-task Gaussian Process models as reviewed in Lawrence et. al . Student T Processes : single and multi output inspired from Shah, Ghahramani et. al Performance improvement to computation of marginal likelihood and posterior predictive distribution in Gaussian Process models. Posterior predictive distribution outputted by the AbstractGPRegression base class is now changed to MultGaussianRV which is added to the dynaml.probability package. Kernels Added StationaryKernel and LocallyStationaryKernel classes in the kernel APIs, converted RBFKernel , CauchyKernel , RationalQuadraticKernel & LaplacianKernel to subclasses of StationaryKernel Added MLPKernel which implements the maximum likelihood perceptron kernel as shown here . Added co-regionalization kernels which are used in Lawrence et. al to formulate kernels for vector valued functions. In this category the following co-regionalization kernels were implemented. CoRegRBFKernel CoRegCauchyKernel CoRegLaplaceKernel CoRegDiracKernel Improved performance when calculating kernel matrices for composite kernels. Added :* operator to kernels so that one can create separable kernels used in co-regionalization models . Optimization Improved performance of CoupledSimulatedAnnealing , enabled use of 4 variants of Coupled Simulated Annealing , adding the ability to set annealing schedule using so called variance control scheme as outlined in de-Souza, Suykens et. al . Pipes Added Scaler and ReversibleScaler traits to represent transformations which input and output into the same domain set, these traits are extensions of DataPipe . Added Discrete Wavelet Transform based on the Haar wavelet. Started work on v1.4.1 with the following progress Linear Algebra API Partitioned Matrices/Vectors and the following operations Addition, Subtraction Matrix, vector multiplication LU, Cholesky A\\y, A\\Y Probability API Added API end points for representing Measurable Functions of random variables. Model Evaluation Added Matthews Correlation Coefficient calculation to BinaryClassificationMetrics via the matthewsCCByThreshold method Data Pipes API Added Encoder[S,D] traits which are reversible data pipes representing an encoding between types S and D . Miscellaneous Updated ammonite version to 0.8.1 Added support for compiling basic R code with renjin . Run R code in the following manner: val toRDF = csvToRDF ( \"dfWine\" , ';' ) val wine_quality_red = toRDF ( \"data/winequality-red.csv\" ) //Descriptive statistics val commands : String = \"\"\" print(summary(dfWine)) print(\"\\n\") print(str(dfWine)) \"\"\" r ( commands ) //Build Linear Model val modelGLM = rdfToGLM ( \"model\" , \"quality\" , Array ( \"fixed.acidity\" , \"citric.acid\" , \"chlorides\" )) modelGLM ( \"dfWine\" ) //Print goodness of fit r ( \"print(summary(model))\" )","title":"June to December"},{"location":"blog/2016-12-11-dynaml-new-features/#ongoing-work","text":"Some projects being worked on right now are. Bayesian optimization using Gaussian Process models. Implementation of Neural Networks using the akka actor API. Implementation of kernels which can be decomposed on data dimensions k((x_1, x_2), (y_1, y_2)) = k_1(x_1, y_1) + k_2(x_2, y_2) k((x_1, x_2), (y_1, y_2)) = k_1(x_1, y_1) + k_2(x_2, y_2)","title":"Ongoing Work"},{"location":"core/core_ann/","text":"Warning This API is deprecated since v1.4.2, users are advised to use the new neural stack API . Feed-forward Network \u00b6 To create a feedforward network we need three entities. The training data (type parameter D ) A data pipe which transforms the original data into a data structure that understood by the FeedForwardNetwork The network architecture (i.e. the network as a graph object) Network graph \u00b6 A standard feedforward network can be created by first initializing the network architecture/graph. val gr = FFNeuralGraph ( num_inputs = 3 , num_outputs = 1 , hidden_layers = 1 , List ( \"logsig\" , \"linear\" ), List ( 5 )) This creates a neural network graph with one hidden layer, 3 input nodes, 1 output node and assigns sigmoid activation in the hidden layer. It also creates 5 neurons in the hidden layer. Next we create a data transform pipe which converts instances of the data input-output patterns to (DenseVector[Double], DenseVector[Double]) , this is required in many data processing applications where the data structure storing the training data is not a breeze vector. Lets say we have data in the form trainingdata: Stream[(DenseVector[Double], Double)] , i.e. we have input features as breeze vectors and scalar output values which help the network learn an unknown function. We can write the transform as. val transform = DataPipe ( ( d : Stream [( DenseVector [ Double ] , Double )]) => d . map ( el => ( el . _1 , DenseVector ( el . _2 ))) ) Model Building \u00b6 We are now in a position to initialize a feed forward neural network model. val model = new FeedForwardNetwork [ Stream [( DenseVector [ Double ] , Double )] ]( trainingdata , gr , transform ) Here the variable trainingdata represents the training input output pairs, which must conform to the type argument given in square brackets (i.e. Stream[(DenseVector[Double], Double)] ). Training the model using back propagation can be done as follows, you can set custom values for the backpropagation parameters like the learning rate, momentum factor, mini batch fraction, regularization and number of learning iterations. model . setLearningRate ( 0.09 ) . setMaxIterations ( 100 ) . setBatchFraction ( 0.85 ) . setMomentum ( 0.45 ) . setRegParam ( 0.0001 ) . learn () The trained model can now be used for prediction, by using either the predict() method or the feedForward() value member both of which are members of FeedForwardNetwork (refer to the api docs for more details). val pattern = DenseVector ( 2.0 , 3.5 , 2.5 ) val prediction = model . predict ( pattern ) Sparse Autoencoder \u00b6 Sparse autoencoders are a feedforward architecture that are useful for unsupervised feature learning. They learn a compressed (or expanded) vector representation of the original data features. This process is known by various terms like feature learning , feature engineering , representation learning etc. Autoencoders are amongst several models used for feature learning. Other notable examples include convolutional neural networks (CNN), principal component analysis (PCA), Singular Value Decomposition (PCA) (a variant of PCA), Discrete Wavelet Transform (DWT), etc. Creation \u00b6 Autoencoders can be created using the AutoEncoder class. Its constructor has the following arguments. import io.github.mandar2812.dynaml.models.neuralnets._ import io.github.mandar2812.dynaml.models.neuralnets.TransferFunctions._ import io.github.mandar2812.dynaml.optimization.BackPropagation //Cast the training data as a stream of (x,x), //where x are the DenseVector of features val trainingData : Stream [( DenseVector [ Double ] , DenseVector [ Double ])] = ... val testData = ... val enc = new AutoEncoder ( inDim = trainingData . head . _1 . length , outDim = 4 , acts = List ( SIGMOID , LIN )) Training \u00b6 The training algorithm used is a modified version of standard back-propagation. The objective function can be seen as an addition of three terms. \\begin{align} \\mathcal{J}(\\mathbf{W}, \\mathbf{X}; \\lambda, \\rho) &= \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) + \\lambda \\mathcal{R}(\\mathbf{W}) + KL(\\hat{\\rho}\\ ||\\ \\rho) \\\\ KL(\\hat{\\rho}\\ ||\\ \\rho) &= \\sum_{i = 1}^{n_h} \\rho log(\\frac{\\rho}{\\hat{\\rho}_i}) + (1 - \\rho) log(\\frac{1-\\rho}{1-\\hat{\\rho}_i}) \\\\ \\hat{\\rho}_i &= \\frac{1}{m} \\sum_{j = 1}^{N} a_{i}(x_j) \\end{align} \\begin{align} \\mathcal{J}(\\mathbf{W}, \\mathbf{X}; \\lambda, \\rho) &= \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) + \\lambda \\mathcal{R}(\\mathbf{W}) + KL(\\hat{\\rho}\\ ||\\ \\rho) \\\\ KL(\\hat{\\rho}\\ ||\\ \\rho) &= \\sum_{i = 1}^{n_h} \\rho log(\\frac{\\rho}{\\hat{\\rho}_i}) + (1 - \\rho) log(\\frac{1-\\rho}{1-\\hat{\\rho}_i}) \\\\ \\hat{\\rho}_i &= \\frac{1}{m} \\sum_{j = 1}^{N} a_{i}(x_j) \\end{align} \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) is the least squares loss. \\mathcal{R}(\\mathbf{W}) \\mathcal{R}(\\mathbf{W}) is the regularization penalty, with parameter \\lambda \\lambda . KL(\\hat{\\rho} \\| \\rho) KL(\\hat{\\rho} \\| \\rho) is the Kullback Leibler divergence, between the average activation (over all data instances x \\in \\mathbf{X} x \\in \\mathbf{X} ) of each hidden node and a specified value \\rho \\in [0,1] \\rho \\in [0,1] which is also known as the sparsity weight . //Set sparsity parameter for back propagation BackPropagation . rho = 0.5 enc . optimizer . setRegParam ( 0.0 ) . setStepSize ( 1.5 ) . setNumIterations ( 200 ) . setMomentum ( 0.4 ) . setSparsityWeight ( 0.9 ) enc . learn ( trainingData . toStream ) val metrics = new MultiRegressionMetrics ( testData . map ( c => ( enc . i ( enc ( c . _1 )), c . _2 )). toList , testData . length )","title":"Old Neural Net API"},{"location":"core/core_ann/#feed-forward-network","text":"To create a feedforward network we need three entities. The training data (type parameter D ) A data pipe which transforms the original data into a data structure that understood by the FeedForwardNetwork The network architecture (i.e. the network as a graph object)","title":"Feed-forward Network"},{"location":"core/core_ann/#network-graph","text":"A standard feedforward network can be created by first initializing the network architecture/graph. val gr = FFNeuralGraph ( num_inputs = 3 , num_outputs = 1 , hidden_layers = 1 , List ( \"logsig\" , \"linear\" ), List ( 5 )) This creates a neural network graph with one hidden layer, 3 input nodes, 1 output node and assigns sigmoid activation in the hidden layer. It also creates 5 neurons in the hidden layer. Next we create a data transform pipe which converts instances of the data input-output patterns to (DenseVector[Double], DenseVector[Double]) , this is required in many data processing applications where the data structure storing the training data is not a breeze vector. Lets say we have data in the form trainingdata: Stream[(DenseVector[Double], Double)] , i.e. we have input features as breeze vectors and scalar output values which help the network learn an unknown function. We can write the transform as. val transform = DataPipe ( ( d : Stream [( DenseVector [ Double ] , Double )]) => d . map ( el => ( el . _1 , DenseVector ( el . _2 ))) )","title":"Network graph"},{"location":"core/core_ann/#model-building","text":"We are now in a position to initialize a feed forward neural network model. val model = new FeedForwardNetwork [ Stream [( DenseVector [ Double ] , Double )] ]( trainingdata , gr , transform ) Here the variable trainingdata represents the training input output pairs, which must conform to the type argument given in square brackets (i.e. Stream[(DenseVector[Double], Double)] ). Training the model using back propagation can be done as follows, you can set custom values for the backpropagation parameters like the learning rate, momentum factor, mini batch fraction, regularization and number of learning iterations. model . setLearningRate ( 0.09 ) . setMaxIterations ( 100 ) . setBatchFraction ( 0.85 ) . setMomentum ( 0.45 ) . setRegParam ( 0.0001 ) . learn () The trained model can now be used for prediction, by using either the predict() method or the feedForward() value member both of which are members of FeedForwardNetwork (refer to the api docs for more details). val pattern = DenseVector ( 2.0 , 3.5 , 2.5 ) val prediction = model . predict ( pattern )","title":"Model Building"},{"location":"core/core_ann/#sparse-autoencoder","text":"Sparse autoencoders are a feedforward architecture that are useful for unsupervised feature learning. They learn a compressed (or expanded) vector representation of the original data features. This process is known by various terms like feature learning , feature engineering , representation learning etc. Autoencoders are amongst several models used for feature learning. Other notable examples include convolutional neural networks (CNN), principal component analysis (PCA), Singular Value Decomposition (PCA) (a variant of PCA), Discrete Wavelet Transform (DWT), etc.","title":"Sparse Autoencoder"},{"location":"core/core_ann/#creation","text":"Autoencoders can be created using the AutoEncoder class. Its constructor has the following arguments. import io.github.mandar2812.dynaml.models.neuralnets._ import io.github.mandar2812.dynaml.models.neuralnets.TransferFunctions._ import io.github.mandar2812.dynaml.optimization.BackPropagation //Cast the training data as a stream of (x,x), //where x are the DenseVector of features val trainingData : Stream [( DenseVector [ Double ] , DenseVector [ Double ])] = ... val testData = ... val enc = new AutoEncoder ( inDim = trainingData . head . _1 . length , outDim = 4 , acts = List ( SIGMOID , LIN ))","title":"Creation"},{"location":"core/core_ann/#training","text":"The training algorithm used is a modified version of standard back-propagation. The objective function can be seen as an addition of three terms. \\begin{align} \\mathcal{J}(\\mathbf{W}, \\mathbf{X}; \\lambda, \\rho) &= \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) + \\lambda \\mathcal{R}(\\mathbf{W}) + KL(\\hat{\\rho}\\ ||\\ \\rho) \\\\ KL(\\hat{\\rho}\\ ||\\ \\rho) &= \\sum_{i = 1}^{n_h} \\rho log(\\frac{\\rho}{\\hat{\\rho}_i}) + (1 - \\rho) log(\\frac{1-\\rho}{1-\\hat{\\rho}_i}) \\\\ \\hat{\\rho}_i &= \\frac{1}{m} \\sum_{j = 1}^{N} a_{i}(x_j) \\end{align} \\begin{align} \\mathcal{J}(\\mathbf{W}, \\mathbf{X}; \\lambda, \\rho) &= \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) + \\lambda \\mathcal{R}(\\mathbf{W}) + KL(\\hat{\\rho}\\ ||\\ \\rho) \\\\ KL(\\hat{\\rho}\\ ||\\ \\rho) &= \\sum_{i = 1}^{n_h} \\rho log(\\frac{\\rho}{\\hat{\\rho}_i}) + (1 - \\rho) log(\\frac{1-\\rho}{1-\\hat{\\rho}_i}) \\\\ \\hat{\\rho}_i &= \\frac{1}{m} \\sum_{j = 1}^{N} a_{i}(x_j) \\end{align} \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) \\mathcal{L}(\\mathbf{W}, \\mathbf{X}) is the least squares loss. \\mathcal{R}(\\mathbf{W}) \\mathcal{R}(\\mathbf{W}) is the regularization penalty, with parameter \\lambda \\lambda . KL(\\hat{\\rho} \\| \\rho) KL(\\hat{\\rho} \\| \\rho) is the Kullback Leibler divergence, between the average activation (over all data instances x \\in \\mathbf{X} x \\in \\mathbf{X} ) of each hidden node and a specified value \\rho \\in [0,1] \\rho \\in [0,1] which is also known as the sparsity weight . //Set sparsity parameter for back propagation BackPropagation . rho = 0.5 enc . optimizer . setRegParam ( 0.0 ) . setStepSize ( 1.5 ) . setNumIterations ( 200 ) . setMomentum ( 0.4 ) . setSparsityWeight ( 0.9 ) enc . learn ( trainingData . toStream ) val metrics = new MultiRegressionMetrics ( testData . map ( c => ( enc . i ( enc ( c . _1 )), c . _2 )). toList , testData . length )","title":"Training"},{"location":"core/core_ann_new/","text":"Warning This API is deprecated since v1.5.2, users are advised to use the new Tensorflow API . Summary In v1.4.2, the neural stack API was introduced. It defines some primitives for modular construction of neural networks. The main idioms will be computational layers & stacks. The neural stack API extends these abstract skeletons by defining two kinds of primitives. Computational layers: Defining how inputs are propagated forward; NeuralLayer Activation functions: Activation [ I ] Computational stacks: composed of a number of layers; GenericNeuralStack Note The classes NeuralLayer and GenericNeuralStack define layers and stacks in an abstract manner, meaning that the parameters could be in principle of any type. The key point to understand is that once a layer or stack is defined, it is immutable i.e. the parameters defining its forward computation can't be changed. The API rather provides factory objects which can spawn a particular layer or stack with any parameter assignments. Activation Functions \u00b6 Activation functions are implemented using the Activation [ I ] object, its apply method requires two arguments. Implementation of the activation Implementation of the derivative of the activation. //Define forward mapping val actFunc : ( I ) => I = _ //Define derivative of forward mapping val gradAct : ( I ) => I = _ val act = Activation ( actFunc , gradAct ) The dynaml.models.neuralnets package also contains implementation of the following activations. Sigmoid g(x) = \\frac{1}{1 + exp(-x)} g(x) = \\frac{1}{1 + exp(-x)} val act = VectorSigmoid Tanh g(x) = tanh(x) g(x) = tanh(x) val act = VectorTansig Linear g(x) = x g(x) = x val act = VectorLinear Rectified Linear g(x) = \\begin{cases} x & x \\geq 0\\\\0 & else\\end{cases} g(x) = \\begin{cases} x & x \\geq 0\\\\0 & else\\end{cases} val act = VectorRecLin Computational Layers \u00b6 Computational layers are the most basic unit of neural networks. They define transformations of their inputs and with that define the forward data flow. Every computational layer generally has a set of parameters describing how this transformation is going to be calculated given the inputs. In DynaML, the central component of the NeuralLayer [ Params , Input , Output ] trait is a MetaPipe [ Params , Input , Output ] ( higher order pipe ) instance. Creating Layers. \u00b6 Creating an immutable computational layer can be done using the NeuralLayer object. import scala.math._ val compute = MetaPipe ( ( params : Double ) => ( x : Double ) => 2 d * Pi * params * x ) val act = Activation ( ( x : Double ) => tanh ( x ), ( x : Double ) => tanh ( x )/( sinh ( x )* cosh ( x ))) val layer = NeuralLayer ( compute , act )( 0.5 ) Vector feed forward layers A common layer is the feed forward vector to vector layer which is given by. $$ \\mathbf{h} = \\sigma(\\mathbf{W} \\mathbf{x} + \\mathbf{b}) $$ Layer Factories. \u00b6 Since the computation and activation are the only two relevant inputs required to spawn any computational layer, the NeuralLayerFactory [ Params , Input , Output ] class is the factory for creating layers on the fly. Layer factories are data pipes which take the layer parameters as input and create computational layers on demand. A layer factory can be created as follows. val fact = NeuralLayerFactory ( compute , act ) val layer1 = fact ( 0.25 ) Vector layer factory Vector layers can be created using the Vec2VecLayerFactory val layerFactory = new Vec2VecLayerFactory ( VectorTansig )( inDim = 4 , outDim = 5 ) Neural Stacks \u00b6 A neural stack is a sequence of computational layers. Every layer represents some computation, so the neural stack is nothing but a sequence of computations or forward data flow. The top level class for neural stacks is GenericNeuralStack . Extending the base class there are two stack implementations. Eagerly evaluated stack: Layers are spawned as soon as the stack is created. val layers : Seq [ NeuralLayer [ P , I , I ]] = _ //Variable argument apply function //so the elements of the sequence //must be enumerated. val stack = NeuralStack ( layers :_ * ) Lazy stack: Layers are spawned only as needed, but once created they are memoized . val layers_func : ( Int ) => NeuralLayer [ P , I , I ] = _ val stack = LazyNeuralStack ( layers_func , num_layers = 4 ) Stack Factories \u00b6 Stack factories like layer factories are pipe lines, which take as input a sequence of layer parameters and return a neural stack of the spawned layers. val layerFactories : Seq [ NeuralLayerFactory [ P , I , I ]] = _ //Create a stack factory from a sequence of layer factories val stackFactory = NeuralStackFactory ( layerFactories :_ * ) //Create a stack factory that creates //feed forward neural stacks that take as inputs //breeze vectors. //Input, Hidden, Output val num_units_by_layer = Seq ( 5 , 8 , 3 ) val acts = Seq ( VectorSigmoid , VectorTansig ) val breezeStackFactory = NeuralStackFactory ( num_units_by_layer )( acts )","title":"Neural Stack API"},{"location":"core/core_ann_new/#activation-functions","text":"Activation functions are implemented using the Activation [ I ] object, its apply method requires two arguments. Implementation of the activation Implementation of the derivative of the activation. //Define forward mapping val actFunc : ( I ) => I = _ //Define derivative of forward mapping val gradAct : ( I ) => I = _ val act = Activation ( actFunc , gradAct ) The dynaml.models.neuralnets package also contains implementation of the following activations. Sigmoid g(x) = \\frac{1}{1 + exp(-x)} g(x) = \\frac{1}{1 + exp(-x)} val act = VectorSigmoid Tanh g(x) = tanh(x) g(x) = tanh(x) val act = VectorTansig Linear g(x) = x g(x) = x val act = VectorLinear Rectified Linear g(x) = \\begin{cases} x & x \\geq 0\\\\0 & else\\end{cases} g(x) = \\begin{cases} x & x \\geq 0\\\\0 & else\\end{cases} val act = VectorRecLin","title":"Activation Functions"},{"location":"core/core_ann_new/#computational-layers","text":"Computational layers are the most basic unit of neural networks. They define transformations of their inputs and with that define the forward data flow. Every computational layer generally has a set of parameters describing how this transformation is going to be calculated given the inputs. In DynaML, the central component of the NeuralLayer [ Params , Input , Output ] trait is a MetaPipe [ Params , Input , Output ] ( higher order pipe ) instance.","title":"Computational Layers"},{"location":"core/core_ann_new/#creating-layers","text":"Creating an immutable computational layer can be done using the NeuralLayer object. import scala.math._ val compute = MetaPipe ( ( params : Double ) => ( x : Double ) => 2 d * Pi * params * x ) val act = Activation ( ( x : Double ) => tanh ( x ), ( x : Double ) => tanh ( x )/( sinh ( x )* cosh ( x ))) val layer = NeuralLayer ( compute , act )( 0.5 ) Vector feed forward layers A common layer is the feed forward vector to vector layer which is given by. $$ \\mathbf{h} = \\sigma(\\mathbf{W} \\mathbf{x} + \\mathbf{b}) $$","title":"Creating Layers."},{"location":"core/core_ann_new/#layer-factories","text":"Since the computation and activation are the only two relevant inputs required to spawn any computational layer, the NeuralLayerFactory [ Params , Input , Output ] class is the factory for creating layers on the fly. Layer factories are data pipes which take the layer parameters as input and create computational layers on demand. A layer factory can be created as follows. val fact = NeuralLayerFactory ( compute , act ) val layer1 = fact ( 0.25 ) Vector layer factory Vector layers can be created using the Vec2VecLayerFactory val layerFactory = new Vec2VecLayerFactory ( VectorTansig )( inDim = 4 , outDim = 5 )","title":"Layer Factories."},{"location":"core/core_ann_new/#neural-stacks","text":"A neural stack is a sequence of computational layers. Every layer represents some computation, so the neural stack is nothing but a sequence of computations or forward data flow. The top level class for neural stacks is GenericNeuralStack . Extending the base class there are two stack implementations. Eagerly evaluated stack: Layers are spawned as soon as the stack is created. val layers : Seq [ NeuralLayer [ P , I , I ]] = _ //Variable argument apply function //so the elements of the sequence //must be enumerated. val stack = NeuralStack ( layers :_ * ) Lazy stack: Layers are spawned only as needed, but once created they are memoized . val layers_func : ( Int ) => NeuralLayer [ P , I , I ] = _ val stack = LazyNeuralStack ( layers_func , num_layers = 4 )","title":"Neural Stacks"},{"location":"core/core_ann_new/#stack-factories","text":"Stack factories like layer factories are pipe lines, which take as input a sequence of layer parameters and return a neural stack of the spawned layers. val layerFactories : Seq [ NeuralLayerFactory [ P , I , I ]] = _ //Create a stack factory from a sequence of layer factories val stackFactory = NeuralStackFactory ( layerFactories :_ * ) //Create a stack factory that creates //feed forward neural stacks that take as inputs //breeze vectors. //Input, Hidden, Output val num_units_by_layer = Seq ( 5 , 8 , 3 ) val acts = Seq ( VectorSigmoid , VectorTansig ) val breezeStackFactory = NeuralStackFactory ( num_units_by_layer )( acts )","title":"Stack Factories"},{"location":"core/core_dtf/","text":"Summary The dtf object can be used to create and transform tensors. To use DynaML's tensorflow API, import it in your code/script/DynaML shell session. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ Creating Tensors. \u00b6 Creating tensors using the dtf object is easy, the user needs to provide a scala collection containing the the data, the shape and data-type of the tensor. There is more than one way to instantiate a tensor. Enumeration of Values \u00b6 import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Create a float tensor val tensor_float = dtf . tensor_from [ Float ]( FLOAT32 , Shape ( 2 , 2 ))( 1 f , 2 f , 3 f , 4 f ) //Prints out a summary of the values in tensor1 tensor_float . summarize () val tensor_double = dtf . tensor_from [ Double ]( FLOAT64 , Shape ( 2 , 2 ))( 1.0 , 2.0 , 3.0 , 4.0 ) tensor_double . summarize () From a Scala Sequence \u00b6 import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val float_seq = Seq ( 1 f , 2 f , 3 f , 4 f ) val double_seq = Seq ( 1.0 , 2.0 , 3.0 , 4.0 ) //Specify data type as a string, and enumerate the shape val tensor_float = dtf . tensor_from [ Float ]( \"FLOAT32\" , 2 , 2 )( float_seq ) //Prints out a summary of the values in tensor1 tensor_float . summarize () val tensor_double = dtf . tensor_from [ Double ]( \"FLOAT64\" , 2 , 2 )( double_seq ) tensor_double . summarize () From an Array of Bytes. \u00b6 When dealing with binary data formats, such as images and other binary numerical formats, it is useful to be able to instantiate tensors from buffers of raw bytes. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val byte_buffer : Array [ Byte ] = _ val shape : Shape = _ val byte_tensor = dtf . tensor_from_buffer ( INT32 , shape )( byte_buffer ) Apart from these functions, there are. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Double tensor val t = dtf . tensor_f64 ( 2 , 2 )( 1.0 , 2.0 , 3.0 , 4.0 ) //32 bit Integer tensor val t_int = dtf . tensor_i32 ( 2 , 3 )( 1 , 2 , 3 , 4 , 5 , 6 ) //Fill a (3, 2, 5) tensor, with the value 1. val t_fill = dtf . fill ( FLOAT32 , 3 , 2 , 5 )( 1 f ) Random Tensors \u00b6 It is also possible to create tensors whose elements are independent and identically distributed , by using the DynaML probability API. import breeze.stats.distributions._ import io.github.mandar2812.dynaml.probability._ import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val rv = RandomVariable ( new LogNormal ( 0.0 , 1.5 )) val random_tensor = dtf . random ( FLOAT64 , 3 , 5 , 2 )( rv ) Operations on Tensors \u00b6 Stack \u00b6 DynaML > val random_tensor1 = dtf . random ( FLOAT64 , 2 , 3 )( rv ) random_tensor1 : Tensor = FLOAT64 [ 2 , 3 ] DynaML > val random_tensor2 = dtf . random ( FLOAT64 , 2 , 3 )( rv ) random_tensor2 : Tensor = FLOAT64 [ 2 , 3 ] DynaML > val t = dtf . stack ( Seq ( random_tensor1 , random_tensor2 ), axis = 1 ) t : Tensor = FLOAT64 [ 2 , 2 , 3 ] DynaML > val t0 = dtf . stack ( Seq ( random_tensor1 , random_tensor2 ), axis = 0 ) t0 : Tensor = FLOAT64 [ 2 , 2 , 3 ] DynaML > random_tensor1 . summarize ( 100 , false ) res18 : String = \"\"\"FLOAT64[2, 3] [[0.3501699906342581, 0.2900664662305818, 0.42806656451314345], [0.3066005571688877, 1.3931959054429162, 0.6366232162759474]]\"\"\" DynaML > random_tensor2 . summarize ( 100 , false ) res19 : String = \"\"\"FLOAT64[2, 3] [[0.21565105620570899, 0.5267519630011802, 6.817248106561024], [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]\"\"\" DynaML > t . summarize ( 100 , false ) res16 : String = \"\"\"FLOAT64[2, 2, 3] [[[0.3501699906342581, 0.2900664662305818, 0.42806656451314345], [0.21565105620570899, 0.5267519630011802, 6.817248106561024]], [[0.3066005571688877, 1.3931959054429162, 0.6366232162759474], [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]]\"\"\" DynaML > t0 . summarize ( 100 , false ) res17 : String = \"\"\"FLOAT64[2, 2, 3] [[[0.3501699906342581, 0.2900664662305818, 0.42806656451314345], [0.3066005571688877, 1.3931959054429162, 0.6366232162759474]], [[0.21565105620570899, 0.5267519630011802, 6.817248106561024], [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]]\"\"\" Concatenate \u00b6 DynaML > val t = dtf . concatenate ( Seq ( random_tensor1 , random_tensor2 ), axis = 0 ) t : Tensor = FLOAT64 [ 4 , 3 ] DynaML > val t1 = dtf . concatenate ( Seq ( random_tensor1 , random_tensor2 ), axis = 1 ) t1 : Tensor = FLOAT64 [ 2 , 6 ] DynaML > t . summarize ( 100 , false ) res28 : String = \"\"\"FLOAT64[4, 3] [[0.3501699906342581, 0.2900664662305818, 0.42806656451314345], [0.3066005571688877, 1.3931959054429162, 0.6366232162759474], [0.21565105620570899, 0.5267519630011802, 6.817248106561024], [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]\"\"\" DynaML > t1 . summarize ( 100 , false ) res29 : String = \"\"\"FLOAT64[2, 6] [[0.3501699906342581, 0.2900664662305818, 0.42806656451314345, 0.21565105620570899, 0.5267519630011802, 6.817248106561024], [0.3066005571688877, 1.3931959054429162, 0.6366232162759474, 0.35121879449734744, 5.487926862392467, 2.3538094624119177]]\"\"\" Unstack \u00b6 DynaML > dtf . unstack ( t1 , axis = 1 ) res31 : Seq [ Tensor ] = ArraySeq ( FLOAT64 [ 2 ], FLOAT64 [ 2 ], FLOAT64 [ 2 ], FLOAT64 [ 2 ], FLOAT64 [ 2 ], FLOAT64 [ 2 ]) DynaML > res31 . map ( t => t . summarize ( 100 , false )) res33 : Seq [ String ] = ArraySeq ( \"\"\"FLOAT64[2] [0.3501699906342581, 0.3066005571688877]\"\"\" , \"\"\"FLOAT64[2] [0.2900664662305818, 1.3931959054429162]\"\"\" , \"\"\"FLOAT64[2] [0.42806656451314345, 0.6366232162759474]\"\"\" , \"\"\"FLOAT64[2] [0.21565105620570899, 0.35121879449734744]\"\"\" , \"\"\"FLOAT64[2] [0.5267519630011802, 5.487926862392467]\"\"\" , \"\"\"FLOAT64[2] [6.817248106561024, 2.3538094624119177]\"\"\" )","title":"Tensorflow Pointer"},{"location":"core/core_dtf/#creating-tensors","text":"Creating tensors using the dtf object is easy, the user needs to provide a scala collection containing the the data, the shape and data-type of the tensor. There is more than one way to instantiate a tensor.","title":"Creating Tensors."},{"location":"core/core_dtf/#enumeration-of-values","text":"import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Create a float tensor val tensor_float = dtf . tensor_from [ Float ]( FLOAT32 , Shape ( 2 , 2 ))( 1 f , 2 f , 3 f , 4 f ) //Prints out a summary of the values in tensor1 tensor_float . summarize () val tensor_double = dtf . tensor_from [ Double ]( FLOAT64 , Shape ( 2 , 2 ))( 1.0 , 2.0 , 3.0 , 4.0 ) tensor_double . summarize ()","title":"Enumeration of Values"},{"location":"core/core_dtf/#from-a-scala-sequence","text":"import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val float_seq = Seq ( 1 f , 2 f , 3 f , 4 f ) val double_seq = Seq ( 1.0 , 2.0 , 3.0 , 4.0 ) //Specify data type as a string, and enumerate the shape val tensor_float = dtf . tensor_from [ Float ]( \"FLOAT32\" , 2 , 2 )( float_seq ) //Prints out a summary of the values in tensor1 tensor_float . summarize () val tensor_double = dtf . tensor_from [ Double ]( \"FLOAT64\" , 2 , 2 )( double_seq ) tensor_double . summarize ()","title":"From a Scala Sequence"},{"location":"core/core_dtf/#from-an-array-of-bytes","text":"When dealing with binary data formats, such as images and other binary numerical formats, it is useful to be able to instantiate tensors from buffers of raw bytes. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val byte_buffer : Array [ Byte ] = _ val shape : Shape = _ val byte_tensor = dtf . tensor_from_buffer ( INT32 , shape )( byte_buffer ) Apart from these functions, there are. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Double tensor val t = dtf . tensor_f64 ( 2 , 2 )( 1.0 , 2.0 , 3.0 , 4.0 ) //32 bit Integer tensor val t_int = dtf . tensor_i32 ( 2 , 3 )( 1 , 2 , 3 , 4 , 5 , 6 ) //Fill a (3, 2, 5) tensor, with the value 1. val t_fill = dtf . fill ( FLOAT32 , 3 , 2 , 5 )( 1 f )","title":"From an Array of Bytes."},{"location":"core/core_dtf/#random-tensors","text":"It is also possible to create tensors whose elements are independent and identically distributed , by using the DynaML probability API. import breeze.stats.distributions._ import io.github.mandar2812.dynaml.probability._ import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val rv = RandomVariable ( new LogNormal ( 0.0 , 1.5 )) val random_tensor = dtf . random ( FLOAT64 , 3 , 5 , 2 )( rv )","title":"Random Tensors"},{"location":"core/core_dtf/#operations-on-tensors","text":"","title":"Operations on Tensors"},{"location":"core/core_dtf/#stack","text":"DynaML > val random_tensor1 = dtf . random ( FLOAT64 , 2 , 3 )( rv ) random_tensor1 : Tensor = FLOAT64 [ 2 , 3 ] DynaML > val random_tensor2 = dtf . random ( FLOAT64 , 2 , 3 )( rv ) random_tensor2 : Tensor = FLOAT64 [ 2 , 3 ] DynaML > val t = dtf . stack ( Seq ( random_tensor1 , random_tensor2 ), axis = 1 ) t : Tensor = FLOAT64 [ 2 , 2 , 3 ] DynaML > val t0 = dtf . stack ( Seq ( random_tensor1 , random_tensor2 ), axis = 0 ) t0 : Tensor = FLOAT64 [ 2 , 2 , 3 ] DynaML > random_tensor1 . summarize ( 100 , false ) res18 : String = \"\"\"FLOAT64[2, 3] [[0.3501699906342581, 0.2900664662305818, 0.42806656451314345], [0.3066005571688877, 1.3931959054429162, 0.6366232162759474]]\"\"\" DynaML > random_tensor2 . summarize ( 100 , false ) res19 : String = \"\"\"FLOAT64[2, 3] [[0.21565105620570899, 0.5267519630011802, 6.817248106561024], [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]\"\"\" DynaML > t . summarize ( 100 , false ) res16 : String = \"\"\"FLOAT64[2, 2, 3] [[[0.3501699906342581, 0.2900664662305818, 0.42806656451314345], [0.21565105620570899, 0.5267519630011802, 6.817248106561024]], [[0.3066005571688877, 1.3931959054429162, 0.6366232162759474], [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]]\"\"\" DynaML > t0 . summarize ( 100 , false ) res17 : String = \"\"\"FLOAT64[2, 2, 3] [[[0.3501699906342581, 0.2900664662305818, 0.42806656451314345], [0.3066005571688877, 1.3931959054429162, 0.6366232162759474]], [[0.21565105620570899, 0.5267519630011802, 6.817248106561024], [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]]\"\"\"","title":"Stack"},{"location":"core/core_dtf/#concatenate","text":"DynaML > val t = dtf . concatenate ( Seq ( random_tensor1 , random_tensor2 ), axis = 0 ) t : Tensor = FLOAT64 [ 4 , 3 ] DynaML > val t1 = dtf . concatenate ( Seq ( random_tensor1 , random_tensor2 ), axis = 1 ) t1 : Tensor = FLOAT64 [ 2 , 6 ] DynaML > t . summarize ( 100 , false ) res28 : String = \"\"\"FLOAT64[4, 3] [[0.3501699906342581, 0.2900664662305818, 0.42806656451314345], [0.3066005571688877, 1.3931959054429162, 0.6366232162759474], [0.21565105620570899, 0.5267519630011802, 6.817248106561024], [0.35121879449734744, 5.487926862392467, 2.3538094624119177]]\"\"\" DynaML > t1 . summarize ( 100 , false ) res29 : String = \"\"\"FLOAT64[2, 6] [[0.3501699906342581, 0.2900664662305818, 0.42806656451314345, 0.21565105620570899, 0.5267519630011802, 6.817248106561024], [0.3066005571688877, 1.3931959054429162, 0.6366232162759474, 0.35121879449734744, 5.487926862392467, 2.3538094624119177]]\"\"\"","title":"Concatenate"},{"location":"core/core_dtf/#unstack","text":"DynaML > dtf . unstack ( t1 , axis = 1 ) res31 : Seq [ Tensor ] = ArraySeq ( FLOAT64 [ 2 ], FLOAT64 [ 2 ], FLOAT64 [ 2 ], FLOAT64 [ 2 ], FLOAT64 [ 2 ], FLOAT64 [ 2 ]) DynaML > res31 . map ( t => t . summarize ( 100 , false )) res33 : Seq [ String ] = ArraySeq ( \"\"\"FLOAT64[2] [0.3501699906342581, 0.3066005571688877]\"\"\" , \"\"\"FLOAT64[2] [0.2900664662305818, 1.3931959054429162]\"\"\" , \"\"\"FLOAT64[2] [0.42806656451314345, 0.6366232162759474]\"\"\" , \"\"\"FLOAT64[2] [0.21565105620570899, 0.35121879449734744]\"\"\" , \"\"\"FLOAT64[2] [0.5267519630011802, 5.487926862392467]\"\"\" , \"\"\"FLOAT64[2] [6.817248106561024, 2.3538094624119177]\"\"\" )","title":"Unstack"},{"location":"core/core_dtfdata/","text":"Summary The DataSet API added in v1.5.3, makes it easy to work with potentially large data sets, perform complex pre-processing tasks and feed these data sets into TensorFlow models. Data Set \u00b6 Basics \u00b6 A DataSet[X] instance is simply a wrapper over an Iterable[X] object, although the user still has access to the underlying collection. Tip The dtfdata object gives the user easy access to the DataSet API. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import io.github.mandar2812.dynaml.tensorflow._ val random_numbers = GaussianRV ( 0.0 , 1.0 ) :* GaussianRV ( 1.0 , 2.0 ) //Create a data set. val dataset1 = dtfdata . dataset ( random_numbers . iid ( 10000 ). draw ) //Access underlying data dataset1 . data Transformations \u00b6 DynaML data sets support several operations of the map-reduce philosophy. Map \u00b6 Transform each element of type X into some other element of type Y ( Y can possibly be the same as X ). import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import io.github.mandar2812.dynaml.tensorflow._ val random_numbers = GaussianRV ( 0.0 , 1.0 ) //A data set of random gaussian numbers. val random_gaussian_dataset = dtfdata . dataset ( random_numbers . iid ( 10000 ). draw ) //Transform data set by applying a scala function val random_chisq_dataset = random_gaussian_dataset . map (( x : Double ) => x * x ) val exp_tr = DataPipe [ Double , Double ]( math . exp _ ) //Can pass a DataPipe instead of a function val random_log_gaussian_dataset = random_gaussian_dataset . map ( exp_tr ) Flat Map \u00b6 Process each element by applying a function which transforms each element into an Iterable , this operation is followed by flattening of the top level Iterable . Schematically, this process is Iterable[X] -> Iterable[Iterable[Y]] -> Iterable[Y] import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import io.github.mandar2812.dynaml.tensorflow._ val random_gaussian_dataset = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) //Transform data set by applying a scala function val gaussian_mixture = random_gaussian_dataset . flatMap ( ( x : Double ) => GaussianRV ( 0.0 , x * x ). iid ( 10 ). draw ) Filter \u00b6 Collect only the elements which satisfy some predicate, i.e. a function which returns true for the elements to be selected (filtered) and false for the ones which should be discarded. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import io.github.mandar2812.dynaml.tensorflow._ val gaussian_dataset = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) val onlyPositive = DataPipe [ Double , Boolean ]( _ > 0.0 ) val truncated_gaussian = gaussian_dataset . filter ( onlyPositive ) val zeroOrGreater = ( x : Double ) => x >= 0.0 //filterNot works in the opposite manner to filter val neg_truncated_gaussian = gaussian_dataset . filterNot ( zeroOrGreater ) Scan & Friends \u00b6 Sometimes, we need to perform operations on a data set which are sequential in nature. In this situation, the scanLeft() and scanRight() are useful. Lets simulate a random walk, we start with x_0 x_0 , a number and add independent gaussian increments to it. \\begin{align*} x_t &= x_{t-1} + \\epsilon \\\\ \\epsilon &\\sim \\mathcal{N}(0, 1) \\end{align*} \\begin{align*} x_t &= x_{t-1} + \\epsilon \\\\ \\epsilon &\\sim \\mathcal{N}(0, 1) \\end{align*} import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import io.github.mandar2812.dynaml.tensorflow._ val gaussian_increments = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) val increment = DataPipe2 [ Double , Double , Double ](( x , i ) => x + i ) //Start the random walk from zero, and keep adding increments. val random_walk = gaussian_increments . scanLeft ( 0.0 )( increment ) The scanRight() works just like the scanLeft() method, except it begins from the last element of the collection. Reduce & Reduce Left \u00b6 The reduce() and reduceLeft() methods help in computing summary values from the entire data collection. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import io.github.mandar2812.dynaml.tensorflow._ val gaussian_increments = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) val increment = DataPipe2 [ Double , Double , Double ](( x , i ) => x + i ) val random_walk = gaussian_increments . scanLeft ( 0.0 )( increment ) val average = random_walk . reduce ( DataPipe2 [ Double , Double , Double ](( x , y ) => x + y ) )/ 10000.0 Other Transformations \u00b6 Some times transformations on data sets cannot be applied on each element individually, but the entire data collection is required for such a transformation. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import io.github.mandar2812.dynaml.tensorflow._ val gaussian_data = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) val resample = DataPipe [ Iterable [ Double ] , Iterable [ Double ]]( coll => ( 0 until 10000 ). map ( _ => coll ( Random . nextInt ( 10000 ))) ) val resampled_data = gaussian_data . transform ( resample ) Note Conversion to TF-Scala Dataset class The TensorFlow scala API also has a Dataset class, from a DynaML DataSet instance, it is possible to obtain a TensorFlow Dataset . import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ import org.platanios.tensorflow.api.types._ val random_numbers = GaussianRV ( 0.0 , 1.0 ) //Create a data set. val dataset1 = dtfdata . dataset ( random_numbers . iid ( 10000 ). draw ) //Convert to TensorFlow data set dataset1 . build [ Tensor , Output , DataType.Aux [ Double ] , DataType , Shape ]( Left ( DataPipe [ Double , Tensor ]( x => dtf . tensor_f64 ( 1 )( x ))), FLOAT64 , Shape ( 1 ) ) Tuple Data & Supervised Data \u00b6 The classes ZipDataSet[X, Y] and SupervisedDataSet[X, Y] both represent data collections which consist of (X, Y) tuples. They can be created in a number of ways. Zip Data \u00b6 The zip() method can be used to create data sets consisting of tuples. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import _root_.breeze.stats.distributions._ import io.github.mandar2812.dynaml.tensorflow._ val gaussian_data = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) val log_normal_data = gaussian_data . map (( x : Double ) => math . exp ( x )) val poisson_data = dtfdata . dataset ( RandomVariable ( Poisson ( 2.5 )). iid ( 10000 ). draw ) val tuple_data1 = poisson_data . zip ( gaussian_data ) val tuple_data2 = poisson_data . zip ( log_normal_data ) //Join on the keys, in this case the //Poisson distributed integers tuple_data1 . join ( tuple_data2 ) Supervised Data \u00b6 For supervised learning operations, we can use the SupervisedDataSet class, which can be instantiated in the following ways. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import _root_.breeze.stats.distributions._ import io.github.mandar2812.dynaml.tensorflow._ val gaussian_data = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) val sup_data1 = gaussian_data . to_supervised ( DataPipe [ Double , ( Double , Double )]( x => ( x , GaussianRV ( 0.0 , x * x ). draw )) ) val targets = gaussian_data . map (( x : Double ) => math . exp ( x )) val sup_data2 = dtfdata . supervised_dataset ( gaussian_data , targets )","title":"Data Set API"},{"location":"core/core_dtfdata/#data-set","text":"","title":"Data Set"},{"location":"core/core_dtfdata/#basics","text":"A DataSet[X] instance is simply a wrapper over an Iterable[X] object, although the user still has access to the underlying collection. Tip The dtfdata object gives the user easy access to the DataSet API. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import io.github.mandar2812.dynaml.tensorflow._ val random_numbers = GaussianRV ( 0.0 , 1.0 ) :* GaussianRV ( 1.0 , 2.0 ) //Create a data set. val dataset1 = dtfdata . dataset ( random_numbers . iid ( 10000 ). draw ) //Access underlying data dataset1 . data","title":"Basics"},{"location":"core/core_dtfdata/#transformations","text":"DynaML data sets support several operations of the map-reduce philosophy.","title":"Transformations"},{"location":"core/core_dtfdata/#map","text":"Transform each element of type X into some other element of type Y ( Y can possibly be the same as X ). import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import io.github.mandar2812.dynaml.tensorflow._ val random_numbers = GaussianRV ( 0.0 , 1.0 ) //A data set of random gaussian numbers. val random_gaussian_dataset = dtfdata . dataset ( random_numbers . iid ( 10000 ). draw ) //Transform data set by applying a scala function val random_chisq_dataset = random_gaussian_dataset . map (( x : Double ) => x * x ) val exp_tr = DataPipe [ Double , Double ]( math . exp _ ) //Can pass a DataPipe instead of a function val random_log_gaussian_dataset = random_gaussian_dataset . map ( exp_tr )","title":"Map"},{"location":"core/core_dtfdata/#flat-map","text":"Process each element by applying a function which transforms each element into an Iterable , this operation is followed by flattening of the top level Iterable . Schematically, this process is Iterable[X] -> Iterable[Iterable[Y]] -> Iterable[Y] import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import io.github.mandar2812.dynaml.tensorflow._ val random_gaussian_dataset = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) //Transform data set by applying a scala function val gaussian_mixture = random_gaussian_dataset . flatMap ( ( x : Double ) => GaussianRV ( 0.0 , x * x ). iid ( 10 ). draw )","title":"Flat Map"},{"location":"core/core_dtfdata/#filter","text":"Collect only the elements which satisfy some predicate, i.e. a function which returns true for the elements to be selected (filtered) and false for the ones which should be discarded. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import io.github.mandar2812.dynaml.tensorflow._ val gaussian_dataset = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) val onlyPositive = DataPipe [ Double , Boolean ]( _ > 0.0 ) val truncated_gaussian = gaussian_dataset . filter ( onlyPositive ) val zeroOrGreater = ( x : Double ) => x >= 0.0 //filterNot works in the opposite manner to filter val neg_truncated_gaussian = gaussian_dataset . filterNot ( zeroOrGreater )","title":"Filter"},{"location":"core/core_dtfdata/#scan-friends","text":"Sometimes, we need to perform operations on a data set which are sequential in nature. In this situation, the scanLeft() and scanRight() are useful. Lets simulate a random walk, we start with x_0 x_0 , a number and add independent gaussian increments to it. \\begin{align*} x_t &= x_{t-1} + \\epsilon \\\\ \\epsilon &\\sim \\mathcal{N}(0, 1) \\end{align*} \\begin{align*} x_t &= x_{t-1} + \\epsilon \\\\ \\epsilon &\\sim \\mathcal{N}(0, 1) \\end{align*} import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import io.github.mandar2812.dynaml.tensorflow._ val gaussian_increments = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) val increment = DataPipe2 [ Double , Double , Double ](( x , i ) => x + i ) //Start the random walk from zero, and keep adding increments. val random_walk = gaussian_increments . scanLeft ( 0.0 )( increment ) The scanRight() works just like the scanLeft() method, except it begins from the last element of the collection.","title":"Scan &amp; Friends"},{"location":"core/core_dtfdata/#reduce-reduce-left","text":"The reduce() and reduceLeft() methods help in computing summary values from the entire data collection. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import io.github.mandar2812.dynaml.tensorflow._ val gaussian_increments = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) val increment = DataPipe2 [ Double , Double , Double ](( x , i ) => x + i ) val random_walk = gaussian_increments . scanLeft ( 0.0 )( increment ) val average = random_walk . reduce ( DataPipe2 [ Double , Double , Double ](( x , y ) => x + y ) )/ 10000.0","title":"Reduce &amp; Reduce Left"},{"location":"core/core_dtfdata/#other-transformations","text":"Some times transformations on data sets cannot be applied on each element individually, but the entire data collection is required for such a transformation. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import io.github.mandar2812.dynaml.tensorflow._ val gaussian_data = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) val resample = DataPipe [ Iterable [ Double ] , Iterable [ Double ]]( coll => ( 0 until 10000 ). map ( _ => coll ( Random . nextInt ( 10000 ))) ) val resampled_data = gaussian_data . transform ( resample ) Note Conversion to TF-Scala Dataset class The TensorFlow scala API also has a Dataset class, from a DynaML DataSet instance, it is possible to obtain a TensorFlow Dataset . import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ import org.platanios.tensorflow.api.types._ val random_numbers = GaussianRV ( 0.0 , 1.0 ) //Create a data set. val dataset1 = dtfdata . dataset ( random_numbers . iid ( 10000 ). draw ) //Convert to TensorFlow data set dataset1 . build [ Tensor , Output , DataType.Aux [ Double ] , DataType , Shape ]( Left ( DataPipe [ Double , Tensor ]( x => dtf . tensor_f64 ( 1 )( x ))), FLOAT64 , Shape ( 1 ) )","title":"Other Transformations"},{"location":"core/core_dtfdata/#tuple-data-supervised-data","text":"The classes ZipDataSet[X, Y] and SupervisedDataSet[X, Y] both represent data collections which consist of (X, Y) tuples. They can be created in a number of ways.","title":"Tuple Data &amp; Supervised Data"},{"location":"core/core_dtfdata/#zip-data","text":"The zip() method can be used to create data sets consisting of tuples. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import _root_.breeze.stats.distributions._ import io.github.mandar2812.dynaml.tensorflow._ val gaussian_data = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) val log_normal_data = gaussian_data . map (( x : Double ) => math . exp ( x )) val poisson_data = dtfdata . dataset ( RandomVariable ( Poisson ( 2.5 )). iid ( 10000 ). draw ) val tuple_data1 = poisson_data . zip ( gaussian_data ) val tuple_data2 = poisson_data . zip ( log_normal_data ) //Join on the keys, in this case the //Poisson distributed integers tuple_data1 . join ( tuple_data2 )","title":"Zip Data"},{"location":"core/core_dtfdata/#supervised-data","text":"For supervised learning operations, we can use the SupervisedDataSet class, which can be instantiated in the following ways. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import scala.util.Random import _root_.breeze.stats.distributions._ import io.github.mandar2812.dynaml.tensorflow._ val gaussian_data = dtfdata . dataset ( GaussianRV ( 0.0 , 1.0 ). iid ( 10000 ). draw ) val sup_data1 = gaussian_data . to_supervised ( DataPipe [ Double , ( Double , Double )]( x => ( x , GaussianRV ( 0.0 , x * x ). draw )) ) val targets = gaussian_data . map (( x : Double ) => math . exp ( x )) val sup_data2 = dtfdata . supervised_dataset ( gaussian_data , targets )","title":"Supervised Data"},{"location":"core/core_dtflearn/","text":"Summary The dtflearn object makes it easy to create and train neural networks of varying complexity. Activation Functions \u00b6 Apart from the activation functions defined in tensorflow for scala, DynaML defines some additional activations. Hyperbolic Tangent val act = dtflearn . Tanh ( \"SomeIdentifier\" ) Cumulative Gaussian val act = dtflearn . Phi ( \"OtherIdentifier\" ) Generalized Logistic val act = dtflearn . GeneralizedLogistic ( \"AnotherId\" ) Layers \u00b6 DynaML aims to supplement and extend the collection of layers available in org.platanios.tensorflow.api.layers , all the layers defined in DynaML's tensorflow package extend the Layer[T, R] class in org.platanios.tensorflow.api.layers . Radial Basis Function Network \u00b6 Radial Basis Function (RBF) networks are an important class of basis functions, each of which are expressed as decaying with distance from a defined central node. \\begin{align} f(x) & = \\sum_{i}{w_{i} \\varphi(||x - c_{i}||/\\sigma)} \\\\ \\varphi(u) & = exp(-u^2/2) \\end{align} \\begin{align} f(x) & = \\sum_{i}{w_{i} \\varphi(||x - c_{i}||/\\sigma)} \\\\ \\varphi(u) & = exp(-u^2/2) \\end{align} The RBF layer implementation in DynaML treats the node center positions c_i c_i and length scales \\sigma_i \\sigma_i as parameters to be learned via gradient based back-propagation. import io.github.mandar2812.dynaml.tensorflow._ val rbf = dtflearn . rbf_layer ( name = \"rbf1\" , num_units = 10 ) Continuous Time RNN \u00b6 Continuous time recurrent neural networks (CTRNN) are an important class of recurrent neural networks. They enable the modelling of non-linear and potentially complex dynamical systems of multiple variables, with feedback. Each state variable is modeled by a single neuron y_i y_i , the evolution of the system y = (y_1, \\cdots, y_n)^T y = (y_1, \\cdots, y_n)^T is governed by a set of coupled ordinary differential equations. These equations can be expressed in vector form as follows. \\begin{align} dy/dt & = - \\Lambda . y + W . \\sigma(G.y + b) \\\\ \\end{align} \\begin{align} dy/dt & = - \\Lambda . y + W . \\sigma(G.y + b) \\\\ \\end{align} The parameters of the system above are. Time Constant/Decay Rate \\begin{equation} \\Lambda = \\begin{pmatrix} \\lambda_1 & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & \\lambda_n \\end{pmatrix} \\end{equation} \\begin{equation} \\Lambda = \\begin{pmatrix} \\lambda_1 & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & \\lambda_n \\end{pmatrix} \\end{equation} Gain \\begin{equation} G = \\begin{pmatrix} g_{11} & \\cdots & g_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ g_{n1} & \\cdots & g_{nn} \\end{pmatrix} \\end{equation} \\begin{equation} G = \\begin{pmatrix} g_{11} & \\cdots & g_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ g_{n1} & \\cdots & g_{nn} \\end{pmatrix} \\end{equation} Bias \\begin{equation} b = \\begin{pmatrix} b_{1}\\\\ \\vdots\\\\ b_{n} \\end{pmatrix} \\end{equation} \\begin{equation} b = \\begin{pmatrix} b_{1}\\\\ \\vdots\\\\ b_{n} \\end{pmatrix} \\end{equation} Weights \\begin{equation} W = \\begin{pmatrix} w_{11} & \\cdots & w_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ w_{n1} & \\cdots & w_{nn} \\end{pmatrix} \\end{equation} \\begin{equation} W = \\begin{pmatrix} w_{11} & \\cdots & w_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ w_{n1} & \\cdots & w_{nn} \\end{pmatrix} \\end{equation} In order to use the CTRNN model in a modelling sequences of finite length, we need to solve its governing equations numerically. This gives us the trajectory of the state upto T T steps y^{0}, \\cdots, y^{T} y^{0}, \\cdots, y^{T} . y^{k+1} = y^{k} + \\Delta t (- \\Lambda . y^{k} + W . \\sigma(G.y^{k} + b)) y^{k+1} = y^{k} + \\Delta t (- \\Lambda . y^{k} + W . \\sigma(G.y^{k} + b)) DynaML's implementation of the CTRNN can be used to learn the trajectory of dynamical systems upto a predefined time horizon. The parameters \\Lambda, G, b, W \\Lambda, G, b, W are learned using gradient based loss minimization. The CTRNN implementations are also instances of Layer[Output, Output] , which take as input tensors of shape n n and produce tensors of shape (n, T) (n, T) , there are two variants that users can choose from. Fixed Time Step Integration \u00b6 When the integration time step \\Delta t \\Delta t is user defined and fixed. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val ctrnn_layer = dtflearn . ctrnn ( name = \"CTRNN_1\" , units = 10 , horizon = 5 , timestep = 0.1 ) Dynamic Time Step Integration \u00b6 When the integration time step \\Delta t \\Delta t is a parameter that can be learned during the training process. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val dctrnn_layer = dtflearn . dctrnn ( name = \"DCTRNN_1\" , units = 10 , horizon = 5 ) Stack & Concatenate \u00b6 Often one would need to combine inputs of previous layers in some manner, the following layers enable these operations. Stack Inputs \u00b6 This is a computational layer which performs the function of dtf.stack() . import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val stk_layer = dtflearn . stack_outputs ( \"StackTensors\" , axis = 1 ) Concatenate Inputs \u00b6 This is a computational layer which performs the function of dtf.concatenate() . import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val concat_layer = dtflearn . stack_outputs ( \"ConcatenateTensors\" , axis = 1 ) Collect Layers \u00b6 A sequence of layers can be collected into a single layer which accepts a sequence of symbolic tensors. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val layers = Seq ( tf . learn . Linear ( \"l1\" , 10 ), dtflearn . identity ( \"Identity\" ), dtflearn . ctrnn ( name = \"CTRNN_1\" , units = 10 , horizon = 5 , timestep = 0.1 ) ) val combined_layer = dtflearn . stack_layers ( \"Collect\" , layers ) Input Pairs \u00b6 To handle inputs consisting of pairs of elements, one can provide a separate layer for processing each of the elements. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val sl = dtflearn . tuple2_layer ( \"tuple2layer\" , dtflearn . rbf_layer ( \"rbf1\" , 10 ), tf . learn . Linear ( \"lin1\" , 10 )) Combining the elements of Tuple2 can be done as follows. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Stack elements of the tuple into one tensor val layer1 = dtflearn . stack_tuple2 ( \"tuple2layer\" , axis = 1 ) //Concatenate elements of the tuple into one tensor val layer2 = dtflearn . concat_tuple2 ( \"tuple2layer\" , axis = 1 ) Stoppage Criteria \u00b6 In order to train tensorflow models using iterative gradient based models, the user must define some stoppage criteria for the training process. This can be done via the method tf.learn.StopCriteria() . The following preset stop criteria call tf.learn.StopCriteria() under the hood. Iterations Based \u00b6 val stopc1 = dtflearn . max_iter_stop ( 10000 ) Change in Loss \u00b6 Absolute Value of Loss \u00b6 val stopc2 = dtflearn . abs_loss_change_stop ( 0.1 ) Relative Value of Loss \u00b6 val stopc2 = dtflearn . rel_loss_change_stop ( 0.1 ) Network Building Blocks \u00b6 To make it convenient to build deeper stacks of neural networks, DynaML includes some common layer design patterns as ready made easy to use methods. Convolutional Neural Nets \u00b6 Convolutional neural networks (CNN) are a crucial building block of deep neural architectures for visual pattern recognition. It turns out that CNN layers must be combined with other computational units such as rectified linear (ReLU) activations, dropout and max pool layers. Currently two abstractions are offered for building large CNN based network stacks Convolutional Unit \u00b6 A single CNN unit is expressed as a convolutional layer followed by a ReLU activation and proceeded by a dropout layer. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Learn 16 filters of shape (2, 2, 4), suitable for 4 channel jpeg images. //Slide the filters over the image in steps of 1 pixel in each direction. val cnn_unit = dtflearn . conv2d_unit ( shape = Shape ( 2 , 2 , 4 , 16 ), stride = ( 1 , 1 ), relu_param = 0.05f , dropout = true , keep_prob = 0.55f )( i = 1 ) Convolutional Pyramid \u00b6 A CNN pyramid builds a stack of CNN units each with a stride multiplied by a factor of 2 and depth divided by a factor of 2 with respect to the previous unit. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Start with a CNN unit of shape (2, 2, 3, 16) stride (1, 1) //End with a CNN unit of shape (2, 2, 8, 4) and stride of (8, 8) val cnn_stack = dtflearn . conv2d_pyramid ( size = 2 , num_channels_input = 3 )( start_num_bits = 4 , end_num_bits = 2 )( relu_param = 0.1f , dropout = true , keep_prob = 0.6F ) Feed-forward Neural Nets \u00b6 Feed-forward networks are the oldest and most frequently used components of neural network architectures, they are often stacked into a number of layers. With dtflearn.feedforward_stack() , you can define feed-forward stacks of arbitrary width and depth. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val net_layer_sizes = Seq ( 10 , 20 , 13 , 15 ) val architecture = dtflearn . feedforward_stack ( ( i : Int ) => dtflearn . Phi ( \"Act_\" + i ), FLOAT64 )( net_layer_sizes ) Building Tensorflow Models \u00b6 After defining the key ingredients needed to build a tensorflow model, dtflearn.build_tf_model() builds a new computational graph and creates a tensorflow model and estimator which is trained on the provided data. In the following example, we bring together all the elements of model training: data, architecture, loss etc. import ammonite.ops._ import io.github.mandar2812.dynaml.tensorflow.dtflearn import org.platanios.tensorflow.api._ import org.platanios.tensorflow.api.ops.NN.SamePadding import org.platanios.tensorflow.data.image.CIFARLoader import java.nio.file.Paths val tempdir = home / \"tmp\" val dataSet = CIFARLoader . load ( Paths . get ( tempdir . toString ()), CIFARLoader . CIFAR_10 ) val trainImages = tf . data . TensorSlicesDataset ( dataSet . trainImages ) val trainLabels = tf . data . TensorSlicesDataset ( dataSet . trainLabels ) val trainData = trainImages . zip ( trainLabels ) . repeat () . shuffle ( 10000 ) . batch ( 128 ) . prefetch ( 10 ) println ( \"Building the classification model.\" ) val input = tf . learn . Input ( UINT8 , Shape ( - 1 , dataSet . trainImages . shape ( 1 ), dataSet . trainImages . shape ( 2 ), dataSet . trainImages . shape ( 3 )) ) val trainInput = tf . learn . Input ( UINT8 , Shape (- 1 )) val architecture = tf . learn . Cast ( \"Input/Cast\" , FLOAT32 ) >> dtflearn . conv2d_pyramid ( 2 , 3 )( 4 , 2 )( 0.1f , true , 0.6F ) >> tf . learn . MaxPool ( \"Layer_3/MaxPool\" , Seq ( 1 , 2 , 2 , 1 ), 1 , 1 , SamePadding ) >> tf . learn . Flatten ( \"Layer_3/Flatten\" ) >> dtflearn . feedforward ( 256 )( id = 4 ) >> tf . learn . ReLU ( \"Layer_4/ReLU\" , 0.1f ) >> dtflearn . feedforward ( 10 )( id = 5 ) val trainingInputLayer = tf . learn . Cast ( \"TrainInput/Cast\" , INT64 ) val loss = tf . learn . SparseSoftmaxCrossEntropy ( \"Loss/CrossEntropy\" ) >> tf . learn . Mean ( \"Loss/Mean\" ) >> tf . learn . ScalarSummary ( \"Loss/Summary\" , \"Loss\" ) val optimizer = tf . train . AdaGrad ( 0.1 ) println ( \"Training the linear regression model.\" ) val summariesDir = java . nio . file . Paths . get ( ( tempdir / \"cifar_summaries\" ). toString () ) val ( model , estimator ) = dtflearn . build_tf_model ( architecture , input , trainInput , trainingInputLayer , loss , optimizer , summariesDir , dtflearn . max_iter_stop ( 1000 ), 100 , 100 , 100 )( trainData )","title":"Building Blocks"},{"location":"core/core_dtflearn/#activation-functions","text":"Apart from the activation functions defined in tensorflow for scala, DynaML defines some additional activations. Hyperbolic Tangent val act = dtflearn . Tanh ( \"SomeIdentifier\" ) Cumulative Gaussian val act = dtflearn . Phi ( \"OtherIdentifier\" ) Generalized Logistic val act = dtflearn . GeneralizedLogistic ( \"AnotherId\" )","title":"Activation Functions"},{"location":"core/core_dtflearn/#layers","text":"DynaML aims to supplement and extend the collection of layers available in org.platanios.tensorflow.api.layers , all the layers defined in DynaML's tensorflow package extend the Layer[T, R] class in org.platanios.tensorflow.api.layers .","title":"Layers"},{"location":"core/core_dtflearn/#radial-basis-function-network","text":"Radial Basis Function (RBF) networks are an important class of basis functions, each of which are expressed as decaying with distance from a defined central node. \\begin{align} f(x) & = \\sum_{i}{w_{i} \\varphi(||x - c_{i}||/\\sigma)} \\\\ \\varphi(u) & = exp(-u^2/2) \\end{align} \\begin{align} f(x) & = \\sum_{i}{w_{i} \\varphi(||x - c_{i}||/\\sigma)} \\\\ \\varphi(u) & = exp(-u^2/2) \\end{align} The RBF layer implementation in DynaML treats the node center positions c_i c_i and length scales \\sigma_i \\sigma_i as parameters to be learned via gradient based back-propagation. import io.github.mandar2812.dynaml.tensorflow._ val rbf = dtflearn . rbf_layer ( name = \"rbf1\" , num_units = 10 )","title":"Radial Basis Function Network"},{"location":"core/core_dtflearn/#continuous-time-rnn","text":"Continuous time recurrent neural networks (CTRNN) are an important class of recurrent neural networks. They enable the modelling of non-linear and potentially complex dynamical systems of multiple variables, with feedback. Each state variable is modeled by a single neuron y_i y_i , the evolution of the system y = (y_1, \\cdots, y_n)^T y = (y_1, \\cdots, y_n)^T is governed by a set of coupled ordinary differential equations. These equations can be expressed in vector form as follows. \\begin{align} dy/dt & = - \\Lambda . y + W . \\sigma(G.y + b) \\\\ \\end{align} \\begin{align} dy/dt & = - \\Lambda . y + W . \\sigma(G.y + b) \\\\ \\end{align} The parameters of the system above are. Time Constant/Decay Rate \\begin{equation} \\Lambda = \\begin{pmatrix} \\lambda_1 & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & \\lambda_n \\end{pmatrix} \\end{equation} \\begin{equation} \\Lambda = \\begin{pmatrix} \\lambda_1 & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & \\lambda_n \\end{pmatrix} \\end{equation} Gain \\begin{equation} G = \\begin{pmatrix} g_{11} & \\cdots & g_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ g_{n1} & \\cdots & g_{nn} \\end{pmatrix} \\end{equation} \\begin{equation} G = \\begin{pmatrix} g_{11} & \\cdots & g_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ g_{n1} & \\cdots & g_{nn} \\end{pmatrix} \\end{equation} Bias \\begin{equation} b = \\begin{pmatrix} b_{1}\\\\ \\vdots\\\\ b_{n} \\end{pmatrix} \\end{equation} \\begin{equation} b = \\begin{pmatrix} b_{1}\\\\ \\vdots\\\\ b_{n} \\end{pmatrix} \\end{equation} Weights \\begin{equation} W = \\begin{pmatrix} w_{11} & \\cdots & w_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ w_{n1} & \\cdots & w_{nn} \\end{pmatrix} \\end{equation} \\begin{equation} W = \\begin{pmatrix} w_{11} & \\cdots & w_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ w_{n1} & \\cdots & w_{nn} \\end{pmatrix} \\end{equation} In order to use the CTRNN model in a modelling sequences of finite length, we need to solve its governing equations numerically. This gives us the trajectory of the state upto T T steps y^{0}, \\cdots, y^{T} y^{0}, \\cdots, y^{T} . y^{k+1} = y^{k} + \\Delta t (- \\Lambda . y^{k} + W . \\sigma(G.y^{k} + b)) y^{k+1} = y^{k} + \\Delta t (- \\Lambda . y^{k} + W . \\sigma(G.y^{k} + b)) DynaML's implementation of the CTRNN can be used to learn the trajectory of dynamical systems upto a predefined time horizon. The parameters \\Lambda, G, b, W \\Lambda, G, b, W are learned using gradient based loss minimization. The CTRNN implementations are also instances of Layer[Output, Output] , which take as input tensors of shape n n and produce tensors of shape (n, T) (n, T) , there are two variants that users can choose from.","title":"Continuous Time RNN"},{"location":"core/core_dtflearn/#fixed-time-step-integration","text":"When the integration time step \\Delta t \\Delta t is user defined and fixed. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val ctrnn_layer = dtflearn . ctrnn ( name = \"CTRNN_1\" , units = 10 , horizon = 5 , timestep = 0.1 )","title":"Fixed Time Step Integration"},{"location":"core/core_dtflearn/#dynamic-time-step-integration","text":"When the integration time step \\Delta t \\Delta t is a parameter that can be learned during the training process. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val dctrnn_layer = dtflearn . dctrnn ( name = \"DCTRNN_1\" , units = 10 , horizon = 5 )","title":"Dynamic Time Step Integration"},{"location":"core/core_dtflearn/#stack-concatenate","text":"Often one would need to combine inputs of previous layers in some manner, the following layers enable these operations.","title":"Stack &amp; Concatenate"},{"location":"core/core_dtflearn/#stack-inputs","text":"This is a computational layer which performs the function of dtf.stack() . import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val stk_layer = dtflearn . stack_outputs ( \"StackTensors\" , axis = 1 )","title":"Stack Inputs"},{"location":"core/core_dtflearn/#concatenate-inputs","text":"This is a computational layer which performs the function of dtf.concatenate() . import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val concat_layer = dtflearn . stack_outputs ( \"ConcatenateTensors\" , axis = 1 )","title":"Concatenate Inputs"},{"location":"core/core_dtflearn/#collect-layers","text":"A sequence of layers can be collected into a single layer which accepts a sequence of symbolic tensors. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val layers = Seq ( tf . learn . Linear ( \"l1\" , 10 ), dtflearn . identity ( \"Identity\" ), dtflearn . ctrnn ( name = \"CTRNN_1\" , units = 10 , horizon = 5 , timestep = 0.1 ) ) val combined_layer = dtflearn . stack_layers ( \"Collect\" , layers )","title":"Collect Layers"},{"location":"core/core_dtflearn/#input-pairs","text":"To handle inputs consisting of pairs of elements, one can provide a separate layer for processing each of the elements. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val sl = dtflearn . tuple2_layer ( \"tuple2layer\" , dtflearn . rbf_layer ( \"rbf1\" , 10 ), tf . learn . Linear ( \"lin1\" , 10 )) Combining the elements of Tuple2 can be done as follows. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Stack elements of the tuple into one tensor val layer1 = dtflearn . stack_tuple2 ( \"tuple2layer\" , axis = 1 ) //Concatenate elements of the tuple into one tensor val layer2 = dtflearn . concat_tuple2 ( \"tuple2layer\" , axis = 1 )","title":"Input Pairs"},{"location":"core/core_dtflearn/#stoppage-criteria","text":"In order to train tensorflow models using iterative gradient based models, the user must define some stoppage criteria for the training process. This can be done via the method tf.learn.StopCriteria() . The following preset stop criteria call tf.learn.StopCriteria() under the hood.","title":"Stoppage Criteria"},{"location":"core/core_dtflearn/#iterations-based","text":"val stopc1 = dtflearn . max_iter_stop ( 10000 )","title":"Iterations Based"},{"location":"core/core_dtflearn/#change-in-loss","text":"","title":"Change in Loss"},{"location":"core/core_dtflearn/#absolute-value-of-loss","text":"val stopc2 = dtflearn . abs_loss_change_stop ( 0.1 )","title":"Absolute Value of Loss"},{"location":"core/core_dtflearn/#relative-value-of-loss","text":"val stopc2 = dtflearn . rel_loss_change_stop ( 0.1 )","title":"Relative Value of Loss"},{"location":"core/core_dtflearn/#network-building-blocks","text":"To make it convenient to build deeper stacks of neural networks, DynaML includes some common layer design patterns as ready made easy to use methods.","title":"Network Building Blocks"},{"location":"core/core_dtflearn/#convolutional-neural-nets","text":"Convolutional neural networks (CNN) are a crucial building block of deep neural architectures for visual pattern recognition. It turns out that CNN layers must be combined with other computational units such as rectified linear (ReLU) activations, dropout and max pool layers. Currently two abstractions are offered for building large CNN based network stacks","title":"Convolutional Neural Nets"},{"location":"core/core_dtflearn/#convolutional-unit","text":"A single CNN unit is expressed as a convolutional layer followed by a ReLU activation and proceeded by a dropout layer. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Learn 16 filters of shape (2, 2, 4), suitable for 4 channel jpeg images. //Slide the filters over the image in steps of 1 pixel in each direction. val cnn_unit = dtflearn . conv2d_unit ( shape = Shape ( 2 , 2 , 4 , 16 ), stride = ( 1 , 1 ), relu_param = 0.05f , dropout = true , keep_prob = 0.55f )( i = 1 )","title":"Convolutional Unit"},{"location":"core/core_dtflearn/#convolutional-pyramid","text":"A CNN pyramid builds a stack of CNN units each with a stride multiplied by a factor of 2 and depth divided by a factor of 2 with respect to the previous unit. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Start with a CNN unit of shape (2, 2, 3, 16) stride (1, 1) //End with a CNN unit of shape (2, 2, 8, 4) and stride of (8, 8) val cnn_stack = dtflearn . conv2d_pyramid ( size = 2 , num_channels_input = 3 )( start_num_bits = 4 , end_num_bits = 2 )( relu_param = 0.1f , dropout = true , keep_prob = 0.6F )","title":"Convolutional Pyramid"},{"location":"core/core_dtflearn/#feed-forward-neural-nets","text":"Feed-forward networks are the oldest and most frequently used components of neural network architectures, they are often stacked into a number of layers. With dtflearn.feedforward_stack() , you can define feed-forward stacks of arbitrary width and depth. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ val net_layer_sizes = Seq ( 10 , 20 , 13 , 15 ) val architecture = dtflearn . feedforward_stack ( ( i : Int ) => dtflearn . Phi ( \"Act_\" + i ), FLOAT64 )( net_layer_sizes )","title":"Feed-forward Neural Nets"},{"location":"core/core_dtflearn/#building-tensorflow-models","text":"After defining the key ingredients needed to build a tensorflow model, dtflearn.build_tf_model() builds a new computational graph and creates a tensorflow model and estimator which is trained on the provided data. In the following example, we bring together all the elements of model training: data, architecture, loss etc. import ammonite.ops._ import io.github.mandar2812.dynaml.tensorflow.dtflearn import org.platanios.tensorflow.api._ import org.platanios.tensorflow.api.ops.NN.SamePadding import org.platanios.tensorflow.data.image.CIFARLoader import java.nio.file.Paths val tempdir = home / \"tmp\" val dataSet = CIFARLoader . load ( Paths . get ( tempdir . toString ()), CIFARLoader . CIFAR_10 ) val trainImages = tf . data . TensorSlicesDataset ( dataSet . trainImages ) val trainLabels = tf . data . TensorSlicesDataset ( dataSet . trainLabels ) val trainData = trainImages . zip ( trainLabels ) . repeat () . shuffle ( 10000 ) . batch ( 128 ) . prefetch ( 10 ) println ( \"Building the classification model.\" ) val input = tf . learn . Input ( UINT8 , Shape ( - 1 , dataSet . trainImages . shape ( 1 ), dataSet . trainImages . shape ( 2 ), dataSet . trainImages . shape ( 3 )) ) val trainInput = tf . learn . Input ( UINT8 , Shape (- 1 )) val architecture = tf . learn . Cast ( \"Input/Cast\" , FLOAT32 ) >> dtflearn . conv2d_pyramid ( 2 , 3 )( 4 , 2 )( 0.1f , true , 0.6F ) >> tf . learn . MaxPool ( \"Layer_3/MaxPool\" , Seq ( 1 , 2 , 2 , 1 ), 1 , 1 , SamePadding ) >> tf . learn . Flatten ( \"Layer_3/Flatten\" ) >> dtflearn . feedforward ( 256 )( id = 4 ) >> tf . learn . ReLU ( \"Layer_4/ReLU\" , 0.1f ) >> dtflearn . feedforward ( 10 )( id = 5 ) val trainingInputLayer = tf . learn . Cast ( \"TrainInput/Cast\" , INT64 ) val loss = tf . learn . SparseSoftmaxCrossEntropy ( \"Loss/CrossEntropy\" ) >> tf . learn . Mean ( \"Loss/Mean\" ) >> tf . learn . ScalarSummary ( \"Loss/Summary\" , \"Loss\" ) val optimizer = tf . train . AdaGrad ( 0.1 ) println ( \"Training the linear regression model.\" ) val summariesDir = java . nio . file . Paths . get ( ( tempdir / \"cifar_summaries\" ). toString () ) val ( model , estimator ) = dtflearn . build_tf_model ( architecture , input , trainInput , trainingInputLayer , loss , optimizer , summariesDir , dtflearn . max_iter_stop ( 1000 ), 100 , 100 , 100 )( trainData )","title":"Building Tensorflow Models"},{"location":"core/core_dynaml_tf/","text":"Summary Since v1.5.2, DynaML has moved towards closer integration with Google Tensorflow. This is done via the Tensorflow for Scala project. The DynaML tensorflow package builds on Tensorflow for Scala and provides a high level API containing several convenience routines and building blocks for deep learning. Google Tensorflow \u00b6 courtesy Google. Tensorflow is a versatile and general computational framework for working with tensors and computational graphs . It provides tensor primitives as well as the ability to define transformations on them. Under the hood, these transformations are baked into a computational graph . Obtaining results of computations now becomes a job of evaluating the relevant nodes of these graphs. courtesy Wikipedia. It turns out that representing computation in this manner is advantageous when you need to compute derivatives of arbitrary functions with respect to any inputs. Tensorflow has the ability to evaluate/execute computational graphs on any hardware architecture, freeing the user from worrying about those details. The tensorflow API is roughly divided into two levels. Low level: Tensor primitives, variables, placeholders, constants, computational graphs. High level: Models, estimators etc. Tensorflow for Scala \u00b6 The tensorflow for scala library provides scala users with access to the low as well as high level API's. Among its packages include. Low level: variables, tensors, placeholders, constants, computational graphs High level: layers, models, estimators, etc DynaML Tensorflow \u00b6 DynaML interfaces with the tensorflow scala API and provides a number of convenience features. The tensorflow pointer dtf Neural network building blocks dtflearn Supporting utilities Data pipes acting on tensorflow based data, dtfpipe The dynaml.tensorflow.utils package. Miscellaneous utilities, dtfutils .","title":"Introduction"},{"location":"core/core_dynaml_tf/#google-tensorflow","text":"courtesy Google. Tensorflow is a versatile and general computational framework for working with tensors and computational graphs . It provides tensor primitives as well as the ability to define transformations on them. Under the hood, these transformations are baked into a computational graph . Obtaining results of computations now becomes a job of evaluating the relevant nodes of these graphs. courtesy Wikipedia. It turns out that representing computation in this manner is advantageous when you need to compute derivatives of arbitrary functions with respect to any inputs. Tensorflow has the ability to evaluate/execute computational graphs on any hardware architecture, freeing the user from worrying about those details. The tensorflow API is roughly divided into two levels. Low level: Tensor primitives, variables, placeholders, constants, computational graphs. High level: Models, estimators etc.","title":"Google Tensorflow"},{"location":"core/core_dynaml_tf/#tensorflow-for-scala","text":"The tensorflow for scala library provides scala users with access to the low as well as high level API's. Among its packages include. Low level: variables, tensors, placeholders, constants, computational graphs High level: layers, models, estimators, etc","title":"Tensorflow for Scala"},{"location":"core/core_dynaml_tf/#dynaml-tensorflow","text":"DynaML interfaces with the tensorflow scala API and provides a number of convenience features. The tensorflow pointer dtf Neural network building blocks dtflearn Supporting utilities Data pipes acting on tensorflow based data, dtfpipe The dynaml.tensorflow.utils package. Miscellaneous utilities, dtfutils .","title":"DynaML Tensorflow"},{"location":"core/core_esgp/","text":"Summary The Extended Skew Gaussian Process (ESGP) uses the MESN distribution to define its finite dimensional probability distribution. It can be viewed as an generalization of the Gaussian Process because when its skewness parameter approaches zero, the calculated probabilities are very close to gaussian probabilities. The ESGP model uses the conditioning property of the MESN distribution, just like the multivariate normal distribution, the MESN retains its form when conditioned on a subset of its dimensions. Creating an ESGP model is very similar to creating a GP model in DynaML. The class ESGPModel [ T , I ] can be instantiated much like the AbstractGPRegressionModel [ T , I ] , using the apply method. //Obtain the data, some generic type val trainingdata : DataType = ... val kernel : LocalScalarKernel [ I ] = _ val noiseKernel : LocalScalarKernel [ I ] = _ val meanFunc : DataPipe [ I , Double ] = _ val lambda = 1.5 val tau = 0.5 //Define how the data is converted to a compatible type implicit val transform : DataPipe [ DataType , Seq [( I , Double )]] = _ val model = ESGPModel ( kernel , noiseKernel , meanFunc , lambda , tau , trainingData )","title":"Extended Skew Gaussian Processes"},{"location":"core/core_ffn_new/","text":"Feed forward neural networks Feed forward neural networks are the most common network architectures in predictive modeling, DynaML has an implementation of feed forward architectures that is trained using Backpropogation with momentum. In a feed forward neural network with a single hidden layer the predicted target y y is expressed using the edge weights and node values in the following manner (this expression is easily extended for multi-layer nets). \\begin{equation} y = W_2 \\sigma(W_1 \\mathbf{x} + b_1) + b_2 \\end{equation} \\begin{equation} y = W_2 \\sigma(W_1 \\mathbf{x} + b_1) + b_2 \\end{equation} Where W_1 , \\ W_2 W_1 , \\ W_2 are matrices representing edge weights for the hidden layer and output layer respectively and \\sigma(.) \\sigma(.) represents a monotonic activation function, the usual choices are sigmoid , tanh , linear or rectified linear functions. The new neural network API extends the same top level traits as the old API, i.e. NeuralNet [ Data , BaseGraph , Input , Output , Graph <: NeuralGraph [ BaseGraph , Input , Output ]] which itself extends the ParameterizedLearner [ Data , Graph , Input , Output , Stream [( Input , Output )]] trait. Tip To learn more about ParameterizedLearner and other major model classes, refer to the model hierarchy specification. In the case of NeuralNet , the parameters are a generic (unknown) type Graph which has to be an extension of NeuralGraph [ BaseGraph , Input , Output ] ] trait. Creating and training feed forward networks can be done by creating a back propagation instance and preparing the training data. Tip For a more in-depth picture of how the neural network API works refer to the neural stack page. //Data is of some generic type val data : DataType = _ //specify how this data can be //converted to a sequence of input and output vectors. val transform : DataPipe [ DataType , Seq [( DenseVector [ Double ] , DenseVector [ Double ])]] = _ //Create the stack factory //and back propagation instance //Input, Hidden, Output val breezeStackFactory = NeuralStackFactory ( Seq ( 5 , 8 , 3 ))( Seq ( VectorSigmoid , VectorTansig ) ) //Random variable which samples layer weights val stackInitializer = GenericFFNeuralNet . getWeightInitializer ( Seq ( 5 , 8 , 3 ) ) val opt_backprop = new FFBackProp ( breezeStackFactory ) . setNumIterations ( 2000 ) . setRegParam ( 0.001 ) . setStepSize ( 0.05 ) . setMiniBatchFraction ( 0.8 ) . momentum_ ( 0.3 ) val ff_neural_model = GenericFFNeuralNet ( opt_backprop , data , transform , stackInitializer ) //train the model ff_neural_model . learn ()","title":"Feed Forward Networks"},{"location":"core/core_glm/","text":"Summary Generalized Linear Models are a class of models which belong to the ordinary least squares framework. They generally consist of a set of parameters \\mathbf{w} \\mathbf{w} , a feature mapping \\varphi() \\varphi() and a link function which dictates how the probability distribution of the output quantity is described. Generalized Linear Models (GLM) are available in the context of regression and binary classification, more specifically in DynaML the following members of the GLM family are implemented. The GeneralizedLinearModel [ T ] class is the base of the GLM hierarchy in DynaML, all linear models are extensions of it. It's companion object is used for the creation of GLM instances as follows. val data : Stream [( DenseVector [ Double ] , Double )] = ... //The task variable is a string which is set to \"regression\" or \"classification\" val task = ... //The map variable defines a possibly higher dimensional function of the input //which is akin to a basis function representation of the original features val map : DenseVector [ Double ] => DenseVector [ Double ] = ... //modeltype is set to \"logit\" or \"probit\" //if one wishes to create a binary classification model, //depending on the classification model involved val modeltype = \"logit\" val glm = GeneralizedLinearModel ( data , task , map , modeltype ) Normal GLM \u00b6 The most common regression model, also known as least squares linear regression , implemented as the class RegularizedGLM which represents a regression model with the following prediction: \\begin{equation} y \\ | \\ \\mathbf{x} \\sim \\mathcal{N}(w^T \\varphi(\\mathbf{x}), \\sigma^{2}) \\end{equation} \\begin{equation} y \\ | \\ \\mathbf{x} \\sim \\mathcal{N}(w^T \\varphi(\\mathbf{x}), \\sigma^{2}) \\end{equation} Here \\varphi(.) \\varphi(.) is an appropriately chosen set of basis functions . The inference problem is formulated as \\begin{equation} \\min_{w} \\ \\mathcal{J}_P(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2 \\end{equation} \\begin{equation} \\min_{w} \\ \\mathcal{J}_P(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2 \\end{equation} Logit GLM \u00b6 In binary classification the most common GLM used is the logistic regression model which is given by $$ \\begin{equation} P(y = 1 | \\mathbf{x}) = \\sigma(w^T \\varphi(\\mathbf{x}) + b) \\end{equation} $$ Where \\sigma(z) = \\frac{1}{1 + exp(-z)} \\sigma(z) = \\frac{1}{1 + exp(-z)} is the logistic function which maps the output of the linear function w^T \\varphi(\\mathbf{x}) + b w^T \\varphi(\\mathbf{x}) + b to a probability value. Probit GLM \u00b6 The probit regression model is an alternative to the logit model it is represented as: $$ \\begin{equation} P(y = 1 | \\mathbf{x}) = \\Phi(w^T \\varphi(\\mathbf{x}) + b) \\end{equation} $$ Where \\Phi(z) \\Phi(z) is the cumulative distribution function of the standard normal distribution. GLS The Generalized Least Squares model which is a more broad formulation of the Ordinary Least Squares (OLS) regression model.","title":"Generalized Linear Models"},{"location":"core/core_glm/#normal-glm","text":"The most common regression model, also known as least squares linear regression , implemented as the class RegularizedGLM which represents a regression model with the following prediction: \\begin{equation} y \\ | \\ \\mathbf{x} \\sim \\mathcal{N}(w^T \\varphi(\\mathbf{x}), \\sigma^{2}) \\end{equation} \\begin{equation} y \\ | \\ \\mathbf{x} \\sim \\mathcal{N}(w^T \\varphi(\\mathbf{x}), \\sigma^{2}) \\end{equation} Here \\varphi(.) \\varphi(.) is an appropriately chosen set of basis functions . The inference problem is formulated as \\begin{equation} \\min_{w} \\ \\mathcal{J}_P(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2 \\end{equation} \\begin{equation} \\min_{w} \\ \\mathcal{J}_P(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2 \\end{equation}","title":"Normal GLM"},{"location":"core/core_glm/#logit-glm","text":"In binary classification the most common GLM used is the logistic regression model which is given by $$ \\begin{equation} P(y = 1 | \\mathbf{x}) = \\sigma(w^T \\varphi(\\mathbf{x}) + b) \\end{equation} $$ Where \\sigma(z) = \\frac{1}{1 + exp(-z)} \\sigma(z) = \\frac{1}{1 + exp(-z)} is the logistic function which maps the output of the linear function w^T \\varphi(\\mathbf{x}) + b w^T \\varphi(\\mathbf{x}) + b to a probability value.","title":"Logit GLM"},{"location":"core/core_glm/#probit-glm","text":"The probit regression model is an alternative to the logit model it is represented as: $$ \\begin{equation} P(y = 1 | \\mathbf{x}) = \\Phi(w^T \\varphi(\\mathbf{x}) + b) \\end{equation} $$ Where \\Phi(z) \\Phi(z) is the cumulative distribution function of the standard normal distribution. GLS The Generalized Least Squares model which is a more broad formulation of the Ordinary Least Squares (OLS) regression model.","title":"Probit GLM"},{"location":"core/core_gls/","text":"Summary The Generalized Least Squares model is a regression formulation which does not assume that the model errors/residuals are independent of each other. Rather it borrows from the Gaussian Process paradigm and assigns a covariance structure to the model residuals. Warning The nomenclature Generalized Least Squares (GLS) and Generalized Linear Models (GLM) can cause much confusion. It is important to remember the context of both. GLS refers to relaxing of the independence of residuals assumption while GLM refers to Ordinary Least Squares OLS based models which are extended to model regression, counts, or classification tasks. Formulation. \u00b6 Let \\mathbf{X} \\in \\mathbb{R}^{n\\times m} \\mathbf{X} \\in \\mathbb{R}^{n\\times m} be a matrix containing data attributes. The GLS model builds a linear predictor of the target quantity of the following form. \\begin{equation} \\mathbf {y} = \\varphi(\\mathbf {X}) \\mathbf {\\beta } +\\mathbf {\\varepsilon } \\end{equation} \\begin{equation} \\mathbf {y} = \\varphi(\\mathbf {X}) \\mathbf {\\beta } +\\mathbf {\\varepsilon } \\end{equation} Where \\varphi(.): \\mathbb{R}^m \\rightarrow \\mathbb{R}^d \\varphi(.): \\mathbb{R}^m \\rightarrow \\mathbb{R}^d is a feature mapping, \\mathbf{y} \\in \\mathbb{R}^n \\mathbf{y} \\in \\mathbb{R}^n is the vector of output values found in the training data set and \\mathbf{\\beta} \\in \\mathbb{R}^d \\mathbf{\\beta} \\in \\mathbb{R}^d is a set of regression parameters. In the GLS framework, it is assumed that the model errors \\varepsilon \\in \\mathbb{R}^n \\varepsilon \\in \\mathbb{R}^n follow a multivariate gaussian distribution given by \\mathbb {E} [\\varepsilon |\\mathbf {X} ] = 0 \\mathbb {E} [\\varepsilon |\\mathbf {X} ] = 0 and \\operatorname{Var} [\\varepsilon |\\mathbf {X} ] = \\mathbf {\\Omega } \\operatorname{Var} [\\varepsilon |\\mathbf {X} ] = \\mathbf {\\Omega } , where \\mathbf{\\Omega} \\mathbf{\\Omega} is a symmetric positive semi-definite covariance matrix. In order to calculate the model parameters \\mathbf{\\beta} \\mathbf{\\beta} , the log-likelihood of the training data outputs must be maximized with respect to the parameters \\mathbf{\\beta} \\mathbf{\\beta} , which leads to. \\begin{equation} \\min_{\\mathbf{\\beta}} \\ \\mathcal{J}_P(\\mathbf{\\beta}) = (\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )^{\\mathtt {T}}\\,\\mathbf {\\Omega } ^{-1}(\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} ) \\end{equation} \\begin{equation} \\min_{\\mathbf{\\beta}} \\ \\mathcal{J}_P(\\mathbf{\\beta}) = (\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )^{\\mathtt {T}}\\,\\mathbf {\\Omega } ^{-1}(\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} ) \\end{equation} For the GLS problem the analytical solution of the above optimization problem can be calculated. {\\displaystyle \\mathbf {\\hat {\\beta }} =\\left(\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\varphi(\\mathbf {X})\\right)^{-1}\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\mathbf {y} .} {\\displaystyle \\mathbf {\\hat {\\beta }} =\\left(\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\varphi(\\mathbf {X})\\right)^{-1}\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\mathbf {y} .} GLS Models \u00b6 You can create a GLS model using the GeneralizedLeastSquaresModel class. //Get the training data val data : Stream [( DenseVector [ Double ] , Double )] = _ //Define a feature mapping //If it is not defined the GLS model //will assume a identity feature map. val feature_map : DenseVector [ Double ] => DenseVector [ Double ] = _ //Initialize a kernel function. val kernel : LocalScalarKernel [ DenseVector [ Double ]] = _ //Construct the covariance matrix for model errors. val covmat = kernel . buildBlockedKernelMatrix ( data , data . length ) val gls_model = new GeneralizedLeastSquaresModel ( data , covmat , feature_map ) //Train the model gls_model . learn ()","title":"Generalized Least Squares"},{"location":"core/core_gls/#formulation","text":"Let \\mathbf{X} \\in \\mathbb{R}^{n\\times m} \\mathbf{X} \\in \\mathbb{R}^{n\\times m} be a matrix containing data attributes. The GLS model builds a linear predictor of the target quantity of the following form. \\begin{equation} \\mathbf {y} = \\varphi(\\mathbf {X}) \\mathbf {\\beta } +\\mathbf {\\varepsilon } \\end{equation} \\begin{equation} \\mathbf {y} = \\varphi(\\mathbf {X}) \\mathbf {\\beta } +\\mathbf {\\varepsilon } \\end{equation} Where \\varphi(.): \\mathbb{R}^m \\rightarrow \\mathbb{R}^d \\varphi(.): \\mathbb{R}^m \\rightarrow \\mathbb{R}^d is a feature mapping, \\mathbf{y} \\in \\mathbb{R}^n \\mathbf{y} \\in \\mathbb{R}^n is the vector of output values found in the training data set and \\mathbf{\\beta} \\in \\mathbb{R}^d \\mathbf{\\beta} \\in \\mathbb{R}^d is a set of regression parameters. In the GLS framework, it is assumed that the model errors \\varepsilon \\in \\mathbb{R}^n \\varepsilon \\in \\mathbb{R}^n follow a multivariate gaussian distribution given by \\mathbb {E} [\\varepsilon |\\mathbf {X} ] = 0 \\mathbb {E} [\\varepsilon |\\mathbf {X} ] = 0 and \\operatorname{Var} [\\varepsilon |\\mathbf {X} ] = \\mathbf {\\Omega } \\operatorname{Var} [\\varepsilon |\\mathbf {X} ] = \\mathbf {\\Omega } , where \\mathbf{\\Omega} \\mathbf{\\Omega} is a symmetric positive semi-definite covariance matrix. In order to calculate the model parameters \\mathbf{\\beta} \\mathbf{\\beta} , the log-likelihood of the training data outputs must be maximized with respect to the parameters \\mathbf{\\beta} \\mathbf{\\beta} , which leads to. \\begin{equation} \\min_{\\mathbf{\\beta}} \\ \\mathcal{J}_P(\\mathbf{\\beta}) = (\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )^{\\mathtt {T}}\\,\\mathbf {\\Omega } ^{-1}(\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} ) \\end{equation} \\begin{equation} \\min_{\\mathbf{\\beta}} \\ \\mathcal{J}_P(\\mathbf{\\beta}) = (\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} )^{\\mathtt {T}}\\,\\mathbf {\\Omega } ^{-1}(\\mathbf {y} - \\varphi(\\mathbf {X}) \\mathbf {\\beta} ) \\end{equation} For the GLS problem the analytical solution of the above optimization problem can be calculated. {\\displaystyle \\mathbf {\\hat {\\beta }} =\\left(\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\varphi(\\mathbf {X})\\right)^{-1}\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\mathbf {y} .} {\\displaystyle \\mathbf {\\hat {\\beta }} =\\left(\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\varphi(\\mathbf {X})\\right)^{-1}\\varphi(\\mathbf {X}) ^{\\mathtt {T}}\\mathbf {\\Omega } ^{-1}\\mathbf {y} .}","title":"Formulation."},{"location":"core/core_gls/#gls-models","text":"You can create a GLS model using the GeneralizedLeastSquaresModel class. //Get the training data val data : Stream [( DenseVector [ Double ] , Double )] = _ //Define a feature mapping //If it is not defined the GLS model //will assume a identity feature map. val feature_map : DenseVector [ Double ] => DenseVector [ Double ] = _ //Initialize a kernel function. val kernel : LocalScalarKernel [ DenseVector [ Double ]] = _ //Construct the covariance matrix for model errors. val covmat = kernel . buildBlockedKernelMatrix ( data , data . length ) val gls_model = new GeneralizedLeastSquaresModel ( data , covmat , feature_map ) //Train the model gls_model . learn ()","title":"GLS Models"},{"location":"core/core_gp/","text":"Gaussian Processes are stochastic processes whose finite dimensional distributions are multivariate gaussians. Gaussian Processes are powerful non-parametric predictive models, which represent probability measures over spaces of functions. Ramussen and Williams is the definitive guide on understanding their applications in machine learning and a gateway to their deeper theoretical foundations. Gaussian Process models are well supported in DynaML, the AbstractGPRegressionModel [ T , I ] and AbstractGPClassification [ T , I ] classes which extend the StochasticProcessModel [ T , I , Y , W ] base trait are the starting point for all GP implementations. Gaussian Process Regression \u00b6 The GP regression framework aims to infer an unknown function f(x) f(x) given y_i y_i which are noise corrupted observations of this unknown function. This is done by adopting an explicit probabilistic formulation to the multi-variate distribution of the noise corrupted observations y_i y_i conditioned on the input features (or design matrix) X X \\begin{align} & y = f(x) + \\epsilon \\\\ & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\ & \\left(\\mathbf{y} \\ \\ \\mathbf{f_*} \\right)^T | X \\sim \\mathcal{N}\\left(\\mathbf{0}, \\left[ \\begin{matrix} K(X, X) + \\sigma^{2} \\it{I} & K(X, X_*) \\\\ K(X_*, X) & K(X_*, X_*) \\end{matrix} \\right ] \\right) \\end{align} \\begin{align} & y = f(x) + \\epsilon \\\\ & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\ & \\left(\\mathbf{y} \\ \\ \\mathbf{f_*} \\right)^T | X \\sim \\mathcal{N}\\left(\\mathbf{0}, \\left[ \\begin{matrix} K(X, X) + \\sigma^{2} \\it{I} & K(X, X_*) \\\\ K(X_*, X) & K(X_*, X_*) \\end{matrix} \\right ] \\right) \\end{align} In the presence of training data X = (x_1, x_2, \\cdot , x_n) \\ \\mathbf{y} = (y_1, y_2, \\cdot , y_n) X = (x_1, x_2, \\cdot , x_n) \\ \\mathbf{y} = (y_1, y_2, \\cdot , y_n) Inference is carried out by calculating the posterior predictive distribution over the unknown targets \\mathbf{f_*}|X,\\mathbf{y},X_* \\mathbf{f_*}|X,\\mathbf{y},X_* assuming X_* X_* , the test inputs are known. \\begin{align} & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim \\mathcal{N}(\\mathbf{\\bar{f_*}}, cov(\\mathbf{f_*})) \\label{eq:posterior}\\\\ & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\ & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*) \\end{align} \\begin{align} & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim \\mathcal{N}(\\mathbf{\\bar{f_*}}, cov(\\mathbf{f_*})) \\label{eq:posterior}\\\\ & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\ & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*) \\end{align} GP models for a single output \u00b6 For univariate GP models (single output), use the GPRegression class (an extension of AbstractGPRegressionModel ). To construct a GP regression model you would need: Training data Kernel/covariance instance to model correlation between values of the latent function at each pair of input features. Kernel instance to model the correlation of the additive noise, generally the DiracKernel (white noise) is used. val trainingdata : Stream [( DenseVector [ Double ] , Double )] = ... val num_features = trainingdata . head . _1 . length // Create an implicit vector field for the creation of the stationary // radial basis function kernel implicit val field = VectorField ( num_features ) val kernel = new RBFKernel ( 2.5 ) val noiseKernel = new DiracKernel ( 1.5 ) val model = new GPRegression ( kernel , noiseKernel , trainingData ) GP models for multiple outputs \u00b6 As reviewed in Lawrence et.al , Gaussian Processes for multiple outputs can be interpreted as single output GP models with an expanded index set. Recall that GPs are stochastic processes and thus are defined on some index set , for example in the equations above it is noted that x \\in \\mathbb{R}^p x \\in \\mathbb{R}^p making \\mathbb{R}^p \\mathbb{R}^p the index set of the process. In case of multiple outputs the index set is expressed as a cartesian product x \\in \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\} x \\in \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\} , where d d is the number of outputs to be modeled. It needs to be noted that now we will also have to define the kernel function on the same index set i.e. \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\} \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\} . In multi-output GP literature a common way to construct kernels on such index sets is to multiply base kernels on each of the parts \\mathbb{R}^p \\mathbb{R}^p and \\{1,2,\\cdots,d\\} \\{1,2,\\cdots,d\\} , such kernels are known as separable kernels . \\begin{equation} K((\\mathbf{x}, d), (\\mathbf{x}', d')) = K_{x}(\\mathbf{x}, \\mathbf{x}') . K_{d}(d, d') \\end{equation} \\begin{equation} K((\\mathbf{x}, d), (\\mathbf{x}', d')) = K_{x}(\\mathbf{x}, \\mathbf{x}') . K_{d}(d, d') \\end{equation} Taking this idea further sum of separable kernels (SoS) are often employed in multi-output GP models. These models are also known as Linear Models of Co-Regionalization (LMC) and the kernels which encode correlation between the outputs K_d(.,.) K_d(.,.) are known as co-regionalization kernels . \\begin{equation} K((\\mathbf{x}, d), (\\mathbf{x}', d')) = \\sum_{i = 1}^{D} K^{i}_{x}(\\mathbf{x}, \\mathbf{x}') . K^{i}_{d}(d, d') \\end{equation} \\begin{equation} K((\\mathbf{x}, d), (\\mathbf{x}', d')) = \\sum_{i = 1}^{D} K^{i}_{x}(\\mathbf{x}, \\mathbf{x}') . K^{i}_{d}(d, d') \\end{equation} Creating separable kernels Creating SoS kernels in DynaML is quite straightforward, use the :* operator to multiply a kernel defined on DenseVector [ Double ] with a kernel defined on Int . val linearK = new PolynomialKernel ( 2 , 1.0 ) val tKernel = new TStudentKernel ( 0.2 ) val d = new DiracKernel ( 0.037 ) val mixedEffects = new MixedEffectRegularizer ( 0.5 ) val coRegCauchyMatrix = new CoRegCauchyKernel ( 10.0 ) val coRegDiracMatrix = new CoRegDiracKernel val sos_kernel : CompositeCovariance [( DenseVector [ Double ] , Int )] = ( linearK :* mixedEffects ) + ( tKernel :* coRegCauchyMatrix ) val sos_noise : CompositeCovariance [( DenseVector [ Double ] , Int )] = d :* coRegDiracMatrix Tip You can use the MOGPRegressionModel [ I ] class to create multi-output GP models. val trainingdata : Stream [( DenseVector [ Double ] , DenseVector [ Double ])] = _ val model = new MOGPRegressionModel [ DenseVector [ Double ]]( sos_kernel , sos_noise , trainingdata , trainingdata . length , trainingdata . head . _2 . length ) Gaussian Process Binary Classification \u00b6 Gaussian process models for classification are formulated using two components. A latent (nuisance) function f(x) f(x) A transfer function \\sigma(.) \\sigma(.) which transforms the value f(x) f(x) to a class probability \\begin{align} & \\pi(x) \\overset{\\triangle}{=} p(y = +1| x) = \\sigma(f(x)) \\\\ & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\ \\end{align} \\begin{align} & \\pi(x) \\overset{\\triangle}{=} p(y = +1| x) = \\sigma(f(x)) \\\\ & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\ \\end{align} Inference is divided into two steps. Computing the distribution of the latent function corresponding to a test case \\begin{align} & p(f_*|X, \\mathbf{y}, x_*) = \\int p(f_*|X, \\mathbf{y}, x_*, \\mathbf{f}) p(\\mathbf{f}|X, \\mathbf{y}) d\\mathbf{f} \\\\ & p(\\mathbf{f}|X, \\mathbf{y}) = p(\\mathbf{y}| \\mathbf{f}) p(\\mathbf{f}|X)/ p(\\mathbf{y}|X) \\end{align} \\begin{align} & p(f_*|X, \\mathbf{y}, x_*) = \\int p(f_*|X, \\mathbf{y}, x_*, \\mathbf{f}) p(\\mathbf{f}|X, \\mathbf{y}) d\\mathbf{f} \\\\ & p(\\mathbf{f}|X, \\mathbf{y}) = p(\\mathbf{y}| \\mathbf{f}) p(\\mathbf{f}|X)/ p(\\mathbf{y}|X) \\end{align} Generating probabilistic prediction for a test case. \\bar{\\pi_*} \\overset{\\triangle}{=} p(y_* = +1| X, \\mathbf{y}, x_*) = \\int \\sigma(f_*) p(f_*|X, \\mathbf{y}, x_*) df_* \\bar{\\pi_*} \\overset{\\triangle}{=} p(y_* = +1| X, \\mathbf{y}, x_*) = \\int \\sigma(f_*) p(f_*|X, \\mathbf{y}, x_*) df_* val trainingdata : Stream [( DenseVector [ Double ] , Double )] = ... val num_features = trainingdata . head . _1 . length // Create an implicit vector field for the creation of the stationary // radial basis function kernel implicit val field = VectorField ( num_features ) val kernel = new RBFKernel ( 2.5 ) val likelihood = new VectorIIDSigmoid () val model = new LaplaceBinaryGPC ( trainingData , kernel , likelihood ) Extending The GP Class \u00b6 In case you want to customize the implementation of Gaussian Process models in DynaML, you can do so by extending the GP abstract skeleton GaussianProcessModel and using your own data structures for the type parameters I and T . import breeze.linalg._ import io.github.mandar2812.dynaml.pipes._ import io.github.mandar2812.dynaml.probability._ import io.github.mandar2812.dynaml.models.gp._ import io.github.mandar2812.dynaml.kernels._ class MyGPRegressionModel [ T , I ]( cov : LocalScalarKernel [ I ], n : LocalScalarKernel [ I ], data : T , num : Int , mean : DataPipe [ I , Double ]) extends AbstractGPRegressionModel [ T , I ]( cov , n , data , num , mean ) { override val covariance = cov override val noiseModel = n override protected val g : T = data /** * Convert from the underlying data structure to * Seq[(I, Y)] where I is the index set of the GP * and Y is the value/label type. **/ override def dataAsSeq ( data : T ) : Seq [( I , Double )] = ??? /** * Calculates the energy of the configuration, * in most global optimization algorithms * we aim to find an approximate value of * the hyper-parameters such that this function * is minimized. * * @param h The value of the hyper-parameters in * the configuration space * @param options Optional parameters * * @return Configuration Energy E(h) * * In this particular case E(h) = -log p(Y|X,h) * also known as log likelihood. **/ override def energy ( h : Map [ String , Double ], options : Map [ String , String ]) : Double = ??? /** * Calculates the gradient energy of the configuration and * subtracts this from the current value of h to yield a new * hyper-parameter configuration. * * Over ride this function if you aim to implement a * gradient based hyper-parameter optimization routine * like ML-II * * @param h The value of the hyper-parameters in the * configuration space * @return Gradient of the objective function * (marginal likelihood) as a Map **/ override def gradEnergy ( h : Map [ String , Double ]) : Map [ String , Double ] = ??? /** * Calculates posterior predictive distribution for * a particular set of test data points. * * @param test A Sequence or Sequence like data structure * storing the values of the input patters. **/ override def predictiveDistribution [ U <: Seq [ I ]]( test : U ) : MultGaussianPRV = ??? //Use the predictive distribution to generate a point prediction override def predict ( point : I ) : Double = ??? }","title":"Gaussian Processes"},{"location":"core/core_gp/#gaussian-process-regression","text":"The GP regression framework aims to infer an unknown function f(x) f(x) given y_i y_i which are noise corrupted observations of this unknown function. This is done by adopting an explicit probabilistic formulation to the multi-variate distribution of the noise corrupted observations y_i y_i conditioned on the input features (or design matrix) X X \\begin{align} & y = f(x) + \\epsilon \\\\ & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\ & \\left(\\mathbf{y} \\ \\ \\mathbf{f_*} \\right)^T | X \\sim \\mathcal{N}\\left(\\mathbf{0}, \\left[ \\begin{matrix} K(X, X) + \\sigma^{2} \\it{I} & K(X, X_*) \\\\ K(X_*, X) & K(X_*, X_*) \\end{matrix} \\right ] \\right) \\end{align} \\begin{align} & y = f(x) + \\epsilon \\\\ & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\ & \\left(\\mathbf{y} \\ \\ \\mathbf{f_*} \\right)^T | X \\sim \\mathcal{N}\\left(\\mathbf{0}, \\left[ \\begin{matrix} K(X, X) + \\sigma^{2} \\it{I} & K(X, X_*) \\\\ K(X_*, X) & K(X_*, X_*) \\end{matrix} \\right ] \\right) \\end{align} In the presence of training data X = (x_1, x_2, \\cdot , x_n) \\ \\mathbf{y} = (y_1, y_2, \\cdot , y_n) X = (x_1, x_2, \\cdot , x_n) \\ \\mathbf{y} = (y_1, y_2, \\cdot , y_n) Inference is carried out by calculating the posterior predictive distribution over the unknown targets \\mathbf{f_*}|X,\\mathbf{y},X_* \\mathbf{f_*}|X,\\mathbf{y},X_* assuming X_* X_* , the test inputs are known. \\begin{align} & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim \\mathcal{N}(\\mathbf{\\bar{f_*}}, cov(\\mathbf{f_*})) \\label{eq:posterior}\\\\ & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\ & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*) \\end{align} \\begin{align} & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim \\mathcal{N}(\\mathbf{\\bar{f_*}}, cov(\\mathbf{f_*})) \\label{eq:posterior}\\\\ & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\ & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*) \\end{align}","title":"Gaussian Process Regression"},{"location":"core/core_gp/#gp-models-for-a-single-output","text":"For univariate GP models (single output), use the GPRegression class (an extension of AbstractGPRegressionModel ). To construct a GP regression model you would need: Training data Kernel/covariance instance to model correlation between values of the latent function at each pair of input features. Kernel instance to model the correlation of the additive noise, generally the DiracKernel (white noise) is used. val trainingdata : Stream [( DenseVector [ Double ] , Double )] = ... val num_features = trainingdata . head . _1 . length // Create an implicit vector field for the creation of the stationary // radial basis function kernel implicit val field = VectorField ( num_features ) val kernel = new RBFKernel ( 2.5 ) val noiseKernel = new DiracKernel ( 1.5 ) val model = new GPRegression ( kernel , noiseKernel , trainingData )","title":"GP models for a single output"},{"location":"core/core_gp/#gp-models-for-multiple-outputs","text":"As reviewed in Lawrence et.al , Gaussian Processes for multiple outputs can be interpreted as single output GP models with an expanded index set. Recall that GPs are stochastic processes and thus are defined on some index set , for example in the equations above it is noted that x \\in \\mathbb{R}^p x \\in \\mathbb{R}^p making \\mathbb{R}^p \\mathbb{R}^p the index set of the process. In case of multiple outputs the index set is expressed as a cartesian product x \\in \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\} x \\in \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\} , where d d is the number of outputs to be modeled. It needs to be noted that now we will also have to define the kernel function on the same index set i.e. \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\} \\mathbb{R}^{p} \\times \\{1,2, \\cdots, d \\} . In multi-output GP literature a common way to construct kernels on such index sets is to multiply base kernels on each of the parts \\mathbb{R}^p \\mathbb{R}^p and \\{1,2,\\cdots,d\\} \\{1,2,\\cdots,d\\} , such kernels are known as separable kernels . \\begin{equation} K((\\mathbf{x}, d), (\\mathbf{x}', d')) = K_{x}(\\mathbf{x}, \\mathbf{x}') . K_{d}(d, d') \\end{equation} \\begin{equation} K((\\mathbf{x}, d), (\\mathbf{x}', d')) = K_{x}(\\mathbf{x}, \\mathbf{x}') . K_{d}(d, d') \\end{equation} Taking this idea further sum of separable kernels (SoS) are often employed in multi-output GP models. These models are also known as Linear Models of Co-Regionalization (LMC) and the kernels which encode correlation between the outputs K_d(.,.) K_d(.,.) are known as co-regionalization kernels . \\begin{equation} K((\\mathbf{x}, d), (\\mathbf{x}', d')) = \\sum_{i = 1}^{D} K^{i}_{x}(\\mathbf{x}, \\mathbf{x}') . K^{i}_{d}(d, d') \\end{equation} \\begin{equation} K((\\mathbf{x}, d), (\\mathbf{x}', d')) = \\sum_{i = 1}^{D} K^{i}_{x}(\\mathbf{x}, \\mathbf{x}') . K^{i}_{d}(d, d') \\end{equation} Creating separable kernels Creating SoS kernels in DynaML is quite straightforward, use the :* operator to multiply a kernel defined on DenseVector [ Double ] with a kernel defined on Int . val linearK = new PolynomialKernel ( 2 , 1.0 ) val tKernel = new TStudentKernel ( 0.2 ) val d = new DiracKernel ( 0.037 ) val mixedEffects = new MixedEffectRegularizer ( 0.5 ) val coRegCauchyMatrix = new CoRegCauchyKernel ( 10.0 ) val coRegDiracMatrix = new CoRegDiracKernel val sos_kernel : CompositeCovariance [( DenseVector [ Double ] , Int )] = ( linearK :* mixedEffects ) + ( tKernel :* coRegCauchyMatrix ) val sos_noise : CompositeCovariance [( DenseVector [ Double ] , Int )] = d :* coRegDiracMatrix Tip You can use the MOGPRegressionModel [ I ] class to create multi-output GP models. val trainingdata : Stream [( DenseVector [ Double ] , DenseVector [ Double ])] = _ val model = new MOGPRegressionModel [ DenseVector [ Double ]]( sos_kernel , sos_noise , trainingdata , trainingdata . length , trainingdata . head . _2 . length )","title":"GP models for multiple outputs"},{"location":"core/core_gp/#gaussian-process-binary-classification","text":"Gaussian process models for classification are formulated using two components. A latent (nuisance) function f(x) f(x) A transfer function \\sigma(.) \\sigma(.) which transforms the value f(x) f(x) to a class probability \\begin{align} & \\pi(x) \\overset{\\triangle}{=} p(y = +1| x) = \\sigma(f(x)) \\\\ & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\ \\end{align} \\begin{align} & \\pi(x) \\overset{\\triangle}{=} p(y = +1| x) = \\sigma(f(x)) \\\\ & f \\sim \\mathcal{GP}(m(x), C(x,x')) \\\\ \\end{align} Inference is divided into two steps. Computing the distribution of the latent function corresponding to a test case \\begin{align} & p(f_*|X, \\mathbf{y}, x_*) = \\int p(f_*|X, \\mathbf{y}, x_*, \\mathbf{f}) p(\\mathbf{f}|X, \\mathbf{y}) d\\mathbf{f} \\\\ & p(\\mathbf{f}|X, \\mathbf{y}) = p(\\mathbf{y}| \\mathbf{f}) p(\\mathbf{f}|X)/ p(\\mathbf{y}|X) \\end{align} \\begin{align} & p(f_*|X, \\mathbf{y}, x_*) = \\int p(f_*|X, \\mathbf{y}, x_*, \\mathbf{f}) p(\\mathbf{f}|X, \\mathbf{y}) d\\mathbf{f} \\\\ & p(\\mathbf{f}|X, \\mathbf{y}) = p(\\mathbf{y}| \\mathbf{f}) p(\\mathbf{f}|X)/ p(\\mathbf{y}|X) \\end{align} Generating probabilistic prediction for a test case. \\bar{\\pi_*} \\overset{\\triangle}{=} p(y_* = +1| X, \\mathbf{y}, x_*) = \\int \\sigma(f_*) p(f_*|X, \\mathbf{y}, x_*) df_* \\bar{\\pi_*} \\overset{\\triangle}{=} p(y_* = +1| X, \\mathbf{y}, x_*) = \\int \\sigma(f_*) p(f_*|X, \\mathbf{y}, x_*) df_* val trainingdata : Stream [( DenseVector [ Double ] , Double )] = ... val num_features = trainingdata . head . _1 . length // Create an implicit vector field for the creation of the stationary // radial basis function kernel implicit val field = VectorField ( num_features ) val kernel = new RBFKernel ( 2.5 ) val likelihood = new VectorIIDSigmoid () val model = new LaplaceBinaryGPC ( trainingData , kernel , likelihood )","title":"Gaussian Process Binary Classification"},{"location":"core/core_gp/#extending-the-gp-class","text":"In case you want to customize the implementation of Gaussian Process models in DynaML, you can do so by extending the GP abstract skeleton GaussianProcessModel and using your own data structures for the type parameters I and T . import breeze.linalg._ import io.github.mandar2812.dynaml.pipes._ import io.github.mandar2812.dynaml.probability._ import io.github.mandar2812.dynaml.models.gp._ import io.github.mandar2812.dynaml.kernels._ class MyGPRegressionModel [ T , I ]( cov : LocalScalarKernel [ I ], n : LocalScalarKernel [ I ], data : T , num : Int , mean : DataPipe [ I , Double ]) extends AbstractGPRegressionModel [ T , I ]( cov , n , data , num , mean ) { override val covariance = cov override val noiseModel = n override protected val g : T = data /** * Convert from the underlying data structure to * Seq[(I, Y)] where I is the index set of the GP * and Y is the value/label type. **/ override def dataAsSeq ( data : T ) : Seq [( I , Double )] = ??? /** * Calculates the energy of the configuration, * in most global optimization algorithms * we aim to find an approximate value of * the hyper-parameters such that this function * is minimized. * * @param h The value of the hyper-parameters in * the configuration space * @param options Optional parameters * * @return Configuration Energy E(h) * * In this particular case E(h) = -log p(Y|X,h) * also known as log likelihood. **/ override def energy ( h : Map [ String , Double ], options : Map [ String , String ]) : Double = ??? /** * Calculates the gradient energy of the configuration and * subtracts this from the current value of h to yield a new * hyper-parameter configuration. * * Over ride this function if you aim to implement a * gradient based hyper-parameter optimization routine * like ML-II * * @param h The value of the hyper-parameters in the * configuration space * @return Gradient of the objective function * (marginal likelihood) as a Map **/ override def gradEnergy ( h : Map [ String , Double ]) : Map [ String , Double ] = ??? /** * Calculates posterior predictive distribution for * a particular set of test data points. * * @param test A Sequence or Sequence like data structure * storing the values of the input patters. **/ override def predictiveDistribution [ U <: Seq [ I ]]( test : U ) : MultGaussianPRV = ??? //Use the predictive distribution to generate a point prediction override def predict ( point : I ) : Double = ??? }","title":"Extending The GP Class"},{"location":"core/core_graphics/","text":"Summary The dynaml.graphics package is a new addition to the API since the v1.5.3 release. It aims to provide more unified access point to producing visualizations. 3D Plots \u00b6 Support for 3d visualisations is provided by the dynaml.graphics.plot3d , under the hood the plot3d object calls the Jzy3d library to produce interactive 3d plots. Producing 3d charts involves similar procedure for each chart kind, use plot3d.draw() to generate a chart object, and use plot3d.show() to display it in a GUI. Surface Plots \u00b6 The most common usage of plot3d is to visualise 3 dimensional surfaces, this can be done in two ways. From defined functions \u00b6 If the user can express the surface as a function of two arguments. import io.github.mandar2812.dynaml.graphics._ val mexican_hat = ( x : Double , y : Double ) => ( 1.0 / math . Pi )*( 1.0 - 0.5 *( x * x + y * y ))* math . exp (- 0.5 *( x * x + y * y )) val mexican_hat_chart = plot3d . draw ( mexican_hat ) plot3d . show ( mexican_hat_chart ) From a set of points \u00b6 If the surface is not determined completely, but only sampled over a discrete set of points, it is still possible to visualise an approximation to the surface, using Delauney triangulation . import io.github.mandar2812.dynaml.graphics._ import io.github.mandar2812.dynaml.probability._ //A function generating the surface need not be known, //as long as one has access to the sampled points. val func = ( x : Double , y : Double ) => math . sin ( x * y + y * x ) - math . cos ( y * y * x - x * x * y ) //Sample the 2d plane using a gaussian distribution val rv2d = GaussianRV ( 0.0 , 2.0 ) :* GaussianRV ( 0.0 , 2.0 ) //Generate some random points and their z values val points = rv2d . iid ( 1000 ). draw . map ( c => ( c , func ( c . _1 , c . _2 ))) val plot = plot3d . draw ( points ) plot3d . show ( plot ) Histograms \u00b6 The plot3d object also allows the user to visualise 3d histograms from collections of points on the 2d plane, expressed as a collection of (Double, Double) . import io.github.mandar2812.dynaml.graphics._ import io.github.mandar2812.dynaml.probability._ import io.github.mandar2812.dynaml.probability.distributions._ //Create two different random variables to //sample x and y coordinates. val rv_x = RandomVariable ( new SkewGaussian ( 2.0 , 0.0 , 1.0 )) val rv_y = RandomVariable ( new SkewGaussian (- 4.0 , 0.0 , 0.5 )) //Sample a point on the 2d plane val rv = rv_x :* rv_y val histogram = plot3d . draw ( rv . iid ( 2000 ). draw , 40 ) plot3d . show ( histogram )","title":"Graphics"},{"location":"core/core_graphics/#3d-plots","text":"Support for 3d visualisations is provided by the dynaml.graphics.plot3d , under the hood the plot3d object calls the Jzy3d library to produce interactive 3d plots. Producing 3d charts involves similar procedure for each chart kind, use plot3d.draw() to generate a chart object, and use plot3d.show() to display it in a GUI.","title":"3D Plots"},{"location":"core/core_graphics/#surface-plots","text":"The most common usage of plot3d is to visualise 3 dimensional surfaces, this can be done in two ways.","title":"Surface Plots"},{"location":"core/core_graphics/#from-defined-functions","text":"If the user can express the surface as a function of two arguments. import io.github.mandar2812.dynaml.graphics._ val mexican_hat = ( x : Double , y : Double ) => ( 1.0 / math . Pi )*( 1.0 - 0.5 *( x * x + y * y ))* math . exp (- 0.5 *( x * x + y * y )) val mexican_hat_chart = plot3d . draw ( mexican_hat ) plot3d . show ( mexican_hat_chart )","title":"From defined functions"},{"location":"core/core_graphics/#from-a-set-of-points","text":"If the surface is not determined completely, but only sampled over a discrete set of points, it is still possible to visualise an approximation to the surface, using Delauney triangulation . import io.github.mandar2812.dynaml.graphics._ import io.github.mandar2812.dynaml.probability._ //A function generating the surface need not be known, //as long as one has access to the sampled points. val func = ( x : Double , y : Double ) => math . sin ( x * y + y * x ) - math . cos ( y * y * x - x * x * y ) //Sample the 2d plane using a gaussian distribution val rv2d = GaussianRV ( 0.0 , 2.0 ) :* GaussianRV ( 0.0 , 2.0 ) //Generate some random points and their z values val points = rv2d . iid ( 1000 ). draw . map ( c => ( c , func ( c . _1 , c . _2 ))) val plot = plot3d . draw ( points ) plot3d . show ( plot )","title":"From a set of points"},{"location":"core/core_graphics/#histograms","text":"The plot3d object also allows the user to visualise 3d histograms from collections of points on the 2d plane, expressed as a collection of (Double, Double) . import io.github.mandar2812.dynaml.graphics._ import io.github.mandar2812.dynaml.probability._ import io.github.mandar2812.dynaml.probability.distributions._ //Create two different random variables to //sample x and y coordinates. val rv_x = RandomVariable ( new SkewGaussian ( 2.0 , 0.0 , 1.0 )) val rv_y = RandomVariable ( new SkewGaussian (- 4.0 , 0.0 , 0.5 )) //Sample a point on the 2d plane val rv = rv_x :* rv_y val histogram = plot3d . draw ( rv . iid ( 2000 ). draw , 40 ) plot3d . show ( histogram )","title":"Histograms"},{"location":"core/core_kernel_nonstat/","text":"Non-stationary covariance functions cannot be expressed as simply a function of the distance between their inputs \\mathbf{x} - \\mathbf{y} \\mathbf{x} - \\mathbf{y} . Locally Stationary Kernels \u00b6 A simple way to construct non-stationary covariances from stationary ones is by scaling the original stationary covariance; K(\\mathbf{x} - \\mathbf{y}) K(\\mathbf{x} - \\mathbf{y}) , by a function of \\mathbf{x} + \\mathbf{y} \\mathbf{x} + \\mathbf{y} . C(\\mathbf{x}, \\mathbf{y}) = G(\\mathbf{x} + \\mathbf{y}) K(\\mathbf{x} - \\mathbf{y}) C(\\mathbf{x}, \\mathbf{y}) = G(\\mathbf{x} + \\mathbf{y}) K(\\mathbf{x} - \\mathbf{y}) Here G(.): \\mathcal{X} \\rightarrow \\mathbb{R} G(.): \\mathcal{X} \\rightarrow \\mathbb{R} is a non-negative function of its inputs. These kernels are called locally stationary kernels . For an in-depth review of locally stationary kernels refer to Genton et. al . //Instantiate the base kernel val kernel : LocalScalarKernel [ I ] = _ val scalingFunction : ( I ) => Double = _ val scKernel = new LocallyStationaryKernel ( kernel , DataPipe ( scalingFunction )) Polynomial Kernel \u00b6 A very popular non-stationary kernel used in machine learning, the polynomial represents the data features as polynomial expansions up to an index d d . C(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^\\intercal \\mathbf{y} + a)^{d} C(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^\\intercal \\mathbf{y} + a)^{d} val fbm = new PolynomialKernel ( 2 , 0.99 ) Fractional Brownian Field (FBM) Kernel \u00b6 C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{2}\\left(||\\mathbf{x}||_{2}^{2H} + ||\\mathbf{y}||_{2}^{2H} - ||\\mathbf{x}-\\mathbf{y}||_{2}^{2H}\\right) C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{2}\\left(||\\mathbf{x}||_{2}^{2H} + ||\\mathbf{y}||_{2}^{2H} - ||\\mathbf{x}-\\mathbf{y}||_{2}^{2H}\\right) val fbm = new FBMKernel ( 0.99 ) The FBM kernel is the generalization of fractional Brownian motion to multi-variate index sets. Fractional Brownian motion is a stochastic process which is the generalization of Brownian motion, it was first studied by Mandelbrot and Von Ness . It is a self similar stochastic process, with stationary increments. However the process itself is non-stationary (as can be seen from the expression for the kernel) and has long range non vanishing covariance. Maximum Likelihood Perceptron Kernel \u00b6 The maximum likelihood perceptron (MLP) kernel, was first arrived at in Radford Neal's thesis , by considering the limiting case of a bayesian feed forward neural network with sigmoid activation. C(\\mathbf{x},\\mathbf{y}) = sin^{-1} \\left (\\frac{w \\mathbf{x}^\\intercal \\mathbf{y} + b}{(w \\mathbf{x}^\\intercal \\mathbf{x} + b) (w \\mathbf{y}^\\intercal \\mathbf{y} + b)} \\right ) C(\\mathbf{x},\\mathbf{y}) = sin^{-1} \\left (\\frac{w \\mathbf{x}^\\intercal \\mathbf{y} + b}{(w \\mathbf{x}^\\intercal \\mathbf{x} + b) (w \\mathbf{y}^\\intercal \\mathbf{y} + b)} \\right ) Neural Network Kernel \u00b6 Also a result of limiting case of bayesian neural networks, albeit with erf(.) erf(.) as the transfer function. C(\\mathbf{x},\\mathbf{y}) = \\frac{2}{\\pi} sin \\left (\\frac{2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{y}}{(2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{x} + 1) (2 \\mathbf{y}^\\intercal \\Sigma \\mathbf{y} + 1)} \\right ) C(\\mathbf{x},\\mathbf{y}) = \\frac{2}{\\pi} sin \\left (\\frac{2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{y}}{(2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{x} + 1) (2 \\mathbf{y}^\\intercal \\Sigma \\mathbf{y} + 1)} \\right )","title":"Non Stationary Kernels"},{"location":"core/core_kernel_nonstat/#locally-stationary-kernels","text":"A simple way to construct non-stationary covariances from stationary ones is by scaling the original stationary covariance; K(\\mathbf{x} - \\mathbf{y}) K(\\mathbf{x} - \\mathbf{y}) , by a function of \\mathbf{x} + \\mathbf{y} \\mathbf{x} + \\mathbf{y} . C(\\mathbf{x}, \\mathbf{y}) = G(\\mathbf{x} + \\mathbf{y}) K(\\mathbf{x} - \\mathbf{y}) C(\\mathbf{x}, \\mathbf{y}) = G(\\mathbf{x} + \\mathbf{y}) K(\\mathbf{x} - \\mathbf{y}) Here G(.): \\mathcal{X} \\rightarrow \\mathbb{R} G(.): \\mathcal{X} \\rightarrow \\mathbb{R} is a non-negative function of its inputs. These kernels are called locally stationary kernels . For an in-depth review of locally stationary kernels refer to Genton et. al . //Instantiate the base kernel val kernel : LocalScalarKernel [ I ] = _ val scalingFunction : ( I ) => Double = _ val scKernel = new LocallyStationaryKernel ( kernel , DataPipe ( scalingFunction ))","title":"Locally Stationary Kernels"},{"location":"core/core_kernel_nonstat/#polynomial-kernel","text":"A very popular non-stationary kernel used in machine learning, the polynomial represents the data features as polynomial expansions up to an index d d . C(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^\\intercal \\mathbf{y} + a)^{d} C(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^\\intercal \\mathbf{y} + a)^{d} val fbm = new PolynomialKernel ( 2 , 0.99 )","title":"Polynomial Kernel"},{"location":"core/core_kernel_nonstat/#fractional-brownian-field-fbm-kernel","text":"C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{2}\\left(||\\mathbf{x}||_{2}^{2H} + ||\\mathbf{y}||_{2}^{2H} - ||\\mathbf{x}-\\mathbf{y}||_{2}^{2H}\\right) C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{2}\\left(||\\mathbf{x}||_{2}^{2H} + ||\\mathbf{y}||_{2}^{2H} - ||\\mathbf{x}-\\mathbf{y}||_{2}^{2H}\\right) val fbm = new FBMKernel ( 0.99 ) The FBM kernel is the generalization of fractional Brownian motion to multi-variate index sets. Fractional Brownian motion is a stochastic process which is the generalization of Brownian motion, it was first studied by Mandelbrot and Von Ness . It is a self similar stochastic process, with stationary increments. However the process itself is non-stationary (as can be seen from the expression for the kernel) and has long range non vanishing covariance.","title":"Fractional Brownian Field (FBM) Kernel"},{"location":"core/core_kernel_nonstat/#maximum-likelihood-perceptron-kernel","text":"The maximum likelihood perceptron (MLP) kernel, was first arrived at in Radford Neal's thesis , by considering the limiting case of a bayesian feed forward neural network with sigmoid activation. C(\\mathbf{x},\\mathbf{y}) = sin^{-1} \\left (\\frac{w \\mathbf{x}^\\intercal \\mathbf{y} + b}{(w \\mathbf{x}^\\intercal \\mathbf{x} + b) (w \\mathbf{y}^\\intercal \\mathbf{y} + b)} \\right ) C(\\mathbf{x},\\mathbf{y}) = sin^{-1} \\left (\\frac{w \\mathbf{x}^\\intercal \\mathbf{y} + b}{(w \\mathbf{x}^\\intercal \\mathbf{x} + b) (w \\mathbf{y}^\\intercal \\mathbf{y} + b)} \\right )","title":"Maximum Likelihood Perceptron Kernel"},{"location":"core/core_kernel_nonstat/#neural-network-kernel","text":"Also a result of limiting case of bayesian neural networks, albeit with erf(.) erf(.) as the transfer function. C(\\mathbf{x},\\mathbf{y}) = \\frac{2}{\\pi} sin \\left (\\frac{2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{y}}{(2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{x} + 1) (2 \\mathbf{y}^\\intercal \\Sigma \\mathbf{y} + 1)} \\right ) C(\\mathbf{x},\\mathbf{y}) = \\frac{2}{\\pi} sin \\left (\\frac{2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{y}}{(2 \\mathbf{x}^\\intercal \\Sigma \\mathbf{x} + 1) (2 \\mathbf{y}^\\intercal \\Sigma \\mathbf{y} + 1)} \\right )","title":"Neural Network Kernel"},{"location":"core/core_kernel_nystrom/","text":"Summary \" Automatic Feature Extraction (AFE) is a technique by which approximate eigen-functions can be extracted from gram matrices constructed via kernel functions. These eigen-functions are features engineered by a particular kernel.\"\" Some Mathematical Background \u00b6 Definitions \u00b6 Let X_k \\ \\in \\ \\mathbb{R}^d \\ , \\ k = 1, \\cdots ,n X_k \\ \\in \\ \\mathbb{R}^d \\ , \\ k = 1, \\cdots ,n be a random sample drawn from a distribution F(x) F(x) . Let C \\in \\mathbb{R}^d C \\in \\mathbb{R}^d be a compact set such that, \\mathcal{H} = \\mathcal{L}^2(C) \\mathcal{H} = \\mathcal{L}^2(C) be a Hilbert space of functions given by the inner product below. \\begin{equation} <f,g>_{\\mathcal{H}} = \\int f(x)g(x) dF(x) \\end{equation} \\begin{equation} <f,g>_{\\mathcal{H}} = \\int f(x)g(x) dF(x) \\end{equation} Further let M(\\mathcal{H}, \\mathcal{H}) M(\\mathcal{H}, \\mathcal{H}) be a class of linear operators from \\mathcal{H} \\mathcal{H} to \\mathcal{H} \\mathcal{H} . Nystr\u00f6m method \u00b6 Automatic Feature Extraction (AFE) using the Nystr\u00f6m method aims at finding a finite dimensional approximation to the kernel eigenfunction expansion of Mercer kernels, as shown below. \\begin{equation} K(x,t) = \\sum_i{\\lambda_i \\phi(x)\\phi(t)} \\end{equation} \\begin{equation} K(x,t) = \\sum_i{\\lambda_i \\phi(x)\\phi(t)} \\end{equation} It is well known that Mercer kernels form a Reproducing Kernel Hilbert Space ( RHKS ) of functions. Every Mercer kernel defines a unique RHKS of functions as shown by the Moore-Aronszajn theorem. For a more involved treatment of RHKS and their applications the reader may refer to the book written by Bertinet et.al. Mercer's theorem states that the spectral decomposition of integral operator of K K , \\mathcal{T} \\in M(\\mathcal{H},\\mathcal{H}) \\mathcal{T} \\in M(\\mathcal{H},\\mathcal{H}) defined below yields the eigenfunctions which span the RHKS generated by K K and having an inner product defined as above. \\begin{equation} (\\mathcal{T}\\phi_i)(t) = \\int K(x,t) \\phi(x) dF(x) \\end{equation} \\begin{equation} (\\mathcal{T}\\phi_i)(t) = \\int K(x,t) \\phi(x) dF(x) \\end{equation} Equation above is more commonly also known as the Fredholm integral equation of the first kind. Nystr\u00f6m's method method approximates this integral using the quadrature constructed by considering a finite kernel matrix constructed out of a prototype set X_k \\ k = 1, \\cdots, m X_k \\ k = 1, \\cdots, m and calculating its spectral decomposition consisting of eigenvalues \\lambda_k \\lambda_k and eigen-vectors u_k u_k . This yields an expression for the approximate non-linear feature map \\hat{\\phi} : \\mathbb{R}^d \\longrightarrow \\mathbb{R}^m \\hat{\\phi} : \\mathbb{R}^d \\longrightarrow \\mathbb{R}^m . \\begin{equation} \\hat{\\phi}_{i}(t) = \\frac{\\sqrt{m}}{\\lambda_i}\\sum_{k=1}^{m}K(X_k, t)u_{k,i} \\end{equation} \\begin{equation} \\hat{\\phi}_{i}(t) = \\frac{\\sqrt{m}}{\\lambda_i}\\sum_{k=1}^{m}K(X_k, t)u_{k,i} \\end{equation} AFE in DynaML Kernels \u00b6 The SVMKernel[M] contains an implementation of AFE in the method featureMapping ( decomposition : ( DenseVector [ Double ], DenseMatrix [ Double ]))( prototypes : List [ DenseVector [ Double ]])( data : DenseVector [ Double ]) : DenseVector [ Double ] Note The SVMKernel class is extended by all the implemented library kernels in DynaML thereby enabling the use of AFE in potentially any model employing kernels.","title":"Automatic Feature Extraction"},{"location":"core/core_kernel_nystrom/#some-mathematical-background","text":"","title":"Some Mathematical Background"},{"location":"core/core_kernel_nystrom/#definitions","text":"Let X_k \\ \\in \\ \\mathbb{R}^d \\ , \\ k = 1, \\cdots ,n X_k \\ \\in \\ \\mathbb{R}^d \\ , \\ k = 1, \\cdots ,n be a random sample drawn from a distribution F(x) F(x) . Let C \\in \\mathbb{R}^d C \\in \\mathbb{R}^d be a compact set such that, \\mathcal{H} = \\mathcal{L}^2(C) \\mathcal{H} = \\mathcal{L}^2(C) be a Hilbert space of functions given by the inner product below. \\begin{equation} <f,g>_{\\mathcal{H}} = \\int f(x)g(x) dF(x) \\end{equation} \\begin{equation} <f,g>_{\\mathcal{H}} = \\int f(x)g(x) dF(x) \\end{equation} Further let M(\\mathcal{H}, \\mathcal{H}) M(\\mathcal{H}, \\mathcal{H}) be a class of linear operators from \\mathcal{H} \\mathcal{H} to \\mathcal{H} \\mathcal{H} .","title":"Definitions"},{"location":"core/core_kernel_nystrom/#nystrom-method","text":"Automatic Feature Extraction (AFE) using the Nystr\u00f6m method aims at finding a finite dimensional approximation to the kernel eigenfunction expansion of Mercer kernels, as shown below. \\begin{equation} K(x,t) = \\sum_i{\\lambda_i \\phi(x)\\phi(t)} \\end{equation} \\begin{equation} K(x,t) = \\sum_i{\\lambda_i \\phi(x)\\phi(t)} \\end{equation} It is well known that Mercer kernels form a Reproducing Kernel Hilbert Space ( RHKS ) of functions. Every Mercer kernel defines a unique RHKS of functions as shown by the Moore-Aronszajn theorem. For a more involved treatment of RHKS and their applications the reader may refer to the book written by Bertinet et.al. Mercer's theorem states that the spectral decomposition of integral operator of K K , \\mathcal{T} \\in M(\\mathcal{H},\\mathcal{H}) \\mathcal{T} \\in M(\\mathcal{H},\\mathcal{H}) defined below yields the eigenfunctions which span the RHKS generated by K K and having an inner product defined as above. \\begin{equation} (\\mathcal{T}\\phi_i)(t) = \\int K(x,t) \\phi(x) dF(x) \\end{equation} \\begin{equation} (\\mathcal{T}\\phi_i)(t) = \\int K(x,t) \\phi(x) dF(x) \\end{equation} Equation above is more commonly also known as the Fredholm integral equation of the first kind. Nystr\u00f6m's method method approximates this integral using the quadrature constructed by considering a finite kernel matrix constructed out of a prototype set X_k \\ k = 1, \\cdots, m X_k \\ k = 1, \\cdots, m and calculating its spectral decomposition consisting of eigenvalues \\lambda_k \\lambda_k and eigen-vectors u_k u_k . This yields an expression for the approximate non-linear feature map \\hat{\\phi} : \\mathbb{R}^d \\longrightarrow \\mathbb{R}^m \\hat{\\phi} : \\mathbb{R}^d \\longrightarrow \\mathbb{R}^m . \\begin{equation} \\hat{\\phi}_{i}(t) = \\frac{\\sqrt{m}}{\\lambda_i}\\sum_{k=1}^{m}K(X_k, t)u_{k,i} \\end{equation} \\begin{equation} \\hat{\\phi}_{i}(t) = \\frac{\\sqrt{m}}{\\lambda_i}\\sum_{k=1}^{m}K(X_k, t)u_{k,i} \\end{equation}","title":"Nystr\u00f6m method"},{"location":"core/core_kernel_nystrom/#afe-in-dynaml-kernels","text":"The SVMKernel[M] contains an implementation of AFE in the method featureMapping ( decomposition : ( DenseVector [ Double ], DenseMatrix [ Double ]))( prototypes : List [ DenseVector [ Double ]])( data : DenseVector [ Double ]) : DenseVector [ Double ] Note The SVMKernel class is extended by all the implemented library kernels in DynaML thereby enabling the use of AFE in potentially any model employing kernels.","title":"AFE in DynaML Kernels"},{"location":"core/core_kernel_stat/","text":"Stationary kernels can be expressed as a function of the difference between their inputs. C(\\mathbf{x}, \\mathbf{y}) = K(||\\mathbf{x} - \\mathbf{y}||_{p}) C(\\mathbf{x}, \\mathbf{y}) = K(||\\mathbf{x} - \\mathbf{y}||_{p}) Note that any norm may be used to quantify the distance between the two vectors \\mathbf{x} \\ \\& \\ \\mathbf{y} \\mathbf{x} \\ \\& \\ \\mathbf{y} . The values p = 1 p = 1 and p = 2 p = 2 represent the Manhattan distance and Euclidean distance respectively. Instantiating Stationary Kernels Stationary kernels are implemented as a subset of the StationaryKernel[T, V, M] class which requires a Field[T] implicit object (an algebraic field which has definitions for addition, subtraction, multiplication and division of its elements much like the number system). You may also import spire.implicits._ in order to load the default field implementations for basic data types like Int , Double and so on. Before instantiating any child class of StationaryKernel one needs to enter the following code. import spire.algebra.Field import io.github.mandar2812.dynaml.analysis.VectorField //Calculate the number of input features //and create a vector field of that dimension val num_features : Int = ... implicit val f = VectorField ( num_features ) Radial Basis Function Kernel \u00b6 C(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2\\sigma^2}\\right) C(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2\\sigma^2}\\right) The RBF kernel is the most popular kernel function applied in machine learning, it represents an inner product space which is spanned by the Hermite polynomials and as such is suitable to model smooth functions. The RBF kernel is also called a universal kernel for the reason that any smooth function can be represented with a high degree of accuracy assuming we can find a suitable value of the bandwidth. val rbf = new RBFKernel ( 4.0 ) Squared Exponential Kernel \u00b6 A generalization of the RBF Kernel is the Squared Exponential Kernel C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2l^2}\\right) C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2l^2}\\right) val rbf = new SEKernel ( 4.0 , 2.0 ) Mahalanobis Kernel \u00b6 This kernel is a further generalization of the SE kernel. It uses the Mahalanobis distance instead of the Euclidean distance between the inputs. C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\\right) C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\\right) The Mahalanobis distance (\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y}) (\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y}) is characterized by a symmetric positive definite matrix \\Sigma \\Sigma . This distance metric reduces to the Euclidean distance if \\Sigma \\Sigma is the identity matrix . Further, if \\Sigma \\Sigma is diagonal, the Mahalanobis kernel becomes the Automatic Relevance Determination version of the SE kernel (SE-ARD). In DynaML the class MahalanobisKernel implements the SE-ARD kernel with diagonal \\Sigma \\Sigma . val bandwidths : DenseVector [ Double ] = _ val amp = 1.5 val maha_kernel = new MahalanobisKernel ( bandwidths , amp ) Student T Kernel \u00b6 C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + ||\\mathbf{x}-\\mathbf{y}||^d} C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + ||\\mathbf{x}-\\mathbf{y}||^d} val tstud = new TStudentKernel ( 2.0 ) Rational Quadratic Kernel \u00b6 C(\\mathbf{x},\\mathbf{y}) = \\left( 1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2 \\mu \\ell^2} \\right)^{-\\frac{1}{2} (dim(\\mathbf{x})+\\mu)} C(\\mathbf{x},\\mathbf{y}) = \\left( 1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2 \\mu \\ell^2} \\right)^{-\\frac{1}{2} (dim(\\mathbf{x})+\\mu)} val rat = new RationalQuadraticKernel ( shape = 1.5 , l = 1.5 ) Cauchy Kernel \u00b6 C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{\\sigma^2}} C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{\\sigma^2}} val cau = new CauchyKernel ( 2.5 ) Gaussian Spectral Kernel \u00b6 C(\\mathbf{x},\\mathbf{y}) = cos(2\\pi \\mu ||\\mathbf{x}-\\mathbf{y}||) \\ exp(-2\\pi^{2} \\sigma^{2} ||\\mathbf{x}-\\mathbf{y}||^{2} ) C(\\mathbf{x},\\mathbf{y}) = cos(2\\pi \\mu ||\\mathbf{x}-\\mathbf{y}||) \\ exp(-2\\pi^{2} \\sigma^{2} ||\\mathbf{x}-\\mathbf{y}||^{2} ) //Define how the hyper-parameter Map gets transformed to the kernel parameters val encoder = Encoder ( ( conf : Map [ String , Double ]) => ( conf ( \"c\" ), conf ( \"s\" )), ( cs : ( Double , Double )) => Map ( \"c\" -> cs . _1 , \"s\" -> cs . _2 )) val gsmKernel = GaussianSpectralKernel [ Double ]( 3.5 , 2.0 , encoder ) Matern Half Integer \u00b6 The Matern kernel is an important family of covariance functions. Matern covariances are parameterized via two quantities i.e. order \\nu \\nu and \\rho \\rho the characteristic length scale. The general matern covariance is defined in terms of modified Bessel functions. C_{\\nu}(\\mathbf{x},\\mathbf{y}) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)^{\\nu} K_{\\nu}\\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) C_{\\nu}(\\mathbf{x},\\mathbf{y}) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)^{\\nu} K_{\\nu}\\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) Where d = ||\\mathbf{x} - \\mathbf{y}|| d = ||\\mathbf{x} - \\mathbf{y}|| is the Euclidean ( L_2 L_2 ) distance between points. For the case \\nu = p + \\frac{1}{2}, p \\in \\mathbb{N} \\nu = p + \\frac{1}{2}, p \\in \\mathbb{N} the expression becomes. C_{\\nu}(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) \\frac{\\Gamma(p+1)}{\\Gamma(2p+1)} \\sum_{i = 0}^{p}{\\frac{(p+1)!}{i!(p-i)!}\\left(\\sqrt{8\\nu}\\frac{d}{\\rho}\\right)^{p-i}} C_{\\nu}(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) \\frac{\\Gamma(p+1)}{\\Gamma(2p+1)} \\sum_{i = 0}^{p}{\\frac{(p+1)!}{i!(p-i)!}\\left(\\sqrt{8\\nu}\\frac{d}{\\rho}\\right)^{p-i}} Currently there is only support for matern half integer kernels. implicit ev = VectorField ( 2 ) val matKern = new GenericMaternKernel ( 1.5 , p = 1 ) Wavelet Kernel \u00b6 The Wavelet kernel ( Zhang et al, 2004 ) comes from Wavelet theory and is given as C(\\mathbf{x},\\mathbf{y}) = \\prod_{i = 1}^{d} h\\left(\\frac{x_i-y_i}{a}\\right) C(\\mathbf{x},\\mathbf{y}) = \\prod_{i = 1}^{d} h\\left(\\frac{x_i-y_i}{a}\\right) Where the function h is known as the mother wavelet function, Zhang et. al suggest the following expression for the mother wavelet function. h(x) = cos(1.75x)exp(-\\frac{1}{2}x^2) h(x) = cos(1.75x)exp(-\\frac{1}{2}x^2) val wv = new WaveletKernel ( x => math . cos ( 1.75 * x )* math . exp (- 1.0 * x * x / 2.0 ))( 1.5 ) Periodic Kernel \u00b6 The periodic kernel has Fourier series as its orthogonal eigenfunctions. It is used when constructing predictive models over quantities which are known to have some periodic behavior. C(\\mathbf{x},\\mathbf{y}) = exp\\left(-2 \\ sin^{2}\\left(\\frac{\\pi \\omega ||\\mathbf{x}-\\mathbf{y}||}{l^2}\\right)\\right) C(\\mathbf{x},\\mathbf{y}) = exp\\left(-2 \\ sin^{2}\\left(\\frac{\\pi \\omega ||\\mathbf{x}-\\mathbf{y}||}{l^2}\\right)\\right) val periodic_kernel = new PeriodicKernel ( lengthscale = 1.5 , freq = 2.5 ) Wave Kernel \u00b6 C(\\mathbf{x},\\mathbf{y}) = \\frac{\\theta}{||\\mathbf{x} - \\mathbf{y}||^2} \\times sin(\\frac{||\\mathbf{x} - \\mathbf{y}||^2}{\\theta}) C(\\mathbf{x},\\mathbf{y}) = \\frac{\\theta}{||\\mathbf{x} - \\mathbf{y}||^2} \\times sin(\\frac{||\\mathbf{x} - \\mathbf{y}||^2}{\\theta}) val wv_kernel = WaveKernel ( th = 1.0 ) Laplacian Kernel \u00b6 The Laplacian kernel is the covariance function of the well known Ornstein Ulhenbeck process , samples drawn from this process are continuous and only once differentiable. \\begin{equation} C(\\mathbf{x},\\mathbf{y}) = exp \\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||_{1}}{2\\beta}\\right) \\end{equation} \\begin{equation} C(\\mathbf{x},\\mathbf{y}) = exp \\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||_{1}}{2\\beta}\\right) \\end{equation} val lap = new LaplacianKernel ( 4.0 )","title":"Stationary Kernels"},{"location":"core/core_kernel_stat/#radial-basis-function-kernel","text":"C(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2\\sigma^2}\\right) C(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2\\sigma^2}\\right) The RBF kernel is the most popular kernel function applied in machine learning, it represents an inner product space which is spanned by the Hermite polynomials and as such is suitable to model smooth functions. The RBF kernel is also called a universal kernel for the reason that any smooth function can be represented with a high degree of accuracy assuming we can find a suitable value of the bandwidth. val rbf = new RBFKernel ( 4.0 )","title":"Radial Basis Function Kernel"},{"location":"core/core_kernel_stat/#squared-exponential-kernel","text":"A generalization of the RBF Kernel is the Squared Exponential Kernel C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2l^2}\\right) C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2l^2}\\right) val rbf = new SEKernel ( 4.0 , 2.0 )","title":"Squared Exponential Kernel"},{"location":"core/core_kernel_stat/#mahalanobis-kernel","text":"This kernel is a further generalization of the SE kernel. It uses the Mahalanobis distance instead of the Euclidean distance between the inputs. C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\\right) C(\\mathbf{x},\\mathbf{y}) = h \\ exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y})\\right) The Mahalanobis distance (\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y}) (\\mathbf{x}-\\mathbf{y})^\\intercal \\Sigma^{-1} (\\mathbf{x}-\\mathbf{y}) is characterized by a symmetric positive definite matrix \\Sigma \\Sigma . This distance metric reduces to the Euclidean distance if \\Sigma \\Sigma is the identity matrix . Further, if \\Sigma \\Sigma is diagonal, the Mahalanobis kernel becomes the Automatic Relevance Determination version of the SE kernel (SE-ARD). In DynaML the class MahalanobisKernel implements the SE-ARD kernel with diagonal \\Sigma \\Sigma . val bandwidths : DenseVector [ Double ] = _ val amp = 1.5 val maha_kernel = new MahalanobisKernel ( bandwidths , amp )","title":"Mahalanobis Kernel"},{"location":"core/core_kernel_stat/#student-t-kernel","text":"C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + ||\\mathbf{x}-\\mathbf{y}||^d} C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + ||\\mathbf{x}-\\mathbf{y}||^d} val tstud = new TStudentKernel ( 2.0 )","title":"Student T Kernel"},{"location":"core/core_kernel_stat/#rational-quadratic-kernel","text":"C(\\mathbf{x},\\mathbf{y}) = \\left( 1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2 \\mu \\ell^2} \\right)^{-\\frac{1}{2} (dim(\\mathbf{x})+\\mu)} C(\\mathbf{x},\\mathbf{y}) = \\left( 1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{2 \\mu \\ell^2} \\right)^{-\\frac{1}{2} (dim(\\mathbf{x})+\\mu)} val rat = new RationalQuadraticKernel ( shape = 1.5 , l = 1.5 )","title":"Rational Quadratic Kernel"},{"location":"core/core_kernel_stat/#cauchy-kernel","text":"C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{\\sigma^2}} C(\\mathbf{x},\\mathbf{y}) = \\frac{1}{1 + \\frac{||\\mathbf{x}-\\mathbf{y}||^2}{\\sigma^2}} val cau = new CauchyKernel ( 2.5 )","title":"Cauchy Kernel"},{"location":"core/core_kernel_stat/#gaussian-spectral-kernel","text":"C(\\mathbf{x},\\mathbf{y}) = cos(2\\pi \\mu ||\\mathbf{x}-\\mathbf{y}||) \\ exp(-2\\pi^{2} \\sigma^{2} ||\\mathbf{x}-\\mathbf{y}||^{2} ) C(\\mathbf{x},\\mathbf{y}) = cos(2\\pi \\mu ||\\mathbf{x}-\\mathbf{y}||) \\ exp(-2\\pi^{2} \\sigma^{2} ||\\mathbf{x}-\\mathbf{y}||^{2} ) //Define how the hyper-parameter Map gets transformed to the kernel parameters val encoder = Encoder ( ( conf : Map [ String , Double ]) => ( conf ( \"c\" ), conf ( \"s\" )), ( cs : ( Double , Double )) => Map ( \"c\" -> cs . _1 , \"s\" -> cs . _2 )) val gsmKernel = GaussianSpectralKernel [ Double ]( 3.5 , 2.0 , encoder )","title":"Gaussian Spectral Kernel"},{"location":"core/core_kernel_stat/#matern-half-integer","text":"The Matern kernel is an important family of covariance functions. Matern covariances are parameterized via two quantities i.e. order \\nu \\nu and \\rho \\rho the characteristic length scale. The general matern covariance is defined in terms of modified Bessel functions. C_{\\nu}(\\mathbf{x},\\mathbf{y}) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)^{\\nu} K_{\\nu}\\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) C_{\\nu}(\\mathbf{x},\\mathbf{y}) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right)^{\\nu} K_{\\nu}\\left(\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) Where d = ||\\mathbf{x} - \\mathbf{y}|| d = ||\\mathbf{x} - \\mathbf{y}|| is the Euclidean ( L_2 L_2 ) distance between points. For the case \\nu = p + \\frac{1}{2}, p \\in \\mathbb{N} \\nu = p + \\frac{1}{2}, p \\in \\mathbb{N} the expression becomes. C_{\\nu}(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) \\frac{\\Gamma(p+1)}{\\Gamma(2p+1)} \\sum_{i = 0}^{p}{\\frac{(p+1)!}{i!(p-i)!}\\left(\\sqrt{8\\nu}\\frac{d}{\\rho}\\right)^{p-i}} C_{\\nu}(\\mathbf{x},\\mathbf{y}) = exp\\left(-\\sqrt{2\\nu}\\frac{d}{\\rho}\\right) \\frac{\\Gamma(p+1)}{\\Gamma(2p+1)} \\sum_{i = 0}^{p}{\\frac{(p+1)!}{i!(p-i)!}\\left(\\sqrt{8\\nu}\\frac{d}{\\rho}\\right)^{p-i}} Currently there is only support for matern half integer kernels. implicit ev = VectorField ( 2 ) val matKern = new GenericMaternKernel ( 1.5 , p = 1 )","title":"Matern Half Integer"},{"location":"core/core_kernel_stat/#wavelet-kernel","text":"The Wavelet kernel ( Zhang et al, 2004 ) comes from Wavelet theory and is given as C(\\mathbf{x},\\mathbf{y}) = \\prod_{i = 1}^{d} h\\left(\\frac{x_i-y_i}{a}\\right) C(\\mathbf{x},\\mathbf{y}) = \\prod_{i = 1}^{d} h\\left(\\frac{x_i-y_i}{a}\\right) Where the function h is known as the mother wavelet function, Zhang et. al suggest the following expression for the mother wavelet function. h(x) = cos(1.75x)exp(-\\frac{1}{2}x^2) h(x) = cos(1.75x)exp(-\\frac{1}{2}x^2) val wv = new WaveletKernel ( x => math . cos ( 1.75 * x )* math . exp (- 1.0 * x * x / 2.0 ))( 1.5 )","title":"Wavelet Kernel"},{"location":"core/core_kernel_stat/#periodic-kernel","text":"The periodic kernel has Fourier series as its orthogonal eigenfunctions. It is used when constructing predictive models over quantities which are known to have some periodic behavior. C(\\mathbf{x},\\mathbf{y}) = exp\\left(-2 \\ sin^{2}\\left(\\frac{\\pi \\omega ||\\mathbf{x}-\\mathbf{y}||}{l^2}\\right)\\right) C(\\mathbf{x},\\mathbf{y}) = exp\\left(-2 \\ sin^{2}\\left(\\frac{\\pi \\omega ||\\mathbf{x}-\\mathbf{y}||}{l^2}\\right)\\right) val periodic_kernel = new PeriodicKernel ( lengthscale = 1.5 , freq = 2.5 )","title":"Periodic Kernel"},{"location":"core/core_kernel_stat/#wave-kernel","text":"C(\\mathbf{x},\\mathbf{y}) = \\frac{\\theta}{||\\mathbf{x} - \\mathbf{y}||^2} \\times sin(\\frac{||\\mathbf{x} - \\mathbf{y}||^2}{\\theta}) C(\\mathbf{x},\\mathbf{y}) = \\frac{\\theta}{||\\mathbf{x} - \\mathbf{y}||^2} \\times sin(\\frac{||\\mathbf{x} - \\mathbf{y}||^2}{\\theta}) val wv_kernel = WaveKernel ( th = 1.0 )","title":"Wave Kernel"},{"location":"core/core_kernel_stat/#laplacian-kernel","text":"The Laplacian kernel is the covariance function of the well known Ornstein Ulhenbeck process , samples drawn from this process are continuous and only once differentiable. \\begin{equation} C(\\mathbf{x},\\mathbf{y}) = exp \\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||_{1}}{2\\beta}\\right) \\end{equation} \\begin{equation} C(\\mathbf{x},\\mathbf{y}) = exp \\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||_{1}}{2\\beta}\\right) \\end{equation} val lap = new LaplacianKernel ( 4.0 )","title":"Laplacian Kernel"},{"location":"core/core_kernels/","text":"The dynaml.kernels package has a highly developed API for creating kernel functions for machine learning applications. Here we give the user an in-depth introduction to its capabilities. Positive definite functions or positive type functions occupy an important place in various areas of mathematics, from the construction of covariances of random variables to quantifying distance measures in Hilbert spaces . Symmetric positive type functions defined on the cartesian product of a set with itself K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R} K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R} are also known as kernel functions in machine learning. They are applied extensively in problems such as. Model non-linear behavior in SVM models: SVM and LSSVM Quantify covariance between input patterns: Gaussian Processes Represent degree of 'closeness' or affinity in unsupervised learning: Kernel Spectral Clustering For an in depth review of the various applications of kernels in the machine learning domain, refer to Scholkopf et. al Nomenclature In the machine learning community the words kernel and covariance function are used interchangeably. Kernel API \u00b6 The kernel class hierarchy all stems from a simple trait shown here. trait Kernel [ T , V ] { def evaluate ( x : T , y : T ) : V } This outlines only one key feature for kernel functions i.e. their evaluation functional which takes two inputs from \\mathcal{X} \\mathcal{X} and yields a scalar value. Kernel vs CovarianceFunction For practical purposes, the Kernel [ T , V ] trait does not have enough functionality for usage in varied models like Gaussian Processes , Student's T Processes , LS-SVM etc. For this reason there is the CovarianceFunction [ T , V , M ] abstract class. It contains methods to construct kernel matrices, keep track of hyper-parameter assignments among other things. Creating arbitrary kernel functions \u00b6 Apart from off the shelf kernel functions, it is also possible to create custom kernels on the fly by using the CovarianceFunction object. Constructing kernels via feature maps \u00b6 It is known from Mercer's theorem that any valid kernel function must be decomposable as a dot product between certain basis function representation of the inputs. This translates mathematically into. \\begin{align} & K(\\mathbf{x}, \\mathbf{y}) = \\varphi^{T}(\\mathbf{x}) . \\varphi(\\mathbf{y}) \\\\ & \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^n \\end{align} \\begin{align} & K(\\mathbf{x}, \\mathbf{y}) = \\varphi^{T}(\\mathbf{x}) . \\varphi(\\mathbf{y}) \\\\ & \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^n \\end{align} The function \\varphi(.) \\varphi(.) is some higher (possibly infinite) dimensional representation of the input features of a data point. Note that the input space \\mathcal{X} \\mathcal{X} could be any of the following (but not limited to). The space of all connection graphs with specific number of nodes. A multi-dimensional vector. The space of all character sequences (binary or otherwise) up to a certain length. The set of all integer tuples e.g. (1,2), (6,10), \\cdots (1,2), (6,10), \\cdots We can use any function from some domain \\mathcal{X} \\mathcal{X} yielding a DenseVector [ Double ] to define a particular inner product/kernel function. // First create a function mapping from some input space to // Breeze dense vectors. val mapFunc = ( vec : DenseVector [ Double ]) => { val mat = vec * vec . t mat . toDenseVector } val kernel = CovarianceFunction ( mapFunc ) Feature map kernels Covariance functions constructed using feature mappings as shown above return a special object; an instance of the FeatureMapCovariance [ T , DenseVector [ Double ]] class. In the section on composite kernels we will see why this is important. Constructing kernels via direct evaluation \u00b6 Instead of defining a feature representation like \\varphi(.) \\varphi(.) as in the section above, you can also directly define the evaluation expression of the kernel. // Create the expression for the required kernel. val mapFunc = ( state : Map [ String , Double ]) => ( x : DenseVector [ Double ], y : DenseVector [ Double ]) => { state ( \"alpha\" )*( x dot y ) + state ( \"intercept\" ) } //Creates kernel with two hyper-parameters: alpha and intercept val kernel = CovarianceFunction ( mapFunc )( Map ( \"alpha\" -> 1.5 , \"intercept\" -> 0.01 ) ) Creating Composite Kernels \u00b6 Algebraic Operations \u00b6 In machine learning it is well known that kernels can be combined to give other valid kernels. The symmetric positive semi-definite property of a kernel is preserved as long as it is added or multiplied to another valid kernel. In DynaML adding and multiplying kernels is elementary. val k1 = new RBFKernel ( 2.5 ) val k2 = new RationalQuadraticKernel ( 2.0 ) val k = k1 + k2 val k3 = k * k2 Composition \u00b6 From Mercer's theorem, every kernel can be expressed as a dot product of feature mappings evaluated at the respective data points. We can use this to construct more complex covariances i.e. by successively applying feature mappings. \\begin{align} C_{a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{a}(\\mathbf{x})^\\intercal \\varphi_{a}(\\mathbf{y}) \\\\ C_{b}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\mathbf{x})^\\intercal \\varphi_{b}(\\mathbf{y}) \\\\ C_{b . a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\varphi_{a}(\\mathbf{x}))^\\intercal \\varphi_{b}(\\varphi_{a}(\\mathbf{y})) \\end{align} \\begin{align} C_{a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{a}(\\mathbf{x})^\\intercal \\varphi_{a}(\\mathbf{y}) \\\\ C_{b}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\mathbf{x})^\\intercal \\varphi_{b}(\\mathbf{y}) \\\\ C_{b . a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\varphi_{a}(\\mathbf{x}))^\\intercal \\varphi_{b}(\\varphi_{a}(\\mathbf{y})) \\end{align} In DynaML, we can create a composite kernel if the kernel represented by the map \\varphi_{a} \\varphi_{a} , is explicitly of type FeatureMapCovariance [ T , DenseVector [ Double ]] val mapFunc = ( vec : DenseVector [ Double ]) => { vec / 2 d } val k1 = CovarianceFunction ( mapFunc ) val k2 = new RationalQuadraticKernel ( 2.0 ) //Composite kernel val k3 = k2 > k1 Scaling Covariances \u00b6 If C(\\mathbf{x}, \\mathbf{y}) C(\\mathbf{x}, \\mathbf{y}) is a valid covariance function, then g(\\mathbf{x}) C(\\mathbf{x}, \\mathbf{y}) g(\\mathbf{x}) g(\\mathbf{x}) C(\\mathbf{x}, \\mathbf{y}) g(\\mathbf{x}) is also a valid covariance function, where g(.): \\mathcal{X} \\rightarrow \\mathbb{R} g(.): \\mathcal{X} \\rightarrow \\mathbb{R} is a non-negative function from the domain of the inputs \\mathcal{X} \\mathcal{X} to the real number line. We call these covariances scaled covariance functions . //Instantiate some kernel val kernel : LocalScalarKernel [ I ] = _ val scalingFunction : ( I ) => Double = _ val scKernel = ScaledKernel ( kernel , DataPipe ( scalingFunction )) Advanced Composite Kernels \u00b6 Sometimes we would like to express a kernel function as a product (or sum) of component kernels each of which act on a sub-set of the dimensions (degree of freedom) of the input attributes. For example; for 4 dimensional input vector, we may define two component kernels acting on the first two and last two dimensions respectively and combine their evaluations via addition or multiplication. For this purpose the dynaml.kernels package has the DecomposableCovariance [ S ] class. In order to create a decomposable kernel you need three components. The component kernels (order matters) An Encoder [ S , Array [ S ]] instance which splits the input into an array of components A Reducer which combines the individual kernel evaluations. //Not required in REPL, already imported import io.github.mandar2812.dynaml.DynaMLPipe._ import io.github.mandar2812.dynaml.pipes._ val kernel1 : LocalScalarKernel [ DenseVector [ Double ]] = _ val kernel2 : LocalScalarKernel [ DenseVector [ Double ]] = _ //Default Reducer is addition val decomp_kernel = new DecomposableCovariance [ DenseVector [ Double ]]( kernel1 , kernel2 )( breezeDVSplitEncoder ( 2 )) val decomp_kernel_mult = new DecomposableCovariance [ DenseVector [ Double ]]( kernel1 , kernel2 )( breezeDVSplitEncoder ( 2 ), Reducer .:*:) Implementing Custom Kernels You can implement your own custom kernels by extending the LocalScalarKernel[T] interface, for example: import breeze.linalg. { DenseMatrix , norm , DenseVector } //You can have any number of constructor parameters class MyNewKernel ( th : Double = 1.0 ) extends LocalScalarKernel [ DenseVector [ Double ]] with Serializable { //One must specify the names of each hyper-parameter override val hyper_parameters = List ( \"theta\" ) //The state variable stores the //current value of all kernel hyper-parameters state = Map ( \"theta\" -> th ) // The implementation of the actual kernel function override def evaluateAt ( config : Map [ String , Double ])( x : DenseVector [ Double ], y : DenseVector [ Double ]) : Double = ??? // Return the gradient of the kernel for each hyper-parameter // for a particular pair of points x,y override def gradientAt ( config : Map [ String , Double ])( x : DenseVector [ Double ], y : DenseVector [ Double ]) : Map [ String , Double ] = ??? }","title":"Kernel API"},{"location":"core/core_kernels/#kernel-api","text":"The kernel class hierarchy all stems from a simple trait shown here. trait Kernel [ T , V ] { def evaluate ( x : T , y : T ) : V } This outlines only one key feature for kernel functions i.e. their evaluation functional which takes two inputs from \\mathcal{X} \\mathcal{X} and yields a scalar value. Kernel vs CovarianceFunction For practical purposes, the Kernel [ T , V ] trait does not have enough functionality for usage in varied models like Gaussian Processes , Student's T Processes , LS-SVM etc. For this reason there is the CovarianceFunction [ T , V , M ] abstract class. It contains methods to construct kernel matrices, keep track of hyper-parameter assignments among other things.","title":"Kernel API"},{"location":"core/core_kernels/#creating-arbitrary-kernel-functions","text":"Apart from off the shelf kernel functions, it is also possible to create custom kernels on the fly by using the CovarianceFunction object.","title":"Creating arbitrary kernel functions"},{"location":"core/core_kernels/#constructing-kernels-via-feature-maps","text":"It is known from Mercer's theorem that any valid kernel function must be decomposable as a dot product between certain basis function representation of the inputs. This translates mathematically into. \\begin{align} & K(\\mathbf{x}, \\mathbf{y}) = \\varphi^{T}(\\mathbf{x}) . \\varphi(\\mathbf{y}) \\\\ & \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^n \\end{align} \\begin{align} & K(\\mathbf{x}, \\mathbf{y}) = \\varphi^{T}(\\mathbf{x}) . \\varphi(\\mathbf{y}) \\\\ & \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^n \\end{align} The function \\varphi(.) \\varphi(.) is some higher (possibly infinite) dimensional representation of the input features of a data point. Note that the input space \\mathcal{X} \\mathcal{X} could be any of the following (but not limited to). The space of all connection graphs with specific number of nodes. A multi-dimensional vector. The space of all character sequences (binary or otherwise) up to a certain length. The set of all integer tuples e.g. (1,2), (6,10), \\cdots (1,2), (6,10), \\cdots We can use any function from some domain \\mathcal{X} \\mathcal{X} yielding a DenseVector [ Double ] to define a particular inner product/kernel function. // First create a function mapping from some input space to // Breeze dense vectors. val mapFunc = ( vec : DenseVector [ Double ]) => { val mat = vec * vec . t mat . toDenseVector } val kernel = CovarianceFunction ( mapFunc ) Feature map kernels Covariance functions constructed using feature mappings as shown above return a special object; an instance of the FeatureMapCovariance [ T , DenseVector [ Double ]] class. In the section on composite kernels we will see why this is important.","title":"Constructing kernels via feature maps"},{"location":"core/core_kernels/#constructing-kernels-via-direct-evaluation","text":"Instead of defining a feature representation like \\varphi(.) \\varphi(.) as in the section above, you can also directly define the evaluation expression of the kernel. // Create the expression for the required kernel. val mapFunc = ( state : Map [ String , Double ]) => ( x : DenseVector [ Double ], y : DenseVector [ Double ]) => { state ( \"alpha\" )*( x dot y ) + state ( \"intercept\" ) } //Creates kernel with two hyper-parameters: alpha and intercept val kernel = CovarianceFunction ( mapFunc )( Map ( \"alpha\" -> 1.5 , \"intercept\" -> 0.01 ) )","title":"Constructing kernels via direct evaluation"},{"location":"core/core_kernels/#creating-composite-kernels","text":"","title":"Creating Composite Kernels"},{"location":"core/core_kernels/#algebraic-operations","text":"In machine learning it is well known that kernels can be combined to give other valid kernels. The symmetric positive semi-definite property of a kernel is preserved as long as it is added or multiplied to another valid kernel. In DynaML adding and multiplying kernels is elementary. val k1 = new RBFKernel ( 2.5 ) val k2 = new RationalQuadraticKernel ( 2.0 ) val k = k1 + k2 val k3 = k * k2","title":"Algebraic Operations"},{"location":"core/core_kernels/#composition","text":"From Mercer's theorem, every kernel can be expressed as a dot product of feature mappings evaluated at the respective data points. We can use this to construct more complex covariances i.e. by successively applying feature mappings. \\begin{align} C_{a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{a}(\\mathbf{x})^\\intercal \\varphi_{a}(\\mathbf{y}) \\\\ C_{b}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\mathbf{x})^\\intercal \\varphi_{b}(\\mathbf{y}) \\\\ C_{b . a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\varphi_{a}(\\mathbf{x}))^\\intercal \\varphi_{b}(\\varphi_{a}(\\mathbf{y})) \\end{align} \\begin{align} C_{a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{a}(\\mathbf{x})^\\intercal \\varphi_{a}(\\mathbf{y}) \\\\ C_{b}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\mathbf{x})^\\intercal \\varphi_{b}(\\mathbf{y}) \\\\ C_{b . a}(\\mathbf{x}, \\mathbf{y}) &= \\varphi_{b}(\\varphi_{a}(\\mathbf{x}))^\\intercal \\varphi_{b}(\\varphi_{a}(\\mathbf{y})) \\end{align} In DynaML, we can create a composite kernel if the kernel represented by the map \\varphi_{a} \\varphi_{a} , is explicitly of type FeatureMapCovariance [ T , DenseVector [ Double ]] val mapFunc = ( vec : DenseVector [ Double ]) => { vec / 2 d } val k1 = CovarianceFunction ( mapFunc ) val k2 = new RationalQuadraticKernel ( 2.0 ) //Composite kernel val k3 = k2 > k1","title":"Composition"},{"location":"core/core_kernels/#scaling-covariances","text":"If C(\\mathbf{x}, \\mathbf{y}) C(\\mathbf{x}, \\mathbf{y}) is a valid covariance function, then g(\\mathbf{x}) C(\\mathbf{x}, \\mathbf{y}) g(\\mathbf{x}) g(\\mathbf{x}) C(\\mathbf{x}, \\mathbf{y}) g(\\mathbf{x}) is also a valid covariance function, where g(.): \\mathcal{X} \\rightarrow \\mathbb{R} g(.): \\mathcal{X} \\rightarrow \\mathbb{R} is a non-negative function from the domain of the inputs \\mathcal{X} \\mathcal{X} to the real number line. We call these covariances scaled covariance functions . //Instantiate some kernel val kernel : LocalScalarKernel [ I ] = _ val scalingFunction : ( I ) => Double = _ val scKernel = ScaledKernel ( kernel , DataPipe ( scalingFunction ))","title":"Scaling Covariances"},{"location":"core/core_kernels/#advanced-composite-kernels","text":"Sometimes we would like to express a kernel function as a product (or sum) of component kernels each of which act on a sub-set of the dimensions (degree of freedom) of the input attributes. For example; for 4 dimensional input vector, we may define two component kernels acting on the first two and last two dimensions respectively and combine their evaluations via addition or multiplication. For this purpose the dynaml.kernels package has the DecomposableCovariance [ S ] class. In order to create a decomposable kernel you need three components. The component kernels (order matters) An Encoder [ S , Array [ S ]] instance which splits the input into an array of components A Reducer which combines the individual kernel evaluations. //Not required in REPL, already imported import io.github.mandar2812.dynaml.DynaMLPipe._ import io.github.mandar2812.dynaml.pipes._ val kernel1 : LocalScalarKernel [ DenseVector [ Double ]] = _ val kernel2 : LocalScalarKernel [ DenseVector [ Double ]] = _ //Default Reducer is addition val decomp_kernel = new DecomposableCovariance [ DenseVector [ Double ]]( kernel1 , kernel2 )( breezeDVSplitEncoder ( 2 )) val decomp_kernel_mult = new DecomposableCovariance [ DenseVector [ Double ]]( kernel1 , kernel2 )( breezeDVSplitEncoder ( 2 ), Reducer .:*:) Implementing Custom Kernels You can implement your own custom kernels by extending the LocalScalarKernel[T] interface, for example: import breeze.linalg. { DenseMatrix , norm , DenseVector } //You can have any number of constructor parameters class MyNewKernel ( th : Double = 1.0 ) extends LocalScalarKernel [ DenseVector [ Double ]] with Serializable { //One must specify the names of each hyper-parameter override val hyper_parameters = List ( \"theta\" ) //The state variable stores the //current value of all kernel hyper-parameters state = Map ( \"theta\" -> th ) // The implementation of the actual kernel function override def evaluateAt ( config : Map [ String , Double ])( x : DenseVector [ Double ], y : DenseVector [ Double ]) : Double = ??? // Return the gradient of the kernel for each hyper-parameter // for a particular pair of points x,y override def gradientAt ( config : Map [ String , Double ])( x : DenseVector [ Double ], y : DenseVector [ Double ]) : Map [ String , Double ] = ??? }","title":"Advanced Composite Kernels"},{"location":"core/core_lssvm/","text":"Least Squares Support Vector Machines are a modification of the classical Support Vector Machine, please see Suykens et. al for a complete background. LSSVM Regression \u00b6 In case of LSSVM regression one solves (by applying the KKT conditions) the following constrained optimization problem. \\begin{align} & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\ & y_k = w^\\intercal \\varphi(x) + b + e_k, \\ k =1, \\cdots, N \\end{align} \\begin{align} & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\ & y_k = w^\\intercal \\varphi(x) + b + e_k, \\ k =1, \\cdots, N \\end{align} Leading to a predictive model of the form. \\begin{equation} y(x) = \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\end{equation} \\begin{equation} y(x) = \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\end{equation} Where the values \\alpha \\ \\& \\ b \\alpha \\ \\& \\ b are the solution of \\begin{equation} \\left[\\begin{array}{c|c} 0 & 1^\\intercal_v \\\\ \\hline 1_v & K + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline y \\end{array}\\right] \\end{equation} \\begin{equation} \\left[\\begin{array}{c|c} 0 & 1^\\intercal_v \\\\ \\hline 1_v & K + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline y \\end{array}\\right] \\end{equation} Here K is the N \\times N N \\times N kernel matrix whose entries are given by K_{kl} = \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N K_{kl} = \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N and I I is the identity matrix of order N N . LSSVM Classification \u00b6 In case of LSSVM for binary classification one solves (by applying the KKT conditions) the following constrained optimization problem. \\begin{align} & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\ & y_k[w^\\intercal \\varphi(x) + b] = 1 - e_k, \\ k =1, \\cdots, N \\end{align} \\begin{align} & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\ & y_k[w^\\intercal \\varphi(x) + b] = 1 - e_k, \\ k =1, \\cdots, N \\end{align} Leading to a classifier of the form. \\begin{equation} y(x) = sign \\left[ \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\right] \\end{equation} \\begin{equation} y(x) = sign \\left[ \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\right] \\end{equation} Where the values \\alpha \\ \\& \\ b \\alpha \\ \\& \\ b are the solution of \\begin{equation} \\left[\\begin{array}{c|c} 0 & y^\\intercal \\\\ \\hline y & \\Omega + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline 1_v \\end{array}\\right] \\end{equation} \\begin{equation} \\left[\\begin{array}{c|c} 0 & y^\\intercal \\\\ \\hline y & \\Omega + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline 1_v \\end{array}\\right] \\end{equation} Here \\Omega \\Omega is the N \\times N N \\times N matrix whose entries are given by \\begin{align} \\Omega_{kl} & = y_{k} y_{l} \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N \\\\ & = y_{k} y_{l} K(x_k, x_l) \\end{align} \\begin{align} \\Omega_{kl} & = y_{k} y_{l} \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N \\\\ & = y_{k} y_{l} K(x_k, x_l) \\end{align} and I I is the identity matrix of order N N . // Create the training data set val data : Stream [( DenseVector [ Double ] , Double )] = ... val numPoints = data . length val num_features = data . head . _1 . length // Create an implicit vector field for the creation of the stationary // radial basis function kernel implicit val field = VectorField ( num_features ) val kern = new RBFKernel ( 2.0 ) //Create the model val lssvmModel = new DLSSVM ( data , numPoints , kern , modelTask = \"regression\" ) //Set the regularization parameter and learn the model model . setRegParam ( 1.5 ). learn ()","title":"Least Squares SVM"},{"location":"core/core_lssvm/#lssvm-regression","text":"In case of LSSVM regression one solves (by applying the KKT conditions) the following constrained optimization problem. \\begin{align} & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\ & y_k = w^\\intercal \\varphi(x) + b + e_k, \\ k =1, \\cdots, N \\end{align} \\begin{align} & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\ & y_k = w^\\intercal \\varphi(x) + b + e_k, \\ k =1, \\cdots, N \\end{align} Leading to a predictive model of the form. \\begin{equation} y(x) = \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\end{equation} \\begin{equation} y(x) = \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\end{equation} Where the values \\alpha \\ \\& \\ b \\alpha \\ \\& \\ b are the solution of \\begin{equation} \\left[\\begin{array}{c|c} 0 & 1^\\intercal_v \\\\ \\hline 1_v & K + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline y \\end{array}\\right] \\end{equation} \\begin{equation} \\left[\\begin{array}{c|c} 0 & 1^\\intercal_v \\\\ \\hline 1_v & K + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline y \\end{array}\\right] \\end{equation} Here K is the N \\times N N \\times N kernel matrix whose entries are given by K_{kl} = \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N K_{kl} = \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N and I I is the identity matrix of order N N .","title":"LSSVM Regression"},{"location":"core/core_lssvm/#lssvm-classification","text":"In case of LSSVM for binary classification one solves (by applying the KKT conditions) the following constrained optimization problem. \\begin{align} & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\ & y_k[w^\\intercal \\varphi(x) + b] = 1 - e_k, \\ k =1, \\cdots, N \\end{align} \\begin{align} & \\min_{w,b,e} \\ \\mathcal{J}_P(w,e) = \\frac{1}{2}w^\\intercal w + \\gamma \\frac{1}{2} \\sum_{k = 1}^{N} e^2_k \\\\ & y_k[w^\\intercal \\varphi(x) + b] = 1 - e_k, \\ k =1, \\cdots, N \\end{align} Leading to a classifier of the form. \\begin{equation} y(x) = sign \\left[ \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\right] \\end{equation} \\begin{equation} y(x) = sign \\left[ \\sum_{k = 1}^{N}\\alpha_k K(x, x_k) + b \\right] \\end{equation} Where the values \\alpha \\ \\& \\ b \\alpha \\ \\& \\ b are the solution of \\begin{equation} \\left[\\begin{array}{c|c} 0 & y^\\intercal \\\\ \\hline y & \\Omega + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline 1_v \\end{array}\\right] \\end{equation} \\begin{equation} \\left[\\begin{array}{c|c} 0 & y^\\intercal \\\\ \\hline y & \\Omega + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline 1_v \\end{array}\\right] \\end{equation} Here \\Omega \\Omega is the N \\times N N \\times N matrix whose entries are given by \\begin{align} \\Omega_{kl} & = y_{k} y_{l} \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N \\\\ & = y_{k} y_{l} K(x_k, x_l) \\end{align} \\begin{align} \\Omega_{kl} & = y_{k} y_{l} \\varphi(x_k)^\\intercal\\varphi(x_l), \\ \\ k,l = 1, \\cdots, N \\\\ & = y_{k} y_{l} K(x_k, x_l) \\end{align} and I I is the identity matrix of order N N . // Create the training data set val data : Stream [( DenseVector [ Double ] , Double )] = ... val numPoints = data . length val num_features = data . head . _1 . length // Create an implicit vector field for the creation of the stationary // radial basis function kernel implicit val field = VectorField ( num_features ) val kern = new RBFKernel ( 2.0 ) //Create the model val lssvmModel = new DLSSVM ( data , numPoints , kern , modelTask = \"regression\" ) //Set the regularization parameter and learn the model model . setRegParam ( 1.5 ). learn ()","title":"LSSVM Classification"},{"location":"core/core_model_evaluation/","text":"Model evaluation is the litmus test for knowing if your modeling effort is headed in the right direction and for comparing various alternative models (or hypothesis) attempting to explain a phenomenon. The evaluation package contains classes and traits to calculate performance metrics for DynaML models. Classes which implement model performance calculation can extend the Metrics [ P ] trait. The Metrics trait requires that its sub-classes implement three methods or behaviors. Print out the performance metrics (whatever they may be) to the screen i.e. print method. Return the key performance indicators in the form of a breeze DenseVector [ Double ] , i.e. the kpi () method. Regression Models \u00b6 Regression models are generally evaluated on a few standard metrics such as mean square error , mean absolute error , coefficient of determination ( R^2 R^2 ), etc. DynaML has implementations for single output and multi-output regression models. Single Output \u00b6 Small Test Set The RegressionMetrics class takes as input a scala list containing the predictions and actual outputs and calculates the following metrics. Mean Absolute Error (mae) Root Mean Square Error (rmse) Correlation Coefficient ( \\rho_{y \\hat{y}} \\rho_{y \\hat{y}} ) Coefficient of Determination ( R^2 R^2 ) //Predictions computed by any model. val predictionAndOutputs : List [( Double , Double )] = ... val metrics = new RegressionMetrics ( predictionAndOutputs , predictionAndOutputs . length ) //Print results on screen metrics . print Large Test Set The RegressionMetricsSpark class takes as input an Apache Spark RDD containing the predictions and actual outputs and calculates the same metrics as above. //Predictions computed by any model. val predictionAndOutputs : RDD [( Double , Double )] = ... val metrics = new RegressionMetricsSpark ( predictionAndOutputs , predictionAndOutputs . length ) //Print results on screen metrics . print Multiple Outputs \u00b6 The MultiRegressionMetrics class calculates regression performance for multi-output models. //Predictions computed by any model. val predictionAndOutputs : List [( DenseVector [ Double ] , DenseVector [ Double ])] = ... val metrics = new MultiRegressionMetrics ( predictionAndOutputs , predictionAndOutputs . length ) //Print results on screen metrics . print Classification Models \u00b6 Currently (as of v1.4) there is only a binary classification implementation for calculating model performance. Binary Classification \u00b6 Small Test Sets The BinaryClassificationMetrics class calculates the following performance indicators. Classification accuracy F-measure Precision-Recall Curve (and area under it). Receiver Operating Characteristic (and area under it) Matthew's Correlation Coefficient val scoresAndLabels : List [( Double , Double )] = ... //Set logisticFlag = true in case outputs are produced via logistic regression val metrics = new BinaryClassificationMetrics ( scoresAndLabels , scoresAndLabels . length , logisticFlag = true ) metrics . print Large Test Sets The BinaryClassificationMetricsSpark class takes as input an Apache Spark RDD containing the predictions and actual labels and calculates the same metrics as above.","title":"Performance Evaluation"},{"location":"core/core_model_evaluation/#regression-models","text":"Regression models are generally evaluated on a few standard metrics such as mean square error , mean absolute error , coefficient of determination ( R^2 R^2 ), etc. DynaML has implementations for single output and multi-output regression models.","title":"Regression Models"},{"location":"core/core_model_evaluation/#single-output","text":"Small Test Set The RegressionMetrics class takes as input a scala list containing the predictions and actual outputs and calculates the following metrics. Mean Absolute Error (mae) Root Mean Square Error (rmse) Correlation Coefficient ( \\rho_{y \\hat{y}} \\rho_{y \\hat{y}} ) Coefficient of Determination ( R^2 R^2 ) //Predictions computed by any model. val predictionAndOutputs : List [( Double , Double )] = ... val metrics = new RegressionMetrics ( predictionAndOutputs , predictionAndOutputs . length ) //Print results on screen metrics . print Large Test Set The RegressionMetricsSpark class takes as input an Apache Spark RDD containing the predictions and actual outputs and calculates the same metrics as above. //Predictions computed by any model. val predictionAndOutputs : RDD [( Double , Double )] = ... val metrics = new RegressionMetricsSpark ( predictionAndOutputs , predictionAndOutputs . length ) //Print results on screen metrics . print","title":"Single Output"},{"location":"core/core_model_evaluation/#multiple-outputs","text":"The MultiRegressionMetrics class calculates regression performance for multi-output models. //Predictions computed by any model. val predictionAndOutputs : List [( DenseVector [ Double ] , DenseVector [ Double ])] = ... val metrics = new MultiRegressionMetrics ( predictionAndOutputs , predictionAndOutputs . length ) //Print results on screen metrics . print","title":"Multiple Outputs"},{"location":"core/core_model_evaluation/#classification-models","text":"Currently (as of v1.4) there is only a binary classification implementation for calculating model performance.","title":"Classification Models"},{"location":"core/core_model_evaluation/#binary-classification","text":"Small Test Sets The BinaryClassificationMetrics class calculates the following performance indicators. Classification accuracy F-measure Precision-Recall Curve (and area under it). Receiver Operating Characteristic (and area under it) Matthew's Correlation Coefficient val scoresAndLabels : List [( Double , Double )] = ... //Set logisticFlag = true in case outputs are produced via logistic regression val metrics = new BinaryClassificationMetrics ( scoresAndLabels , scoresAndLabels . length , logisticFlag = true ) metrics . print Large Test Sets The BinaryClassificationMetricsSpark class takes as input an Apache Spark RDD containing the predictions and actual labels and calculates the same metrics as above.","title":"Binary Classification"},{"location":"core/core_model_hierarchy/","text":"Model Classes \u00b6 In DynaML all model implementations fit into a well defined class hierarchy. In fact every DynaML machine learning model is an extension of the Model [ T , Q , R ] trait. Model [ T , Q , R ] The Model trait is quite bare bones: machine learning models are viewed as objects containing two parts or components. A training data set (of type T ). A method predict ( point : Q ) : R to generate a prediction of type R given a data point of type Q . Parameterized Models \u00b6 Many predictive models calculate predictions by formulating an expression which includes a set of parameters which are used along with the data points to generate predictions, the ParameterizedLearner[G, T, Q, R, S] class represents a skeleton for all parametric machine learning models such as Generalized Linear Models , Neural Networks , etc. Tip The defining characteristic of classes which extend ParameterizedLearner is that they must contain a member variable optimizer: RegularizedOptimizer[T, Q, R, S] which represents a regularization enabled optimizer implementation along with a learn () method which uses the optimizer member to calculate approximate values of the model parameters given the training data. Linear Models \u00b6 Linear models; represented by the LinearModel [ T , P , Q , R , S ] trait are extensions of ParameterizedLearner , this top level trait is extended to yield many useful linear prediction models. Generalized Linear Models which are linear in parameters expression for the predictions y y given a vector of processed features \\phi(x) \\phi(x) or basis functions. \\begin{equation} y = w^T\\varphi(x) + \\epsilon \\end{equation} \\begin{equation} y = w^T\\varphi(x) + \\epsilon \\end{equation} Stochastic Processes \u00b6 Stochastic processes (or random functions) are general probabilistic models which can be used to construct finite dimensional distributions over a set of sampled domain points. More specifically a stochastic process is a probabilistic function f(.) f(.) defined on any domain or index set \\mathcal{X} \\mathcal{X} such that for any finite collection x_i \\in \\mathcal{X}, i = 1 \\cdots N x_i \\in \\mathcal{X}, i = 1 \\cdots N , the finite dimensional distribution P(f(x_1), \\cdots, f(x_N)) P(f(x_1), \\cdots, f(x_N)) is coherently defined. Tip The StochasticProcessModel [ T , I , Y , W ] trait extends Model [ T , I , Y ] and is the top level trait for the implementation of general stochastic processes. In order to extend it, one must implement among others a function to output the posterior predictive distribution predictiveDistribution () . Continuous Processes \u00b6 By continuous processes, we mean processes whose values lie on a continuous domain (such as \\mathbb{R}^d \\mathbb{R}^d ). The ContinuousProcessModel [ T , I , Y , W ] abstract class provides a template which can be extended to implement continuous random process models. Tip The ContinuousProcessModel class contains the method predictionWithErrorBars() which takes inputs test data and number of standard deviations, and generates predictions with upper and lower error bars around them. In order to create a sub-class of ContinuousProcessModel , you must implement the method predictionWithErrorBars() . Second Order Processes \u00b6 Second order stochastic processes can be described by specifying the mean (first order statistic) and variance (second order statistic) of their finite dimensional distribution. The SecondOrderProcessModel [ T , I , Y , K , M , W ] trait is an abstract skeleton which describes what elements a second order process model must have i.e. the mean and covariance functions. Meta Models/Model Ensembles \u00b6 Meta models use predictions from several candidate models and derive a prediction that is a meaningful combination of the individual predictions. This may be achieved in several ways some of which are. Average of predictions/voting Weighted predictions: Problem is now transferred to calculating appropriate weights. Learning some non-trivial functional transformation of the individual prediction, also known as gating networks . Currently the DynaML API has the following classes providing capabilities of meta models. Abstract Classes MetaModel [ D , D1 , BaseModel ] CommitteeModel [ D , D1 , BaseModel ] Implementations LS-SVM Committee Neural Committee","title":"Model Hierarchy"},{"location":"core/core_model_hierarchy/#model-classes","text":"In DynaML all model implementations fit into a well defined class hierarchy. In fact every DynaML machine learning model is an extension of the Model [ T , Q , R ] trait. Model [ T , Q , R ] The Model trait is quite bare bones: machine learning models are viewed as objects containing two parts or components. A training data set (of type T ). A method predict ( point : Q ) : R to generate a prediction of type R given a data point of type Q .","title":"Model Classes"},{"location":"core/core_model_hierarchy/#parameterized-models","text":"Many predictive models calculate predictions by formulating an expression which includes a set of parameters which are used along with the data points to generate predictions, the ParameterizedLearner[G, T, Q, R, S] class represents a skeleton for all parametric machine learning models such as Generalized Linear Models , Neural Networks , etc. Tip The defining characteristic of classes which extend ParameterizedLearner is that they must contain a member variable optimizer: RegularizedOptimizer[T, Q, R, S] which represents a regularization enabled optimizer implementation along with a learn () method which uses the optimizer member to calculate approximate values of the model parameters given the training data.","title":"Parameterized Models"},{"location":"core/core_model_hierarchy/#linear-models","text":"Linear models; represented by the LinearModel [ T , P , Q , R , S ] trait are extensions of ParameterizedLearner , this top level trait is extended to yield many useful linear prediction models. Generalized Linear Models which are linear in parameters expression for the predictions y y given a vector of processed features \\phi(x) \\phi(x) or basis functions. \\begin{equation} y = w^T\\varphi(x) + \\epsilon \\end{equation} \\begin{equation} y = w^T\\varphi(x) + \\epsilon \\end{equation}","title":"Linear Models"},{"location":"core/core_model_hierarchy/#stochastic-processes","text":"Stochastic processes (or random functions) are general probabilistic models which can be used to construct finite dimensional distributions over a set of sampled domain points. More specifically a stochastic process is a probabilistic function f(.) f(.) defined on any domain or index set \\mathcal{X} \\mathcal{X} such that for any finite collection x_i \\in \\mathcal{X}, i = 1 \\cdots N x_i \\in \\mathcal{X}, i = 1 \\cdots N , the finite dimensional distribution P(f(x_1), \\cdots, f(x_N)) P(f(x_1), \\cdots, f(x_N)) is coherently defined. Tip The StochasticProcessModel [ T , I , Y , W ] trait extends Model [ T , I , Y ] and is the top level trait for the implementation of general stochastic processes. In order to extend it, one must implement among others a function to output the posterior predictive distribution predictiveDistribution () .","title":"Stochastic Processes"},{"location":"core/core_model_hierarchy/#continuous-processes","text":"By continuous processes, we mean processes whose values lie on a continuous domain (such as \\mathbb{R}^d \\mathbb{R}^d ). The ContinuousProcessModel [ T , I , Y , W ] abstract class provides a template which can be extended to implement continuous random process models. Tip The ContinuousProcessModel class contains the method predictionWithErrorBars() which takes inputs test data and number of standard deviations, and generates predictions with upper and lower error bars around them. In order to create a sub-class of ContinuousProcessModel , you must implement the method predictionWithErrorBars() .","title":"Continuous Processes"},{"location":"core/core_model_hierarchy/#second-order-processes","text":"Second order stochastic processes can be described by specifying the mean (first order statistic) and variance (second order statistic) of their finite dimensional distribution. The SecondOrderProcessModel [ T , I , Y , K , M , W ] trait is an abstract skeleton which describes what elements a second order process model must have i.e. the mean and covariance functions.","title":"Second Order Processes"},{"location":"core/core_model_hierarchy/#meta-modelsmodel-ensembles","text":"Meta models use predictions from several candidate models and derive a prediction that is a meaningful combination of the individual predictions. This may be achieved in several ways some of which are. Average of predictions/voting Weighted predictions: Problem is now transferred to calculating appropriate weights. Learning some non-trivial functional transformation of the individual prediction, also known as gating networks . Currently the DynaML API has the following classes providing capabilities of meta models. Abstract Classes MetaModel [ D , D1 , BaseModel ] CommitteeModel [ D , D1 , BaseModel ] Implementations LS-SVM Committee Neural Committee","title":"Meta Models/Model Ensembles"},{"location":"core/core_multi_output_t/","text":"Summary The multi-output matrix T regression model was first described by Conti and O' Hagan in their paper on Bayesian emulation of multi-output computer codes. It has been available in the dynaml.models.stp package of the dynaml-core module since v1.4.2 . Formulation \u00b6 The model starts from the multi-output gaussian process framework. The quantity of interest is some unknown function \\mathbf{f}: \\mathcal{X} \\rightarrow \\mathbb{R}^q \\mathbf{f}: \\mathcal{X} \\rightarrow \\mathbb{R}^q , which maps inputs in \\mathcal{X} \\mathcal{X} (an arbitrary input space) to a q q dimensional vector outputs. \\begin{align} \\mathbf{f}(.)|B,\\Sigma, \\theta &\\sim \\mathcal{GP}(\\mathbf{m}(.), c(.,.)\\Sigma) \\\\ \\mathbf{m}(x) &= B^\\intercal \\varphi(x) \\end{align} \\begin{align} \\mathbf{f}(.)|B,\\Sigma, \\theta &\\sim \\mathcal{GP}(\\mathbf{m}(.), c(.,.)\\Sigma) \\\\ \\mathbf{m}(x) &= B^\\intercal \\varphi(x) \\end{align} The input x x is transformed through \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^m \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^m which is a deterministic feature mapping which then calculates the inputs for a linear mean function \\mathbf{m}(.) \\mathbf{m}(.) . The parameters of this linear trend are contained in the matrix B \\in \\mathbb{R}^{m \\times q} B \\in \\mathbb{R}^{m \\times q} and \\theta \\theta contains all the covariance function hyper-parameters. The prior distribution of the multi-output function is represented as a matrix normal distribution, with c(.,.) c(.,.) representing the covariance between two input points, and the entries of \\Sigma \\Sigma being the covariance between the output dimensions. The predictive distribution when the output data D \\in \\mathbb{R}^{n\\times q} D \\in \\mathbb{R}^{n\\times q} is observed is calculated by first computing the conditional predictive distribution of \\mathbf{f}(.) | D, \\Sigma, B, \\theta \\mathbf{f}(.) | D, \\Sigma, B, \\theta and then integrating this distribution with respect to the posterior distributions \\Sigma|D \\Sigma|D and B|D B|D . The resulting predictive distribution \\mathbf{f}(.)| \\theta, D \\mathbf{f}(.)| \\theta, D has the following structure. \\begin{align} \\mathbf{f}(.)|\\theta,D &\\sim \\mathcal{T}(\\mathbf{m}^{**}(.), c^{**}\\Sigma_{GLS};n-m) \\\\ \\end{align} \\begin{align} \\mathbf{f}(.)|\\theta,D &\\sim \\mathcal{T}(\\mathbf{m}^{**}(.), c^{**}\\Sigma_{GLS};n-m) \\\\ \\end{align} The distribution is a matrix variate T distribution . It is described by Mean \\mathbf{m}^{**}(x) \\mathbf{m}^{**}(x) . Covariance between rows c^{**}(x_{1}, x_{2}) c^{**}(x_{1}, x_{2}) Covariance function between output columns \\Sigma_{GLS} \\Sigma_{GLS} Degrees of freedom n-m n-m . \\begin{align} \\mathbf{m}^{**}(x_{1}) &= B_{GLS}^{\\intercal}\\varphi(x_{1}) + (D-\\varphi(X)B_{GLS})^{\\intercal} C^{-1}c(x_{1},.)\\\\ c^{**}(x_{1}, x_{2}) &= \\bar{c}(x_{1}, x_{2}) + \\hat{c}(x_{1}, x_{2})\\\\ \\bar{c}(x_{1}, x_{2}) &= c(x_{1}, x_{2}) - C(x_{1},.)^{\\intercal}C^{-1}C(x_{2},.) \\\\ \\hat{c}(x_{1}, x_{2}) &= H(x_{1})^{\\intercal}.A^{-1}.H(x_{2})\\\\ H(x) &= (\\varphi(x) - \\varphi(X)C^{-1}c(x,.)) \\\\ A &= \\varphi(X)^{\\intercal}C^{-1}\\varphi(X)\\\\ \\end{align} \\begin{align} \\mathbf{m}^{**}(x_{1}) &= B_{GLS}^{\\intercal}\\varphi(x_{1}) + (D-\\varphi(X)B_{GLS})^{\\intercal} C^{-1}c(x_{1},.)\\\\ c^{**}(x_{1}, x_{2}) &= \\bar{c}(x_{1}, x_{2}) + \\hat{c}(x_{1}, x_{2})\\\\ \\bar{c}(x_{1}, x_{2}) &= c(x_{1}, x_{2}) - C(x_{1},.)^{\\intercal}C^{-1}C(x_{2},.) \\\\ \\hat{c}(x_{1}, x_{2}) &= H(x_{1})^{\\intercal}.A^{-1}.H(x_{2})\\\\ H(x) &= (\\varphi(x) - \\varphi(X)C^{-1}c(x,.)) \\\\ A &= \\varphi(X)^{\\intercal}C^{-1}\\varphi(X)\\\\ \\end{align} The matrices B_{GLS} = (\\varphi(X)^{\\intercal}C^{-1}\\varphi(X))^{-1}\\varphi(X)^{\\intercal}C^{-1}D B_{GLS} = (\\varphi(X)^{\\intercal}C^{-1}\\varphi(X))^{-1}\\varphi(X)^{\\intercal}C^{-1}D and \\Sigma_{GLS} = (n-m)^{-1}(D - \\varphi(X)B_{GLS})^{\\intercal}C^{-1}(D - \\varphi(X)B_{GLS}) \\Sigma_{GLS} = (n-m)^{-1}(D - \\varphi(X)B_{GLS})^{\\intercal}C^{-1}(D - \\varphi(X)B_{GLS}) are the generalized least squares estimators for the matrices B B and \\Sigma \\Sigma which we saw in the formulation above. Multi-output Regression \u00b6 An implementation of the multi-output matrix T model is available via the class MVStudentsTModel . Instantiating the model is very similar to other stochastic process models in DynaML i.e. by specifying the covariance structures on signal and noise, training data, etc. //Obtain the data, some generic type val trainingdata : DataType = _ val num_data_points : Int = _ val num_outputs : Int = _ val kernel : LocalScalarKernel [ I ] = _ val noiseKernel : LocalScalarKernel [ I ] = _ val feature_map : DataPipe [ I , Double ] = _ //Define how the data is converted to a compatible type implicit val transform : DataPipe [ DataType , Seq [( I , Double )]] = _ val model = MVStudentsTModel ( kernel , noiseKernel , feature_map )( trainingData , num_data_points , num_outputs )","title":"Multi-output Matrix T Process"},{"location":"core/core_multi_output_t/#formulation","text":"The model starts from the multi-output gaussian process framework. The quantity of interest is some unknown function \\mathbf{f}: \\mathcal{X} \\rightarrow \\mathbb{R}^q \\mathbf{f}: \\mathcal{X} \\rightarrow \\mathbb{R}^q , which maps inputs in \\mathcal{X} \\mathcal{X} (an arbitrary input space) to a q q dimensional vector outputs. \\begin{align} \\mathbf{f}(.)|B,\\Sigma, \\theta &\\sim \\mathcal{GP}(\\mathbf{m}(.), c(.,.)\\Sigma) \\\\ \\mathbf{m}(x) &= B^\\intercal \\varphi(x) \\end{align} \\begin{align} \\mathbf{f}(.)|B,\\Sigma, \\theta &\\sim \\mathcal{GP}(\\mathbf{m}(.), c(.,.)\\Sigma) \\\\ \\mathbf{m}(x) &= B^\\intercal \\varphi(x) \\end{align} The input x x is transformed through \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^m \\varphi(.): \\mathcal{X} \\rightarrow \\mathbb{R}^m which is a deterministic feature mapping which then calculates the inputs for a linear mean function \\mathbf{m}(.) \\mathbf{m}(.) . The parameters of this linear trend are contained in the matrix B \\in \\mathbb{R}^{m \\times q} B \\in \\mathbb{R}^{m \\times q} and \\theta \\theta contains all the covariance function hyper-parameters. The prior distribution of the multi-output function is represented as a matrix normal distribution, with c(.,.) c(.,.) representing the covariance between two input points, and the entries of \\Sigma \\Sigma being the covariance between the output dimensions. The predictive distribution when the output data D \\in \\mathbb{R}^{n\\times q} D \\in \\mathbb{R}^{n\\times q} is observed is calculated by first computing the conditional predictive distribution of \\mathbf{f}(.) | D, \\Sigma, B, \\theta \\mathbf{f}(.) | D, \\Sigma, B, \\theta and then integrating this distribution with respect to the posterior distributions \\Sigma|D \\Sigma|D and B|D B|D . The resulting predictive distribution \\mathbf{f}(.)| \\theta, D \\mathbf{f}(.)| \\theta, D has the following structure. \\begin{align} \\mathbf{f}(.)|\\theta,D &\\sim \\mathcal{T}(\\mathbf{m}^{**}(.), c^{**}\\Sigma_{GLS};n-m) \\\\ \\end{align} \\begin{align} \\mathbf{f}(.)|\\theta,D &\\sim \\mathcal{T}(\\mathbf{m}^{**}(.), c^{**}\\Sigma_{GLS};n-m) \\\\ \\end{align} The distribution is a matrix variate T distribution . It is described by Mean \\mathbf{m}^{**}(x) \\mathbf{m}^{**}(x) . Covariance between rows c^{**}(x_{1}, x_{2}) c^{**}(x_{1}, x_{2}) Covariance function between output columns \\Sigma_{GLS} \\Sigma_{GLS} Degrees of freedom n-m n-m . \\begin{align} \\mathbf{m}^{**}(x_{1}) &= B_{GLS}^{\\intercal}\\varphi(x_{1}) + (D-\\varphi(X)B_{GLS})^{\\intercal} C^{-1}c(x_{1},.)\\\\ c^{**}(x_{1}, x_{2}) &= \\bar{c}(x_{1}, x_{2}) + \\hat{c}(x_{1}, x_{2})\\\\ \\bar{c}(x_{1}, x_{2}) &= c(x_{1}, x_{2}) - C(x_{1},.)^{\\intercal}C^{-1}C(x_{2},.) \\\\ \\hat{c}(x_{1}, x_{2}) &= H(x_{1})^{\\intercal}.A^{-1}.H(x_{2})\\\\ H(x) &= (\\varphi(x) - \\varphi(X)C^{-1}c(x,.)) \\\\ A &= \\varphi(X)^{\\intercal}C^{-1}\\varphi(X)\\\\ \\end{align} \\begin{align} \\mathbf{m}^{**}(x_{1}) &= B_{GLS}^{\\intercal}\\varphi(x_{1}) + (D-\\varphi(X)B_{GLS})^{\\intercal} C^{-1}c(x_{1},.)\\\\ c^{**}(x_{1}, x_{2}) &= \\bar{c}(x_{1}, x_{2}) + \\hat{c}(x_{1}, x_{2})\\\\ \\bar{c}(x_{1}, x_{2}) &= c(x_{1}, x_{2}) - C(x_{1},.)^{\\intercal}C^{-1}C(x_{2},.) \\\\ \\hat{c}(x_{1}, x_{2}) &= H(x_{1})^{\\intercal}.A^{-1}.H(x_{2})\\\\ H(x) &= (\\varphi(x) - \\varphi(X)C^{-1}c(x,.)) \\\\ A &= \\varphi(X)^{\\intercal}C^{-1}\\varphi(X)\\\\ \\end{align} The matrices B_{GLS} = (\\varphi(X)^{\\intercal}C^{-1}\\varphi(X))^{-1}\\varphi(X)^{\\intercal}C^{-1}D B_{GLS} = (\\varphi(X)^{\\intercal}C^{-1}\\varphi(X))^{-1}\\varphi(X)^{\\intercal}C^{-1}D and \\Sigma_{GLS} = (n-m)^{-1}(D - \\varphi(X)B_{GLS})^{\\intercal}C^{-1}(D - \\varphi(X)B_{GLS}) \\Sigma_{GLS} = (n-m)^{-1}(D - \\varphi(X)B_{GLS})^{\\intercal}C^{-1}(D - \\varphi(X)B_{GLS}) are the generalized least squares estimators for the matrices B B and \\Sigma \\Sigma which we saw in the formulation above.","title":"Formulation"},{"location":"core/core_multi_output_t/#multi-output-regression","text":"An implementation of the multi-output matrix T model is available via the class MVStudentsTModel . Instantiating the model is very similar to other stochastic process models in DynaML i.e. by specifying the covariance structures on signal and noise, training data, etc. //Obtain the data, some generic type val trainingdata : DataType = _ val num_data_points : Int = _ val num_outputs : Int = _ val kernel : LocalScalarKernel [ I ] = _ val noiseKernel : LocalScalarKernel [ I ] = _ val feature_map : DataPipe [ I , Double ] = _ //Define how the data is converted to a compatible type implicit val transform : DataPipe [ DataType , Seq [( I , Double )]] = _ val model = MVStudentsTModel ( kernel , noiseKernel , feature_map )( trainingData , num_data_points , num_outputs )","title":"Multi-output Regression"},{"location":"core/core_opt_convex/","text":"Model Solvers \u00b6 Model solvers are implementations which either solve for the parameters/coefficients which determine the prediction of a model. Below is a list of all model solvers currently implemented, they are all sub-classes/subtraits of the top level optimization API. Refer to the wiki page on optimizers for more details on extending the API and writing your own optimizers. Gradient Descent \u00b6 The bread and butter of any machine learning framework, the GradientDescent class in the dynaml . optimization package provides gradient based optimization primitives for solving optimization problems of the form. \\begin{equation} f(w) := \\lambda\\, R(w) + \\frac1n \\sum_{k=1}^n L(w;x_k,y_k) \\label{eq:regPrimal} \\ . \\end{equation} \\begin{equation} f(w) := \\lambda\\, R(w) + \\frac1n \\sum_{k=1}^n L(w;x_k,y_k) \\label{eq:regPrimal} \\ . \\end{equation} Gradients \u00b6 Name Class Equation Logistic Gradient LogisticGradient L = \\frac1n \\sum_{k=1}^n \\log(1+\\exp( -y_k w^T x_k)), y_k \\in \\{-1, +1\\} L = \\frac1n \\sum_{k=1}^n \\log(1+\\exp( -y_k w^T x_k)), y_k \\in \\{-1, +1\\} Least Squares Gradient LeastSquaresGradient L = \\frac1n \\sum_{k=1}^n \\|w^{T} \\cdot x_k - y_k\\|^2 L = \\frac1n \\sum_{k=1}^n \\|w^{T} \\cdot x_k - y_k\\|^2 Updaters \u00b6 Name Class Equation L_1 L_1 Updater L1Updater R = \\|\\|w\\|\\|_{1} R = \\|\\|w\\|\\|_{1} L_2 L_2 Updater SquaredL2Updater R = \\frac{1}{2} \\|\\|w\\|\\|^2 R = \\frac{1}{2} \\|\\|w\\|\\|^2 BFGS Updater SimpleBFGSUpdater val data : Stream [( DenseVector [ Double ] , Double )] = ... val num_points = data . length val initial_params : DenseVector [ Double ] = ... val optimizer = new GradientDescent ( new LogisticGradient , new SquaredL2Updater ) val params = optimizer . setRegParam ( 0.002 ). optimize ( num_points , data , initial_params ) Quasi-Newton (BFGS) \u00b6 The Broydon-Fletcher-Goldfarb-Shanno (BFGS) is a Quasi-Newton based second order optimization method. To calculate an update to the parameters, it requires calculation of the inverse Hessian \\mathit{H}^{-1} \\mathit{H}^{-1} as well as the gradient at each iteration. val optimizer = QuasiNewtonOptimizer ( new LeastSquaresGradient , new SimpleBFGSUpdater ) val data : Stream [( DenseVector [ Double ] , Double )] = ... val num_points = data . length val initial_params : DenseVector [ Double ] = ... val params = optimizer . setRegParam ( 0.002 ). optimize ( num_points , data , initial_params ) Regularized Least Squares \u00b6 This subroutine solves the regularized least squares optimization problem as shown below. \\begin{equation} \\min_{w} \\ \\mathcal{J}_{P}(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2 \\end{equation} \\begin{equation} \\min_{w} \\ \\mathcal{J}_{P}(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2 \\end{equation} val num_dim = ... val designMatrix : DenseMatrix [ Double ] = ... val response : DenseVector [ Double ] = ... val optimizer = new RegularizedLSSolver () val x = optimizer . setRegParam ( 0.05 ). optimize ( designMatrix . nrow , ( designMatrix , response ), DenseVector . ones [ Double ]( num_dim )) Back propagation with Momentum \u00b6 This is the most common learning methods for supervised training of feed forward neural networks, the edge weights are adjusted using the generalized delta rule . val data : Seq [( DenseVector [ Double ] , DenseVector [ Double ])] = _ //Input, Hidden, Output val num_units_by_layer = Seq ( 5 , 8 , 3 ) val acts = Seq ( VectorSigmoid , VectorTansig ) val breezeStackFactory = NeuralStackFactory ( num_units_by_layer )( acts ) //Random variable which samples layer weights val stackInitializer = GenericFFNeuralNet . getWeightInitializer ( num_units_by_layer ) val opt_backprop = new FFBackProp ( breezeStackFactory ) val learned_stack = opt_backprop . optimize ( data . length , data , stackInitializer . draw ) Deprecated back propagation API val data : Stream [( DenseVector [ Double ] , DenseVector [ Double ])] = ... val initParam = FFNeuralGraph ( num_inputs = data . head . _1 . length , num_outputs = data . head . _2 . length , hidden_layers = 1 , List ( \"logsig\" , \"linear\" ), List ( 5 )) val optimizer = new BackPropogation () . setNumIterations ( 100 ) . setStepSize ( 0.01 ) val newparams = optimizer . optimize ( data . length , data , initParam ) Conjugate Gradient \u00b6 The conjugate gradient method is used to solve linear systems of the form Ax = b Ax = b where A A is a symmetric positive definite matrix. val num_dim = ... val A : DenseMatrix [ Double ] = ... val b : DenseVector [ Double ] = ... ///Solves A.x = b val x = ConjugateGradient . runCG ( A , b , DenseVector . ones [ Double ]( num_dim ), epsilon = 0.005 , MAX_ITERATIONS = 50 ) Dual LSSVM Solver \u00b6 The LSSVM solver solves the linear program that results from the application of the Karush, Kuhn Tucker conditions on the LSSVM optimization problem. \\begin{equation} \\left[\\begin{array}{c|c} 0 & 1^\\intercal_v \\\\ \\hline 1_v & K + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline y \\end{array}\\right] \\end{equation} \\begin{equation} \\left[\\begin{array}{c|c} 0 & 1^\\intercal_v \\\\ \\hline 1_v & K + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline y \\end{array}\\right] \\end{equation} val data : Stream [( DenseVector [ Double ] , Double )] = ... val kernelMatrix : DenseMatrix [ Double ] = ... val initParam = DenseVector . ones [ Double ]( num_points + 1 ) val optimizer = new LSSVMLinearSolver () val alpha = optimizer . optimize ( num_points , ( kernelMatrix , DenseVector ( data . map ( _ . _2 ). toArray )), initParam ) Committee Model Solver \u00b6 The committee model solver aims to find the optimum values of weights applied to the predictions of a set of base models. The weights are calculated as follows. \\alpha = \\frac{C^{-1} \\overrightarrow{1}}{\\overrightarrow{1}^T C^{-1} \\overrightarrow{1}} \\alpha = \\frac{C^{-1} \\overrightarrow{1}}{\\overrightarrow{1}^T C^{-1} \\overrightarrow{1}} Where C C is the sample correlation matrix of errors for all combinations of the base models calculated on the training data. val optimizer = new CommitteeModelSolver () //Data Structure containing for each training point the following couple //(predictions from base models as a vector, actual target) val predictionsTargets : Stream [( DenseVector [ Double ] , Double )] = ... val params = optimizer . optimize ( num_points , predictionsTargets , DenseVector . ones [ Double ]( num_of_models ))","title":"Convex"},{"location":"core/core_opt_convex/#model-solvers","text":"Model solvers are implementations which either solve for the parameters/coefficients which determine the prediction of a model. Below is a list of all model solvers currently implemented, they are all sub-classes/subtraits of the top level optimization API. Refer to the wiki page on optimizers for more details on extending the API and writing your own optimizers.","title":"Model Solvers"},{"location":"core/core_opt_convex/#gradient-descent","text":"The bread and butter of any machine learning framework, the GradientDescent class in the dynaml . optimization package provides gradient based optimization primitives for solving optimization problems of the form. \\begin{equation} f(w) := \\lambda\\, R(w) + \\frac1n \\sum_{k=1}^n L(w;x_k,y_k) \\label{eq:regPrimal} \\ . \\end{equation} \\begin{equation} f(w) := \\lambda\\, R(w) + \\frac1n \\sum_{k=1}^n L(w;x_k,y_k) \\label{eq:regPrimal} \\ . \\end{equation}","title":"Gradient Descent"},{"location":"core/core_opt_convex/#gradients","text":"Name Class Equation Logistic Gradient LogisticGradient L = \\frac1n \\sum_{k=1}^n \\log(1+\\exp( -y_k w^T x_k)), y_k \\in \\{-1, +1\\} L = \\frac1n \\sum_{k=1}^n \\log(1+\\exp( -y_k w^T x_k)), y_k \\in \\{-1, +1\\} Least Squares Gradient LeastSquaresGradient L = \\frac1n \\sum_{k=1}^n \\|w^{T} \\cdot x_k - y_k\\|^2 L = \\frac1n \\sum_{k=1}^n \\|w^{T} \\cdot x_k - y_k\\|^2","title":"Gradients"},{"location":"core/core_opt_convex/#updaters","text":"Name Class Equation L_1 L_1 Updater L1Updater R = \\|\\|w\\|\\|_{1} R = \\|\\|w\\|\\|_{1} L_2 L_2 Updater SquaredL2Updater R = \\frac{1}{2} \\|\\|w\\|\\|^2 R = \\frac{1}{2} \\|\\|w\\|\\|^2 BFGS Updater SimpleBFGSUpdater val data : Stream [( DenseVector [ Double ] , Double )] = ... val num_points = data . length val initial_params : DenseVector [ Double ] = ... val optimizer = new GradientDescent ( new LogisticGradient , new SquaredL2Updater ) val params = optimizer . setRegParam ( 0.002 ). optimize ( num_points , data , initial_params )","title":"Updaters"},{"location":"core/core_opt_convex/#quasi-newton-bfgs","text":"The Broydon-Fletcher-Goldfarb-Shanno (BFGS) is a Quasi-Newton based second order optimization method. To calculate an update to the parameters, it requires calculation of the inverse Hessian \\mathit{H}^{-1} \\mathit{H}^{-1} as well as the gradient at each iteration. val optimizer = QuasiNewtonOptimizer ( new LeastSquaresGradient , new SimpleBFGSUpdater ) val data : Stream [( DenseVector [ Double ] , Double )] = ... val num_points = data . length val initial_params : DenseVector [ Double ] = ... val params = optimizer . setRegParam ( 0.002 ). optimize ( num_points , data , initial_params )","title":"Quasi-Newton (BFGS)"},{"location":"core/core_opt_convex/#regularized-least-squares","text":"This subroutine solves the regularized least squares optimization problem as shown below. \\begin{equation} \\min_{w} \\ \\mathcal{J}_{P}(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2 \\end{equation} \\begin{equation} \\min_{w} \\ \\mathcal{J}_{P}(w) = \\frac{1}{2} \\gamma \\ w^Tw + \\frac{1}{2} \\sum_{k = 1}^{N} (y_k - w^T \\varphi(x_k))^2 \\end{equation} val num_dim = ... val designMatrix : DenseMatrix [ Double ] = ... val response : DenseVector [ Double ] = ... val optimizer = new RegularizedLSSolver () val x = optimizer . setRegParam ( 0.05 ). optimize ( designMatrix . nrow , ( designMatrix , response ), DenseVector . ones [ Double ]( num_dim ))","title":"Regularized Least Squares"},{"location":"core/core_opt_convex/#back-propagation-with-momentum","text":"This is the most common learning methods for supervised training of feed forward neural networks, the edge weights are adjusted using the generalized delta rule . val data : Seq [( DenseVector [ Double ] , DenseVector [ Double ])] = _ //Input, Hidden, Output val num_units_by_layer = Seq ( 5 , 8 , 3 ) val acts = Seq ( VectorSigmoid , VectorTansig ) val breezeStackFactory = NeuralStackFactory ( num_units_by_layer )( acts ) //Random variable which samples layer weights val stackInitializer = GenericFFNeuralNet . getWeightInitializer ( num_units_by_layer ) val opt_backprop = new FFBackProp ( breezeStackFactory ) val learned_stack = opt_backprop . optimize ( data . length , data , stackInitializer . draw ) Deprecated back propagation API val data : Stream [( DenseVector [ Double ] , DenseVector [ Double ])] = ... val initParam = FFNeuralGraph ( num_inputs = data . head . _1 . length , num_outputs = data . head . _2 . length , hidden_layers = 1 , List ( \"logsig\" , \"linear\" ), List ( 5 )) val optimizer = new BackPropogation () . setNumIterations ( 100 ) . setStepSize ( 0.01 ) val newparams = optimizer . optimize ( data . length , data , initParam )","title":"Back propagation with Momentum"},{"location":"core/core_opt_convex/#conjugate-gradient","text":"The conjugate gradient method is used to solve linear systems of the form Ax = b Ax = b where A A is a symmetric positive definite matrix. val num_dim = ... val A : DenseMatrix [ Double ] = ... val b : DenseVector [ Double ] = ... ///Solves A.x = b val x = ConjugateGradient . runCG ( A , b , DenseVector . ones [ Double ]( num_dim ), epsilon = 0.005 , MAX_ITERATIONS = 50 )","title":"Conjugate Gradient"},{"location":"core/core_opt_convex/#dual-lssvm-solver","text":"The LSSVM solver solves the linear program that results from the application of the Karush, Kuhn Tucker conditions on the LSSVM optimization problem. \\begin{equation} \\left[\\begin{array}{c|c} 0 & 1^\\intercal_v \\\\ \\hline 1_v & K + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline y \\end{array}\\right] \\end{equation} \\begin{equation} \\left[\\begin{array}{c|c} 0 & 1^\\intercal_v \\\\ \\hline 1_v & K + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline y \\end{array}\\right] \\end{equation} val data : Stream [( DenseVector [ Double ] , Double )] = ... val kernelMatrix : DenseMatrix [ Double ] = ... val initParam = DenseVector . ones [ Double ]( num_points + 1 ) val optimizer = new LSSVMLinearSolver () val alpha = optimizer . optimize ( num_points , ( kernelMatrix , DenseVector ( data . map ( _ . _2 ). toArray )), initParam )","title":"Dual LSSVM Solver"},{"location":"core/core_opt_convex/#committee-model-solver","text":"The committee model solver aims to find the optimum values of weights applied to the predictions of a set of base models. The weights are calculated as follows. \\alpha = \\frac{C^{-1} \\overrightarrow{1}}{\\overrightarrow{1}^T C^{-1} \\overrightarrow{1}} \\alpha = \\frac{C^{-1} \\overrightarrow{1}}{\\overrightarrow{1}^T C^{-1} \\overrightarrow{1}} Where C C is the sample correlation matrix of errors for all combinations of the base models calculated on the training data. val optimizer = new CommitteeModelSolver () //Data Structure containing for each training point the following couple //(predictions from base models as a vector, actual target) val predictionsTargets : Stream [( DenseVector [ Double ] , Double )] = ... val params = optimizer . optimize ( num_points , predictionsTargets , DenseVector . ones [ Double ]( num_of_models ))","title":"Committee Model Solver"},{"location":"core/core_opt_global/","text":"Model Selection Routines \u00b6 These routines are also known as global optimizers , paradigms/algorithms such as genetic algorithms, gibbs sampling, simulated annealing, evolutionary optimization fall under this category. They can be used in situations when the objective function in not \"smooth\". In DynaML they are most prominently used in hyper-parameter optimization in kernel based learning methods. All global optimizers in DynaML extend the GlobalOptimizer trait, which implies that they provide an implementation for its optimize method. In order to use a global optimization routine on an model, the model implementation in question must be extending the GloballyOptimizable trait in the dynaml.optimization package, this trait has only one method called energy which is to be implemented by all sub-classes/traits. The energy method calculates the value of the global objective function for a particular configuration i.e. for particular values of model hyper-parameters. This objective function can be defined differently for each model class (marginal likelihood for Gaussian Processes, cross validation score for parametric models, etc). The following model selection routines are available in DynaML so far. Grid Search \u00b6 The most elementary (naive) method of model selection is to evaluate its performance (value returned by energy ) on a fixed set of grid points which are initialized for the model hyper-parameters. val kernel = ... val noise = ... val data = ... val model = new GPRegression ( kernel , noise , data ) val grid = 5 val step = 0.2 val gs = new GridSearch [ model.type ]( model ) . setGridSize ( grid ) . setStepSize ( step ) . setLogScale ( false ) val startConf = kernel . state ++ noise . state val ( _ , conf ) = gs . optimize ( startConf , opt ) model . setState ( conf ) Coupled Simulated Annealing \u00b6 Coupled Simulated Annealing (CSA) is an iterative search procedure which evaluates model performance on a grid and in each iteration perturbs the grid points in a randomized manner. Each perturbed point is accepted using a certain acceptance probability which is a function of the performance on the whole grid. Coupled Simulated Annealing can be seen as an extension to the classical Simulated Annealing algorithm, since the acceptance probability and perturbation function are design choices, we can formulate a number of variants of CSA. Any CSA-like algorithm must have the following components. An ensemble or grid of points x_i \\in \\Theta x_i \\in \\Theta . A perturbation distribution or function $P: x_i \\rightarrow y_i $. A coupling term \\gamma \\gamma for an ensemble. An acceptance probability function A_{\\Theta}(\\gamma, x_i \\rightarrow y_i) A_{\\Theta}(\\gamma, x_i \\rightarrow y_i) . An annealing schedule T_{k}^{ac}, k = 0, 1, \\cdots T_{k}^{ac}, k = 0, 1, \\cdots . The CoupledSimulatedAnnealing class has a companion object with the following available variants. Variant Acceptance Probability Coupling term \\gamma \\gamma SA : Classical Simulated Annealing 1/(1 + exp(\\frac{E(y) - E(x)}{T^{ac}_{k}})) 1/(1 + exp(\\frac{E(y) - E(x)}{T^{ac}_{k}})) - MuSA : Multi-state Simulated Annealing : Direct generalization of Simulated Annealing exp(-E(y_i))/(exp(-E(y_i)) + \\gamma) exp(-E(y_i))/(exp(-E(y_i)) + \\gamma) \\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})} \\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})} BA : Blind Acceptance CSA 1 - exp(-E(x_i)/T_{k}^{ac})/\\gamma 1 - exp(-E(x_i)/T_{k}^{ac})/\\gamma \\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})} \\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})} M : Modified CSA exp(E(x_i)/T_{k}^{ac})/\\gamma exp(E(x_i)/T_{k}^{ac})/\\gamma \\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})} \\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})} MwVC : Modified CSA with Variance Control: Employs an annealing schedule that controls the variance of the acceptance probabilities of states exp(E(x_i)/T_{k}^{ac})/\\gamma exp(E(x_i)/T_{k}^{ac})/\\gamma \\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})} \\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})} val kernel = ... val noise = ... val data = ... val model = new GPRegression ( kernel , noise , data ) //The default variant of CSA is Mw-VC val gs = new CoupledSimulatedAnnealing [ model.type ]( model ) . setGridSize ( grid ) . setStepSize ( step ) . setLogScale ( false ) . setVariant ( CoupledSimulatedAnnealing . MuSA ) val startConf = kernel . state ++ noise . state val ( _ , conf ) = gs . optimize ( startConf , opt ) model . setState ( conf ) Gradient based Model Selection \u00b6 Gradient based model selection can be used if the model fitness function implemented in the energy method has differentiability properties (e.g. using marginal likelihood in the case of stochastic process inference). The GloballyOptWithGrad trait is an extension of GlobalOptimizer and adds a method gradEnergy that should return the gradient of the fitness function in each hyper-parameter in the form of a Map [ String , Double ] . Maximum Likelihood ML-II \u00b6 In the Maximum Likelihood (ML-II) algorithm (refer to Ramussen & Williams for more details), we aim to maximize the log marginal likelihood by calculating its gradient with respect to the hyper-parameters \\theta_j \\theta_j in each iteration and performing steepest ascent . The calculations are summarized below. \\begin{equation} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = - \\frac{1}{2} \\mathbf{y}^T K^{-1} \\mathbf{y} - \\frac{1}{2} log |K| - \\frac{n}{2} log 2\\pi \\end{equation} \\begin{equation} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = - \\frac{1}{2} \\mathbf{y}^T K^{-1} \\mathbf{y} - \\frac{1}{2} log |K| - \\frac{n}{2} log 2\\pi \\end{equation} \\begin{align} & \\frac{\\partial }{\\partial \\theta_j} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = \\frac{1}{2} tr ((\\mathbf{\\alpha} \\mathbf{\\alpha}^T - K^{-1}) \\frac{\\partial K}{\\partial \\theta_j}) \\\\ & \\mathbf{\\alpha} = K^{-1} \\mathbf{y} \\end{align} \\begin{align} & \\frac{\\partial }{\\partial \\theta_j} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = \\frac{1}{2} tr ((\\mathbf{\\alpha} \\mathbf{\\alpha}^T - K^{-1}) \\frac{\\partial K}{\\partial \\theta_j}) \\\\ & \\mathbf{\\alpha} = K^{-1} \\mathbf{y} \\end{align} The GPMLOptimizer [ I , T , M ] class implements ML-II, by using the gradEnergy method implemented by the system : M member value (which refers to a model extending GloballyOptWithGrad ). val kernel = ... val noise = ... val data = ... val model = new GPRegression ( kernel , noise , data ) val ml = new GPMLOptimizer [ DenseVector [ Double ] , Seq [( DenseVector [ Double ] , Double )] , GPRegression ]( model ) val startConf = kernel . state ++ noise . state val ( _ , conf ) = ml . optimize ( startConf , opt ) model . setState ( conf )","title":"Global"},{"location":"core/core_opt_global/#model-selection-routines","text":"These routines are also known as global optimizers , paradigms/algorithms such as genetic algorithms, gibbs sampling, simulated annealing, evolutionary optimization fall under this category. They can be used in situations when the objective function in not \"smooth\". In DynaML they are most prominently used in hyper-parameter optimization in kernel based learning methods. All global optimizers in DynaML extend the GlobalOptimizer trait, which implies that they provide an implementation for its optimize method. In order to use a global optimization routine on an model, the model implementation in question must be extending the GloballyOptimizable trait in the dynaml.optimization package, this trait has only one method called energy which is to be implemented by all sub-classes/traits. The energy method calculates the value of the global objective function for a particular configuration i.e. for particular values of model hyper-parameters. This objective function can be defined differently for each model class (marginal likelihood for Gaussian Processes, cross validation score for parametric models, etc). The following model selection routines are available in DynaML so far.","title":"Model Selection Routines"},{"location":"core/core_opt_global/#grid-search","text":"The most elementary (naive) method of model selection is to evaluate its performance (value returned by energy ) on a fixed set of grid points which are initialized for the model hyper-parameters. val kernel = ... val noise = ... val data = ... val model = new GPRegression ( kernel , noise , data ) val grid = 5 val step = 0.2 val gs = new GridSearch [ model.type ]( model ) . setGridSize ( grid ) . setStepSize ( step ) . setLogScale ( false ) val startConf = kernel . state ++ noise . state val ( _ , conf ) = gs . optimize ( startConf , opt ) model . setState ( conf )","title":"Grid Search"},{"location":"core/core_opt_global/#coupled-simulated-annealing","text":"Coupled Simulated Annealing (CSA) is an iterative search procedure which evaluates model performance on a grid and in each iteration perturbs the grid points in a randomized manner. Each perturbed point is accepted using a certain acceptance probability which is a function of the performance on the whole grid. Coupled Simulated Annealing can be seen as an extension to the classical Simulated Annealing algorithm, since the acceptance probability and perturbation function are design choices, we can formulate a number of variants of CSA. Any CSA-like algorithm must have the following components. An ensemble or grid of points x_i \\in \\Theta x_i \\in \\Theta . A perturbation distribution or function $P: x_i \\rightarrow y_i $. A coupling term \\gamma \\gamma for an ensemble. An acceptance probability function A_{\\Theta}(\\gamma, x_i \\rightarrow y_i) A_{\\Theta}(\\gamma, x_i \\rightarrow y_i) . An annealing schedule T_{k}^{ac}, k = 0, 1, \\cdots T_{k}^{ac}, k = 0, 1, \\cdots . The CoupledSimulatedAnnealing class has a companion object with the following available variants. Variant Acceptance Probability Coupling term \\gamma \\gamma SA : Classical Simulated Annealing 1/(1 + exp(\\frac{E(y) - E(x)}{T^{ac}_{k}})) 1/(1 + exp(\\frac{E(y) - E(x)}{T^{ac}_{k}})) - MuSA : Multi-state Simulated Annealing : Direct generalization of Simulated Annealing exp(-E(y_i))/(exp(-E(y_i)) + \\gamma) exp(-E(y_i))/(exp(-E(y_i)) + \\gamma) \\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})} \\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})} BA : Blind Acceptance CSA 1 - exp(-E(x_i)/T_{k}^{ac})/\\gamma 1 - exp(-E(x_i)/T_{k}^{ac})/\\gamma \\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})} \\sum_{x_j \\in \\Theta}{exp(-E(x_j)/T^{ac}_{k})} M : Modified CSA exp(E(x_i)/T_{k}^{ac})/\\gamma exp(E(x_i)/T_{k}^{ac})/\\gamma \\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})} \\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})} MwVC : Modified CSA with Variance Control: Employs an annealing schedule that controls the variance of the acceptance probabilities of states exp(E(x_i)/T_{k}^{ac})/\\gamma exp(E(x_i)/T_{k}^{ac})/\\gamma \\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})} \\sum_{x_j \\in \\Theta}{exp(E(x_j)/T^{ac}_{k})} val kernel = ... val noise = ... val data = ... val model = new GPRegression ( kernel , noise , data ) //The default variant of CSA is Mw-VC val gs = new CoupledSimulatedAnnealing [ model.type ]( model ) . setGridSize ( grid ) . setStepSize ( step ) . setLogScale ( false ) . setVariant ( CoupledSimulatedAnnealing . MuSA ) val startConf = kernel . state ++ noise . state val ( _ , conf ) = gs . optimize ( startConf , opt ) model . setState ( conf )","title":"Coupled Simulated Annealing"},{"location":"core/core_opt_global/#gradient-based-model-selection","text":"Gradient based model selection can be used if the model fitness function implemented in the energy method has differentiability properties (e.g. using marginal likelihood in the case of stochastic process inference). The GloballyOptWithGrad trait is an extension of GlobalOptimizer and adds a method gradEnergy that should return the gradient of the fitness function in each hyper-parameter in the form of a Map [ String , Double ] .","title":"Gradient based Model Selection"},{"location":"core/core_opt_global/#maximum-likelihood-ml-ii","text":"In the Maximum Likelihood (ML-II) algorithm (refer to Ramussen & Williams for more details), we aim to maximize the log marginal likelihood by calculating its gradient with respect to the hyper-parameters \\theta_j \\theta_j in each iteration and performing steepest ascent . The calculations are summarized below. \\begin{equation} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = - \\frac{1}{2} \\mathbf{y}^T K^{-1} \\mathbf{y} - \\frac{1}{2} log |K| - \\frac{n}{2} log 2\\pi \\end{equation} \\begin{equation} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = - \\frac{1}{2} \\mathbf{y}^T K^{-1} \\mathbf{y} - \\frac{1}{2} log |K| - \\frac{n}{2} log 2\\pi \\end{equation} \\begin{align} & \\frac{\\partial }{\\partial \\theta_j} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = \\frac{1}{2} tr ((\\mathbf{\\alpha} \\mathbf{\\alpha}^T - K^{-1}) \\frac{\\partial K}{\\partial \\theta_j}) \\\\ & \\mathbf{\\alpha} = K^{-1} \\mathbf{y} \\end{align} \\begin{align} & \\frac{\\partial }{\\partial \\theta_j} log \\ p(\\mathbf{y}| X, \\mathbf{\\theta}) = \\frac{1}{2} tr ((\\mathbf{\\alpha} \\mathbf{\\alpha}^T - K^{-1}) \\frac{\\partial K}{\\partial \\theta_j}) \\\\ & \\mathbf{\\alpha} = K^{-1} \\mathbf{y} \\end{align} The GPMLOptimizer [ I , T , M ] class implements ML-II, by using the gradEnergy method implemented by the system : M member value (which refers to a model extending GloballyOptWithGrad ). val kernel = ... val noise = ... val data = ... val model = new GPRegression ( kernel , noise , data ) val ml = new GPMLOptimizer [ DenseVector [ Double ] , Seq [( DenseVector [ Double ] , Double )] , GPRegression ]( model ) val startConf = kernel . state ++ noise . state val ( _ , conf ) = ml . optimize ( startConf , opt ) model . setState ( conf )","title":"Maximum Likelihood ML-II"},{"location":"core/core_opt_hierarchy/","text":"","title":"Core opt hierarchy"},{"location":"core/core_prob_dist/","text":"Summary The DynaML dynaml.probability.distributions package leverages and extends the breeze.stats.distributions package. Below is a list of distributions implemented. Specifying Distributions \u00b6 Every probability density function \\rho(x) \\rho(x) defined over some domain x \\in \\mathcal{X} x \\in \\mathcal{X} can be represented as \\rho(x) = \\frac{1}{Z} f(x) \\rho(x) = \\frac{1}{Z} f(x) , where f(x) f(x) is the un-normalized probability weight and Z Z is the normalization constant. The normalization constant ensures that the density function sums to 1 1 over the whole domain \\mathcal{X} \\mathcal{X} . Describing Skewness \u00b6 An important analytical way to create skewed distributions was described by Azzalani et. al . It consists of four components. A symmetric probability density \\varphi(.) \\varphi(.) An odd function w(.) w(.) A cumulative distribution function G(.) G(.) of some symmetric density A cut-off parameter \\tau \\tau \\rho(x) = \\frac{1}{G(\\tau)} \\times \\varphi(x)\\times G(w(x) + \\tau) \\rho(x) = \\frac{1}{G(\\tau)} \\times \\varphi(x)\\times G(w(x) + \\tau) Distributions API \u00b6 The Density [ T ] and Rand [ T ] traits form the API entry points for implementing probability distributions in breeze. In the dynaml.probability.distributions package, these two traits are inherited by GenericDistribution [ T ] which is extended by AbstractContinuousDistr [ T ] and AbstractDiscreteDistr [ T ] classes. Distributions which can produce confidence intervals The trait HasErrorBars [ T ] can be used as a mix in to provide the ability of producing error bars to distributions. To extend it, one has to implement the confidenceInterval ( s : Double ) : ( T , T ) method. Skewness The SkewSymmDistribution [ T ] class is the generic base implementations for skew symmetric family of distributions in DynaML. Distributions Library \u00b6 Apart from the distributions defined in the breeze . stats . distributions , users have access to the following distributions implemented in the dynaml . probability . distributions . Multivariate Students T \u00b6 Defines a Students' T distribution over the domain of finite dimensional vectors. \\mathcal{X} \\equiv \\mathbb{R}^{n} \\mathcal{X} \\equiv \\mathbb{R}^{n} f(x) = \\left[1+{\\frac {1}{\\nu }}({\\mathbf {x} }-{\\boldsymbol {\\mu }})^{\\rm {T}}{\\boldsymbol {\\Sigma }}^{-1}({\\mathbf {x} }-{\\boldsymbol {\\mu }})\\right]^{-(\\nu +p)/2} f(x) = \\left[1+{\\frac {1}{\\nu }}({\\mathbf {x} }-{\\boldsymbol {\\mu }})^{\\rm {T}}{\\boldsymbol {\\Sigma }}^{-1}({\\mathbf {x} }-{\\boldsymbol {\\mu }})\\right]^{-(\\nu +p)/2} Z = \\frac{\\Gamma \\left[(\\nu +p)/2\\right]}{\\Gamma (\\nu /2)\\nu ^{p/2}\\pi ^{p/2}\\left|{\\boldsymbol {\\Sigma }}\\right|^{1/2}} Z = \\frac{\\Gamma \\left[(\\nu +p)/2\\right]}{\\Gamma (\\nu /2)\\nu ^{p/2}\\pi ^{p/2}\\left|{\\boldsymbol {\\Sigma }}\\right|^{1/2}} Usage : val mu = 2.5 val mean = DenseVector ( 1.0 , 0.0 ) val cov = DenseMatrix (( 1.5 , 0.5 ), ( 0.5 , 2.5 )) val d = MultivariateStudentsT ( mu , mean , cov ) Matrix T \u00b6 Defines a Students' T distribution over the domain of matrices. \\mathcal{X} \\equiv \\mathbb{R}^{n \\times p} \\mathcal{X} \\equiv \\mathbb{R}^{n \\times p} f(x) = \\left|{\\mathbf {I}}_{n}+{\\boldsymbol \\Sigma }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}}){\\boldsymbol \\Omega }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}})^{{{\\rm {T}}}}\\right|^{{-{\\frac {\\nu +n+p-1}{2}}}} f(x) = \\left|{\\mathbf {I}}_{n}+{\\boldsymbol \\Sigma }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}}){\\boldsymbol \\Omega }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}})^{{{\\rm {T}}}}\\right|^{{-{\\frac {\\nu +n+p-1}{2}}}} Z = {\\frac {\\Gamma_{p}\\left({\\frac {\\nu +n+p-1}{2}}\\right)}{(\\pi )^{{\\frac {np}{2}}}\\Gamma _{p}\\left({\\frac {\\nu +p-1}{2}}\\right)}}|{\\boldsymbol \\Omega }|^{{-{\\frac {n}{2}}}}|{\\boldsymbol \\Sigma }|^{{-{\\frac {p}{2}}}} Z = {\\frac {\\Gamma_{p}\\left({\\frac {\\nu +n+p-1}{2}}\\right)}{(\\pi )^{{\\frac {np}{2}}}\\Gamma _{p}\\left({\\frac {\\nu +p-1}{2}}\\right)}}|{\\boldsymbol \\Omega }|^{{-{\\frac {n}{2}}}}|{\\boldsymbol \\Sigma }|^{{-{\\frac {p}{2}}}} Usage : val mu = 2.5 val mean = DenseMatrix ((- 1.5 , - 0.5 ), ( 3.5 , - 2.5 )) val cov_rows = DenseMatrix (( 1.5 , 0.5 ), ( 0.5 , 2.5 )) val cov_cols = DenseMatrix (( 0.5 , 0.1 ), ( 0.1 , 1.5 )) val d = MatrixT ( mu , mean , cov_rows , cov_cols ) Matrix Normal \u00b6 Defines a Gaussian distribution over the domain of matrices. \\mathcal{X} \\equiv \\mathbb{R}^{n \\times p} \\mathcal{X} \\equiv \\mathbb{R}^{n \\times p} f(x) = \\exp\\left( -\\frac{1}{2} \\, \\mathrm{tr}\\left[ \\mathbf{V}^{-1} (\\mathbf{X} - \\mathbf{M})^{T} \\mathbf{U}^{-1} (\\mathbf{X} - \\mathbf{M}) \\right] \\right) f(x) = \\exp\\left( -\\frac{1}{2} \\, \\mathrm{tr}\\left[ \\mathbf{V}^{-1} (\\mathbf{X} - \\mathbf{M})^{T} \\mathbf{U}^{-1} (\\mathbf{X} - \\mathbf{M}) \\right] \\right) Z = (2\\pi)^{np/2} |\\mathbf{V}|^{n/2} |\\mathbf{U}|^{p/2} Z = (2\\pi)^{np/2} |\\mathbf{V}|^{n/2} |\\mathbf{U}|^{p/2} Usage : val mean = DenseMatrix ((- 1.5 , - 0.5 ), ( 3.5 , - 2.5 )) val cov_rows = DenseMatrix (( 1.5 , 0.5 ), ( 0.5 , 2.5 )) val cov_cols = DenseMatrix (( 0.5 , 0.1 ), ( 0.1 , 1.5 )) val d = MatrixNormal ( mean , cov_rows , cov_cols ) Truncated Normal \u00b6 Defines a univariate Gaussian distribution that is defined in a finite domain. \\mathcal{X} \\equiv [a, b] \\mathcal{X} \\equiv [a, b] f(x) = \\begin{cases} \\phi ({\\frac {x-\\mu }{\\sigma }}) & a \\leq x \\leq b\\\\0 & else\\end{cases} f(x) = \\begin{cases} \\phi ({\\frac {x-\\mu }{\\sigma }}) & a \\leq x \\leq b\\\\0 & else\\end{cases} Z = \\sigma \\left(\\Phi ({\\frac {b-\\mu }{\\sigma }})-\\Phi ({\\frac {a-\\mu }{\\sigma }})\\right) Z = \\sigma \\left(\\Phi ({\\frac {b-\\mu }{\\sigma }})-\\Phi ({\\frac {a-\\mu }{\\sigma }})\\right) \\phi() \\phi() and \\Phi() \\Phi() being the gaussian density function and cumulative distribution function respectively Usage : val mean = 1.5 val sigma = 1.5 val ( a , b ) = (- 0.5 , 2.5 ) val d = TruncatedGaussian ( mean , sigma , a , b ) Skew Gaussian \u00b6 Univariate \u00b6 \\mathcal{X} \\equiv \\mathbb{R} \\mathcal{X} \\equiv \\mathbb{R} f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma})) f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma})) Z = \\frac{1}{2} Z = \\frac{1}{2} \\phi() \\phi() and \\Phi() \\Phi() being the standard gaussian density function and cumulative distribution function respectively Multivariate \u00b6 \\mathcal{X} \\equiv \\mathbb{R}^d \\mathcal{X} \\equiv \\mathbb{R}^d f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu})) f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu})) Z = \\frac{1}{2} Z = \\frac{1}{2} \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) and \\Phi() \\Phi() are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively and L L is the lower triangular Cholesky decomposition of \\Sigma \\Sigma . Skewness parameter \\alpha \\alpha The parameter \\alpha \\alpha determines the skewness of the distribution and its sign tells us in which direction the distribution has a fatter tail. In the univariate case the parameter \\alpha \\alpha is a scalar, while in the multivariate case \\alpha \\in \\mathbb{R}^d \\alpha \\in \\mathbb{R}^d , so for the multivariate skew gaussian distribution, there is a skewness value for each dimension. Usage : //Univariate val mean = 1.5 val sigma = 1.5 val a = - 0.5 val d = SkewGaussian ( a , mean , sigma ) //Multivariate val mu = DenseVector . ones [ Double ]( 4 ) val alpha = DenseVector . fill [ Double ]( 4 )( 1.2 ) val cov = DenseMatrix . eye [ Double ]( 4 )* 1.5 val md = MultivariateSkewNormal ( alpha , mu , cov ) Extended Skew Gaussian \u00b6 Univariate \u00b6 The generalization of the univariate skew Gaussian distribution. \\mathcal{X} \\equiv \\mathbb{R} \\mathcal{X} \\equiv \\mathbb{R} f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}) + \\tau\\sqrt{1 + \\alpha^{2}}) f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}) + \\tau\\sqrt{1 + \\alpha^{2}}) Z = \\Phi(\\tau) Z = \\Phi(\\tau) \\phi() \\phi() and \\Phi() \\Phi() being the standard gaussian density function and cumulative distribution function respectively Multivariate \u00b6 \\mathcal{X} \\equiv \\mathbb{R}^d \\mathcal{X} \\equiv \\mathbb{R}^d f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\mathbf{\\alpha}}) f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\mathbf{\\alpha}}) Z = \\Phi(\\tau) Z = \\Phi(\\tau) \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) and \\Phi() \\Phi() are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively and L L is the lower triangular Cholesky decomposition of \\Sigma \\Sigma . Usage : //Univariate val mean = 1.5 val sigma = 1.5 val a = - 0.5 val c = 0.5 val d = ExtendedSkewGaussian ( c , a , mean , sigma ) //Multivariate val mu = DenseVector . ones [ Double ]( 4 ) val alpha = DenseVector . fill [ Double ]( 4 )( 1.2 ) val cov = DenseMatrix . eye [ Double ]( 4 )* 1.5 val tau = 0.2 val md = ExtendedMultivariateSkewNormal ( tau , alpha , mu , cov ) Confusing Nomenclature The following distribution has a very similar form and name to the extended skew gaussian distribution shown above. But despite its deceptively similar formula, it is a very different object. We use the name MESN to denote the variant below instead of its expanded form. MESN \u00b6 The Multivariate Extended Skew Normal or MESN distribution was formulated by Adcock and Schutes . It is given by \\mathcal{X} \\equiv \\mathbb{R}^d \\mathcal{X} \\equiv \\mathbb{R}^d f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu} + \\mathbf{\\alpha}\\tau, {\\Sigma} + \\mathbf{\\alpha}\\mathbf{\\alpha}^\\intercal) \\Phi\\left(\\frac{\\mathbf{\\alpha}^{\\intercal} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau}{\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\Sigma^{-1}\\mathbf{\\alpha}}}\\right) f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu} + \\mathbf{\\alpha}\\tau, {\\Sigma} + \\mathbf{\\alpha}\\mathbf{\\alpha}^\\intercal) \\Phi\\left(\\frac{\\mathbf{\\alpha}^{\\intercal} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau}{\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\Sigma^{-1}\\mathbf{\\alpha}}}\\right) Z = \\Phi(\\tau) Z = \\Phi(\\tau) \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) and \\Phi() \\Phi() are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively. Usage : //Univariate val mean = 1.5 val sigma = 1.5 val a = - 0.5 val c = 0.5 val d = UESN ( c , a , mean , sigma ) //Multivariate val mu = DenseVector . ones [ Double ]( 4 ) val alpha = DenseVector . fill [ Double ]( 4 )( 1.2 ) val cov = DenseMatrix . eye [ Double ]( 4 )* 1.5 val tau = 0.2 val md = MESN ( tau , alpha , mu , cov ) Extended Skew Gaussian Process ESGP The MESN distribution is used to define the finite dimensional probabilities for the ESGP process.","title":"Probability Distributions"},{"location":"core/core_prob_dist/#specifying-distributions","text":"Every probability density function \\rho(x) \\rho(x) defined over some domain x \\in \\mathcal{X} x \\in \\mathcal{X} can be represented as \\rho(x) = \\frac{1}{Z} f(x) \\rho(x) = \\frac{1}{Z} f(x) , where f(x) f(x) is the un-normalized probability weight and Z Z is the normalization constant. The normalization constant ensures that the density function sums to 1 1 over the whole domain \\mathcal{X} \\mathcal{X} .","title":"Specifying Distributions"},{"location":"core/core_prob_dist/#describing-skewness","text":"An important analytical way to create skewed distributions was described by Azzalani et. al . It consists of four components. A symmetric probability density \\varphi(.) \\varphi(.) An odd function w(.) w(.) A cumulative distribution function G(.) G(.) of some symmetric density A cut-off parameter \\tau \\tau \\rho(x) = \\frac{1}{G(\\tau)} \\times \\varphi(x)\\times G(w(x) + \\tau) \\rho(x) = \\frac{1}{G(\\tau)} \\times \\varphi(x)\\times G(w(x) + \\tau)","title":"Describing Skewness"},{"location":"core/core_prob_dist/#distributions-api","text":"The Density [ T ] and Rand [ T ] traits form the API entry points for implementing probability distributions in breeze. In the dynaml.probability.distributions package, these two traits are inherited by GenericDistribution [ T ] which is extended by AbstractContinuousDistr [ T ] and AbstractDiscreteDistr [ T ] classes. Distributions which can produce confidence intervals The trait HasErrorBars [ T ] can be used as a mix in to provide the ability of producing error bars to distributions. To extend it, one has to implement the confidenceInterval ( s : Double ) : ( T , T ) method. Skewness The SkewSymmDistribution [ T ] class is the generic base implementations for skew symmetric family of distributions in DynaML.","title":"Distributions API"},{"location":"core/core_prob_dist/#distributions-library","text":"Apart from the distributions defined in the breeze . stats . distributions , users have access to the following distributions implemented in the dynaml . probability . distributions .","title":"Distributions Library"},{"location":"core/core_prob_dist/#multivariate-students-t","text":"Defines a Students' T distribution over the domain of finite dimensional vectors. \\mathcal{X} \\equiv \\mathbb{R}^{n} \\mathcal{X} \\equiv \\mathbb{R}^{n} f(x) = \\left[1+{\\frac {1}{\\nu }}({\\mathbf {x} }-{\\boldsymbol {\\mu }})^{\\rm {T}}{\\boldsymbol {\\Sigma }}^{-1}({\\mathbf {x} }-{\\boldsymbol {\\mu }})\\right]^{-(\\nu +p)/2} f(x) = \\left[1+{\\frac {1}{\\nu }}({\\mathbf {x} }-{\\boldsymbol {\\mu }})^{\\rm {T}}{\\boldsymbol {\\Sigma }}^{-1}({\\mathbf {x} }-{\\boldsymbol {\\mu }})\\right]^{-(\\nu +p)/2} Z = \\frac{\\Gamma \\left[(\\nu +p)/2\\right]}{\\Gamma (\\nu /2)\\nu ^{p/2}\\pi ^{p/2}\\left|{\\boldsymbol {\\Sigma }}\\right|^{1/2}} Z = \\frac{\\Gamma \\left[(\\nu +p)/2\\right]}{\\Gamma (\\nu /2)\\nu ^{p/2}\\pi ^{p/2}\\left|{\\boldsymbol {\\Sigma }}\\right|^{1/2}} Usage : val mu = 2.5 val mean = DenseVector ( 1.0 , 0.0 ) val cov = DenseMatrix (( 1.5 , 0.5 ), ( 0.5 , 2.5 )) val d = MultivariateStudentsT ( mu , mean , cov )","title":"Multivariate Students T"},{"location":"core/core_prob_dist/#matrix-t","text":"Defines a Students' T distribution over the domain of matrices. \\mathcal{X} \\equiv \\mathbb{R}^{n \\times p} \\mathcal{X} \\equiv \\mathbb{R}^{n \\times p} f(x) = \\left|{\\mathbf {I}}_{n}+{\\boldsymbol \\Sigma }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}}){\\boldsymbol \\Omega }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}})^{{{\\rm {T}}}}\\right|^{{-{\\frac {\\nu +n+p-1}{2}}}} f(x) = \\left|{\\mathbf {I}}_{n}+{\\boldsymbol \\Sigma }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}}){\\boldsymbol \\Omega }^{{-1}}({\\mathbf {X}}-{\\mathbf {M}})^{{{\\rm {T}}}}\\right|^{{-{\\frac {\\nu +n+p-1}{2}}}} Z = {\\frac {\\Gamma_{p}\\left({\\frac {\\nu +n+p-1}{2}}\\right)}{(\\pi )^{{\\frac {np}{2}}}\\Gamma _{p}\\left({\\frac {\\nu +p-1}{2}}\\right)}}|{\\boldsymbol \\Omega }|^{{-{\\frac {n}{2}}}}|{\\boldsymbol \\Sigma }|^{{-{\\frac {p}{2}}}} Z = {\\frac {\\Gamma_{p}\\left({\\frac {\\nu +n+p-1}{2}}\\right)}{(\\pi )^{{\\frac {np}{2}}}\\Gamma _{p}\\left({\\frac {\\nu +p-1}{2}}\\right)}}|{\\boldsymbol \\Omega }|^{{-{\\frac {n}{2}}}}|{\\boldsymbol \\Sigma }|^{{-{\\frac {p}{2}}}} Usage : val mu = 2.5 val mean = DenseMatrix ((- 1.5 , - 0.5 ), ( 3.5 , - 2.5 )) val cov_rows = DenseMatrix (( 1.5 , 0.5 ), ( 0.5 , 2.5 )) val cov_cols = DenseMatrix (( 0.5 , 0.1 ), ( 0.1 , 1.5 )) val d = MatrixT ( mu , mean , cov_rows , cov_cols )","title":"Matrix T"},{"location":"core/core_prob_dist/#matrix-normal","text":"Defines a Gaussian distribution over the domain of matrices. \\mathcal{X} \\equiv \\mathbb{R}^{n \\times p} \\mathcal{X} \\equiv \\mathbb{R}^{n \\times p} f(x) = \\exp\\left( -\\frac{1}{2} \\, \\mathrm{tr}\\left[ \\mathbf{V}^{-1} (\\mathbf{X} - \\mathbf{M})^{T} \\mathbf{U}^{-1} (\\mathbf{X} - \\mathbf{M}) \\right] \\right) f(x) = \\exp\\left( -\\frac{1}{2} \\, \\mathrm{tr}\\left[ \\mathbf{V}^{-1} (\\mathbf{X} - \\mathbf{M})^{T} \\mathbf{U}^{-1} (\\mathbf{X} - \\mathbf{M}) \\right] \\right) Z = (2\\pi)^{np/2} |\\mathbf{V}|^{n/2} |\\mathbf{U}|^{p/2} Z = (2\\pi)^{np/2} |\\mathbf{V}|^{n/2} |\\mathbf{U}|^{p/2} Usage : val mean = DenseMatrix ((- 1.5 , - 0.5 ), ( 3.5 , - 2.5 )) val cov_rows = DenseMatrix (( 1.5 , 0.5 ), ( 0.5 , 2.5 )) val cov_cols = DenseMatrix (( 0.5 , 0.1 ), ( 0.1 , 1.5 )) val d = MatrixNormal ( mean , cov_rows , cov_cols )","title":"Matrix Normal"},{"location":"core/core_prob_dist/#truncated-normal","text":"Defines a univariate Gaussian distribution that is defined in a finite domain. \\mathcal{X} \\equiv [a, b] \\mathcal{X} \\equiv [a, b] f(x) = \\begin{cases} \\phi ({\\frac {x-\\mu }{\\sigma }}) & a \\leq x \\leq b\\\\0 & else\\end{cases} f(x) = \\begin{cases} \\phi ({\\frac {x-\\mu }{\\sigma }}) & a \\leq x \\leq b\\\\0 & else\\end{cases} Z = \\sigma \\left(\\Phi ({\\frac {b-\\mu }{\\sigma }})-\\Phi ({\\frac {a-\\mu }{\\sigma }})\\right) Z = \\sigma \\left(\\Phi ({\\frac {b-\\mu }{\\sigma }})-\\Phi ({\\frac {a-\\mu }{\\sigma }})\\right) \\phi() \\phi() and \\Phi() \\Phi() being the gaussian density function and cumulative distribution function respectively Usage : val mean = 1.5 val sigma = 1.5 val ( a , b ) = (- 0.5 , 2.5 ) val d = TruncatedGaussian ( mean , sigma , a , b )","title":"Truncated Normal"},{"location":"core/core_prob_dist/#skew-gaussian","text":"","title":"Skew Gaussian"},{"location":"core/core_prob_dist/#univariate","text":"\\mathcal{X} \\equiv \\mathbb{R} \\mathcal{X} \\equiv \\mathbb{R} f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma})) f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma})) Z = \\frac{1}{2} Z = \\frac{1}{2} \\phi() \\phi() and \\Phi() \\Phi() being the standard gaussian density function and cumulative distribution function respectively","title":"Univariate"},{"location":"core/core_prob_dist/#multivariate","text":"\\mathcal{X} \\equiv \\mathbb{R}^d \\mathcal{X} \\equiv \\mathbb{R}^d f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu})) f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu})) Z = \\frac{1}{2} Z = \\frac{1}{2} \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) and \\Phi() \\Phi() are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively and L L is the lower triangular Cholesky decomposition of \\Sigma \\Sigma . Skewness parameter \\alpha \\alpha The parameter \\alpha \\alpha determines the skewness of the distribution and its sign tells us in which direction the distribution has a fatter tail. In the univariate case the parameter \\alpha \\alpha is a scalar, while in the multivariate case \\alpha \\in \\mathbb{R}^d \\alpha \\in \\mathbb{R}^d , so for the multivariate skew gaussian distribution, there is a skewness value for each dimension. Usage : //Univariate val mean = 1.5 val sigma = 1.5 val a = - 0.5 val d = SkewGaussian ( a , mean , sigma ) //Multivariate val mu = DenseVector . ones [ Double ]( 4 ) val alpha = DenseVector . fill [ Double ]( 4 )( 1.2 ) val cov = DenseMatrix . eye [ Double ]( 4 )* 1.5 val md = MultivariateSkewNormal ( alpha , mu , cov )","title":"Multivariate"},{"location":"core/core_prob_dist/#extended-skew-gaussian","text":"","title":"Extended Skew Gaussian"},{"location":"core/core_prob_dist/#univariate_1","text":"The generalization of the univariate skew Gaussian distribution. \\mathcal{X} \\equiv \\mathbb{R} \\mathcal{X} \\equiv \\mathbb{R} f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}) + \\tau\\sqrt{1 + \\alpha^{2}}) f(x) = \\phi(\\frac{x - \\mu}{\\sigma}) \\Phi(\\alpha (\\frac{x-\\mu}{\\sigma}) + \\tau\\sqrt{1 + \\alpha^{2}}) Z = \\Phi(\\tau) Z = \\Phi(\\tau) \\phi() \\phi() and \\Phi() \\Phi() being the standard gaussian density function and cumulative distribution function respectively","title":"Univariate"},{"location":"core/core_prob_dist/#multivariate_1","text":"\\mathcal{X} \\equiv \\mathbb{R}^d \\mathcal{X} \\equiv \\mathbb{R}^d f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\mathbf{\\alpha}}) f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu}, {\\Sigma}) \\Phi(\\mathbf{\\alpha}^{\\intercal} L^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\mathbf{\\alpha}}) Z = \\Phi(\\tau) Z = \\Phi(\\tau) \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) and \\Phi() \\Phi() are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively and L L is the lower triangular Cholesky decomposition of \\Sigma \\Sigma . Usage : //Univariate val mean = 1.5 val sigma = 1.5 val a = - 0.5 val c = 0.5 val d = ExtendedSkewGaussian ( c , a , mean , sigma ) //Multivariate val mu = DenseVector . ones [ Double ]( 4 ) val alpha = DenseVector . fill [ Double ]( 4 )( 1.2 ) val cov = DenseMatrix . eye [ Double ]( 4 )* 1.5 val tau = 0.2 val md = ExtendedMultivariateSkewNormal ( tau , alpha , mu , cov ) Confusing Nomenclature The following distribution has a very similar form and name to the extended skew gaussian distribution shown above. But despite its deceptively similar formula, it is a very different object. We use the name MESN to denote the variant below instead of its expanded form.","title":"Multivariate"},{"location":"core/core_prob_dist/#mesn","text":"The Multivariate Extended Skew Normal or MESN distribution was formulated by Adcock and Schutes . It is given by \\mathcal{X} \\equiv \\mathbb{R}^d \\mathcal{X} \\equiv \\mathbb{R}^d f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu} + \\mathbf{\\alpha}\\tau, {\\Sigma} + \\mathbf{\\alpha}\\mathbf{\\alpha}^\\intercal) \\Phi\\left(\\frac{\\mathbf{\\alpha}^{\\intercal} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau}{\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\Sigma^{-1}\\mathbf{\\alpha}}}\\right) f(x) = \\phi_{d}(\\mathbf{x}; \\mathbf{\\mu} + \\mathbf{\\alpha}\\tau, {\\Sigma} + \\mathbf{\\alpha}\\mathbf{\\alpha}^\\intercal) \\Phi\\left(\\frac{\\mathbf{\\alpha}^{\\intercal} \\Sigma^{-1}(\\mathbf{x} - \\mathbf{\\mu}) + \\tau}{\\sqrt{1 + \\mathbf{\\alpha}^{\\intercal}\\Sigma^{-1}\\mathbf{\\alpha}}}\\right) Z = \\Phi(\\tau) Z = \\Phi(\\tau) \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) \\phi_{d}(.; \\mathbf{\\mu}, {\\Sigma}) and \\Phi() \\Phi() are the multivariate gaussian density function and standard gaussian univariate cumulative distribution function respectively. Usage : //Univariate val mean = 1.5 val sigma = 1.5 val a = - 0.5 val c = 0.5 val d = UESN ( c , a , mean , sigma ) //Multivariate val mu = DenseVector . ones [ Double ]( 4 ) val alpha = DenseVector . fill [ Double ]( 4 )( 1.2 ) val cov = DenseMatrix . eye [ Double ]( 4 )* 1.5 val tau = 0.2 val md = MESN ( tau , alpha , mu , cov ) Extended Skew Gaussian Process ESGP The MESN distribution is used to define the finite dimensional probabilities for the ESGP process.","title":"MESN"},{"location":"core/core_prob_model/","text":"DynaML takes a sampling first approach to a probability API and so there are many operations and transformations that can easily be applied to random variables created in the DynaML REPL. Probability models enable the user to express multivariate distributions in terms of conditional probability factorizations. p(x,\\theta) = p(x|\\theta) \\times p(\\theta) = p(\\theta|x) \\times p(x) p(x,\\theta) = p(x|\\theta) \\times p(\\theta) = p(\\theta|x) \\times p(x) Conditional probability factorizations are at the center of Bayes Theorem p(\\theta|x) = \\frac{p(x|\\theta) \\times p(\\theta)}{p(x)} p(\\theta|x) = \\frac{p(x|\\theta) \\times p(\\theta)}{p(x)} In Bayesian analysis, p(\\theta) p(\\theta) <span><span class=\"MathJax_Preview\">p(\\theta)</span><script type=\"math/tex\">p(\\theta) is known as the prior probability or just prior while $$ p(x|\\theta) $$ is called the data likelihood or just likelihood . The prior distribution encodes our belief about which areas of the parameter space are more likely. The likelihood distribution states how likely it is for some data x x <span><span class=\"MathJax_Preview\">x</span><script type=\"math/tex\">x is produced for a specific value of parameters \\theta \\theta <span><span class=\"MathJax_Preview\">\\theta</span><script type=\"math/tex\">\\theta . Probability Models \u00b6 In DynaML the ProbabilityModel class can be used to create arbitrary kinds of conditional probability factorizations, for example consider the simple Beta-Bernoulli coin toss model. The Beta-Bernoulli model can be specified as follows. Prior The prior is a Beta distribution with parameters \\alpha, \\beta \\alpha, \\beta <span><span class=\"MathJax_Preview\">\\alpha, \\beta</span><script type=\"math/tex\">\\alpha, \\beta . \\begin{align} p(\\theta) &= \\frac{1}{B(\\alpha, \\beta)} \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} \\\\ B(\\alpha, \\beta) &= \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)} \\end{align} \\begin{align} p(\\theta) &= \\frac{1}{B(\\alpha, \\beta)} \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} \\\\ B(\\alpha, \\beta) &= \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)} \\end{align} Likelihood For a value \\theta \\theta <span><span class=\"MathJax_Preview\">\\theta</span><script type=\"math/tex\">\\theta sampled from the prior distribution, we generate n n <span><span class=\"MathJax_Preview\">n</span><script type=\"math/tex\">n coin tosses with probability of heads for each toss being \\theta \\theta <span><span class=\"MathJax_Preview\">\\theta</span><script type=\"math/tex\">\\theta . The Binomial distribution gives the probability that out of such n n <span><span class=\"MathJax_Preview\">n</span><script type=\"math/tex\">n loaded coin tosses; k k <span><span class=\"MathJax_Preview\">k</span><script type=\"math/tex\">k tosses will turn up heads. \\begin{align} p(x = k|\\theta;n) = \\binom{n}{k}\\theta^k (1 - \\theta)^{n-k} \\end{align} \\begin{align} p(x = k|\\theta;n) = \\binom{n}{k}\\theta^k (1 - \\theta)^{n-k} \\end{align} Creation \u00b6 //Start with a beta prior val p = RandomVariable ( new Beta ( 7.5 , 7.5 )) //Simulate 500 coin tosses with probability of heads; p ~ Beta(7.5, 7.5) val coinLikelihood = DataPipe (( p : Double ) => new BinomialRV ( 500 , p )) //Construct a probability model. val c_model = ProbabilityModel ( p , coinLikihood ) Prior Sampling \u00b6 We can now visualize the prior. histogram (( 1 to 2000 ). map ( _ => p . sample ())) Posterior Sampling \u00b6 The ProbabilityModel class has a built in value member called posterior , which is an instance of the RandomVariable class. It can be used to sample from the posterior distribution of the model parameters given a set of data observations. In the Beta-Bernoulli coin toss example, we created a likelihood model that was a Binomial distribution over 500 coin tosses. To generate samples from the posterior distribution, we must provide data or in our case; the number of heads observed in 500 coin tosses. // The posterior distribution for the situation // when 350 heads are observed out of 500 coin tosses val post = c_model . posterior ( 350 ) val postSample = ( 1 to 2000 ). map ( _ => post . sample ()) //Generate samples from posterior and visualize as a histogram. hold () histogram ( postSample ) unhold () From the posterior samples generated we can now examine sufficient statistics, such as the posterior mean. From Bayesian theory it is known that for a Beta-Bernoulli model, the posterior is another Beta distribution specified by $$ p(\\theta|x = k) = Beta(\\alpha + k, \\beta + n - k) $$. From the properties of the Beta distribution the mean is given in our case by \\frac{\\alpha+k}{\\alpha + \\beta + n} \\frac{\\alpha+k}{\\alpha + \\beta + n} <span><span class=\"MathJax_Preview\">\\frac{\\alpha+k}{\\alpha + \\beta + n}</span><script type=\"math/tex\">\\frac{\\alpha+k}{\\alpha + \\beta + n} giving a value of about 0.694174 0.694174 <span><span class=\"MathJax_Preview\">0.694174</span><script type=\"math/tex\">0.694174 . We can verify this from out posterior sample. val postMean = postSample . sum / postSample . length postMean : Double = 0.6851085955685318","title":"Core prob model"},{"location":"core/core_prob_model/#probability-models","text":"In DynaML the ProbabilityModel class can be used to create arbitrary kinds of conditional probability factorizations, for example consider the simple Beta-Bernoulli coin toss model. The Beta-Bernoulli model can be specified as follows. Prior The prior is a Beta distribution with parameters \\alpha, \\beta \\alpha, \\beta <span><span class=\"MathJax_Preview\">\\alpha, \\beta</span><script type=\"math/tex\">\\alpha, \\beta . \\begin{align} p(\\theta) &= \\frac{1}{B(\\alpha, \\beta)} \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} \\\\ B(\\alpha, \\beta) &= \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)} \\end{align} \\begin{align} p(\\theta) &= \\frac{1}{B(\\alpha, \\beta)} \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} \\\\ B(\\alpha, \\beta) &= \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)} \\end{align} Likelihood For a value \\theta \\theta <span><span class=\"MathJax_Preview\">\\theta</span><script type=\"math/tex\">\\theta sampled from the prior distribution, we generate n n <span><span class=\"MathJax_Preview\">n</span><script type=\"math/tex\">n coin tosses with probability of heads for each toss being \\theta \\theta <span><span class=\"MathJax_Preview\">\\theta</span><script type=\"math/tex\">\\theta . The Binomial distribution gives the probability that out of such n n <span><span class=\"MathJax_Preview\">n</span><script type=\"math/tex\">n loaded coin tosses; k k <span><span class=\"MathJax_Preview\">k</span><script type=\"math/tex\">k tosses will turn up heads. \\begin{align} p(x = k|\\theta;n) = \\binom{n}{k}\\theta^k (1 - \\theta)^{n-k} \\end{align} \\begin{align} p(x = k|\\theta;n) = \\binom{n}{k}\\theta^k (1 - \\theta)^{n-k} \\end{align}","title":"Probability Models"},{"location":"core/core_prob_model/#creation","text":"//Start with a beta prior val p = RandomVariable ( new Beta ( 7.5 , 7.5 )) //Simulate 500 coin tosses with probability of heads; p ~ Beta(7.5, 7.5) val coinLikelihood = DataPipe (( p : Double ) => new BinomialRV ( 500 , p )) //Construct a probability model. val c_model = ProbabilityModel ( p , coinLikihood )","title":"Creation"},{"location":"core/core_prob_model/#prior-sampling","text":"We can now visualize the prior. histogram (( 1 to 2000 ). map ( _ => p . sample ()))","title":"Prior Sampling"},{"location":"core/core_prob_model/#posterior-sampling","text":"The ProbabilityModel class has a built in value member called posterior , which is an instance of the RandomVariable class. It can be used to sample from the posterior distribution of the model parameters given a set of data observations. In the Beta-Bernoulli coin toss example, we created a likelihood model that was a Binomial distribution over 500 coin tosses. To generate samples from the posterior distribution, we must provide data or in our case; the number of heads observed in 500 coin tosses. // The posterior distribution for the situation // when 350 heads are observed out of 500 coin tosses val post = c_model . posterior ( 350 ) val postSample = ( 1 to 2000 ). map ( _ => post . sample ()) //Generate samples from posterior and visualize as a histogram. hold () histogram ( postSample ) unhold () From the posterior samples generated we can now examine sufficient statistics, such as the posterior mean. From Bayesian theory it is known that for a Beta-Bernoulli model, the posterior is another Beta distribution specified by $$ p(\\theta|x = k) = Beta(\\alpha + k, \\beta + n - k) $$. From the properties of the Beta distribution the mean is given in our case by \\frac{\\alpha+k}{\\alpha + \\beta + n} \\frac{\\alpha+k}{\\alpha + \\beta + n} <span><span class=\"MathJax_Preview\">\\frac{\\alpha+k}{\\alpha + \\beta + n}</span><script type=\"math/tex\">\\frac{\\alpha+k}{\\alpha + \\beta + n} giving a value of about 0.694174 0.694174 <span><span class=\"MathJax_Preview\">0.694174</span><script type=\"math/tex\">0.694174 . We can verify this from out posterior sample. val postMean = postSample . sum / postSample . length postMean : Double = 0.6851085955685318","title":"Posterior Sampling"},{"location":"core/core_prob_operations/","text":"Apart from just creating wrapper code around sampling procedures which represent random variables, it is also important to do transformations on random variables to yield new more interesting random variables and distributions. In statistics one often formulates certain random variables as algebraic operations on other simpler random variables. Algebraic Operations \u00b6 It is possible to do common algebraic operations on instances of continuous random variables. import spire.implicits._ val b = RandomVariable ( new Beta ( 7.5 , 7.5 )) val g = RandomVariable ( new Gamma ( 1.5 , 1.2 )) val n = GaussianRV ( 0.0 , 1.0 ) val addR = b + n - g val multR = b * ( n + g ) histogram (( 1 to 1000 ). map ( _ => multR . sample ())) Measurable Functions \u00b6 In many cases random variables can be expressed as functions of one another, for example chi square random variables are obtained by squaring normally distributed samples. val chsq = MeasurableFunction ( n , DataPipe (( x : Double ) => x * x )) //Generate a chi square distribution with one degree of freedom histogram (( 1 to 1000 ). map ( _ => chsq . sample ())) Push-forward Maps \u00b6","title":"Operations on Random Variables"},{"location":"core/core_prob_operations/#algebraic-operations","text":"It is possible to do common algebraic operations on instances of continuous random variables. import spire.implicits._ val b = RandomVariable ( new Beta ( 7.5 , 7.5 )) val g = RandomVariable ( new Gamma ( 1.5 , 1.2 )) val n = GaussianRV ( 0.0 , 1.0 ) val addR = b + n - g val multR = b * ( n + g ) histogram (( 1 to 1000 ). map ( _ => multR . sample ()))","title":"Algebraic Operations"},{"location":"core/core_prob_operations/#measurable-functions","text":"In many cases random variables can be expressed as functions of one another, for example chi square random variables are obtained by squaring normally distributed samples. val chsq = MeasurableFunction ( n , DataPipe (( x : Double ) => x * x )) //Generate a chi square distribution with one degree of freedom histogram (( 1 to 1000 ). map ( _ => chsq . sample ()))","title":"Measurable Functions"},{"location":"core/core_prob_operations/#push-forward-maps","text":"","title":"Push-forward Maps"},{"location":"core/core_prob_randomvar/","text":"What I cannot create, I do not understand - Richard Feynman Summary Since version 1.4 a new package called probability has been added to the core api with an aim to aid in the modeling of random variables and measurable functions. Random variables and probability distributions form the bedrock of modern statistical based approaches to inference. Furthermore, analytically tractable inference is only possible for a small number of models while a wealth of interesting model structures don't yield themselves to analytical inference and approximate sampling based approaches are often employed. Random Variable API \u00b6 Although both random variable with tractable and intractable distributions can be constructed, the emphasis is on the sampling capabilities of random variable objects. The probability package class hierarchy consists of classes and traits which represent continuous and discrete random variables along with ability to endow them with distributions. DynaML Random Variable \u00b6 The RandomVariable [ Domain ] forms the top of the class hierarchy in the probability package. It is a light weight trait which takes a form like so. abstract class RandomVariable [ Domain ] { val sample : DataPipe [ Unit , Domain ] def :*[ Domain1 ]( other : RandomVariable [ Domain1 ]) : RandomVariable [( Domain , Domain1 )] = { val sam = this . sample RandomVariable ( BifurcationPipe ( sam , other . sample )) } } A RandomVariable instance is defined by its type parameter Domain , in Mathematics this is the underlying space (referred to as the support ) over which the random variable is defined ( \\mathbb{R}^p \\mathbb{R}^p for continuos variables, \\mathbb{N} \\mathbb{N} for discrete variables). The two main functionalities are as follows. sample which is a data pipe having no input and outputs a sample from the random variables distribution whenever invoked. :* the 'composition' operator between random variables, evaluating an expression like randomVar1 :* randomVar2 creates a new random variable whose domain is a cartesian product of the domains of randomVar1 and randomVar2 . Continuous and discrete distribution random variables are implemented through the ContinuousDistrRV [ Domain ] and DiscreteDistrRV [ Domain ] respectively. Creating Random Variables \u00b6 Creating random variables can be created by a number of ways. import breeze.stats.distributions._ import spire.implicits._ //Create a sampling function val sampF : () => Double = ... val rv = RandomVariable ( sampF ) //Also works with a pipe val sampF : DataPipe [ Unit , Double ] = ... val rv = RandomVariable ( sampF ) Sampling is the core functionality of the classes extending RandomVariable but in some cases representing random variables having an underlying (tractable and known) distribution is a requirement, for that purpose there exists the RandomVarWithDistr [ Domain , Dist ] trait which is a bare bones extension of RandomVariable ; it contains only one other member, underlyingDist which is of abstract type Dist . The type Dist can be any breeze distribution, which is either contained in the package breeze . stats . distributions or a user written extension of a breeze probability distribution. Creating random variables from breeze distributions Creating a random variable backed by a breeze distribution is easy, simply pass the breeze distribution to the RandomVariable companion object. val p = RandomVariable ( new Beta ( 7.5 , 7.5 )) The RandomVariable object recognizes the breeze distribution passed to it and creates a continuous or discrete random variable accordingly.","title":"Random Variables"},{"location":"core/core_prob_randomvar/#random-variable-api","text":"Although both random variable with tractable and intractable distributions can be constructed, the emphasis is on the sampling capabilities of random variable objects. The probability package class hierarchy consists of classes and traits which represent continuous and discrete random variables along with ability to endow them with distributions.","title":"Random Variable API"},{"location":"core/core_prob_randomvar/#dynaml-random-variable","text":"The RandomVariable [ Domain ] forms the top of the class hierarchy in the probability package. It is a light weight trait which takes a form like so. abstract class RandomVariable [ Domain ] { val sample : DataPipe [ Unit , Domain ] def :*[ Domain1 ]( other : RandomVariable [ Domain1 ]) : RandomVariable [( Domain , Domain1 )] = { val sam = this . sample RandomVariable ( BifurcationPipe ( sam , other . sample )) } } A RandomVariable instance is defined by its type parameter Domain , in Mathematics this is the underlying space (referred to as the support ) over which the random variable is defined ( \\mathbb{R}^p \\mathbb{R}^p for continuos variables, \\mathbb{N} \\mathbb{N} for discrete variables). The two main functionalities are as follows. sample which is a data pipe having no input and outputs a sample from the random variables distribution whenever invoked. :* the 'composition' operator between random variables, evaluating an expression like randomVar1 :* randomVar2 creates a new random variable whose domain is a cartesian product of the domains of randomVar1 and randomVar2 . Continuous and discrete distribution random variables are implemented through the ContinuousDistrRV [ Domain ] and DiscreteDistrRV [ Domain ] respectively.","title":"DynaML Random Variable"},{"location":"core/core_prob_randomvar/#creating-random-variables","text":"Creating random variables can be created by a number of ways. import breeze.stats.distributions._ import spire.implicits._ //Create a sampling function val sampF : () => Double = ... val rv = RandomVariable ( sampF ) //Also works with a pipe val sampF : DataPipe [ Unit , Double ] = ... val rv = RandomVariable ( sampF ) Sampling is the core functionality of the classes extending RandomVariable but in some cases representing random variables having an underlying (tractable and known) distribution is a requirement, for that purpose there exists the RandomVarWithDistr [ Domain , Dist ] trait which is a bare bones extension of RandomVariable ; it contains only one other member, underlyingDist which is of abstract type Dist . The type Dist can be any breeze distribution, which is either contained in the package breeze . stats . distributions or a user written extension of a breeze probability distribution. Creating random variables from breeze distributions Creating a random variable backed by a breeze distribution is easy, simply pass the breeze distribution to the RandomVariable companion object. val p = RandomVariable ( new Beta ( 7.5 , 7.5 )) The RandomVariable object recognizes the breeze distribution passed to it and creates a continuous or discrete random variable accordingly.","title":"Creating Random Variables"},{"location":"core/core_stp/","text":"Student T Processes (STP) can be viewed as a generalization of Gaussian Processes, in GP models we use the multivariate normal distribution to model noisy observations of an unknown function. Likewise for STP models, we employ the multivariate student t distribution. Formally a student t process is a stochastic process where the finite dimensional distribution is multivariate t. \\begin{align} \\mathbf{y} & \\in \\mathbb{R}^n \\\\ \\mathbf{y} & \\sim MVT_{n}(\\nu, \\phi, K) \\\\ p(\\mathbf{y}) & = \\frac{\\Gamma(\\frac{\\nu + n}{2})}{(\\nu \\pi)^{n/2} \\Gamma(\\nu/2)} |K|^{-1/2} \\\\ & \\times (1 + \\frac{(\\mathbf{y} - \\phi)^T K^{-1} (\\mathbf{y} - \\phi)}{\\nu})^{-\\frac{\\nu + n}{2}} \\end{align} \\begin{align} \\mathbf{y} & \\in \\mathbb{R}^n \\\\ \\mathbf{y} & \\sim MVT_{n}(\\nu, \\phi, K) \\\\ p(\\mathbf{y}) & = \\frac{\\Gamma(\\frac{\\nu + n}{2})}{(\\nu \\pi)^{n/2} \\Gamma(\\nu/2)} |K|^{-1/2} \\\\ & \\times (1 + \\frac{(\\mathbf{y} - \\phi)^T K^{-1} (\\mathbf{y} - \\phi)}{\\nu})^{-\\frac{\\nu + n}{2}} \\end{align} It is known that as \\nu \\rightarrow \\infty \\nu \\rightarrow \\infty , the MVT_{n}(\\nu, \\phi, K) MVT_{n}(\\nu, \\phi, K) tends towards the multivariate normal distribution \\mathcal{N}_{n}(\\phi, K) \\mathcal{N}_{n}(\\phi, K) . Regression with Student T Processes \u00b6 The regression formulation for STP models is identical to the GP regression framework, to summarize the posterior predictive distribution takes the following form. Suppose \\mathbf{t} \\sim MVT_{n_{tr} + n_t}(\\nu, \\mathbf{0}, K) \\mathbf{t} \\sim MVT_{n_{tr} + n_t}(\\nu, \\mathbf{0}, K) is the process producing the data. Let [\\mathbf{f_*}]_{n_{t} \\times 1} [\\mathbf{f_*}]_{n_{t} \\times 1} represent the values of the function on the test inputs and [\\mathbf{y}]_{n_{tr} \\times 1} [\\mathbf{y}]_{n_{tr} \\times 1} represent noisy observations made on the training data points. \\begin{align} & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim MVT_{\\nu + n_{tr}}(\\mathbf{\\bar{f_*}}, \\frac{\\nu + \\beta - 2}{\\nu + n_{tr} - 2} \\times cov(\\mathbf{f_*})) \\label{eq:posterior}\\\\ & \\beta = \\mathbf{y}^T K^{-1} \\mathbf{y} \\\\ & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\ & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*) \\end{align} \\begin{align} & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim MVT_{\\nu + n_{tr}}(\\mathbf{\\bar{f_*}}, \\frac{\\nu + \\beta - 2}{\\nu + n_{tr} - 2} \\times cov(\\mathbf{f_*})) \\label{eq:posterior}\\\\ & \\beta = \\mathbf{y}^T K^{-1} \\mathbf{y} \\\\ & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\ & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*) \\end{align} STP models for a single output \u00b6 For univariate GP models (single output), use the StudentTRegressionModel class (an extension of AbstractSTPRegressionModel ). To construct a STP regression model you would need: The degrees of freedom \\nu \\nu Kernel/covariance instance to model correlation between values of the latent function at each pair of input features. Kernel instance to model the correlation of the additive noise, generally the DiracKernel (white noise) is used. Training data val trainingdata : Stream [( DenseVector [ Double ] , Double )] = ... val num_features = trainingdata . head . _1 . length // Create an implicit vector field for the creation of the stationary // radial basis function kernel implicit val field = VectorField ( num_features ) val kernel = new RBFKernel ( 2.5 ) val noiseKernel = new DiracKernel ( 1.5 ) val model = new StudentTRegression ( 1.5 , kernel , noiseKernel , trainingData ) STP models for Multiple Outputs \u00b6 You can use the MOStudentTRegression [ I ] class to create multi-output GP models. val trainingdata : Stream [( DenseVector [ Double ] , DenseVector [ Double ])] = ... val model = new MOStudentTRegression [ DenseVector [ Double ]]( sos_kernel , sos_noise , trainingdata , trainingdata . length , trainingdata . head . _2 . length ) Tip Working with multi-output Student T models is similar to multi-output GP models. We need to create a kernel function over the combined index set ( DenseVector [ Double ], Int ) . This can be done using the sum of separable kernel idea. val linearK = new PolynomialKernel ( 2 , 1.0 ) val tKernel = new TStudentKernel ( 0.2 ) val d = new DiracKernel ( 0.037 ) val mixedEffects = new MixedEffectRegularizer ( 0.5 ) val coRegCauchyMatrix = new CoRegCauchyKernel ( 10.0 ) val coRegDiracMatrix = new CoRegDiracKernel val sos_kernel : CompositeCovariance [( DenseVector [ Double ] , Int )] = ( linearK :* mixedEffects ) + ( tKernel :* coRegCauchyMatrix ) val sos_noise : CompositeCovariance [( DenseVector [ Double ] , Int )] = d :* coRegDiracMatrix","title":"Students T Processes"},{"location":"core/core_stp/#regression-with-student-t-processes","text":"The regression formulation for STP models is identical to the GP regression framework, to summarize the posterior predictive distribution takes the following form. Suppose \\mathbf{t} \\sim MVT_{n_{tr} + n_t}(\\nu, \\mathbf{0}, K) \\mathbf{t} \\sim MVT_{n_{tr} + n_t}(\\nu, \\mathbf{0}, K) is the process producing the data. Let [\\mathbf{f_*}]_{n_{t} \\times 1} [\\mathbf{f_*}]_{n_{t} \\times 1} represent the values of the function on the test inputs and [\\mathbf{y}]_{n_{tr} \\times 1} [\\mathbf{y}]_{n_{tr} \\times 1} represent noisy observations made on the training data points. \\begin{align} & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim MVT_{\\nu + n_{tr}}(\\mathbf{\\bar{f_*}}, \\frac{\\nu + \\beta - 2}{\\nu + n_{tr} - 2} \\times cov(\\mathbf{f_*})) \\label{eq:posterior}\\\\ & \\beta = \\mathbf{y}^T K^{-1} \\mathbf{y} \\\\ & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\ & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*) \\end{align} \\begin{align} & \\mathbf{f_*}|X,\\mathbf{y},X_* \\sim MVT_{\\nu + n_{tr}}(\\mathbf{\\bar{f_*}}, \\frac{\\nu + \\beta - 2}{\\nu + n_{tr} - 2} \\times cov(\\mathbf{f_*})) \\label{eq:posterior}\\\\ & \\beta = \\mathbf{y}^T K^{-1} \\mathbf{y} \\\\ & \\mathbf{\\bar{f_*}} \\overset{\\triangle}{=} \\mathbb{E}[\\mathbf{f_*}|X,y,X_*] = K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1} \\mathbf{y} \\label{eq:posterior:mean} \\\\ & cov(\\mathbf{f_*}) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^{2}_n \\it{I}]^{-1}K(X,X_*) \\end{align}","title":"Regression with Student T Processes"},{"location":"core/core_stp/#stp-models-for-a-single-output","text":"For univariate GP models (single output), use the StudentTRegressionModel class (an extension of AbstractSTPRegressionModel ). To construct a STP regression model you would need: The degrees of freedom \\nu \\nu Kernel/covariance instance to model correlation between values of the latent function at each pair of input features. Kernel instance to model the correlation of the additive noise, generally the DiracKernel (white noise) is used. Training data val trainingdata : Stream [( DenseVector [ Double ] , Double )] = ... val num_features = trainingdata . head . _1 . length // Create an implicit vector field for the creation of the stationary // radial basis function kernel implicit val field = VectorField ( num_features ) val kernel = new RBFKernel ( 2.5 ) val noiseKernel = new DiracKernel ( 1.5 ) val model = new StudentTRegression ( 1.5 , kernel , noiseKernel , trainingData )","title":"STP models for a single output"},{"location":"core/core_stp/#stp-models-for-multiple-outputs","text":"You can use the MOStudentTRegression [ I ] class to create multi-output GP models. val trainingdata : Stream [( DenseVector [ Double ] , DenseVector [ Double ])] = ... val model = new MOStudentTRegression [ DenseVector [ Double ]]( sos_kernel , sos_noise , trainingdata , trainingdata . length , trainingdata . head . _2 . length ) Tip Working with multi-output Student T models is similar to multi-output GP models. We need to create a kernel function over the combined index set ( DenseVector [ Double ], Int ) . This can be done using the sum of separable kernel idea. val linearK = new PolynomialKernel ( 2 , 1.0 ) val tKernel = new TStudentKernel ( 0.2 ) val d = new DiracKernel ( 0.037 ) val mixedEffects = new MixedEffectRegularizer ( 0.5 ) val coRegCauchyMatrix = new CoRegCauchyKernel ( 10.0 ) val coRegDiracMatrix = new CoRegDiracKernel val sos_kernel : CompositeCovariance [( DenseVector [ Double ] , Int )] = ( linearK :* mixedEffects ) + ( tKernel :* coRegCauchyMatrix ) val sos_noise : CompositeCovariance [( DenseVector [ Double ] , Int )] = d :* coRegDiracMatrix","title":"STP models for Multiple Outputs"},{"location":"core/core_tf_misc/","text":"Summary The dynaml.tensorflow.dtfutils provides some miscellaneous computational functions.","title":"Core tf misc"},{"location":"core/partitioned_alg_utils/","text":"Summary The algebra package has utility functions for commonly used operations on block matrices and vectors. We give the user a glimpse here. Warning The routines in this section assume that the block sizes of the input matrix are homogenous i.e. the number of row blocks is equal to number of column blocks. Blocked Operations \u00b6 Following operations are the blocked implementations of standard algorithms used for matrices. LU LU Decomposition \u00b6 LU LU decomposition consists of decomposing a square matrix into lower and upper triangular factors. //Initialize a square matrix val sq_mat : PartitionedMatrix = _ val ( lower , upper ) = bLU ( sq_mat ) Cholesky Decomposition \u00b6 Cholesky decomposition consists of decomposing a symmetric positive semi-definite matrix uniquely into lower and upper triangular factors. //Initialize a psd matrix val psd_mat : PartitionedPSDMatrix = _ val ( lower , upper ) = bcholesky ( psd_mat ) Trace \u00b6 Trace of a square matrix is the sum of the diagonal elements. //Initialize a square matrix val sq_mat : PartitionedMatrix = _ val tr = btrace ( sq_mat ) Determinant \u00b6 The determinant of a square matrix represents the scaling factor of the transformation described by the matrix //Initialize a square matrix val sq_mat : PartitionedMatrix = _ val de = bdet ( sq_mat ) Diagonal \u00b6 Obtain diagonal elements of a square block matrix in the form of a block vector. //Initialize a square matrix val sq_mat : PartitionedMatrix = _ val dia : PartitionedVector = bdiagonal ( sq_mat ) Quadratic Forms \u00b6 Quadratic forms are often encountered in algebra, they involve products on inverse positive semi-definite matrices with vectors. The two common quadratic forms are. Self Quadratic Forms : \\mathbf{x}^\\intercal \\Omega^{-1} \\mathbf{x} \\mathbf{x}^\\intercal \\Omega^{-1} \\mathbf{x} Cross Quadratic Form : \\mathbf{y}^\\intercal \\Omega^{-1} \\mathbf{x} \\mathbf{y}^\\intercal \\Omega^{-1} \\mathbf{x} val ( x , y ) : ( DenseVector [ Double ], DenseVector [ Double ]) = ( _ , _ ) val omega : DenseMatrix [ Double ] = _ //Use breeze function val lower = cholesky ( omega ) val x_omega_x = quadraticForm ( lower , x ) val y_omega_x = crossQuadraticForm ( y , lower , x ) //Blocked Version of the same. val ( xb , yb ) : ( PartitionedVector , PartitionedVector ) = ( _ , _ ) val omegab : PartitionedPSDMatrix = _ //Use DynaML algebra function val lowerb = bcholesky ( omegab ) val x_omega_x_b = blockedQuadraticForm ( lowerb , xb ) val y_omega_x_b = blockedCrossQuadraticForm ( yb , lowerb , xb )","title":"Block Algebra Utilities"},{"location":"core/partitioned_alg_utils/#blocked-operations","text":"Following operations are the blocked implementations of standard algorithms used for matrices.","title":"Blocked Operations"},{"location":"core/partitioned_alg_utils/#lulu-decomposition","text":"LU LU decomposition consists of decomposing a square matrix into lower and upper triangular factors. //Initialize a square matrix val sq_mat : PartitionedMatrix = _ val ( lower , upper ) = bLU ( sq_mat )","title":"LULU Decomposition"},{"location":"core/partitioned_alg_utils/#cholesky-decomposition","text":"Cholesky decomposition consists of decomposing a symmetric positive semi-definite matrix uniquely into lower and upper triangular factors. //Initialize a psd matrix val psd_mat : PartitionedPSDMatrix = _ val ( lower , upper ) = bcholesky ( psd_mat )","title":"Cholesky Decomposition"},{"location":"core/partitioned_alg_utils/#trace","text":"Trace of a square matrix is the sum of the diagonal elements. //Initialize a square matrix val sq_mat : PartitionedMatrix = _ val tr = btrace ( sq_mat )","title":"Trace"},{"location":"core/partitioned_alg_utils/#determinant","text":"The determinant of a square matrix represents the scaling factor of the transformation described by the matrix //Initialize a square matrix val sq_mat : PartitionedMatrix = _ val de = bdet ( sq_mat )","title":"Determinant"},{"location":"core/partitioned_alg_utils/#diagonal","text":"Obtain diagonal elements of a square block matrix in the form of a block vector. //Initialize a square matrix val sq_mat : PartitionedMatrix = _ val dia : PartitionedVector = bdiagonal ( sq_mat )","title":"Diagonal"},{"location":"core/partitioned_alg_utils/#quadratic-forms","text":"Quadratic forms are often encountered in algebra, they involve products on inverse positive semi-definite matrices with vectors. The two common quadratic forms are. Self Quadratic Forms : \\mathbf{x}^\\intercal \\Omega^{-1} \\mathbf{x} \\mathbf{x}^\\intercal \\Omega^{-1} \\mathbf{x} Cross Quadratic Form : \\mathbf{y}^\\intercal \\Omega^{-1} \\mathbf{x} \\mathbf{y}^\\intercal \\Omega^{-1} \\mathbf{x} val ( x , y ) : ( DenseVector [ Double ], DenseVector [ Double ]) = ( _ , _ ) val omega : DenseMatrix [ Double ] = _ //Use breeze function val lower = cholesky ( omega ) val x_omega_x = quadraticForm ( lower , x ) val y_omega_x = crossQuadraticForm ( y , lower , x ) //Blocked Version of the same. val ( xb , yb ) : ( PartitionedVector , PartitionedVector ) = ( _ , _ ) val omegab : PartitionedPSDMatrix = _ //Use DynaML algebra function val lowerb = bcholesky ( omegab ) val x_omega_x_b = blockedQuadraticForm ( lowerb , xb ) val y_omega_x_b = blockedCrossQuadraticForm ( yb , lowerb , xb )","title":"Quadratic Forms"},{"location":"core/partitioned_matrices/","text":"Summary Here we show how to use the block matrix API The algebra package contains a number of block matrix implementations. Class Represents Notes PartitionedMatrix A general block matrix User facing i.e. can be instantiated directly LowerTriPartitionedMatrix Lower triangular block matrix Result of algebra API calls UpperTriPartitionedMatrix Upper triangular block matrix Result of algebra API calls PartitionedPSDMatrix Symmetric positive semi-definite matrix Result of applying kernel function on data. Creation \u00b6 Block can be created in two major ways. From Input Blocks \u00b6 val index_set = ( 0L until 10L ). toStream //Create the data blocks val data_blocks : Stream [(( Long , Long ) , DenseVector [ Double ])] = utils . combine ( index_set ). map ( indices => ( ( indices . head , indices . last ), DenseMatrix . ones [ Double ]( 500 , 500 ) ) ) //Instantiate the partitioned matrix //must provide dimensions val part_matrix = PartitionedMatrix ( data_blocks , numrows = 5000L , numcols = 5000L ) From Tabulating Functions \u00b6 val tabFunc : ( Long , Long ) => Double = ( indexR : Long , indexC : Long ) => { math . sin ( 2 d * math . Pi * indexR / 5000 d )* math . cos ( 2 d * math . Pi * indexC / 5000 d ) } //Instantiate the partitioned matrix val part_matrix = PartitionedMatrix ( nRows = 5000L , nCols = 5000L , numElementsPerRBlock = 1000 , numElementsPerCBlock = 1000 , tabFunc ) From Outer Product \u00b6 A PartitionedMatrix can also be constructed from the product of a PartitionedDualVector and PartitionedVector . val random_var = RandomVariable ( new Beta ( 1.5 , 2.5 )) val rand_vec1 = PartitionedVector . rand ( 2000L , 500 , random_var ) val rand_vec2 = PartitionedVector . rand ( 2000L , 500 , random_var ) val p_mat = rand_vec1 * rand_vec2 . t Matrix Concatenation \u00b6 You can vertically join matrices, as long as the number of rows and row blocks match. val mat1 : PartitionedMatrix = _ val mat2 : PartitionedMatrix = _ val mat3 = PartitionedMatrix . vertcat ( mat1 , mat2 ) Positive Semi-Definite Matrices The class PartitionedPSDMatrix can be instantiated in two ways. From outer product. val random_var = RandomVariable ( new Beta ( 1.5 , 2.5 )) val rand_vec = PartitionedVector . rand ( 2000L , 500 , random_var ) val psd_mat = PartitionedPSDMatrix . fromOuterProduct ( rand_vec ) From kernel evaluation //Obtain data val data : Seq [ DenseVector [ Double ]] = _ //Create kernel instance val kernel : LocalScalarKernel [ DenseVector [ Double ]] = _ val psd_gram_mat = kernel . buildBlockedKernelMatrix ( data , data . length ) Algebraic Operations \u00b6 Partitioned vectors and dual vectors have a number of algebraic operations available in the API. val beta_var = RandomVariable ( Beta ( 1.5 , 2.5 )) val gamma_var = RandomVariable ( Gamma ( 1.5 , 2.5 )) val p_vec_beta = PartitionedVector . rand ( 5000L , 1000 , beta_var ) val p_vec_gamma = PartitionedVector . rand ( 5000L , 1000 , gamma_var ) val dvec_beta = p_vec_beta . t val dvec_gamma = p_vec_gamma . t val mat1 = p_vec_gamma * dvec_gamma val mat2 = p_vec_beta * dvec_beta //Addition val add_mat = mat1 + mat2 //Subtraction val sub_mat = mat2 - mat1 //Element wise multiplication val mult_mat = mat1 :* mat2 //Matrix matrix product val prod_mat = mat1 * mat2 //matrix vector Product val prod = mat1 * p_vec_beta val prod_dual = dvec_gamma * mat2 //Scaler multiplication val sc_mat = mat1 * 1.5 Misc. Operations \u00b6 Map Partitions \u00b6 Map each index, partition pair by a Scala function. val vec : PartitionedMatrix = _ val other_vec = vec . map ( ( pair : (( Long , Long ), DenseMatrix [ Double ])) => ( pair . _1 , pair . _2 * 1.5 ) ) Slice \u00b6 Obtain subset of elements, the new matrix is repartitioned and re-indexed accordingly. val vec : PartitionedVector = PartitionedVector . ones ( 5000L , 1000 ) val mat = vec * vec . t val other_mat = vec ( 999L until 2000L , 0L until 999L ) Upper and Lower Triangular Sections \u00b6 val vec : PartitionedVector = PartitionedVector . ones ( 5000L , 1000 ) val mat = vec * vec . t val lower_tri : LowerTriPartitionedMatrix = mat . L val upper_tri : UpperTriPartitionedMatrix = mat . U Convert to Breeze Matrix \u00b6 val mat : PartitionedMatrix = _ //Do not use on large vectors as //it might lead to overflow of memory. val breeze_mat = mat . toBreezeMatrix","title":"Block Matrices"},{"location":"core/partitioned_matrices/#creation","text":"Block can be created in two major ways.","title":"Creation"},{"location":"core/partitioned_matrices/#from-input-blocks","text":"val index_set = ( 0L until 10L ). toStream //Create the data blocks val data_blocks : Stream [(( Long , Long ) , DenseVector [ Double ])] = utils . combine ( index_set ). map ( indices => ( ( indices . head , indices . last ), DenseMatrix . ones [ Double ]( 500 , 500 ) ) ) //Instantiate the partitioned matrix //must provide dimensions val part_matrix = PartitionedMatrix ( data_blocks , numrows = 5000L , numcols = 5000L )","title":"From Input Blocks"},{"location":"core/partitioned_matrices/#from-tabulating-functions","text":"val tabFunc : ( Long , Long ) => Double = ( indexR : Long , indexC : Long ) => { math . sin ( 2 d * math . Pi * indexR / 5000 d )* math . cos ( 2 d * math . Pi * indexC / 5000 d ) } //Instantiate the partitioned matrix val part_matrix = PartitionedMatrix ( nRows = 5000L , nCols = 5000L , numElementsPerRBlock = 1000 , numElementsPerCBlock = 1000 , tabFunc )","title":"From Tabulating Functions"},{"location":"core/partitioned_matrices/#from-outer-product","text":"A PartitionedMatrix can also be constructed from the product of a PartitionedDualVector and PartitionedVector . val random_var = RandomVariable ( new Beta ( 1.5 , 2.5 )) val rand_vec1 = PartitionedVector . rand ( 2000L , 500 , random_var ) val rand_vec2 = PartitionedVector . rand ( 2000L , 500 , random_var ) val p_mat = rand_vec1 * rand_vec2 . t","title":"From Outer Product"},{"location":"core/partitioned_matrices/#matrix-concatenation","text":"You can vertically join matrices, as long as the number of rows and row blocks match. val mat1 : PartitionedMatrix = _ val mat2 : PartitionedMatrix = _ val mat3 = PartitionedMatrix . vertcat ( mat1 , mat2 ) Positive Semi-Definite Matrices The class PartitionedPSDMatrix can be instantiated in two ways. From outer product. val random_var = RandomVariable ( new Beta ( 1.5 , 2.5 )) val rand_vec = PartitionedVector . rand ( 2000L , 500 , random_var ) val psd_mat = PartitionedPSDMatrix . fromOuterProduct ( rand_vec ) From kernel evaluation //Obtain data val data : Seq [ DenseVector [ Double ]] = _ //Create kernel instance val kernel : LocalScalarKernel [ DenseVector [ Double ]] = _ val psd_gram_mat = kernel . buildBlockedKernelMatrix ( data , data . length )","title":"Matrix Concatenation"},{"location":"core/partitioned_matrices/#algebraic-operations","text":"Partitioned vectors and dual vectors have a number of algebraic operations available in the API. val beta_var = RandomVariable ( Beta ( 1.5 , 2.5 )) val gamma_var = RandomVariable ( Gamma ( 1.5 , 2.5 )) val p_vec_beta = PartitionedVector . rand ( 5000L , 1000 , beta_var ) val p_vec_gamma = PartitionedVector . rand ( 5000L , 1000 , gamma_var ) val dvec_beta = p_vec_beta . t val dvec_gamma = p_vec_gamma . t val mat1 = p_vec_gamma * dvec_gamma val mat2 = p_vec_beta * dvec_beta //Addition val add_mat = mat1 + mat2 //Subtraction val sub_mat = mat2 - mat1 //Element wise multiplication val mult_mat = mat1 :* mat2 //Matrix matrix product val prod_mat = mat1 * mat2 //matrix vector Product val prod = mat1 * p_vec_beta val prod_dual = dvec_gamma * mat2 //Scaler multiplication val sc_mat = mat1 * 1.5","title":"Algebraic Operations"},{"location":"core/partitioned_matrices/#misc-operations","text":"","title":"Misc. Operations"},{"location":"core/partitioned_matrices/#map-partitions","text":"Map each index, partition pair by a Scala function. val vec : PartitionedMatrix = _ val other_vec = vec . map ( ( pair : (( Long , Long ), DenseMatrix [ Double ])) => ( pair . _1 , pair . _2 * 1.5 ) )","title":"Map Partitions"},{"location":"core/partitioned_matrices/#slice","text":"Obtain subset of elements, the new matrix is repartitioned and re-indexed accordingly. val vec : PartitionedVector = PartitionedVector . ones ( 5000L , 1000 ) val mat = vec * vec . t val other_mat = vec ( 999L until 2000L , 0L until 999L )","title":"Slice"},{"location":"core/partitioned_matrices/#upper-and-lower-triangular-sections","text":"val vec : PartitionedVector = PartitionedVector . ones ( 5000L , 1000 ) val mat = vec * vec . t val lower_tri : LowerTriPartitionedMatrix = mat . L val upper_tri : UpperTriPartitionedMatrix = mat . U","title":"Upper and Lower Triangular Sections"},{"location":"core/partitioned_matrices/#convert-to-breeze-matrix","text":"val mat : PartitionedMatrix = _ //Do not use on large vectors as //it might lead to overflow of memory. val breeze_mat = mat . toBreezeMatrix","title":"Convert to Breeze Matrix"},{"location":"core/partitioned_objects/","text":"Partitioned Vectors & Dual Vectors The dynaml . algebra package contains a number of classes and utilities for constructing blocked linear algebra objects. DynaML makes extensive use of the breeze linear algebra library for matrix-vector operations. Breeze is an attractive option because it is easy to use and has the ability to use low level implementations such as LAPACK and BLAS for performance speed-up. If we are working with linear algebra objects which are large in size as compared to the available JVM memory, it may be necessary to not construct the entire vector in an eager fashion. In the Scala language, there are lazy data structures, which are not computed unless they are required in further computation. The dynaml.algebra package leverages lazy data structures to create blocked vectors and matrices. Each partition/block of a blocked object is a breeze vector or matrix. The proceeding pages give the user a glimpse of how to use and manipulate objects of the algebra package.","title":"Blocked Algebraic Objects"},{"location":"core/partitioned_vectors/","text":"Summary Here we show how to use blocked vectors and blocked dual vectors. Blocked vectors and dual vectors in the algebra package are wrappers around Stream [( Long , DenseVector [ Double ])] each partition consists of an ordered index and the partition content which is in the form of a breeze vector. The relevant API endpoints are PartitionedVector and PartitionedDualVector . In order to access these objects, you must do import io.github.mandar2812.dynaml.algebra._ (already loaded by default in the DynaML shell). Creation \u00b6 Block vectors can be created in a number of ways. PartitionedVector and PartitionedDualVector are column row vectors respectively and treated as transposes of each other. From Input Blocks \u00b6 //Create the data blocks val data_blocks : Stream [( Long , DenseVector [ Double ])] = ( 0L until 10L ). toStream . map ( index => ( index , DenseVector . ones [ Double ]( 500 ))) //Instantiate the partitioned vector val part_vector = PartitionedVector ( data_blocks ) //Optionally you may also provide the total length //of the partitioned vector val part_vector = PartitionedVector ( data_blocks , num_rows : Long = 5000L ) //Created Block Dual Vector val part_dvector = PartitionedDualVector ( data_blocks ) //Optionally you may also provide the total length //of the partitioned dual vector val part_dvector = PartitionedDualVector ( data_blocks , num_rows : Long = 5000L ) From Tabulating Functions \u00b6 val tabFunc : ( Long ) => Double = ( index : Long ) => math . sin ( 2 d * math . Pi * index / 5000 d ) //Instantiate the partitioned vector val part_vector = PartitionedVector ( length = 5000L , numElementsPerBlock = 500 , tabFunc ) //Instantiate the partitioned dual vector val part_dvector = PartitionedDualVector ( length = 5000L , numElementsPerBlock = 500 , tabFunc ) From a Stream \u00b6 //Create the data stream val data : Stream [ Double ] = Stream . fill [ Double ]( 5000 )( 1.0 ) //Instantiate the partitioned vector val part_vector = PartitionedVector ( data , length = 5000L , num_elements_per_block = 500 ) From a Breeze Vector \u00b6 //Create the data blocks val data_vector = DenseVector . ones [ Double ]( 5000 ) //Instantiate the partitioned vector val part_vector = PartitionedVector ( data_vector , num_elements_per_block = 500 ) Apart from the above methods of creation there are a number of convenience functions available. Vector with Filled Values \u00b6 Vector of zeros \u00b6 val ones_vec = PartitionedVector . zeros ( 5000L , 500 ) Vector of Ones \u00b6 val ones_vec = PartitionedVector . ones ( 5000L , 500 ) Vector of Random Values \u00b6 val random_var = RandomVariable ( new Beta ( 1.5 , 2.5 )) val rand_vec = PartitionedVector . rand ( 5000L , 500 , random_var ) Vector Concatenation \u00b6 val random_var = RandomVariable ( new Beta ( 1.5 , 2.5 )) val rand_vec1 = PartitionedVector . rand ( 2000L , 500 , random_var ) val rand_vec2 = PartitionedVector . rand ( 2000L , 500 , random_var ) //Vector of length 4000, having 8 blocks of 500 elements each val vec = PartitionedVector . vertcat ( rand_vec1 , rand_vec2 ) Tip A PartitionedDualVector can be created via the transpose operation on a PartitionedVector instance and vice versa. val random_var = RandomVariable ( new Beta ( 1.5 , 2.5 )) val p_vec = PartitionedVector . rand ( 5000L , 500 , random_var ) val p_dvec = p_vec . t Algebraic Operations \u00b6 Partitioned vectors and dual vectors have a number of algebraic operations available in the API. val beta_var = RandomVariable ( Beta ( 1.5 , 2.5 )) val gamma_var = RandomVariable ( Gamma ( 1.5 , 2.5 )) val p_vec_beta = PartitionedVector . rand ( 5000L , 500 , beta_var ) val p_vec_gamma = PartitionedVector . rand ( 5000L , 500 , gamma_var ) val dvec_beta = p_vec_beta . t val dvec_gamma = p_vec_gamma . t //Addition val add_vec = p_vec_beta + p_vec_gamma val add_dvec = dvec_beta + dvec_gamma //Subtraction val sub_vec = p_vec_beta - p_vec_gamma val sub_dvec = dvec_beta - dvec_gamma //Element wise multiplication val mult_vec = p_vec_beta :* p_vec_gamma //Element wise division val div_vec = p_vec_beta :/ p_vec_gamma //Inner Product val prod = dvec_gamma * p_vec_beta //Scaler multiplication val sc_vec = add_vec * 1.5 val sc_dvec = add_dvec * 2.5 Misc. Operations \u00b6 Map Partitions \u00b6 Map each index, partition pair by a scala function. val vec : PartitionedVector = _ val other_vec = vec . map ( ( pair : ( Long , DenseVector [ Double ])) => ( pair . _1 , pair . _2 * 1.5 ) ) Slice \u00b6 Obtain subset of elements, the new vector is repartitioned and re-indexed accordingly. val vec : PartitionedVector = PartitionedVector . ones ( 5000L , 500 ) val other_vec = vec ( 999L until 2000L ) Reverse \u00b6 Reverse a block vector val vec : PartitionedVector = PartitionedVector . ones ( 5000L , 500 ) val reverse_vec = vec . reverse Convert to Breeze Vector \u00b6 val vec : PartitionedVector = PartitionedVector . ones ( 5000L , 500 ) //Do not use on large vectors as //it might lead to overflow of memory. val breeze_vec = vec . toBreezeVector","title":"Block Vectors"},{"location":"core/partitioned_vectors/#creation","text":"Block vectors can be created in a number of ways. PartitionedVector and PartitionedDualVector are column row vectors respectively and treated as transposes of each other.","title":"Creation"},{"location":"core/partitioned_vectors/#from-input-blocks","text":"//Create the data blocks val data_blocks : Stream [( Long , DenseVector [ Double ])] = ( 0L until 10L ). toStream . map ( index => ( index , DenseVector . ones [ Double ]( 500 ))) //Instantiate the partitioned vector val part_vector = PartitionedVector ( data_blocks ) //Optionally you may also provide the total length //of the partitioned vector val part_vector = PartitionedVector ( data_blocks , num_rows : Long = 5000L ) //Created Block Dual Vector val part_dvector = PartitionedDualVector ( data_blocks ) //Optionally you may also provide the total length //of the partitioned dual vector val part_dvector = PartitionedDualVector ( data_blocks , num_rows : Long = 5000L )","title":"From Input Blocks"},{"location":"core/partitioned_vectors/#from-tabulating-functions","text":"val tabFunc : ( Long ) => Double = ( index : Long ) => math . sin ( 2 d * math . Pi * index / 5000 d ) //Instantiate the partitioned vector val part_vector = PartitionedVector ( length = 5000L , numElementsPerBlock = 500 , tabFunc ) //Instantiate the partitioned dual vector val part_dvector = PartitionedDualVector ( length = 5000L , numElementsPerBlock = 500 , tabFunc )","title":"From Tabulating Functions"},{"location":"core/partitioned_vectors/#from-a-stream","text":"//Create the data stream val data : Stream [ Double ] = Stream . fill [ Double ]( 5000 )( 1.0 ) //Instantiate the partitioned vector val part_vector = PartitionedVector ( data , length = 5000L , num_elements_per_block = 500 )","title":"From a Stream"},{"location":"core/partitioned_vectors/#from-a-breeze-vector","text":"//Create the data blocks val data_vector = DenseVector . ones [ Double ]( 5000 ) //Instantiate the partitioned vector val part_vector = PartitionedVector ( data_vector , num_elements_per_block = 500 ) Apart from the above methods of creation there are a number of convenience functions available.","title":"From a Breeze Vector"},{"location":"core/partitioned_vectors/#vector-with-filled-values","text":"","title":"Vector with Filled Values"},{"location":"core/partitioned_vectors/#vector-of-zeros","text":"val ones_vec = PartitionedVector . zeros ( 5000L , 500 )","title":"Vector of zeros"},{"location":"core/partitioned_vectors/#vector-of-ones","text":"val ones_vec = PartitionedVector . ones ( 5000L , 500 )","title":"Vector of Ones"},{"location":"core/partitioned_vectors/#vector-of-random-values","text":"val random_var = RandomVariable ( new Beta ( 1.5 , 2.5 )) val rand_vec = PartitionedVector . rand ( 5000L , 500 , random_var )","title":"Vector of Random Values"},{"location":"core/partitioned_vectors/#vector-concatenation","text":"val random_var = RandomVariable ( new Beta ( 1.5 , 2.5 )) val rand_vec1 = PartitionedVector . rand ( 2000L , 500 , random_var ) val rand_vec2 = PartitionedVector . rand ( 2000L , 500 , random_var ) //Vector of length 4000, having 8 blocks of 500 elements each val vec = PartitionedVector . vertcat ( rand_vec1 , rand_vec2 ) Tip A PartitionedDualVector can be created via the transpose operation on a PartitionedVector instance and vice versa. val random_var = RandomVariable ( new Beta ( 1.5 , 2.5 )) val p_vec = PartitionedVector . rand ( 5000L , 500 , random_var ) val p_dvec = p_vec . t","title":"Vector Concatenation"},{"location":"core/partitioned_vectors/#algebraic-operations","text":"Partitioned vectors and dual vectors have a number of algebraic operations available in the API. val beta_var = RandomVariable ( Beta ( 1.5 , 2.5 )) val gamma_var = RandomVariable ( Gamma ( 1.5 , 2.5 )) val p_vec_beta = PartitionedVector . rand ( 5000L , 500 , beta_var ) val p_vec_gamma = PartitionedVector . rand ( 5000L , 500 , gamma_var ) val dvec_beta = p_vec_beta . t val dvec_gamma = p_vec_gamma . t //Addition val add_vec = p_vec_beta + p_vec_gamma val add_dvec = dvec_beta + dvec_gamma //Subtraction val sub_vec = p_vec_beta - p_vec_gamma val sub_dvec = dvec_beta - dvec_gamma //Element wise multiplication val mult_vec = p_vec_beta :* p_vec_gamma //Element wise division val div_vec = p_vec_beta :/ p_vec_gamma //Inner Product val prod = dvec_gamma * p_vec_beta //Scaler multiplication val sc_vec = add_vec * 1.5 val sc_dvec = add_dvec * 2.5","title":"Algebraic Operations"},{"location":"core/partitioned_vectors/#misc-operations","text":"","title":"Misc. Operations"},{"location":"core/partitioned_vectors/#map-partitions","text":"Map each index, partition pair by a scala function. val vec : PartitionedVector = _ val other_vec = vec . map ( ( pair : ( Long , DenseVector [ Double ])) => ( pair . _1 , pair . _2 * 1.5 ) )","title":"Map Partitions"},{"location":"core/partitioned_vectors/#slice","text":"Obtain subset of elements, the new vector is repartitioned and re-indexed accordingly. val vec : PartitionedVector = PartitionedVector . ones ( 5000L , 500 ) val other_vec = vec ( 999L until 2000L )","title":"Slice"},{"location":"core/partitioned_vectors/#reverse","text":"Reverse a block vector val vec : PartitionedVector = PartitionedVector . ones ( 5000L , 500 ) val reverse_vec = vec . reverse","title":"Reverse"},{"location":"core/partitioned_vectors/#convert-to-breeze-vector","text":"val vec : PartitionedVector = PartitionedVector . ones ( 5000L , 500 ) //Do not use on large vectors as //it might lead to overflow of memory. val breeze_vec = vec . toBreezeVector","title":"Convert to Breeze Vector"},{"location":"installation/include/","text":"Maven \u00b6 To include DynaML in your maven JVM project edit your pom.xml file as follows <repositories> <repository> <id> jitpack.io </id> <url> https://jitpack.io </url> </repository> </repositories> <dependency> <groupId> com.github.transcendent-ai-labs </groupId> <artifactId> DynaML </artifactId> <version> v1.4 </version> </dependency> SBT \u00b6 For sbt projects edit your build.sbt (see JitPack for more details) resolvers += \"jitpack\" at \"https://jitpack.io\" libraryDependencies += \"com.github.transcendent-ai-labs\" % \"DynaML\" % version Gradle \u00b6 In your gradle project, add the following to the root build.gradle as follows allprojects { repositories { ... maven { url \"https://jitpack.io\" } } } and then add the dependency like dependencies { compile 'com.github.User:Repo:Tag' } Leinengen \u00b6 In project.clj :repositories [[ \"jitpack\" \"https://jitpack.io\" ]] :dependencies [[ com.github.User/Repo \"Tag\" ]]","title":"Import"},{"location":"installation/include/#maven","text":"To include DynaML in your maven JVM project edit your pom.xml file as follows <repositories> <repository> <id> jitpack.io </id> <url> https://jitpack.io </url> </repository> </repositories> <dependency> <groupId> com.github.transcendent-ai-labs </groupId> <artifactId> DynaML </artifactId> <version> v1.4 </version> </dependency>","title":"Maven"},{"location":"installation/include/#sbt","text":"For sbt projects edit your build.sbt (see JitPack for more details) resolvers += \"jitpack\" at \"https://jitpack.io\" libraryDependencies += \"com.github.transcendent-ai-labs\" % \"DynaML\" % version","title":"SBT"},{"location":"installation/include/#gradle","text":"In your gradle project, add the following to the root build.gradle as follows allprojects { repositories { ... maven { url \"https://jitpack.io\" } } } and then add the dependency like dependencies { compile 'com.github.User:Repo:Tag' }","title":"Gradle"},{"location":"installation/include/#leinengen","text":"In project.clj :repositories [[ \"jitpack\" \"https://jitpack.io\" ]] :dependencies [[ com.github.User/Repo \"Tag\" ]]","title":"Leinengen"},{"location":"installation/installation/","text":"Platform Compatibility \u00b6 Currently DynaML installs and runs on *nix platforms. Pre-requisites \u00b6 sbt A modern HTML5 enabled browser (to view plots generated by Wisp) BLAS, LAPACK and ARPACK binaries for your platform. In case they are not installed, it is possible to disable this feature by commenting out ( // ) the section of the build.sbt file given below. \"org.scalanlp\" % \"breeze-natives_2.11\" % \"0.11.2\" % \"compile\" , Tip Using the install script There are two scripts for building DynaML. install.sh : This builds the binary and adds a symlink to the system path (sudo priviledges needed). sbt-shell.sh : This starts the sbt shell, for build/compile/test workflows. $ ./install.sh 8096m sbt-shell.sh and install.sh both take three parameters. The JVM heap size to use for the packaged executable (defaults to 4096m ). Whether to use GPU accelerated tensorflow (defaults to False). Whether to use a user compiled/built tensorflow library (True) or a pre-packaged one (False, the default). Note TensorFlow Nvidia GPU support If you want to use Nvidia GPU acceleration when DynaML calls TensorFlow, giving the appropriate flags to install.sh or sbt-shell.sh should work, but in order for your GPU to be actually utilised, you must ensure that: Your Nvidia GPU is compliant , i.e. has a compute capability of atleast 4.1 Cuda v9.0 is installed on your system and its installation location is added to LD_LIBRARY_PATH bash variable. cuDNN v7 is installed on your system and is appended to LD_LIBRARY_PATH Using user compiled TensorFlow By default, DynaML uses the Tensorflow dynamic library bundled with TensorFlow Scala distribution, but it is possible for the user to build Tensorflow from source for their platform. For example, to use DynaML with Cuda 10.0, we would need to build the tensorflow binary with Cuda 10.x, add it to LD_LIBRARY_PATH and build DynaML with the corresponding flag set to True. In most applications, the bundled TensorFlow is adequate for user requirements. Steps \u00b6 Clone the repository. $ git clone https://github.com/transcendent-ai-labs/DynaML.git Use the sbt shell launcher sbt-shell.sh : # Build with GPU support and use user-built # Tensorflow binary (accessible in LD_LIBRARY_PATH) $ ./sbt-shell.sh 8096m True True The sbt shell will open [ info ] Loading project definition from ~/DynaML/project [ info ] Set current project to DynaML sbt:DynaML> Build the source sbt:DynaML>stage After the project builds, exit from the sbt console and execute the DynaML start script from the bash shell. Make sure you have execute permissions on the DynaML start script. $ ./target/universal/stage/bin/dynaml You should see the prompt. Welcome to DynaML v2.0-SNAPSHOT Interactive Scala shell for Machine Learning Research Currently running on: (Scala 2.12.10 Java 11.0.7) DynaML>","title":"Installation"},{"location":"installation/installation/#platform-compatibility","text":"Currently DynaML installs and runs on *nix platforms.","title":"Platform Compatibility"},{"location":"installation/installation/#pre-requisites","text":"sbt A modern HTML5 enabled browser (to view plots generated by Wisp) BLAS, LAPACK and ARPACK binaries for your platform. In case they are not installed, it is possible to disable this feature by commenting out ( // ) the section of the build.sbt file given below. \"org.scalanlp\" % \"breeze-natives_2.11\" % \"0.11.2\" % \"compile\" , Tip Using the install script There are two scripts for building DynaML. install.sh : This builds the binary and adds a symlink to the system path (sudo priviledges needed). sbt-shell.sh : This starts the sbt shell, for build/compile/test workflows. $ ./install.sh 8096m sbt-shell.sh and install.sh both take three parameters. The JVM heap size to use for the packaged executable (defaults to 4096m ). Whether to use GPU accelerated tensorflow (defaults to False). Whether to use a user compiled/built tensorflow library (True) or a pre-packaged one (False, the default). Note TensorFlow Nvidia GPU support If you want to use Nvidia GPU acceleration when DynaML calls TensorFlow, giving the appropriate flags to install.sh or sbt-shell.sh should work, but in order for your GPU to be actually utilised, you must ensure that: Your Nvidia GPU is compliant , i.e. has a compute capability of atleast 4.1 Cuda v9.0 is installed on your system and its installation location is added to LD_LIBRARY_PATH bash variable. cuDNN v7 is installed on your system and is appended to LD_LIBRARY_PATH Using user compiled TensorFlow By default, DynaML uses the Tensorflow dynamic library bundled with TensorFlow Scala distribution, but it is possible for the user to build Tensorflow from source for their platform. For example, to use DynaML with Cuda 10.0, we would need to build the tensorflow binary with Cuda 10.x, add it to LD_LIBRARY_PATH and build DynaML with the corresponding flag set to True. In most applications, the bundled TensorFlow is adequate for user requirements.","title":"Pre-requisites"},{"location":"installation/installation/#steps","text":"Clone the repository. $ git clone https://github.com/transcendent-ai-labs/DynaML.git Use the sbt shell launcher sbt-shell.sh : # Build with GPU support and use user-built # Tensorflow binary (accessible in LD_LIBRARY_PATH) $ ./sbt-shell.sh 8096m True True The sbt shell will open [ info ] Loading project definition from ~/DynaML/project [ info ] Set current project to DynaML sbt:DynaML> Build the source sbt:DynaML>stage After the project builds, exit from the sbt console and execute the DynaML start script from the bash shell. Make sure you have execute permissions on the DynaML start script. $ ./target/universal/stage/bin/dynaml You should see the prompt. Welcome to DynaML v2.0-SNAPSHOT Interactive Scala shell for Machine Learning Research Currently running on: (Scala 2.12.10 Java 11.0.7) DynaML>","title":"Steps"},{"location":"pipes/feature_processing/","text":"Feature Processing \u00b6 Extract features and targets \u00b6 splitFeaturesAndTargets Type : DataPipe [ Stream [ String ] , Stream [( DenseVector [ Double ] , Double )]] Result : Take each line which is a comma separated string and extract all but the last element into a feature vector and leave the last element as the \"target\" value. Extract Specific Columns \u00b6 extractTrainingFeatures ( columns , missingVals ) Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Extract a subset of columns from a stream of comma separated string also replace any missing value strings with the empty string. Usage : DynaMLPipe . extractTrainingFeatures ( List ( 1 , 2 , 3 ), Map ( 1 -> \"N.A.\" , 2 -> \"NA\" , 3 -> \"na\" )) Gaussian Scaling of Data \u00b6 gaussianScaling Result : Perform gaussian normalization of features & targets, on a data stream which is a of the form Stream [( DenseVector [ Double ] , Double )] . Gaussian Scaling of Train/Test Splits \u00b6 gaussianScalingTrainTest Result : Perform gaussian normalization of features & targets, on a data stream which is a Tuple2 of the form ( Stream ( training data ), Stream ( test data )) . Min-Max Scaling of Data \u00b6 minMaxScaling Result : Perform 0-1 scaling of features & targets, on a data stream which is a of the form Stream [( DenseVector [ Double ] , Double )] . Min-Max Scaling of Train/Test Splits \u00b6 minMaxScalingTrainTest Result : Perform 0-1 scaling of features & targets, on a data stream which is a Tuple2 of the form ( Stream ( training_data ), Stream ( test_data )) .","title":"Feature Processing"},{"location":"pipes/feature_processing/#feature-processing","text":"","title":"Feature Processing"},{"location":"pipes/feature_processing/#extract-features-and-targets","text":"splitFeaturesAndTargets Type : DataPipe [ Stream [ String ] , Stream [( DenseVector [ Double ] , Double )]] Result : Take each line which is a comma separated string and extract all but the last element into a feature vector and leave the last element as the \"target\" value.","title":"Extract features and targets"},{"location":"pipes/feature_processing/#extract-specific-columns","text":"extractTrainingFeatures ( columns , missingVals ) Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Extract a subset of columns from a stream of comma separated string also replace any missing value strings with the empty string. Usage : DynaMLPipe . extractTrainingFeatures ( List ( 1 , 2 , 3 ), Map ( 1 -> \"N.A.\" , 2 -> \"NA\" , 3 -> \"na\" ))","title":"Extract Specific Columns"},{"location":"pipes/feature_processing/#gaussian-scaling-of-data","text":"gaussianScaling Result : Perform gaussian normalization of features & targets, on a data stream which is a of the form Stream [( DenseVector [ Double ] , Double )] .","title":"Gaussian Scaling of Data"},{"location":"pipes/feature_processing/#gaussian-scaling-of-traintest-splits","text":"gaussianScalingTrainTest Result : Perform gaussian normalization of features & targets, on a data stream which is a Tuple2 of the form ( Stream ( training data ), Stream ( test data )) .","title":"Gaussian Scaling of Train/Test Splits"},{"location":"pipes/feature_processing/#min-max-scaling-of-data","text":"minMaxScaling Result : Perform 0-1 scaling of features & targets, on a data stream which is a of the form Stream [( DenseVector [ Double ] , Double )] .","title":"Min-Max Scaling of Data"},{"location":"pipes/feature_processing/#min-max-scaling-of-traintest-splits","text":"minMaxScalingTrainTest Result : Perform 0-1 scaling of features & targets, on a data stream which is a Tuple2 of the form ( Stream ( training_data ), Stream ( test_data )) .","title":"Min-Max Scaling of Train/Test Splits"},{"location":"pipes/file_processing/","text":"Summary A List of data pipes useful for processing contents of data files. Data Pre-processing \u00b6 File to Stream of Lines \u00b6 fileToStream Type : DataPipe [ String , Stream [ String ]] Result : Converts a text file (inputted as a file path string) into Stream [ String ] Write Stream of Lines to File \u00b6 streamToFile ( fileName : String ) Type : DataPipe [ Stream [ String ] , Unit ] Result : Writes a stream of lines to the file specified by filePath Drop first line in Stream \u00b6 dropHead Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Drop the first element of a Stream of String Replace Occurrences in of a String \u00b6 replace ( original , newString ) Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Replace all occurrences of a regular expression or string in a Stream of String with with a specified replacement string. Replace White Spaces \u00b6 replaceWhiteSpaces Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Replace all white space characters in a stream of lines. Remove Trailing White Spaces \u00b6 Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Trim white spaces from both sides of every line. Remove White Spaces \u00b6 replaceWhiteSpaces Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Replace all white space characters in a stream of lines. Remove Missing Records \u00b6 removeMissingLines Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Remove all lines/records which contain missing values Create Train/Test splits \u00b6 splitTrainingTest ( num_training , num_test ) Type : DataPipe [( Stream [( DenseVector [ Double ] , Double )] , Stream [( DenseVector [ Double ] , Double )]) , ( Stream [( DenseVector [ Double ] , Double )] , Stream [( DenseVector [ Double ] , Double )])] Result : Extract a subset of the data into a Tuple2 which can be used as a training, test combo for model learning and evaluation.","title":"String & File Processing"},{"location":"pipes/file_processing/#data-pre-processing","text":"","title":"Data Pre-processing"},{"location":"pipes/file_processing/#file-to-stream-of-lines","text":"fileToStream Type : DataPipe [ String , Stream [ String ]] Result : Converts a text file (inputted as a file path string) into Stream [ String ]","title":"File to Stream of Lines"},{"location":"pipes/file_processing/#write-stream-of-lines-to-file","text":"streamToFile ( fileName : String ) Type : DataPipe [ Stream [ String ] , Unit ] Result : Writes a stream of lines to the file specified by filePath","title":"Write Stream of Lines to File"},{"location":"pipes/file_processing/#drop-first-line-in-stream","text":"dropHead Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Drop the first element of a Stream of String","title":"Drop first line in Stream"},{"location":"pipes/file_processing/#replace-occurrences-in-of-a-string","text":"replace ( original , newString ) Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Replace all occurrences of a regular expression or string in a Stream of String with with a specified replacement string.","title":"Replace Occurrences in of a String"},{"location":"pipes/file_processing/#replace-white-spaces","text":"replaceWhiteSpaces Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Replace all white space characters in a stream of lines.","title":"Replace White Spaces"},{"location":"pipes/file_processing/#remove-trailing-white-spaces","text":"Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Trim white spaces from both sides of every line.","title":"Remove Trailing White Spaces"},{"location":"pipes/file_processing/#remove-white-spaces","text":"replaceWhiteSpaces Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Replace all white space characters in a stream of lines.","title":"Remove White Spaces"},{"location":"pipes/file_processing/#remove-missing-records","text":"removeMissingLines Type : DataPipe [ Stream [ String ] , Stream [ String ]] Result : Remove all lines/records which contain missing values","title":"Remove Missing Records"},{"location":"pipes/file_processing/#create-traintest-splits","text":"splitTrainingTest ( num_training , num_test ) Type : DataPipe [( Stream [( DenseVector [ Double ] , Double )] , Stream [( DenseVector [ Double ] , Double )]) , ( Stream [( DenseVector [ Double ] , Double )] , Stream [( DenseVector [ Double ] , Double )])] Result : Extract a subset of the data into a Tuple2 which can be used as a training, test combo for model learning and evaluation.","title":"Create Train/Test splits"},{"location":"pipes/model_pipes/","text":"Summary Model pipes define pipelines which involve predictive models. Note The classes described here exist in the dynaml.modelpipe package of the dynaml-core module. Although they are not strictly part of the pipes module, they are included here for clarity and continuity. The pipes module gives the user the ability to create workflows of arbitrary complexity. In order to enable end to end machine learning, we need pipelines which involve predictive models. These pipelines can be of two types. Pipelines which take data as input and output a predictive model. It is evident that the model creation itself is a common step in the data analysis workflow, therefore one needs library pipes which create machine learning models given the training data and other relevant inputs. Pipelines which encapsulate predictive models and generate predictions for test data splits. Once a model has been tuned/trained, it can be a part of a pipeline which generates predictions for previously unobserved data. Model Creation \u00b6 All pipelines which return predictive models as outputs extend the ModelPipe trait. Generalized Linear Model Pipe \u00b6 //Pre-process data val pre : ( Source ) => Stream [( DenseVector [ Double ] , Double )] = _ val feature_map : ( DenseVector [ Double ]) => ( DenseVector [ Double ]) = _ val glm_pipe = GLMPipe [( DenseMatrix [ Double ] , DenseVector [ Double ]) , Source ]( pre , map , task = \"regression\" , modelType = \"\" ) val dataSource : Source = _ val glm_model = glm_pipe ( dataSource ) Type : DataPipe [ Source , GeneralizedLinearModel [ T ]] Result : Takes as input a data of type Source and outputs a Generalized Linear Model . Generalized Least Squares Model Pipe \u00b6 val kernel : LocalScalarKernel [ DenseVector [ Double ]] val gls_pipe2 = GeneralizedLeastSquaresPipe2 ( kernel ) val featuremap : ( DenseVector [ Double ]) => ( DenseVector [ Double ]) = _ val data : Stream [( DenseVector [ Double ] , Double )] = _ val gls_model = gls_pipe2 ( data , featuremap ) Type : DataPipe2 [ Stream [( DenseVector [ Double ] , Double )] , DataPipe [ DenseVector [ Double ] , DenseVector [ Double ]] , GeneralizedLeastSquaresModel ] ] Result : Takes as inputs data and a feature mapping and outputs a Generalized Least Squares Model . Gaussian Process Regression Model Pipe \u00b6 //Pre-process data val pre : ( Source ) => Stream [( DenseVector [ Double ] , Double )] = _ //Declare kernel and noise val kernel : LocalScalarKernel [ DenseVector [ Double ]] = _ val noise : LocalScalarKernel [ DenseVector [ Double ]] = _ GPRegressionPipe ( pre , kernel , noise , order : Int = 0 , ex : Int = 0 ) Type : DataPipe [ Source , M ] Result : Takes as input data of type Source and outputs a Gaussian Process regression model as the output. Dual LS-SVM Model Pipe \u00b6 //Pre-process data val pre : ( Source ) => Stream [( DenseVector [ Double ] , Double )] = _ //Declare kernel val kernel : LocalScalarKernel [ DenseVector [ Double ]] = _ DLSSVMPipe ( pre , kernel , task = \"regression\" ) Type : DataPipe [ Source , DLSSVM ] Result : Takes as input data of type Source and outputs a LS-SVM regression/classification model as the output. Model Prediction \u00b6 Prediction pipelines encapsulate predictive models, the ModelPredictionPipe class provides an expressive API for creating prediction pipelines. //Any model val model : Model [ T , Q , R ] = _ //Data pre and post processing val preprocessing : DataPipe [ P , Q ] = _ val postprocessing : DataPipe [ R , S ] = _ val prediction_pipeline = ModelPredictionPipe ( preprocessing , model , postprocessing ) //In case no pre or post processing is done. val prediction_pipeline2 = ModelPredictionPipe ( model ) //Incase feature and target scaling is performed val featureScaling : ReversibleScaler [ Q ] = _ val targetScaling : ReversibleScaler [ R ] = _ val prediction_pipeline3 = ModelPredictionPipe ( featureScaling , model , targetScaling )","title":"Model Pipes"},{"location":"pipes/model_pipes/#model-creation","text":"All pipelines which return predictive models as outputs extend the ModelPipe trait.","title":"Model Creation"},{"location":"pipes/model_pipes/#generalized-linear-model-pipe","text":"//Pre-process data val pre : ( Source ) => Stream [( DenseVector [ Double ] , Double )] = _ val feature_map : ( DenseVector [ Double ]) => ( DenseVector [ Double ]) = _ val glm_pipe = GLMPipe [( DenseMatrix [ Double ] , DenseVector [ Double ]) , Source ]( pre , map , task = \"regression\" , modelType = \"\" ) val dataSource : Source = _ val glm_model = glm_pipe ( dataSource ) Type : DataPipe [ Source , GeneralizedLinearModel [ T ]] Result : Takes as input a data of type Source and outputs a Generalized Linear Model .","title":"Generalized Linear Model Pipe"},{"location":"pipes/model_pipes/#generalized-least-squares-model-pipe","text":"val kernel : LocalScalarKernel [ DenseVector [ Double ]] val gls_pipe2 = GeneralizedLeastSquaresPipe2 ( kernel ) val featuremap : ( DenseVector [ Double ]) => ( DenseVector [ Double ]) = _ val data : Stream [( DenseVector [ Double ] , Double )] = _ val gls_model = gls_pipe2 ( data , featuremap ) Type : DataPipe2 [ Stream [( DenseVector [ Double ] , Double )] , DataPipe [ DenseVector [ Double ] , DenseVector [ Double ]] , GeneralizedLeastSquaresModel ] ] Result : Takes as inputs data and a feature mapping and outputs a Generalized Least Squares Model .","title":"Generalized Least Squares Model Pipe"},{"location":"pipes/model_pipes/#gaussian-process-regression-model-pipe","text":"//Pre-process data val pre : ( Source ) => Stream [( DenseVector [ Double ] , Double )] = _ //Declare kernel and noise val kernel : LocalScalarKernel [ DenseVector [ Double ]] = _ val noise : LocalScalarKernel [ DenseVector [ Double ]] = _ GPRegressionPipe ( pre , kernel , noise , order : Int = 0 , ex : Int = 0 ) Type : DataPipe [ Source , M ] Result : Takes as input data of type Source and outputs a Gaussian Process regression model as the output.","title":"Gaussian Process Regression Model Pipe"},{"location":"pipes/model_pipes/#dual-ls-svm-model-pipe","text":"//Pre-process data val pre : ( Source ) => Stream [( DenseVector [ Double ] , Double )] = _ //Declare kernel val kernel : LocalScalarKernel [ DenseVector [ Double ]] = _ DLSSVMPipe ( pre , kernel , task = \"regression\" ) Type : DataPipe [ Source , DLSSVM ] Result : Takes as input data of type Source and outputs a LS-SVM regression/classification model as the output.","title":"Dual LS-SVM Model Pipe"},{"location":"pipes/model_pipes/#model-prediction","text":"Prediction pipelines encapsulate predictive models, the ModelPredictionPipe class provides an expressive API for creating prediction pipelines. //Any model val model : Model [ T , Q , R ] = _ //Data pre and post processing val preprocessing : DataPipe [ P , Q ] = _ val postprocessing : DataPipe [ R , S ] = _ val prediction_pipeline = ModelPredictionPipe ( preprocessing , model , postprocessing ) //In case no pre or post processing is done. val prediction_pipeline2 = ModelPredictionPipe ( model ) //Incase feature and target scaling is performed val featureScaling : ReversibleScaler [ Q ] = _ val targetScaling : ReversibleScaler [ R ] = _ val prediction_pipeline3 = ModelPredictionPipe ( featureScaling , model , targetScaling )","title":"Model Prediction"},{"location":"pipes/pipes/","text":"Summary In this section we attempt to give a simple yet effective introduction to the data pipes module of DynaML. Motivation \u00b6 Machine Learning involves operations that can be thought of as being part of different stages. Data pre-processing Data munging or pre-processing is one of the most time consuming activities in the analysis and modeling cycle, yet very few libraries do justice to this need. Modeling: train, validation & test : Training and testing models on data is a cyclical process and in the interest of keeping things manageable, it is important to separate operations in stage 1 from stage 2 and 3. Post processing: produce and summarize results via reports, visualizations etc. What are Data Pipes? \u00b6 At their heart data pipes in DynaML are (wrapped) Scala functions. Every machine learning workflow can be thought of as a chain of functional transformations on data. These functional transformations are applied one after another (in fancy language composed ) to yield a result which is then suitable for modeling/training. Creating a Pipe \u00b6 As we mentioned earlier a DynaML pipe is nothing but a thin wrapper around a scala function. Creating a new data pipe is very easy, you just create a scala function and give it to the `#!scala DataPipe() object. val func = ( x : Double ) => math . sin ( 2.0 * x )* math . exp (- 2.0 * x ) val tPipe = DataPipe ( func ) Stacking/Composing Data Pipes \u00b6 val pre_processing = DataPipe (( x : Double ) => math . sin ( 2.0 * x )* math . exp (- 2.0 * x )) val post_processing = DataPipe (( x : Double ) => if ( x <= 0.2 ) \"Y\" else \"N\" ) //Compose the two pipes //The result will be \"Y\" val tPipe = pre_processing > post_processing tPipe ( 15.5 ) Tip It is possible to create a pipe from any scala type to another, inclucing Unit . For example the statement val p = DataPipe (() => scala . Random . nextGaussian ()) creates a pipe which when executed samples from a univariate gaussian distribution val sample = p . run () You can compose or stack any number of pipes using the > character to create a composite data workflow. There is only one constraint when joining two pipes, that the destination type of the first pipe must be the same as the source type of the second pipe, in other words: dont put square pegs into round holes","title":"DynaML Pipes"},{"location":"pipes/pipes/#motivation","text":"Machine Learning involves operations that can be thought of as being part of different stages. Data pre-processing Data munging or pre-processing is one of the most time consuming activities in the analysis and modeling cycle, yet very few libraries do justice to this need. Modeling: train, validation & test : Training and testing models on data is a cyclical process and in the interest of keeping things manageable, it is important to separate operations in stage 1 from stage 2 and 3. Post processing: produce and summarize results via reports, visualizations etc.","title":"Motivation"},{"location":"pipes/pipes/#what-are-data-pipes","text":"At their heart data pipes in DynaML are (wrapped) Scala functions. Every machine learning workflow can be thought of as a chain of functional transformations on data. These functional transformations are applied one after another (in fancy language composed ) to yield a result which is then suitable for modeling/training.","title":"What are Data Pipes?"},{"location":"pipes/pipes/#creating-a-pipe","text":"As we mentioned earlier a DynaML pipe is nothing but a thin wrapper around a scala function. Creating a new data pipe is very easy, you just create a scala function and give it to the `#!scala DataPipe() object. val func = ( x : Double ) => math . sin ( 2.0 * x )* math . exp (- 2.0 * x ) val tPipe = DataPipe ( func )","title":"Creating a Pipe"},{"location":"pipes/pipes/#stackingcomposing-data-pipes","text":"val pre_processing = DataPipe (( x : Double ) => math . sin ( 2.0 * x )* math . exp (- 2.0 * x )) val post_processing = DataPipe (( x : Double ) => if ( x <= 0.2 ) \"Y\" else \"N\" ) //Compose the two pipes //The result will be \"Y\" val tPipe = pre_processing > post_processing tPipe ( 15.5 ) Tip It is possible to create a pipe from any scala type to another, inclucing Unit . For example the statement val p = DataPipe (() => scala . Random . nextGaussian ()) creates a pipe which when executed samples from a univariate gaussian distribution val sample = p . run () You can compose or stack any number of pipes using the > character to create a composite data workflow. There is only one constraint when joining two pipes, that the destination type of the first pipe must be the same as the source type of the second pipe, in other words: dont put square pegs into round holes","title":"Stacking/Composing Data Pipes"},{"location":"pipes/pipes_api/","text":"Base \u00b6 At the top of the pipes hierarchy is the base trait DataPipe[Source, Destination] which is a thin wrapper for a Scala function having the type (Source) => Destination . Along with that the base trait also defines how pipes are composed with each other to yield more complex workflows. Pipes in Parallel \u00b6 The ParallelPipe[Source1, Result1, Source2, Result2] trait models pipes which are attached to each other, from an implementation point of view these can be seen as data pipes taking input from (Source1, Source2) and yielding values from (Result1, Result2) . They can be created in two ways: By supplying two pipes to the DataPipe() object. val pipe1 = DataPipe (( x : Double ) => math . sin ( 2.0 * x )* math . exp (- 2.0 * x )) val pipe2 = DataPipe (( x : Double ) => if ( x <= 0.2 ) \"Y\" else \"N\" ) val pipe3 = DataPipe ( pipe1 , pipe2 ) //Returns (-0.013, \"N\") pipe3 (( 2.0 , 15.0 )) By duplicating a single pipe using DynaMLPipe.duplicate //Already imported in DynaML repl //but should be imported when using DynaML API //outside of its provided repl environment. import io.github.mandar2812.dynaml.DynaMLPipe._ val pipe1 = DataPipe (( x : Double ) => math . sin ( 2.0 * x )* math . exp (- 2.0 * x )) val pipe3 = duplicate ( pipe1 ) //Returns (-0.013, -9E-14) pipe3 (( 2.0 , 15.0 )) Diverging Pipes \u00b6 The BifurcationPipe[Source, Result1, Result2] trait represents pipes which start from the same source and yield two result types, from an implementation point of view these can be seen as data pipes taking input from Source1 and yielding values from (Result1, Result2) . They can be created in two ways: By supplying a function of type (Source) => (Result1, Result2) to the DataPipe() object. val pipe1 = DataPipe (( x : Double ) => ( 1.0 * math . sin ( 2.0 * x )* math . exp (- 2.0 * x ), math . exp (- 2.0 * x ))) pipe1 ( 2.0 ) By using the BifurcationPipe() object val pipe1 = DataPipe (( x : Double ) => math . sin ( 2.0 * x )* math . exp (- 2.0 * x )) val pipe2 = DataPipe (( x : Double ) => if ( x <= 0.2 ) \"Y\" else \"N\" ) val pipe3 = BifurcationPipe ( pipe1 , pipe2 ) pipe3 ( 2.0 ) Side Effects \u00b6 In order to enable pipes which have side effects i.e. writing to disk, the SideEffectPipe[Source] trait is used. Conceptually it is a pipe taking as input a value from Source but has a return type of Unit . Stream Processing \u00b6 To simplify writing pipes for scala streams, the StreamDataPipe[I, J, K] and its subclasses implement workflows on streams. Map \u00b6 Map every element of a stream. val pipe1 = StreamDataPipe (( x : Double ) => math . sin ( 2.0 * x )* math . exp (- 2.0 * x )) val str : Stream [ Double ] = ( 1 to 5 ). map ( _ . toDouble ). toStream pipe1 ( str ) Filter \u00b6 Filter certain elements of a stream. val pipe1 = StreamDataPipe (( x : Double ) => x <= 2.5 ) val str : Stream [ Double ] = ( 1 to 5 ). map ( _ . toDouble ). toStream pipe1 ( str ) Bifurcate stream \u00b6 val pipe1 = StreamPartitionPipe (( x : Double ) => x <= 2.5 ) val str : Stream [ Double ] = ( 1 to 5 ). map ( _ . toDouble ). toStream pipe1 ( str ) Side effect \u00b6 val pipe1 = StreamDataPipe (( x : Double ) => println ( \"Number is: \" + x )) val str : Stream [ Double ] = ( 1 to 5 ). map ( _ . toDouble ). toStream pipe1 ( str ) The following API members were added in v1.4.1 Flat Map \u00b6 StreamFlatMapPipe carries out the scala flat-map operation on a stream. val mapFunc = ( n : Int ) => ( 1 to n ). sliding ( 2 ). toStream val streamFMPipe = StreamFlatMapPipe ( mapFunc ) streamFMPipe (( 1 to 20 ). toStream ) Pipes on Spark RDDs It is also possible to create pipes acting on Spark RDDs. val num = 20 val sc : SparkContext = _ val numbers = sc . parallelize ( 1 to num ) val convPipe = RDDPipe (( n : Int ) => n . toDouble ) val sqPipe = RDDPipe (( x : Double ) => x * x ) val sqrtPipe = RDDPipe (( x : Double ) => math . sqrt ( x )) val resultPipe = RDDPipe (( r : RDD [ Double ]) => r . reduce ( _ + _ ). toInt ) val netPipeline = convPipe > sqPipe > sqrtPipe > resultPipe netPipeline ( numbers ) Advanced Pipes \u00b6 Apart from the basic capabilities offered by the DataPipe [ Source , Destination ] interface and its family, users can also work with more complex workflow components some of which are shown below. The advanced components of the pipes API enable two key extensions. Data pipes which take more than one argument 1 . Data pipes which take an argument and return a data pipe 2 Data Pipe 2 \u00b6 DataPipe2 [ A , B , C ] arguments : 2 of type A and B respectively returns : result of type C val f2 : ( A , B ) => C = _ val pipe2 = DataPipe2 ( f2 ) DataPipe 3 \u00b6 DataPipe3 [ A , B , C , D ] arguments : 3 of type A , B and C respectively returns : result of type D val f3 : ( A , B , C ) => D = _ val pipe3 = DataPipe3 ( f3 ) DataPipe 4 \u00b6 DataPipe4 [ A , B , C , D , E ] arguments : 4 of type A , B , C and D respectively returns : result of type E val f4 : ( A , B , C , D ) => E = _ val pipe4 = DataPipe4 ( f4 ) Meta Pipe \u00b6 MetaPipe [ A , B , C ] Takes an argument returns a DataPipe val mpipe = MetaPipe ( ( omega : DenseVector [ Double ]) => ( x : DenseVector [ Double ]) => math . exp (- omega . t * x )) //Returns a pipe which computes exp(-2pi/10 1Tx) val expPipe = mpipe ( DenseVector . fill [ Double ]( 10 )( 2.0 * math . Pi / 10.0 )) Meta Pipe (2, 1) \u00b6 MetaPipe21 [ A , B , C , D ] Takes 2 arguments returns a DataPipe . val pipe21 = MetaPipe21 ( ( alpha : Double , beta : Double ) => ( rv : ContinuousRandomVariable [ Double ]) => ( rv * beta ) + alpha ) val random_func = pipe21 ( 1.5 , - 0.5 ) val result_rv = random_func ( RandomVariable ( Gamma ( 1.5 , 2.5 ))) //Draw samples from resulting random variable result_rv . iid ( 500 ). draw Meta Pipe (1, 2) \u00b6 MetaPipe12 [ A , B , C , D ] Takes an argument returns a DataPipe2 > and >> operators on higher order pipes Although the > operator is defined on higher order pipes, it quickly becomes difficult to imagine what transformation it can be joined with. For example the > operator applied after a MetaPipe [ I , J , K ] instance would expect a DataPipe [ DataPipe [ J , K ] , _ ] instance in order for the join to proceed. The >> operator on the other hand has a different and easier purpose. val mpipe = MetaPipe ( ( omega : DenseVector [ Double ]) => ( x : DenseVector [ Double ]) => math . exp (- omega . t * x )) val further_pipe = DataPipe (( y : Double ) => y * y + 2.0 ) //Returns MetaPipe[DenseVector[Double], DenseVector[Double], Double] //Computes exp(2*omega.x) + 2.0 val final_pipe = mpipe >> further_pipe so far DynaML has support till 4 \u21a9 similar to curried functions in scala. \u21a9","title":"Pipes API"},{"location":"pipes/pipes_api/#base","text":"At the top of the pipes hierarchy is the base trait DataPipe[Source, Destination] which is a thin wrapper for a Scala function having the type (Source) => Destination . Along with that the base trait also defines how pipes are composed with each other to yield more complex workflows.","title":"Base"},{"location":"pipes/pipes_api/#pipes-in-parallel","text":"The ParallelPipe[Source1, Result1, Source2, Result2] trait models pipes which are attached to each other, from an implementation point of view these can be seen as data pipes taking input from (Source1, Source2) and yielding values from (Result1, Result2) . They can be created in two ways: By supplying two pipes to the DataPipe() object. val pipe1 = DataPipe (( x : Double ) => math . sin ( 2.0 * x )* math . exp (- 2.0 * x )) val pipe2 = DataPipe (( x : Double ) => if ( x <= 0.2 ) \"Y\" else \"N\" ) val pipe3 = DataPipe ( pipe1 , pipe2 ) //Returns (-0.013, \"N\") pipe3 (( 2.0 , 15.0 )) By duplicating a single pipe using DynaMLPipe.duplicate //Already imported in DynaML repl //but should be imported when using DynaML API //outside of its provided repl environment. import io.github.mandar2812.dynaml.DynaMLPipe._ val pipe1 = DataPipe (( x : Double ) => math . sin ( 2.0 * x )* math . exp (- 2.0 * x )) val pipe3 = duplicate ( pipe1 ) //Returns (-0.013, -9E-14) pipe3 (( 2.0 , 15.0 ))","title":"Pipes in Parallel"},{"location":"pipes/pipes_api/#diverging-pipes","text":"The BifurcationPipe[Source, Result1, Result2] trait represents pipes which start from the same source and yield two result types, from an implementation point of view these can be seen as data pipes taking input from Source1 and yielding values from (Result1, Result2) . They can be created in two ways: By supplying a function of type (Source) => (Result1, Result2) to the DataPipe() object. val pipe1 = DataPipe (( x : Double ) => ( 1.0 * math . sin ( 2.0 * x )* math . exp (- 2.0 * x ), math . exp (- 2.0 * x ))) pipe1 ( 2.0 ) By using the BifurcationPipe() object val pipe1 = DataPipe (( x : Double ) => math . sin ( 2.0 * x )* math . exp (- 2.0 * x )) val pipe2 = DataPipe (( x : Double ) => if ( x <= 0.2 ) \"Y\" else \"N\" ) val pipe3 = BifurcationPipe ( pipe1 , pipe2 ) pipe3 ( 2.0 )","title":"Diverging Pipes"},{"location":"pipes/pipes_api/#side-effects","text":"In order to enable pipes which have side effects i.e. writing to disk, the SideEffectPipe[Source] trait is used. Conceptually it is a pipe taking as input a value from Source but has a return type of Unit .","title":"Side Effects"},{"location":"pipes/pipes_api/#stream-processing","text":"To simplify writing pipes for scala streams, the StreamDataPipe[I, J, K] and its subclasses implement workflows on streams.","title":"Stream Processing"},{"location":"pipes/pipes_api/#map","text":"Map every element of a stream. val pipe1 = StreamDataPipe (( x : Double ) => math . sin ( 2.0 * x )* math . exp (- 2.0 * x )) val str : Stream [ Double ] = ( 1 to 5 ). map ( _ . toDouble ). toStream pipe1 ( str )","title":"Map"},{"location":"pipes/pipes_api/#filter","text":"Filter certain elements of a stream. val pipe1 = StreamDataPipe (( x : Double ) => x <= 2.5 ) val str : Stream [ Double ] = ( 1 to 5 ). map ( _ . toDouble ). toStream pipe1 ( str )","title":"Filter"},{"location":"pipes/pipes_api/#bifurcate-stream","text":"val pipe1 = StreamPartitionPipe (( x : Double ) => x <= 2.5 ) val str : Stream [ Double ] = ( 1 to 5 ). map ( _ . toDouble ). toStream pipe1 ( str )","title":"Bifurcate stream"},{"location":"pipes/pipes_api/#side-effect","text":"val pipe1 = StreamDataPipe (( x : Double ) => println ( \"Number is: \" + x )) val str : Stream [ Double ] = ( 1 to 5 ). map ( _ . toDouble ). toStream pipe1 ( str ) The following API members were added in v1.4.1","title":"Side effect"},{"location":"pipes/pipes_api/#flat-map","text":"StreamFlatMapPipe carries out the scala flat-map operation on a stream. val mapFunc = ( n : Int ) => ( 1 to n ). sliding ( 2 ). toStream val streamFMPipe = StreamFlatMapPipe ( mapFunc ) streamFMPipe (( 1 to 20 ). toStream ) Pipes on Spark RDDs It is also possible to create pipes acting on Spark RDDs. val num = 20 val sc : SparkContext = _ val numbers = sc . parallelize ( 1 to num ) val convPipe = RDDPipe (( n : Int ) => n . toDouble ) val sqPipe = RDDPipe (( x : Double ) => x * x ) val sqrtPipe = RDDPipe (( x : Double ) => math . sqrt ( x )) val resultPipe = RDDPipe (( r : RDD [ Double ]) => r . reduce ( _ + _ ). toInt ) val netPipeline = convPipe > sqPipe > sqrtPipe > resultPipe netPipeline ( numbers )","title":"Flat Map"},{"location":"pipes/pipes_api/#advanced-pipes","text":"Apart from the basic capabilities offered by the DataPipe [ Source , Destination ] interface and its family, users can also work with more complex workflow components some of which are shown below. The advanced components of the pipes API enable two key extensions. Data pipes which take more than one argument 1 . Data pipes which take an argument and return a data pipe 2","title":"Advanced Pipes"},{"location":"pipes/pipes_api/#data-pipe-2","text":"DataPipe2 [ A , B , C ] arguments : 2 of type A and B respectively returns : result of type C val f2 : ( A , B ) => C = _ val pipe2 = DataPipe2 ( f2 )","title":"Data Pipe 2"},{"location":"pipes/pipes_api/#datapipe-3","text":"DataPipe3 [ A , B , C , D ] arguments : 3 of type A , B and C respectively returns : result of type D val f3 : ( A , B , C ) => D = _ val pipe3 = DataPipe3 ( f3 )","title":"DataPipe 3"},{"location":"pipes/pipes_api/#datapipe-4","text":"DataPipe4 [ A , B , C , D , E ] arguments : 4 of type A , B , C and D respectively returns : result of type E val f4 : ( A , B , C , D ) => E = _ val pipe4 = DataPipe4 ( f4 )","title":"DataPipe 4"},{"location":"pipes/pipes_api/#meta-pipe","text":"MetaPipe [ A , B , C ] Takes an argument returns a DataPipe val mpipe = MetaPipe ( ( omega : DenseVector [ Double ]) => ( x : DenseVector [ Double ]) => math . exp (- omega . t * x )) //Returns a pipe which computes exp(-2pi/10 1Tx) val expPipe = mpipe ( DenseVector . fill [ Double ]( 10 )( 2.0 * math . Pi / 10.0 ))","title":"Meta Pipe"},{"location":"pipes/pipes_api/#meta-pipe-2-1","text":"MetaPipe21 [ A , B , C , D ] Takes 2 arguments returns a DataPipe . val pipe21 = MetaPipe21 ( ( alpha : Double , beta : Double ) => ( rv : ContinuousRandomVariable [ Double ]) => ( rv * beta ) + alpha ) val random_func = pipe21 ( 1.5 , - 0.5 ) val result_rv = random_func ( RandomVariable ( Gamma ( 1.5 , 2.5 ))) //Draw samples from resulting random variable result_rv . iid ( 500 ). draw","title":"Meta Pipe (2, 1)"},{"location":"pipes/pipes_api/#meta-pipe-1-2","text":"MetaPipe12 [ A , B , C , D ] Takes an argument returns a DataPipe2 > and >> operators on higher order pipes Although the > operator is defined on higher order pipes, it quickly becomes difficult to imagine what transformation it can be joined with. For example the > operator applied after a MetaPipe [ I , J , K ] instance would expect a DataPipe [ DataPipe [ J , K ] , _ ] instance in order for the join to proceed. The >> operator on the other hand has a different and easier purpose. val mpipe = MetaPipe ( ( omega : DenseVector [ Double ]) => ( x : DenseVector [ Double ]) => math . exp (- omega . t * x )) val further_pipe = DataPipe (( y : Double ) => y * y + 2.0 ) //Returns MetaPipe[DenseVector[Double], DenseVector[Double], Double] //Computes exp(2*omega.x) + 2.0 val final_pipe = mpipe >> further_pipe so far DynaML has support till 4 \u21a9 similar to curried functions in scala. \u21a9","title":"Meta Pipe (1, 2)"},{"location":"pipes/pipes_library/","text":"DynaML Library Pipes \u00b6 DynaML comes bundled with a set of data pipes which enable certain standard data processing tasks, they are defined in the DynaMLPipe object in the io.github.mandar2812.dynaml.pipes package and they can be invoked as DynaMLPipe.<pipe name> . Example \u00b6 As a simple motivating example consider the following hypothetical csv data file called sample.csv . a b c NA e f r s q t l m u v w x z d Lets say one wants to extract only the first, fourth and last columns of this file for further processing, also one is only interested in records which do not have missing values in any of the columns we want to extract. One can think of a data pipe as follows. Replace the erratic white space separators with a consistent separator character Extract a subset of the columns Remove the records with missing values NA Write output to another file processedsample.csv with the comma character as separator We can do this by 'composing' data flow pipes which achieve each of the sub tasks. //Import the workflow library. import io.github.mandar2812.dynaml.DynaMLPipe._ val columns = List ( 0 , 3 , 5 ) val dataPipe = fileToStream > replaceWhiteSpaces > extractTrainingFeatures ( columns , Map ( 0 -> \"NA\" , 3 -> \"NA\" , 5 -> \"NA\" ) ) > removeMissingLines > streamToFile ( \"processed_sample.csv\" ) val result = dataPipe ( \"sample.csv\" ) Lets go over the code snippet piece by piece. First convert the text file to a Stream using fileToStream Replace white spaces in each line by using replaceWhiteSpaces Extract the required columns by extractTrainingFeatures , be sure to supply it the column numbers (indexed from 0) and the missing value strings for each column to be extracted. Remove missing records removeMissingLines Write the resulting data stream to a file streamToFile(\"processed_sample.csv\")","title":"Pipes Example"},{"location":"pipes/pipes_library/#dynaml-library-pipes","text":"DynaML comes bundled with a set of data pipes which enable certain standard data processing tasks, they are defined in the DynaMLPipe object in the io.github.mandar2812.dynaml.pipes package and they can be invoked as DynaMLPipe.<pipe name> .","title":"DynaML Library Pipes"},{"location":"pipes/pipes_library/#example","text":"As a simple motivating example consider the following hypothetical csv data file called sample.csv . a b c NA e f r s q t l m u v w x z d Lets say one wants to extract only the first, fourth and last columns of this file for further processing, also one is only interested in records which do not have missing values in any of the columns we want to extract. One can think of a data pipe as follows. Replace the erratic white space separators with a consistent separator character Extract a subset of the columns Remove the records with missing values NA Write output to another file processedsample.csv with the comma character as separator We can do this by 'composing' data flow pipes which achieve each of the sub tasks. //Import the workflow library. import io.github.mandar2812.dynaml.DynaMLPipe._ val columns = List ( 0 , 3 , 5 ) val dataPipe = fileToStream > replaceWhiteSpaces > extractTrainingFeatures ( columns , Map ( 0 -> \"NA\" , 3 -> \"NA\" , 5 -> \"NA\" ) ) > removeMissingLines > streamToFile ( \"processed_sample.csv\" ) val result = dataPipe ( \"sample.csv\" ) Lets go over the code snippet piece by piece. First convert the text file to a Stream using fileToStream Replace white spaces in each line by using replaceWhiteSpaces Extract the required columns by extractTrainingFeatures , be sure to supply it the column numbers (indexed from 0) and the missing value strings for each column to be extracted. Remove missing records removeMissingLines Write the resulting data stream to a file streamToFile(\"processed_sample.csv\")","title":"Example"},{"location":"pipes/pipes_misc/","text":"General \u00b6 Duplicate a pipe \u00b6 duplicate [ S , D ]( pipe : DataPipe [ S , D ]) Type : DataPipe[(S, S), (D, D)] Result : Takes a base pipe and creates a parallel pipe by duplicating it.","title":"Miscellaneuos"},{"location":"pipes/pipes_misc/#general","text":"","title":"General"},{"location":"pipes/pipes_misc/#duplicate-a-pipe","text":"duplicate [ S , D ]( pipe : DataPipe [ S , D ]) Type : DataPipe[(S, S), (D, D)] Result : Takes a base pipe and creates a parallel pipe by duplicating it.","title":"Duplicate a pipe"},{"location":"pipes/pipes_models/","text":"Operations on Models \u00b6 Train a parametric model \u00b6 trainParametricModel [ G , T , Q , R , S , M <: ParameterizedLearner [ G , T , Q , R , S ]]( regParameter : Double , step : Double , maxIt : Int , mini : Double ) Type : DataPipe[M, M] Result : Takes as input a parametric model i.e. a subclass of ParameterizedLearner[G, T, Q, R, S] , trains it and outputs the trained model. Tune a model using global optimization \u00b6 modelTuning [ M <: GloballyOptWithGrad ]( startingState : Map [ String , Double ], globalOpt : String , grid : Int , step : Double ) Type : DataPipe[(S, S), (D, D)] Result : Takes as input a parametric model i.e. a subclass of GloballyOptimizableWithGrad , tunes it using a global optimization procedure globalOpt and outputs the tuned model.","title":"Workflows on Models"},{"location":"pipes/pipes_models/#operations-on-models","text":"","title":"Operations on Models"},{"location":"pipes/pipes_models/#train-a-parametric-model","text":"trainParametricModel [ G , T , Q , R , S , M <: ParameterizedLearner [ G , T , Q , R , S ]]( regParameter : Double , step : Double , maxIt : Int , mini : Double ) Type : DataPipe[M, M] Result : Takes as input a parametric model i.e. a subclass of ParameterizedLearner[G, T, Q, R, S] , trains it and outputs the trained model.","title":"Train a parametric model"},{"location":"pipes/pipes_models/#tune-a-model-using-global-optimization","text":"modelTuning [ M <: GloballyOptWithGrad ]( startingState : Map [ String , Double ], globalOpt : String , grid : Int , step : Double ) Type : DataPipe[(S, S), (D, D)] Result : Takes as input a parametric model i.e. a subclass of GloballyOptimizableWithGrad , tunes it using a global optimization procedure globalOpt and outputs the tuned model.","title":"Tune a model using global optimization"},{"location":"pipes/pipes_scalers_encoders/","text":"Summary The pipes API provides a good foundation to construct data processing pipelines, in this section we show how it is extended for application to a specific application i.e. attribute scaling & transformation. Transforming data attributes is an often repeated task, some examples include re-scaling values in a finite domain [min, max] [min, max] , gaussian centering, principal component analysis (PCA), discreet Haar wavelet (DWT) transform etc. The pipes API contains traits for these tasks, they are abstract skeletons which can be extended by the user to create arbitrary feature re-scaling transformations. Encoders \u00b6 Encoder [ I , J ] are an extension of DataPipe [ I , J ] class which has an extra value member i : DataPipe [ J , I ] which represents the inverse transformation. Note Encoder [ I , J ] implies a reversible, one to one transformation of the input. Mathematically this can be expressed as \\begin{align} g: \\mathcal{X} &\\rightarrow \\mathcal{Y} \\\\ h: \\mathcal{Y} &\\rightarrow \\mathcal{X} \\\\ h(g(x)) &= x \\ \\ \\ \\forall x \\in \\mathcal{X} \\\\ h &\\equiv g^{-1} \\end{align} \\begin{align} g: \\mathcal{X} &\\rightarrow \\mathcal{Y} \\\\ h: \\mathcal{Y} &\\rightarrow \\mathcal{X} \\\\ h(g(x)) &= x \\ \\ \\ \\forall x \\in \\mathcal{X} \\\\ h &\\equiv g^{-1} \\end{align} An encoder can be created by calling the apply method of the Encoder object. //Converts a point expressed in cartesian coordinates //into a point expressed in polar coordinates and vice versa. val cartesianToPolar = Encoder ( ( pointCart : ( Double , Double )) => { val ( x , y ) = pointCart val r = math . sqrt ( x * x + y * y ) if ( r != 0.0 ) ( r , math . arcsin ( y / r )) else ( 0.0 , 0.0 ) }), ( pointPolar : ( Double , Double )) => { val ( r , theta ) = pointPolar ( r * math . cos ( theta ), r * math . sin ( theta )) } ) Note In the above example, we created a cartesian to polar coordinate converter by specifying the forward and reverse transformations as anonymous scala functions. But we could as well have passed the forward and reverse transforms as DataPipe instances. val forwardTransform : DataPipe [ I , J ] = _ val reverseTransform : DataPipe [ J , I ] = _ //Still works. val enc = Encoder ( forwardTransform , reverseTransform ) Scalers \u00b6 Scaler [ I ] is an extension of the DataPipe [ I , I ] trait. Represents transformations of inputs which don't change their type. val linTr = Scaler (( x : Double ) => x * 5.0 + - 1.5 ) Reversible Scalers \u00b6 ReversibleScaler [ I ] extends Scaler [ I ] along with Encoder [ I , J ] , a reversible re-scaling of inputs. The > and * for scalers and encoders Since Encoder [ S , D ] , Scaler [ S ] and ReversibleScaler [ S , D ] are inherit the DataPipe trait, they can be composed with any data pipeline as usual, but there are special cases. If an Encoder [ I , J ] instance is composed with Encoder [ J , K ] , the result is of type Encoder [ I , K ] and accordingly for Scaler [ I ] and ReversibleScaler [ I ] . The * can be used to create cartesian products of encoders and scalers. val enc1 : Encoder [ I , J ] = _ val enc2 : Encoder [ K , L ] = _ val enc3 : Encoder [( I , K ) , ( J , L )] = enc1 * enc2 Tip Common attribute transformations like gaussian centering, min-max scaling, etc are included in the dynaml . utils package, click here to see their syntax.","title":"Scalers & Encoders"},{"location":"pipes/pipes_scalers_encoders/#encoders","text":"Encoder [ I , J ] are an extension of DataPipe [ I , J ] class which has an extra value member i : DataPipe [ J , I ] which represents the inverse transformation. Note Encoder [ I , J ] implies a reversible, one to one transformation of the input. Mathematically this can be expressed as \\begin{align} g: \\mathcal{X} &\\rightarrow \\mathcal{Y} \\\\ h: \\mathcal{Y} &\\rightarrow \\mathcal{X} \\\\ h(g(x)) &= x \\ \\ \\ \\forall x \\in \\mathcal{X} \\\\ h &\\equiv g^{-1} \\end{align} \\begin{align} g: \\mathcal{X} &\\rightarrow \\mathcal{Y} \\\\ h: \\mathcal{Y} &\\rightarrow \\mathcal{X} \\\\ h(g(x)) &= x \\ \\ \\ \\forall x \\in \\mathcal{X} \\\\ h &\\equiv g^{-1} \\end{align} An encoder can be created by calling the apply method of the Encoder object. //Converts a point expressed in cartesian coordinates //into a point expressed in polar coordinates and vice versa. val cartesianToPolar = Encoder ( ( pointCart : ( Double , Double )) => { val ( x , y ) = pointCart val r = math . sqrt ( x * x + y * y ) if ( r != 0.0 ) ( r , math . arcsin ( y / r )) else ( 0.0 , 0.0 ) }), ( pointPolar : ( Double , Double )) => { val ( r , theta ) = pointPolar ( r * math . cos ( theta ), r * math . sin ( theta )) } ) Note In the above example, we created a cartesian to polar coordinate converter by specifying the forward and reverse transformations as anonymous scala functions. But we could as well have passed the forward and reverse transforms as DataPipe instances. val forwardTransform : DataPipe [ I , J ] = _ val reverseTransform : DataPipe [ J , I ] = _ //Still works. val enc = Encoder ( forwardTransform , reverseTransform )","title":"Encoders"},{"location":"pipes/pipes_scalers_encoders/#scalers","text":"Scaler [ I ] is an extension of the DataPipe [ I , I ] trait. Represents transformations of inputs which don't change their type. val linTr = Scaler (( x : Double ) => x * 5.0 + - 1.5 )","title":"Scalers"},{"location":"pipes/pipes_scalers_encoders/#reversible-scalers","text":"ReversibleScaler [ I ] extends Scaler [ I ] along with Encoder [ I , J ] , a reversible re-scaling of inputs. The > and * for scalers and encoders Since Encoder [ S , D ] , Scaler [ S ] and ReversibleScaler [ S , D ] are inherit the DataPipe trait, they can be composed with any data pipeline as usual, but there are special cases. If an Encoder [ I , J ] instance is composed with Encoder [ J , K ] , the result is of type Encoder [ I , K ] and accordingly for Scaler [ I ] and ReversibleScaler [ I ] . The * can be used to create cartesian products of encoders and scalers. val enc1 : Encoder [ I , J ] = _ val enc2 : Encoder [ K , L ] = _ val enc3 : Encoder [( I , K ) , ( J , L )] = enc1 * enc2 Tip Common attribute transformations like gaussian centering, min-max scaling, etc are included in the dynaml . utils package, click here to see their syntax.","title":"Reversible Scalers"},{"location":"pipes/pipes_time_series/","text":"Extract Data as Univariate Time Series \u00b6 extractTimeSeries ( Tfunc ) Type : DataPipe [ Stream [ String ] , Stream [( Double , Double )]] Result : This pipe assumes its input to be of the form YYYY,Day,Hour,Value . It takes as input a function (TFunc) which converts a ( Double , Double , Double ) into a single timestamp like value. The pipe processes its data source line by line and outputs a Tuple2 in the following format (Timestamp,Value) . Extract data as Multivariate Time Series \u00b6 extractTimeSeriesVec ( Tfunc ) Type : DataPipe [ Stream [ String ] , Stream [( Double , DenseVector [ Double ])]] Result : This pipe is similar to extractTimeSeries but for application in multivariate time series analysis such as nonlinear autoregressive models with exogenous inputs. The pipe processes its data source line by line and outputs a ( Double , DenseVector [ Double ]) in the following format (Timestamp,Values) . Construct Time differenced Data \u00b6 deltaOperation ( deltaT , timelag ) Type : DataPipe [ Stream [( Double , Double )] , Stream [( DenseVector [ Double ] , Double )]] Result : In order to generate features for auto-regressive models, one needs to construct sliding windows in time. This function takes two parameters deltaT : the auto-regressive order and timelag : the time lag after which the windowing is conducted. E.g Let deltaT = 2 and timelag = 1 This pipe will take stream data of the form (t, y(t)) (t, y(t)) and output a stream which looks like (t, [y(t-2), y(t-3)]) (t, [y(t-2), y(t-3)]) Construct multivariate Time differenced Data \u00b6 deltaOperationVec ( deltaT : Int ) Type : DataPipe [ Stream [( Double , Double )] , Stream [( DenseVector [ Double ] , Double )]] Result : A variant of deltaOperation for NARX models. Haar Discrete Wavelet Transform \u00b6 haarWaveletFilter ( order : Int ) Type : DataPipe [ DenseVector [ Double ] , DenseVector [ Double ]] Result : A Haar Discrete wavelet transform.","title":"Time Series"},{"location":"pipes/pipes_time_series/#extract-data-as-univariate-time-series","text":"extractTimeSeries ( Tfunc ) Type : DataPipe [ Stream [ String ] , Stream [( Double , Double )]] Result : This pipe assumes its input to be of the form YYYY,Day,Hour,Value . It takes as input a function (TFunc) which converts a ( Double , Double , Double ) into a single timestamp like value. The pipe processes its data source line by line and outputs a Tuple2 in the following format (Timestamp,Value) .","title":"Extract Data as Univariate Time Series"},{"location":"pipes/pipes_time_series/#extract-data-as-multivariate-time-series","text":"extractTimeSeriesVec ( Tfunc ) Type : DataPipe [ Stream [ String ] , Stream [( Double , DenseVector [ Double ])]] Result : This pipe is similar to extractTimeSeries but for application in multivariate time series analysis such as nonlinear autoregressive models with exogenous inputs. The pipe processes its data source line by line and outputs a ( Double , DenseVector [ Double ]) in the following format (Timestamp,Values) .","title":"Extract data as Multivariate Time Series"},{"location":"pipes/pipes_time_series/#construct-time-differenced-data","text":"deltaOperation ( deltaT , timelag ) Type : DataPipe [ Stream [( Double , Double )] , Stream [( DenseVector [ Double ] , Double )]] Result : In order to generate features for auto-regressive models, one needs to construct sliding windows in time. This function takes two parameters deltaT : the auto-regressive order and timelag : the time lag after which the windowing is conducted. E.g Let deltaT = 2 and timelag = 1 This pipe will take stream data of the form (t, y(t)) (t, y(t)) and output a stream which looks like (t, [y(t-2), y(t-3)]) (t, [y(t-2), y(t-3)])","title":"Construct Time differenced Data"},{"location":"pipes/pipes_time_series/#construct-multivariate-time-differenced-data","text":"deltaOperationVec ( deltaT : Int ) Type : DataPipe [ Stream [( Double , Double )] , Stream [( DenseVector [ Double ] , Double )]] Result : A variant of deltaOperation for NARX models.","title":"Construct multivariate Time differenced Data"},{"location":"pipes/pipes_time_series/#haar-discrete-wavelet-transform","text":"haarWaveletFilter ( order : Int ) Type : DataPipe [ DenseVector [ Double ] , DenseVector [ Double ]] Result : A Haar Discrete wavelet transform.","title":"Haar Discrete Wavelet Transform"},{"location":"releases/mydoc_release_notes_14/","text":"Version 1.4 of DynaML, released Sept 23, 2016, implements a number of new models (multi-output GP, student T process, random variables, etc) and features (Variance control for CSA, etc). Models \u00b6 The following inference models have been added. Meta Models & Ensembles \u00b6 LSSVM committees. Stochastic Processes \u00b6 Multi-output, multi-task Gaussian Process models as reviewed in Lawrence et. al . Student T Processes : single and multi output inspired from Shah, Ghahramani et. al Performance improvement to computation of marginal likelihood and posterior predictive distribution in Gaussian Process models. Posterior predictive distribution outputted by the AbstractGPRegression base class is now changed to MultGaussianRV which is added to the dynaml . probability package. Kernels \u00b6 Added StationaryKernel and LocallyStationaryKernel classes in the kernel APIs, converted RBFKernel , CauchyKernel , RationalQuadraticKernel & LaplacianKernel to subclasses of StationaryKernel Added MLPKernel which implements the maximum likelihood perceptron kernel as shown here . Added co-regionalization kernels which are used in Lawrence et. al to formulate kernels for vector valued functions. In this category the following co-regionalization kernels were implemented. CoRegRBFKernel CoRegCauchyKernel CoRegLaplaceKernel CoRegDiracKernel Improved performance when calculating kernel matrices for composite kernels. Added :* operator to kernels so that one can create separable kernels used in co-regionalization models . Optimization \u00b6 Improved performance of CoupledSimulatedAnnealing , enabled use of 4 variants of Coupled Simulated Annealing , adding the ability to set annealing schedule using so called variance control scheme as outlined in de-Souza, Suykens et. al . Pipes \u00b6 Added Scaler and ReversibleScaler traits to represent transformations which input and output into the same domain set, these traits are extensions of DataPipe . Added Discrete Wavelet Transform based on the Haar wavelet.","title":"v1.4"},{"location":"releases/mydoc_release_notes_14/#models","text":"The following inference models have been added.","title":"Models"},{"location":"releases/mydoc_release_notes_14/#meta-models-ensembles","text":"LSSVM committees.","title":"Meta Models &amp; Ensembles"},{"location":"releases/mydoc_release_notes_14/#stochastic-processes","text":"Multi-output, multi-task Gaussian Process models as reviewed in Lawrence et. al . Student T Processes : single and multi output inspired from Shah, Ghahramani et. al Performance improvement to computation of marginal likelihood and posterior predictive distribution in Gaussian Process models. Posterior predictive distribution outputted by the AbstractGPRegression base class is now changed to MultGaussianRV which is added to the dynaml . probability package.","title":"Stochastic Processes"},{"location":"releases/mydoc_release_notes_14/#kernels","text":"Added StationaryKernel and LocallyStationaryKernel classes in the kernel APIs, converted RBFKernel , CauchyKernel , RationalQuadraticKernel & LaplacianKernel to subclasses of StationaryKernel Added MLPKernel which implements the maximum likelihood perceptron kernel as shown here . Added co-regionalization kernels which are used in Lawrence et. al to formulate kernels for vector valued functions. In this category the following co-regionalization kernels were implemented. CoRegRBFKernel CoRegCauchyKernel CoRegLaplaceKernel CoRegDiracKernel Improved performance when calculating kernel matrices for composite kernels. Added :* operator to kernels so that one can create separable kernels used in co-regionalization models .","title":"Kernels"},{"location":"releases/mydoc_release_notes_14/#optimization","text":"Improved performance of CoupledSimulatedAnnealing , enabled use of 4 variants of Coupled Simulated Annealing , adding the ability to set annealing schedule using so called variance control scheme as outlined in de-Souza, Suykens et. al .","title":"Optimization"},{"location":"releases/mydoc_release_notes_14/#pipes","text":"Added Scaler and ReversibleScaler traits to represent transformations which input and output into the same domain set, these traits are extensions of DataPipe . Added Discrete Wavelet Transform based on the Haar wavelet.","title":"Pipes"},{"location":"releases/mydoc_release_notes_141/","text":"Version 1.4.1 of DynaML, released March 26, 2017, implements a number of new models (Extended Skew GP, student T process, generalized least squares, etc) and features. Pipes API \u00b6 Additions \u00b6 The pipes API has been vastly extended by creating pipes which encapsulate functions of multiple arguments leading to the following end points. DataPipe2 [ A , B , C ] : Pipe which takes 2 arguments DataPipe3 [ A , B , C , D ] : Pipe which takes 3 arguments DataPipe4 [ A , B , C , D , E ] : Pipe which takes 4 arguments Furthermore there is now the ability to create pipes which return pipes, something akin to curried functions in functional programming. MetaPipe : Takes an argument returns a DataPipe MetaPipe21 : Takes 2 arguments returns a DataPipe MetaPipe12 : Takes an argument returns a DataPipe2 A new kind of Stream data pipe, StreamFlatMapPipe is added to represent data pipelines which can perform flat map like operations on streams. val mapFunc : ( I ) => Stream [ J ] = ... val streamFMPipe = StreamFlatMapPipe ( mapFunc ) Added Data Pipes API for Apache Spark RDDs. val num = 20 val numbers = sc . parallelize ( 1 to num ) val convPipe = RDDPipe (( n : Int ) => n . toDouble ) val sqPipe = RDDPipe (( x : Double ) => x * x ) val sqrtPipe = RDDPipe (( x : Double ) => math . sqrt ( x )) val resultPipe = RDDPipe (( r : RDD [ Double ]) => r . reduce ( _ + _ ). toInt ) val netPipeline = convPipe > sqPipe > sqrtPipe > resultPipe netPipeline ( numbers ) Added UnivariateGaussianScaler class for gaussian scaling of univariate data. Core API \u00b6 Additions \u00b6 Package dynaml . models . bayes This new package will house stochastic prior models, currently there is support for GP and Skew GP priors, to see a starting example see stochasticPriors.sc in the scripts directory of the DynaML source. Package dynaml . kernels Added evaluateAt ( h )( x , y ) and gradientAt ( h )( x , y ) ; expressing evaluate ( x , y ) and gradient ( x , y ) in terms of them Added asPipe method for Covariance Functions For backwards compatibility users are advised to extend LocalSVMKernel in their custom Kernel implementations incase they do not want to implement the evaluateAt API endpoints. Added FeatureMapKernel , representing kernels which can be explicitly decomposed into feature mappings. Added Matern half integer kernel GenericMaternKernel [ I ] Added block ( S : String* ) method to block any hyper-parameters of kernels. Added NeuralNetworkKernel and GaussianSpectralKernel . Added DecomposableCovariance import io.github.mandar2812.dynaml.DynaMLPipe._ import io.github.mandar2812.dynaml.kernels._ implicit val ev = VectorField ( 6 ) implicit val sp = breezeDVSplitEncoder ( 2 ) implicit val sumR = sumReducer val kernel = new LaplacianKernel ( 1.5 ) val other_kernel = new PolynomialKernel ( 1 , 0.05 ) val decompKernel = new DecomposableCovariance ( kernel , other_kernel )( sp , sumReducer ) val other_kernel1 = new FBMKernel ( 1.0 ) val decompKernel1 = new DecomposableCovariance ( decompKernel , other_kernel1 )( sp , sumReducer ) val veca = DenseVector . tabulate [ Double ]( 8 )( math . sin ( _ )) val vecb = DenseVector . tabulate [ Double ]( 8 )( math . cos ( _ )) decompKernel1 . evaluate ( veca , vecb ) Package dynaml . algebra Partitioned Matrices/Vectors and the following operations Addition, Subtraction Matrix, vector multiplication LU, Cholesky A \\ y , A \\ Y Added calculation of quadratic forms, namely: quadraticForm which calculates \\mathbf{x}^\\intercal A^{-1} \\mathbf{x} \\mathbf{x}^\\intercal A^{-1} \\mathbf{x} crossQuadraticForm which calculates \\mathbf{y}^\\intercal A^{-1} \\mathbf{x} \\mathbf{y}^\\intercal A^{-1} \\mathbf{x} Where A is assumed to be a symmetric positive semi-definite matrix Usage: import io.github.mandar2812.dynaml.algebra._ val x : DenseVector [ Double ] = ... val y : DenseVector [ Double ] = ... val a : DenseMatrix [ Double ] = ... quadraticForm ( a , x ) crossQuadraticForm ( y , a , x ) Package dynaml . modelpipe New package created, moved all inheriting classes of ModelPipe to this package. Added the following: GLMPipe2 A pipe taking two arguments and returning a GeneralizedLinearModel instance GeneralizedLeastSquaresPipe2 : GeneralizedLeastSquaresPipe3 : Package dynaml . models Added a new Neural Networks API: NeuralNet and GenericFFNeuralNet , for an example refer to TestNNDelve in dynaml - examples . GeneralizedLeastSquaresModel : The GLS model. ESGPModel : The implementation of a skew gaussian process regression model Warped Gaussian Process models WIP Added mean function capability to Gaussian Process and Student T process models. Added Apache Spark implementation of Generalized Linear Models; see SparkGLM , SparkLogisticModel , SparkProbitGLM Package dynaml.probability MultivariateSkewNormal as specified in Azzalani et. al ExtendedMultivariateSkewNormal UESN and MESN representing an alternative formulation of the skew gaussian family from Adcock and Shutes. TruncatedGaussian : Truncated version of the Gaussian distribution. Matrix Normal Distribution Added Expectation operator for RandomVariable implementations in the io . github . mandar2812 . dynaml . probability package object. Usage example given below. SkewGaussian , ExtendedSkewGaussian : An breeze implementation of the SkewGaussian and extended Skew-Gaussian distributions respectively PushforwardMap , DifferentiableMap added: PushforwardMap enables creating new random variables with defined density from base random variables. import io.github.mandar2812.dynaml.analysis._ import io.github.mandar2812.dynaml.probability._ import io.github.mandar2812.dynaml.probability.distributions._ val g = GaussianRV ( 0.0 , 0.25 ) val sg = RandomVariable ( SkewGaussian ( 1.0 , 0.0 , 0.25 )) //Define a determinant implementation for the Jacobian type (Double in this case) implicit val detImpl = identityPipe [ Double ] //Defines a homeomorphism y = exp(x) x = log(y) val h : PushforwardMap [ Double , Double , Double ] = PushforwardMap ( DataPipe (( x : Double ) => math . exp ( x )), DifferentiableMap ( ( x : Double ) => math . log ( x ), ( x : Double ) => 1.0 / x ) ) //Creates a log-normal random variable val p = h -> g //Creates a log-skew-gaussian random variable val q = h -> sg //Calculate expectation of q println ( \"E[Q] = \" + E ( q )) - Added Markov Chain Monte Carlo (MCMC) based inference schemes ContinuousMCMC and the underlying sampling implementation in GeneralMetropolisHastings . - Added implementation of Approximate Bayesian Computation (ABC) in the ApproxBayesComputation class. //The mean val center : DenseMatrix [ Double ] = ... //Covariance (positive semi-def) matrix among rows val sigmaRows : DenseMatrix [ Double ] = ... //Covariance (positive semi-def) matrix among columns val sigmaCols : DenseMatrix [ Double ] = ... val matD = MatrixNormal ( center , sigmaRows , sigmaCols ) - Matrix T Distribution ( Experimental ) //The degrees of freedom (must be > 2.0 for existence of finite moments) val mu : Double = ... //The mean val center : DenseMatrix [ Double ] = ... //Covariance (positive semi-def) matrix among rows val sigmaRows : DenseMatrix [ Double ] = ... //Covariance (positive semi-def) matrix among columns val sigmaCols : DenseMatrix [ Double ] = ... val matD = MatrixT ( mu , center , sigmaCols , sigmaRows ) Package dynaml . optimization Added ProbGPCommMachine which performs grid search or CSA and then instead of selecting a single hyper-parameter configuration calculates a weighted Gaussian Process committee where the weights correspond to probabilities or confidence on each model instance (hyper-parameter configuration). Package dynaml . utils Added multivariate gamma function //Returns logarithm of multivariate gamma function val g = mvlgamma ( 5 , 1.5 ) Package dynaml . dataformat Added support for reading MATLAB .mat files in the MAT object. Improvements/Bug Fixes \u00b6 Package dynaml . probability Removed ProbabilityModel and replaced with JointProbabilityScheme and BayesJointProbabilityScheme , major refactoring to RandomVariable API. Package dynaml . optimization Improved logging of CoupledSimulatedAnnealing Refactored GPMLOptimizer to GradBasedGlobalOptimizer Package dynaml . utils - Correction to utils . getStats method used for calculating mean and variance of data sets consisting of DenseVector [ Double ] . - minMaxScalingTrainTest minMaxScaling of DynaMLPipe using GaussianScaler instead of MinMaxScaler for processing of features. Package dynaml . kernels Fix to CoRegCauchyKernel : corrected mismatch of hyper-parameter string Fix to SVMKernel objects matrix gradient computation in the case when kernel dimensions are not multiples of block size. Correction to gradient calculation in RBF kernel family. Speed up of kernel gradient computation, kernel and kernel gradient matrices with respect to the model hyper-parameters now calculated in a single pass through the data.","title":"v1.4.1"},{"location":"releases/mydoc_release_notes_141/#pipes-api","text":"","title":"Pipes API"},{"location":"releases/mydoc_release_notes_141/#additions","text":"The pipes API has been vastly extended by creating pipes which encapsulate functions of multiple arguments leading to the following end points. DataPipe2 [ A , B , C ] : Pipe which takes 2 arguments DataPipe3 [ A , B , C , D ] : Pipe which takes 3 arguments DataPipe4 [ A , B , C , D , E ] : Pipe which takes 4 arguments Furthermore there is now the ability to create pipes which return pipes, something akin to curried functions in functional programming. MetaPipe : Takes an argument returns a DataPipe MetaPipe21 : Takes 2 arguments returns a DataPipe MetaPipe12 : Takes an argument returns a DataPipe2 A new kind of Stream data pipe, StreamFlatMapPipe is added to represent data pipelines which can perform flat map like operations on streams. val mapFunc : ( I ) => Stream [ J ] = ... val streamFMPipe = StreamFlatMapPipe ( mapFunc ) Added Data Pipes API for Apache Spark RDDs. val num = 20 val numbers = sc . parallelize ( 1 to num ) val convPipe = RDDPipe (( n : Int ) => n . toDouble ) val sqPipe = RDDPipe (( x : Double ) => x * x ) val sqrtPipe = RDDPipe (( x : Double ) => math . sqrt ( x )) val resultPipe = RDDPipe (( r : RDD [ Double ]) => r . reduce ( _ + _ ). toInt ) val netPipeline = convPipe > sqPipe > sqrtPipe > resultPipe netPipeline ( numbers ) Added UnivariateGaussianScaler class for gaussian scaling of univariate data.","title":"Additions"},{"location":"releases/mydoc_release_notes_141/#core-api","text":"","title":"Core API"},{"location":"releases/mydoc_release_notes_141/#additions_1","text":"Package dynaml . models . bayes This new package will house stochastic prior models, currently there is support for GP and Skew GP priors, to see a starting example see stochasticPriors.sc in the scripts directory of the DynaML source. Package dynaml . kernels Added evaluateAt ( h )( x , y ) and gradientAt ( h )( x , y ) ; expressing evaluate ( x , y ) and gradient ( x , y ) in terms of them Added asPipe method for Covariance Functions For backwards compatibility users are advised to extend LocalSVMKernel in their custom Kernel implementations incase they do not want to implement the evaluateAt API endpoints. Added FeatureMapKernel , representing kernels which can be explicitly decomposed into feature mappings. Added Matern half integer kernel GenericMaternKernel [ I ] Added block ( S : String* ) method to block any hyper-parameters of kernels. Added NeuralNetworkKernel and GaussianSpectralKernel . Added DecomposableCovariance import io.github.mandar2812.dynaml.DynaMLPipe._ import io.github.mandar2812.dynaml.kernels._ implicit val ev = VectorField ( 6 ) implicit val sp = breezeDVSplitEncoder ( 2 ) implicit val sumR = sumReducer val kernel = new LaplacianKernel ( 1.5 ) val other_kernel = new PolynomialKernel ( 1 , 0.05 ) val decompKernel = new DecomposableCovariance ( kernel , other_kernel )( sp , sumReducer ) val other_kernel1 = new FBMKernel ( 1.0 ) val decompKernel1 = new DecomposableCovariance ( decompKernel , other_kernel1 )( sp , sumReducer ) val veca = DenseVector . tabulate [ Double ]( 8 )( math . sin ( _ )) val vecb = DenseVector . tabulate [ Double ]( 8 )( math . cos ( _ )) decompKernel1 . evaluate ( veca , vecb ) Package dynaml . algebra Partitioned Matrices/Vectors and the following operations Addition, Subtraction Matrix, vector multiplication LU, Cholesky A \\ y , A \\ Y Added calculation of quadratic forms, namely: quadraticForm which calculates \\mathbf{x}^\\intercal A^{-1} \\mathbf{x} \\mathbf{x}^\\intercal A^{-1} \\mathbf{x} crossQuadraticForm which calculates \\mathbf{y}^\\intercal A^{-1} \\mathbf{x} \\mathbf{y}^\\intercal A^{-1} \\mathbf{x} Where A is assumed to be a symmetric positive semi-definite matrix Usage: import io.github.mandar2812.dynaml.algebra._ val x : DenseVector [ Double ] = ... val y : DenseVector [ Double ] = ... val a : DenseMatrix [ Double ] = ... quadraticForm ( a , x ) crossQuadraticForm ( y , a , x ) Package dynaml . modelpipe New package created, moved all inheriting classes of ModelPipe to this package. Added the following: GLMPipe2 A pipe taking two arguments and returning a GeneralizedLinearModel instance GeneralizedLeastSquaresPipe2 : GeneralizedLeastSquaresPipe3 : Package dynaml . models Added a new Neural Networks API: NeuralNet and GenericFFNeuralNet , for an example refer to TestNNDelve in dynaml - examples . GeneralizedLeastSquaresModel : The GLS model. ESGPModel : The implementation of a skew gaussian process regression model Warped Gaussian Process models WIP Added mean function capability to Gaussian Process and Student T process models. Added Apache Spark implementation of Generalized Linear Models; see SparkGLM , SparkLogisticModel , SparkProbitGLM Package dynaml.probability MultivariateSkewNormal as specified in Azzalani et. al ExtendedMultivariateSkewNormal UESN and MESN representing an alternative formulation of the skew gaussian family from Adcock and Shutes. TruncatedGaussian : Truncated version of the Gaussian distribution. Matrix Normal Distribution Added Expectation operator for RandomVariable implementations in the io . github . mandar2812 . dynaml . probability package object. Usage example given below. SkewGaussian , ExtendedSkewGaussian : An breeze implementation of the SkewGaussian and extended Skew-Gaussian distributions respectively PushforwardMap , DifferentiableMap added: PushforwardMap enables creating new random variables with defined density from base random variables. import io.github.mandar2812.dynaml.analysis._ import io.github.mandar2812.dynaml.probability._ import io.github.mandar2812.dynaml.probability.distributions._ val g = GaussianRV ( 0.0 , 0.25 ) val sg = RandomVariable ( SkewGaussian ( 1.0 , 0.0 , 0.25 )) //Define a determinant implementation for the Jacobian type (Double in this case) implicit val detImpl = identityPipe [ Double ] //Defines a homeomorphism y = exp(x) x = log(y) val h : PushforwardMap [ Double , Double , Double ] = PushforwardMap ( DataPipe (( x : Double ) => math . exp ( x )), DifferentiableMap ( ( x : Double ) => math . log ( x ), ( x : Double ) => 1.0 / x ) ) //Creates a log-normal random variable val p = h -> g //Creates a log-skew-gaussian random variable val q = h -> sg //Calculate expectation of q println ( \"E[Q] = \" + E ( q )) - Added Markov Chain Monte Carlo (MCMC) based inference schemes ContinuousMCMC and the underlying sampling implementation in GeneralMetropolisHastings . - Added implementation of Approximate Bayesian Computation (ABC) in the ApproxBayesComputation class. //The mean val center : DenseMatrix [ Double ] = ... //Covariance (positive semi-def) matrix among rows val sigmaRows : DenseMatrix [ Double ] = ... //Covariance (positive semi-def) matrix among columns val sigmaCols : DenseMatrix [ Double ] = ... val matD = MatrixNormal ( center , sigmaRows , sigmaCols ) - Matrix T Distribution ( Experimental ) //The degrees of freedom (must be > 2.0 for existence of finite moments) val mu : Double = ... //The mean val center : DenseMatrix [ Double ] = ... //Covariance (positive semi-def) matrix among rows val sigmaRows : DenseMatrix [ Double ] = ... //Covariance (positive semi-def) matrix among columns val sigmaCols : DenseMatrix [ Double ] = ... val matD = MatrixT ( mu , center , sigmaCols , sigmaRows ) Package dynaml . optimization Added ProbGPCommMachine which performs grid search or CSA and then instead of selecting a single hyper-parameter configuration calculates a weighted Gaussian Process committee where the weights correspond to probabilities or confidence on each model instance (hyper-parameter configuration). Package dynaml . utils Added multivariate gamma function //Returns logarithm of multivariate gamma function val g = mvlgamma ( 5 , 1.5 ) Package dynaml . dataformat Added support for reading MATLAB .mat files in the MAT object.","title":"Additions"},{"location":"releases/mydoc_release_notes_141/#improvementsbug-fixes","text":"Package dynaml . probability Removed ProbabilityModel and replaced with JointProbabilityScheme and BayesJointProbabilityScheme , major refactoring to RandomVariable API. Package dynaml . optimization Improved logging of CoupledSimulatedAnnealing Refactored GPMLOptimizer to GradBasedGlobalOptimizer Package dynaml . utils - Correction to utils . getStats method used for calculating mean and variance of data sets consisting of DenseVector [ Double ] . - minMaxScalingTrainTest minMaxScaling of DynaMLPipe using GaussianScaler instead of MinMaxScaler for processing of features. Package dynaml . kernels Fix to CoRegCauchyKernel : corrected mismatch of hyper-parameter string Fix to SVMKernel objects matrix gradient computation in the case when kernel dimensions are not multiples of block size. Correction to gradient calculation in RBF kernel family. Speed up of kernel gradient computation, kernel and kernel gradient matrices with respect to the model hyper-parameters now calculated in a single pass through the data.","title":"Improvements/Bug Fixes"},{"location":"releases/mydoc_release_notes_142/","text":"Version 1.4.2 of DynaML, released May 7, 2017. Updates, improvements and new features. Core API \u00b6 Additions \u00b6 Package dynaml . models . neuralnets Added GenericAutoEncoder [ LayerP , I ] , the class AutoEncoder is now deprecated Added GenericNeuralStack [ P , I , T ] as a base class for Neural Stack API Added LazyNeuralStack [ P , I ] where the layers are lazily spawned. Package dynaml . kernels Added ScaledKernel [ I ] representing kernels of scaled Gaussian Processes. Package dynaml . models . bayes Added * method to GaussianProcessPrior [ I , M ] which creates a scaled Gaussian Process prior using the newly minted ScaledKernel [ I ] class Added Kronecker product GP priors with the CoRegGPPrior [ I , J , M ] class Package dynaml . models . stp Added multi-output Students' T Regression model of Conti & O' Hagan in class MVStudentsTModel Package dynaml . probability . distributions Added HasErrorBars [ T ] generic trait representing distributions which can generate confidence intervals around their mean value. Improvements \u00b6 Package dynaml . probability Fixed issue with creation of MeasurableFunction instances from RandomVariable instances Package dynaml . probability . distributions Changed error bar calculations and sampling of Students T distributions (vector and matrix) and Matrix Normal distribution. Package dynaml . models . gp Added Kronecker structure speed up to energy (marginal likelihood) calculation of multi-output GP models Package dynaml . kernels - Improved implicit paramterization of Matern Covariance classes General Updated breeze version to latest. Updated Ammonite version to latest","title":"v1.4.2"},{"location":"releases/mydoc_release_notes_142/#core-api","text":"","title":"Core API"},{"location":"releases/mydoc_release_notes_142/#additions","text":"Package dynaml . models . neuralnets Added GenericAutoEncoder [ LayerP , I ] , the class AutoEncoder is now deprecated Added GenericNeuralStack [ P , I , T ] as a base class for Neural Stack API Added LazyNeuralStack [ P , I ] where the layers are lazily spawned. Package dynaml . kernels Added ScaledKernel [ I ] representing kernels of scaled Gaussian Processes. Package dynaml . models . bayes Added * method to GaussianProcessPrior [ I , M ] which creates a scaled Gaussian Process prior using the newly minted ScaledKernel [ I ] class Added Kronecker product GP priors with the CoRegGPPrior [ I , J , M ] class Package dynaml . models . stp Added multi-output Students' T Regression model of Conti & O' Hagan in class MVStudentsTModel Package dynaml . probability . distributions Added HasErrorBars [ T ] generic trait representing distributions which can generate confidence intervals around their mean value.","title":"Additions"},{"location":"releases/mydoc_release_notes_142/#improvements","text":"Package dynaml . probability Fixed issue with creation of MeasurableFunction instances from RandomVariable instances Package dynaml . probability . distributions Changed error bar calculations and sampling of Students T distributions (vector and matrix) and Matrix Normal distribution. Package dynaml . models . gp Added Kronecker structure speed up to energy (marginal likelihood) calculation of multi-output GP models Package dynaml . kernels - Improved implicit paramterization of Matern Covariance classes General Updated breeze version to latest. Updated Ammonite version to latest","title":"Improvements"},{"location":"releases/mydoc_release_notes_143/","text":"Version 1.4.3 of DynaML, released June 13, 2017. Updates, improvements and new features. DynaML REPL \u00b6 Additions \u00b6 Module scripts Added gp_mcmc_santafe.sc worksheet to try new MCMC feature on GP models; applied on the Santa Fe laser data set. General Updated Ammonite version to 0.9.9 Pipes API \u00b6 Additions \u00b6 Package dynaml.pipes Added ._1 and ._2 members in ParallelPipe Core API \u00b6 Additions \u00b6 Package dynaml.models.neuralnets Added SELU activation function proposed by Hochreiter et. al Package dynaml.models.bayes Added * method to CoRegGPPrior which scales it with to a ParallelPipe Package dynaml.probability.mcmc Added HyperParameterMCMC for performing MCMC sampling for models extending GloballyOptimizable . Package dynaml.utils Added trait HyperParameters outlining methods that must be implemented by entities having hyper-parameters Added MeanScaler , PCAScaler to perform mean centering and PCA on data sets. Also added to DynaMLPipe pipe library. Added tail recursive computation of the Chebyshev polynomials of the first and second kind in utils . chebyshev . Improvements \u00b6 Package dynaml.models.bayes Added trendParamsEncoder which converts the trend/mean parameters into a scala Map [ String , Double ] making them consistent with covariance parameters. Added to GaussianProcessPrior and ESGPPrior families.","title":"v1.4.3"},{"location":"releases/mydoc_release_notes_143/#dynaml-repl","text":"","title":"DynaML REPL"},{"location":"releases/mydoc_release_notes_143/#additions","text":"Module scripts Added gp_mcmc_santafe.sc worksheet to try new MCMC feature on GP models; applied on the Santa Fe laser data set. General Updated Ammonite version to 0.9.9","title":"Additions"},{"location":"releases/mydoc_release_notes_143/#pipes-api","text":"","title":"Pipes API"},{"location":"releases/mydoc_release_notes_143/#additions_1","text":"Package dynaml.pipes Added ._1 and ._2 members in ParallelPipe","title":"Additions"},{"location":"releases/mydoc_release_notes_143/#core-api","text":"","title":"Core API"},{"location":"releases/mydoc_release_notes_143/#additions_2","text":"Package dynaml.models.neuralnets Added SELU activation function proposed by Hochreiter et. al Package dynaml.models.bayes Added * method to CoRegGPPrior which scales it with to a ParallelPipe Package dynaml.probability.mcmc Added HyperParameterMCMC for performing MCMC sampling for models extending GloballyOptimizable . Package dynaml.utils Added trait HyperParameters outlining methods that must be implemented by entities having hyper-parameters Added MeanScaler , PCAScaler to perform mean centering and PCA on data sets. Also added to DynaMLPipe pipe library. Added tail recursive computation of the Chebyshev polynomials of the first and second kind in utils . chebyshev .","title":"Additions"},{"location":"releases/mydoc_release_notes_143/#improvements","text":"Package dynaml.models.bayes Added trendParamsEncoder which converts the trend/mean parameters into a scala Map [ String , Double ] making them consistent with covariance parameters. Added to GaussianProcessPrior and ESGPPrior families.","title":"Improvements"},{"location":"releases/mydoc_release_notes_15/","text":"Version 1.5 of DynaML, released August 11, 2017. Updates to global optimization api, improvements and new features in the gaussian process and stochastic process api. Additions \u00b6 Package dynaml.algebra Added support for dual numbers . //Zero Dual val zero = DualNumber . zero [ Double ] val dnum = DualNumber ( 1.5 , - 1.0 ) val dnum1 = DualNumber (- 1.5 , 1.0 ) //Algebraic operations: multiplication and addition/subtraction dnum1 * dnum2 dnum1 - dnum dnum * zero Package dynaml.probability Added support for mixture distributions and mixture random variables. MixtureRV , ContinuousDistrMixture for random variables and MixtureDistribution for constructing mixtures of breeze distributions. Package dynaml.optimization Added ModelTuner[T, T1] trait as a super trait to GlobalOptimizer[T] GridSearch and CoupledSimulatedAnnealing now extend AbstractGridSearch and AbstractCSA respectively. Added ProbGPMixtureMachine : constructs a mixture model after a CSA or grid search routine by calculating the mixture probabilities of members of the final hyper-parameter ensemble. Stochastic Mixture Models \u00b6 Package dynaml.models Added StochasticProcessMixtureModel as top level class for stochastic mixture models. Added GaussianProcessMixture : implementation of gaussian process mixture models. Added MVTMixture : implementation of mixture model over multioutput matrix T processes. Kulback-Leibler Divergence \u00b6 Package dynaml.probability Added method KL() to probability package object, to calculate the Kulback Leibler divergence between two continuous random variables backed by breeze distributions. Adaptive Metropolis Algorithms. \u00b6 AdaptiveHyperParameterMCMC which adapts the exploration covariance with each sample. HyperParameterSCAM adapts the exploration covariance for each hyper-parameter independently. Splines and B-Spline Generators \u00b6 Package dynaml.analysis B-Spline generators Bernstein b-spline generators. Arbitrary spline functions can be created using the SplineGenerator class. Cubic Spline Interpolation Kernels \u00b6 Package dynaml.kernels Added cubic spline interpolation kernel CubicSplineKernel and its ARD analogue CubicSplineARDKernel Gaussian Process Models for Linear Partial Differential Equations \u00b6 Based on a legacy ICML 2003 paper by Graepel . DynaML now ships with capability of performing PDE forward and inverse inference using the Gaussian Process API. Package dynaml.models.gp GPOperatorModel : models a quantity of interest which is governed by a linear PDE in space and time. Package dynaml.kernels LinearPDEKernel : The core kernel primitive accepted by the GPOperatorModel class. GenExpSpaceTimeKernel : a kernel of the exponential family which can serve as a handy base kernel for LinearPDEKernel class. Basis Function Gaussian Processes \u00b6 DynaML now supports GP models with explicitly incorporated basis functions as linear mean/trend functions. Package dynaml.models.gp GPBasisFuncRegressionModel can be used to create GP models with trends incorporated as a linear combination of basis functions. Log Gaussian Processes \u00b6 LogGaussianProcessModel represents a stochastic process whose natural logarithm follows a gaussian process. Improvements \u00b6 Package dynaml.probability Changes to RandomVarWithDistr : made type parameter Dist covariant. Reform to IIDRandomVar hierarchy. Package dynaml.probability.mcmc Bug-fixes to the HyperParameterMCMC class. General DynaML now ships with Ammonite v1.0.0 . Fixes \u00b6 Package dynaml.optimization Corrected energy calculation in CoupledSimulatedAnnealing ; added log likelihood due to hyper-prior. Package dynaml.optimization Corrected energy calculation in CoupledSimulatedAnnealing ; added log likelihood due to hyper-prior.","title":"v1.5"},{"location":"releases/mydoc_release_notes_15/#additions","text":"Package dynaml.algebra Added support for dual numbers . //Zero Dual val zero = DualNumber . zero [ Double ] val dnum = DualNumber ( 1.5 , - 1.0 ) val dnum1 = DualNumber (- 1.5 , 1.0 ) //Algebraic operations: multiplication and addition/subtraction dnum1 * dnum2 dnum1 - dnum dnum * zero Package dynaml.probability Added support for mixture distributions and mixture random variables. MixtureRV , ContinuousDistrMixture for random variables and MixtureDistribution for constructing mixtures of breeze distributions. Package dynaml.optimization Added ModelTuner[T, T1] trait as a super trait to GlobalOptimizer[T] GridSearch and CoupledSimulatedAnnealing now extend AbstractGridSearch and AbstractCSA respectively. Added ProbGPMixtureMachine : constructs a mixture model after a CSA or grid search routine by calculating the mixture probabilities of members of the final hyper-parameter ensemble.","title":"Additions"},{"location":"releases/mydoc_release_notes_15/#stochastic-mixture-models","text":"Package dynaml.models Added StochasticProcessMixtureModel as top level class for stochastic mixture models. Added GaussianProcessMixture : implementation of gaussian process mixture models. Added MVTMixture : implementation of mixture model over multioutput matrix T processes.","title":"Stochastic Mixture Models"},{"location":"releases/mydoc_release_notes_15/#kulback-leibler-divergence","text":"Package dynaml.probability Added method KL() to probability package object, to calculate the Kulback Leibler divergence between two continuous random variables backed by breeze distributions.","title":"Kulback-Leibler Divergence"},{"location":"releases/mydoc_release_notes_15/#adaptive-metropolis-algorithms","text":"AdaptiveHyperParameterMCMC which adapts the exploration covariance with each sample. HyperParameterSCAM adapts the exploration covariance for each hyper-parameter independently.","title":"Adaptive Metropolis Algorithms."},{"location":"releases/mydoc_release_notes_15/#splines-and-b-spline-generators","text":"Package dynaml.analysis B-Spline generators Bernstein b-spline generators. Arbitrary spline functions can be created using the SplineGenerator class.","title":"Splines and B-Spline Generators"},{"location":"releases/mydoc_release_notes_15/#cubic-spline-interpolation-kernels","text":"Package dynaml.kernels Added cubic spline interpolation kernel CubicSplineKernel and its ARD analogue CubicSplineARDKernel","title":"Cubic Spline Interpolation Kernels"},{"location":"releases/mydoc_release_notes_15/#gaussian-process-models-for-linear-partial-differential-equations","text":"Based on a legacy ICML 2003 paper by Graepel . DynaML now ships with capability of performing PDE forward and inverse inference using the Gaussian Process API. Package dynaml.models.gp GPOperatorModel : models a quantity of interest which is governed by a linear PDE in space and time. Package dynaml.kernels LinearPDEKernel : The core kernel primitive accepted by the GPOperatorModel class. GenExpSpaceTimeKernel : a kernel of the exponential family which can serve as a handy base kernel for LinearPDEKernel class.","title":"Gaussian Process Models for Linear Partial Differential Equations"},{"location":"releases/mydoc_release_notes_15/#basis-function-gaussian-processes","text":"DynaML now supports GP models with explicitly incorporated basis functions as linear mean/trend functions. Package dynaml.models.gp GPBasisFuncRegressionModel can be used to create GP models with trends incorporated as a linear combination of basis functions.","title":"Basis Function Gaussian Processes"},{"location":"releases/mydoc_release_notes_15/#log-gaussian-processes","text":"LogGaussianProcessModel represents a stochastic process whose natural logarithm follows a gaussian process.","title":"Log Gaussian Processes"},{"location":"releases/mydoc_release_notes_15/#improvements","text":"Package dynaml.probability Changes to RandomVarWithDistr : made type parameter Dist covariant. Reform to IIDRandomVar hierarchy. Package dynaml.probability.mcmc Bug-fixes to the HyperParameterMCMC class. General DynaML now ships with Ammonite v1.0.0 .","title":"Improvements"},{"location":"releases/mydoc_release_notes_15/#fixes","text":"Package dynaml.optimization Corrected energy calculation in CoupledSimulatedAnnealing ; added log likelihood due to hyper-prior. Package dynaml.optimization Corrected energy calculation in CoupledSimulatedAnnealing ; added log likelihood due to hyper-prior.","title":"Fixes"},{"location":"releases/mydoc_release_notes_151/","text":"Version 1.5.1 of DynaML, released September 20, 2017, introduces bug fixes and some useful new features Additions \u00b6 Package dynaml.probability.distributions Added Kumaraswamy distribution, an alternative to the Beta distribution. Added Erlang distribution, a special case of the Gamma distribution. Package dynaml.analysis Added Radial Basis Function generators. Gaussian Inverse Multi-Quadric Multi-Quadric Matern Half-Integer Added an inner product space implementation for Tuple2 Bug Fixes \u00b6 Package dynaml.kernels Fixed bug concerning hyper-parameter blocking in CompositeCovariance and its children. Package dynaml.probability.distributions Fixed calculation error for normalisation constant of multivariate T and Gaussian family.","title":"v1.5.1"},{"location":"releases/mydoc_release_notes_151/#additions","text":"Package dynaml.probability.distributions Added Kumaraswamy distribution, an alternative to the Beta distribution. Added Erlang distribution, a special case of the Gamma distribution. Package dynaml.analysis Added Radial Basis Function generators. Gaussian Inverse Multi-Quadric Multi-Quadric Matern Half-Integer Added an inner product space implementation for Tuple2","title":"Additions"},{"location":"releases/mydoc_release_notes_151/#bug-fixes","text":"Package dynaml.kernels Fixed bug concerning hyper-parameter blocking in CompositeCovariance and its children. Package dynaml.probability.distributions Fixed calculation error for normalisation constant of multivariate T and Gaussian family.","title":"Bug Fixes"},{"location":"releases/mydoc_release_notes_152/","text":"Version 1.5.2 of DynaML, released March 5, 2017, introduces functionality through improvement in the pipes API and increased integration with Tensorflow. Additions \u00b6 Tensorflow Integration \u00b6 Tensorflow (beta) support now live, thanks to the tensorflow_scala project! Try it out in: CIFAR-10 example script MNIST example script Package dynaml.tensorflow The dtf package object houses utility functions related to tensorflow primitives. Currently supports creation of tensors from arrays. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Create a FLOAT32 Tensor of shape (2, 2), i.e. a square matrix val mat = dtf . tensor_f32 ( 2 , 2 )( 1 d , 2 d , 3 d , 4 d ) //Create a random 2 * 3 matrix with independent standard normal entries val rand_mat = dtf . random ( FLOAT32 , 2 , 3 )( GaussianRV ( 0 d , 1 d ) > DataPipe (( x : Double ) => x . toFloat ) ) //Multiply matrices val prod = mat . matmul ( rand_mat ) println ( prod . summarize ()) val another_rand_mat = dtf . random ( FLOAT32 , 2 , 3 )( GaussianRV ( 0 d , 1 d ) > DataPipe (( x : Double ) => x . toFloat ) ) //Stack tensors vertically, i.e. row wise val vert_tensor = dtf . stack ( Seq ( rand_mat , another_rand_mat ), axis = 0 ) //Stack vectors horizontally, i.e. column wise val horz_tensor = dtf . stack ( Seq ( rand_mat , another_rand_mat ), axis = 1 ) The dtflearn package object deals with basic neural network building blocks which are often needed while constructing prediction architectures. //Create a simple neural architecture with one convolutional layer //followed by a max pool and feedforward layer val net = tf . learn . Cast ( \"Input/Cast\" , FLOAT32 ) >> dtflearn . conv2d_pyramid ( 2 , 3 )( 4 , 2 )( 0.1f , true , 0.6F ) >> tf . learn . MaxPool ( \"Layer_3/MaxPool\" , Seq ( 1 , 2 , 2 , 1 ), 1 , 1 , SamePadding ) >> tf . learn . Flatten ( \"Layer_3/Flatten\" ) >> dtflearn . feedforward ( 256 )( id = 4 ) >> tf . learn . ReLU ( \"Layer_4/ReLU\" , 0.1f ) >> dtflearn . feedforward ( 10 )( id = 5 ) Library Organisation \u00b6 Added dynaml-repl and dynaml-notebook modules to repository. DynaML Server \u00b6 DynaML ssh server now available (only in Local mode) $ ./target/universal/stage/bin/dynaml --server To login to the server open a separate shell and type, (when prompted for password, just press ENTER) $ ssh repl@localhost -p22222 Basis Generators \u00b6 Legrendre polynomial basis generators Bugfixes \u00b6 Acceptance rule of HyperParameterMCMC and related classes. Changes \u00b6 Increased pretty printing to screen instead of logging. Cleanup \u00b6 Package dynaml.models.svm - Removal of deprecated model classes from svm package","title":"v1.5.2"},{"location":"releases/mydoc_release_notes_152/#additions","text":"","title":"Additions"},{"location":"releases/mydoc_release_notes_152/#tensorflow-integration","text":"Tensorflow (beta) support now live, thanks to the tensorflow_scala project! Try it out in: CIFAR-10 example script MNIST example script Package dynaml.tensorflow The dtf package object houses utility functions related to tensorflow primitives. Currently supports creation of tensors from arrays. import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Create a FLOAT32 Tensor of shape (2, 2), i.e. a square matrix val mat = dtf . tensor_f32 ( 2 , 2 )( 1 d , 2 d , 3 d , 4 d ) //Create a random 2 * 3 matrix with independent standard normal entries val rand_mat = dtf . random ( FLOAT32 , 2 , 3 )( GaussianRV ( 0 d , 1 d ) > DataPipe (( x : Double ) => x . toFloat ) ) //Multiply matrices val prod = mat . matmul ( rand_mat ) println ( prod . summarize ()) val another_rand_mat = dtf . random ( FLOAT32 , 2 , 3 )( GaussianRV ( 0 d , 1 d ) > DataPipe (( x : Double ) => x . toFloat ) ) //Stack tensors vertically, i.e. row wise val vert_tensor = dtf . stack ( Seq ( rand_mat , another_rand_mat ), axis = 0 ) //Stack vectors horizontally, i.e. column wise val horz_tensor = dtf . stack ( Seq ( rand_mat , another_rand_mat ), axis = 1 ) The dtflearn package object deals with basic neural network building blocks which are often needed while constructing prediction architectures. //Create a simple neural architecture with one convolutional layer //followed by a max pool and feedforward layer val net = tf . learn . Cast ( \"Input/Cast\" , FLOAT32 ) >> dtflearn . conv2d_pyramid ( 2 , 3 )( 4 , 2 )( 0.1f , true , 0.6F ) >> tf . learn . MaxPool ( \"Layer_3/MaxPool\" , Seq ( 1 , 2 , 2 , 1 ), 1 , 1 , SamePadding ) >> tf . learn . Flatten ( \"Layer_3/Flatten\" ) >> dtflearn . feedforward ( 256 )( id = 4 ) >> tf . learn . ReLU ( \"Layer_4/ReLU\" , 0.1f ) >> dtflearn . feedforward ( 10 )( id = 5 )","title":"Tensorflow Integration"},{"location":"releases/mydoc_release_notes_152/#library-organisation","text":"Added dynaml-repl and dynaml-notebook modules to repository.","title":"Library Organisation"},{"location":"releases/mydoc_release_notes_152/#dynaml-server","text":"DynaML ssh server now available (only in Local mode) $ ./target/universal/stage/bin/dynaml --server To login to the server open a separate shell and type, (when prompted for password, just press ENTER) $ ssh repl@localhost -p22222","title":"DynaML Server"},{"location":"releases/mydoc_release_notes_152/#basis-generators","text":"Legrendre polynomial basis generators","title":"Basis Generators"},{"location":"releases/mydoc_release_notes_152/#bugfixes","text":"Acceptance rule of HyperParameterMCMC and related classes.","title":"Bugfixes"},{"location":"releases/mydoc_release_notes_152/#changes","text":"Increased pretty printing to screen instead of logging.","title":"Changes"},{"location":"releases/mydoc_release_notes_152/#cleanup","text":"Package dynaml.models.svm - Removal of deprecated model classes from svm package","title":"Cleanup"},{"location":"releases/mydoc_release_notes_153/","text":"Version 1.5.3 of DynaML, released August 14, 2017, introduces a new API for handling data sets. It also features greater TensorFlow related integrations, notably the Inception v2 cell. Additions \u00b6 Data Set API \u00b6 The DataSet family of classes helps the user to create and transform potentially large number of data instances. Users can create and perform complex transformations on data sets, using the DataPipe API or simple Scala functions. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import io.github.mandar2812.dynaml.tensorflow._ val random_numbers = GaussianRV ( 0.0 , 1.0 ) :* GaussianRV ( 1.0 , 2.0 ) //Create a data set. val dataset1 = dtfdata . dataset ( random_numbers . iid ( 10000 ). draw ) val filter_gr_zero = DataPipe [( Double , Double ) , Boolean ]( c => c . _1 > 0 d && c . _2 > 0 d ) //Filter elements val data_gr_zero = dataset1 . filter ( filter_gr_zero ) val abs_func : ( Double , Double ) => ( Double , Double ) = ( c : ( Double , Double )) => ( math . abs ( c . _1 ), math . abs ( c . _2 )) //Map elements val data_abs = dataset1 . map ( abs_func ) Find out more about the DataSet API and its capabilities in the user guide . Tensorflow Integration \u00b6 Package dynaml.tensorflow Batch Normalisation \u00b6 Batch normalisation is used to standardize activations of convolutional layers and to speed up training of deep neural nets. Usage import io.github.mandar2812.dynaml.tensorflow._ val bn = dtflearn . batch_norm ( \"BatchNorm1\" ) Inception v2 \u00b6 The Inception architecture, proposed by Google is an important building block of convolutional neural network architectures used in vision applications. In a subsequent paper , the authors introduced optimizations in the Inception architecture, known colloquially as Inception v2 . In Inception v2 , larger convolutions (i.e. 3 x 3 and 5 x 5 ) are implemented in a factorized manner to reduce the number of parameters to be learned. For example the 3 x 3 convolution is expressed as a combination of 1 x 3 and 3 x 1 convolutions. Similarly the 5 x 5 convolutions can be expressed a combination of two 3 x 3 convolutions DynaML now offers the Inception cell as a computational layer. Usage import io.github.mandar2812.dynaml.pipes._ import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Create an RELU activation, given a string name/identifier. val relu_act = DataPipe ( tf . learn . ReLU ( _ )) //Learn 10 filters in each branch of the inception cell val filters = Seq ( 10 , 10 , 10 , 10 ) val inception_cell = dtflearn . inception_unit ( channels = 3 , num_filters = filters , relu_act , //Apply batch normalisation after each convolution use_batch_norm = true )( layer_index = 1 ) Dynamical Systems: Continuous Time RNN \u00b6 Continuous time recurrent neural networks (CTRNN) are an important class of recurrent neural networks. They enable the modelling of non-linear and potentially complex dynamical systems of multiple variables, with feedback. Added CTRNN layer: dtflearn.ctrnn Added CTRNN layer with inferable time step: dtflearn.dctrnn . Added a projection layer for CTRNN based models dtflearn.ts_linear . Training Stopping Criteria Create common and simple training stop criteria such as. Stop after fixed number of iterations dtflearn.max_iter_stop(100000) Stop after change in value of loss goes below a threshold. dtflearn.abs_loss_change_stop(0.0001) Stop after change in relative value of loss goes below a threshold. dtflearn.rel_loss_change_stop(0.001) Neural Network Building Blocks Added helper method dtlearn.build_tf_model() for training tensorflow models/estimators. Usage import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ import org.platanios.tensorflow.data.image.MNISTLoader import ammonite.ops._ val tempdir = home / \"tmp\" val dataSet = MNISTLoader . load ( java . nio . file . Paths . get ( tempdir . toString ()) ) val trainImages = tf . data . TensorSlicesDataset ( dataSet . trainImages ) val trainLabels = tf . data . TensorSlicesDataset ( dataSet . trainLabels ) val trainData = trainImages . zip ( trainLabels ) . repeat () . shuffle ( 10000 ) . batch ( 256 ) . prefetch ( 10 ) // Create the MLP model. val input = tf . learn . Input ( UINT8 , Shape ( - 1 , dataSet . trainImages . shape ( 1 ), dataSet . trainImages . shape ( 2 )) ) val trainInput = tf . learn . Input ( UINT8 , Shape (- 1 )) val architecture = tf . learn . Flatten ( \"Input/Flatten\" ) >> tf . learn . Cast ( \"Input/Cast\" , FLOAT32 ) >> tf . learn . Linear ( \"Layer_0/Linear\" , 128 ) >> tf . learn . ReLU ( \"Layer_0/ReLU\" , 0.1f ) >> tf . learn . Linear ( \"Layer_1/Linear\" , 64 ) >> tf . learn . ReLU ( \"Layer_1/ReLU\" , 0.1f ) >> tf . learn . Linear ( \"Layer_2/Linear\" , 32 ) >> tf . learn . ReLU ( \"Layer_2/ReLU\" , 0.1f ) >> tf . learn . Linear ( \"OutputLayer/Linear\" , 10 ) val trainingInputLayer = tf . learn . Cast ( \"TrainInput/Cast\" , INT64 ) val loss = tf . learn . SparseSoftmaxCrossEntropy ( \"Loss/CrossEntropy\" ) >> tf . learn . Mean ( \"Loss/Mean\" ) >> tf . learn . ScalarSummary ( \"Loss/Summary\" , \"Loss\" ) val optimizer = tf . train . AdaGrad ( 0.1 ) // Directory in which to save summaries and checkpoints val summariesDir = java . nio . file . Paths . get ( ( tempdir / \"mnist_summaries\" ). toString () ) val ( model , estimator ) = dtflearn . build_tf_model ( architecture , input , trainInput , trainingInputLayer , loss , optimizer , summariesDir , dtflearn . max_iter_stop ( 1000 ), 100 , 100 , 100 )( trainData ) Build feedforward layers and feedforward layer stacks easier. Usage import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Create a single feedforward layer val layer = dtflearn . feedforward ( num_units = 10 , useBias = true )( id = 1 ) //Create a stack of feedforward layers val net_layer_sizes = Seq ( 10 , 5 , 3 ) val stack = dtflearn . feedforward_stack ( ( i : Int ) => dtflearn . Phi ( \"Act_\" + i ), FLOAT64 )( net_layer_sizes ) 3D Graphics \u00b6 Package dynaml.graphics Create 3d plots of surfaces, for a use case, see the jzydemo.sc and tf_wave_pde.sc Library Organisation \u00b6 Removed the dynaml-notebook module. Bug Fixes \u00b6 Fixed bug related to scalar method of VectorField , innerProdDouble and other inner product implementations. Improvements and Upgrades \u00b6 Bumped up Ammonite version to 1.1.0 RegressionMetrics and RegressionMetricsTF now also compute Spearman rank correlation as one of the performance metrics. Changes \u00b6","title":"v1.5.3"},{"location":"releases/mydoc_release_notes_153/#additions","text":"","title":"Additions"},{"location":"releases/mydoc_release_notes_153/#data-set-api","text":"The DataSet family of classes helps the user to create and transform potentially large number of data instances. Users can create and perform complex transformations on data sets, using the DataPipe API or simple Scala functions. import _root_.io.github.mandar2812.dynaml.probability._ import _root_.io.github.mandar2812.dynaml.pipes._ import io.github.mandar2812.dynaml.tensorflow._ val random_numbers = GaussianRV ( 0.0 , 1.0 ) :* GaussianRV ( 1.0 , 2.0 ) //Create a data set. val dataset1 = dtfdata . dataset ( random_numbers . iid ( 10000 ). draw ) val filter_gr_zero = DataPipe [( Double , Double ) , Boolean ]( c => c . _1 > 0 d && c . _2 > 0 d ) //Filter elements val data_gr_zero = dataset1 . filter ( filter_gr_zero ) val abs_func : ( Double , Double ) => ( Double , Double ) = ( c : ( Double , Double )) => ( math . abs ( c . _1 ), math . abs ( c . _2 )) //Map elements val data_abs = dataset1 . map ( abs_func ) Find out more about the DataSet API and its capabilities in the user guide .","title":"Data Set API"},{"location":"releases/mydoc_release_notes_153/#tensorflow-integration","text":"Package dynaml.tensorflow","title":"Tensorflow Integration"},{"location":"releases/mydoc_release_notes_153/#batch-normalisation","text":"Batch normalisation is used to standardize activations of convolutional layers and to speed up training of deep neural nets. Usage import io.github.mandar2812.dynaml.tensorflow._ val bn = dtflearn . batch_norm ( \"BatchNorm1\" )","title":"Batch Normalisation"},{"location":"releases/mydoc_release_notes_153/#inception-v2","text":"The Inception architecture, proposed by Google is an important building block of convolutional neural network architectures used in vision applications. In a subsequent paper , the authors introduced optimizations in the Inception architecture, known colloquially as Inception v2 . In Inception v2 , larger convolutions (i.e. 3 x 3 and 5 x 5 ) are implemented in a factorized manner to reduce the number of parameters to be learned. For example the 3 x 3 convolution is expressed as a combination of 1 x 3 and 3 x 1 convolutions. Similarly the 5 x 5 convolutions can be expressed a combination of two 3 x 3 convolutions DynaML now offers the Inception cell as a computational layer. Usage import io.github.mandar2812.dynaml.pipes._ import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Create an RELU activation, given a string name/identifier. val relu_act = DataPipe ( tf . learn . ReLU ( _ )) //Learn 10 filters in each branch of the inception cell val filters = Seq ( 10 , 10 , 10 , 10 ) val inception_cell = dtflearn . inception_unit ( channels = 3 , num_filters = filters , relu_act , //Apply batch normalisation after each convolution use_batch_norm = true )( layer_index = 1 )","title":"Inception v2"},{"location":"releases/mydoc_release_notes_153/#dynamical-systems-continuous-time-rnn","text":"Continuous time recurrent neural networks (CTRNN) are an important class of recurrent neural networks. They enable the modelling of non-linear and potentially complex dynamical systems of multiple variables, with feedback. Added CTRNN layer: dtflearn.ctrnn Added CTRNN layer with inferable time step: dtflearn.dctrnn . Added a projection layer for CTRNN based models dtflearn.ts_linear . Training Stopping Criteria Create common and simple training stop criteria such as. Stop after fixed number of iterations dtflearn.max_iter_stop(100000) Stop after change in value of loss goes below a threshold. dtflearn.abs_loss_change_stop(0.0001) Stop after change in relative value of loss goes below a threshold. dtflearn.rel_loss_change_stop(0.001) Neural Network Building Blocks Added helper method dtlearn.build_tf_model() for training tensorflow models/estimators. Usage import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ import org.platanios.tensorflow.data.image.MNISTLoader import ammonite.ops._ val tempdir = home / \"tmp\" val dataSet = MNISTLoader . load ( java . nio . file . Paths . get ( tempdir . toString ()) ) val trainImages = tf . data . TensorSlicesDataset ( dataSet . trainImages ) val trainLabels = tf . data . TensorSlicesDataset ( dataSet . trainLabels ) val trainData = trainImages . zip ( trainLabels ) . repeat () . shuffle ( 10000 ) . batch ( 256 ) . prefetch ( 10 ) // Create the MLP model. val input = tf . learn . Input ( UINT8 , Shape ( - 1 , dataSet . trainImages . shape ( 1 ), dataSet . trainImages . shape ( 2 )) ) val trainInput = tf . learn . Input ( UINT8 , Shape (- 1 )) val architecture = tf . learn . Flatten ( \"Input/Flatten\" ) >> tf . learn . Cast ( \"Input/Cast\" , FLOAT32 ) >> tf . learn . Linear ( \"Layer_0/Linear\" , 128 ) >> tf . learn . ReLU ( \"Layer_0/ReLU\" , 0.1f ) >> tf . learn . Linear ( \"Layer_1/Linear\" , 64 ) >> tf . learn . ReLU ( \"Layer_1/ReLU\" , 0.1f ) >> tf . learn . Linear ( \"Layer_2/Linear\" , 32 ) >> tf . learn . ReLU ( \"Layer_2/ReLU\" , 0.1f ) >> tf . learn . Linear ( \"OutputLayer/Linear\" , 10 ) val trainingInputLayer = tf . learn . Cast ( \"TrainInput/Cast\" , INT64 ) val loss = tf . learn . SparseSoftmaxCrossEntropy ( \"Loss/CrossEntropy\" ) >> tf . learn . Mean ( \"Loss/Mean\" ) >> tf . learn . ScalarSummary ( \"Loss/Summary\" , \"Loss\" ) val optimizer = tf . train . AdaGrad ( 0.1 ) // Directory in which to save summaries and checkpoints val summariesDir = java . nio . file . Paths . get ( ( tempdir / \"mnist_summaries\" ). toString () ) val ( model , estimator ) = dtflearn . build_tf_model ( architecture , input , trainInput , trainingInputLayer , loss , optimizer , summariesDir , dtflearn . max_iter_stop ( 1000 ), 100 , 100 , 100 )( trainData ) Build feedforward layers and feedforward layer stacks easier. Usage import io.github.mandar2812.dynaml.tensorflow._ import org.platanios.tensorflow.api._ //Create a single feedforward layer val layer = dtflearn . feedforward ( num_units = 10 , useBias = true )( id = 1 ) //Create a stack of feedforward layers val net_layer_sizes = Seq ( 10 , 5 , 3 ) val stack = dtflearn . feedforward_stack ( ( i : Int ) => dtflearn . Phi ( \"Act_\" + i ), FLOAT64 )( net_layer_sizes )","title":"Dynamical Systems: Continuous Time RNN"},{"location":"releases/mydoc_release_notes_153/#3d-graphics","text":"Package dynaml.graphics Create 3d plots of surfaces, for a use case, see the jzydemo.sc and tf_wave_pde.sc","title":"3D Graphics"},{"location":"releases/mydoc_release_notes_153/#library-organisation","text":"Removed the dynaml-notebook module.","title":"Library Organisation"},{"location":"releases/mydoc_release_notes_153/#bug-fixes","text":"Fixed bug related to scalar method of VectorField , innerProdDouble and other inner product implementations.","title":"Bug Fixes"},{"location":"releases/mydoc_release_notes_153/#improvements-and-upgrades","text":"Bumped up Ammonite version to 1.1.0 RegressionMetrics and RegressionMetricsTF now also compute Spearman rank correlation as one of the performance metrics.","title":"Improvements and Upgrades"},{"location":"releases/mydoc_release_notes_153/#changes","text":"","title":"Changes"},{"location":"repl-examples/p2_boston_housing/","text":"The Housing data set is a popular regression benchmarking data set hosted on the UCI Machine Learning Repository . It contains 506 records consisting of multivariate data attributes for various real estate zones and their housing price indices. The task is then to learn a regression model that can predict the price index or range. Attribute Information: \u00b6 CRIM : per capita crime rate by town ZN : proportion of residential land zoned for lots over 25,000 sq.ft. INDUS : proportion of non-retail business acres per town CHAS : Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) NOX : nitric oxides concentration (parts per 10 million) RM : average number of rooms per dwelling AGE : proportion of owner-occupied units built prior to 1940 DIS : weighted distances to five Boston employment centres RAD : index of accessibility to radial highways TAX : full-value property-tax rate per $10,000 PTRATIO : pupil-teacher ratio by town B : 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town LSTAT : % lower status of the population MEDV : Median value of owner-occupied homes in $1000's Model \u00b6 Below is a GP model for predicting the MEDV \\begin{align} & MEDV(\\mathbf{u}) = f(\\mathbf{u}) + \\epsilon(\\mathbf{u}) \\\\ & f \\sim \\mathcal{GP}(m(\\mathbf{u}), K(\\mathbf{u},\\mathbf{v})) \\\\ & \\mathbb{E}[\\epsilon(\\mathbf{u}).\\epsilon(\\mathbf{v})] = K_{noise}(\\mathbf{u}, \\mathbf{v})\\\\ \\end{align} \\begin{align} & MEDV(\\mathbf{u}) = f(\\mathbf{u}) + \\epsilon(\\mathbf{u}) \\\\ & f \\sim \\mathcal{GP}(m(\\mathbf{u}), K(\\mathbf{u},\\mathbf{v})) \\\\ & \\mathbb{E}[\\epsilon(\\mathbf{u}).\\epsilon(\\mathbf{v})] = K_{noise}(\\mathbf{u}, \\mathbf{v})\\\\ \\end{align} Syntax \u00b6 The TestGPHousing() program can be run in the REPL, below is a description of each of its arguments. Parameter | Type | Default value |Notes --------|-----------|-----------|------------| kernel | CovarianceFunction | - | The kernel function driving the GP model. noise | CovarianceFunction | - | The additive noise that corrupts the values of the latent function. trainFraction | Double | 0.75 | Fraction of the data to be used for model training and hyper-parameter selection. columns | List [ Int ] | 13, 0,.., 12 | The columns to be selected for analysis (indexed from 0), first one is the target column. grid| Int | 5 | The number of grid points for each hyper-parameter step | Double | 0.2| The space between grid points. globalOpt | String | ML | The model selection procedure \"GS\" , \"CSA\" , or \"ML\" stepSize | Double | 0.01 | Only relevant if globalOpt = \"ML\" , determines step size of steepest ascent. maxIt | Int | 300 | Maximum iterations for ML model selection procedure. TestGPHousing ( kernel = new FBMKernel ( 0.55 ) + new LaplacianKernel ( 2.5 ), noise = new RBFKernel ( 1.5 ), grid = 5 , step = 0.03 , globalOpt = \"GS\" , trainFraction = 0.45 ) 16/03/03 20:45:41 INFO GridSearch: Optimum value of energy is: 278.1603309851301 Configuration: Map(hurst -> 0.4, beta -> 2.35, bandwidth -> 1.35) 16/03/03 20:45:41 INFO SVMKernel$: Constructing kernel matrix. 16/03/03 20:45:42 INFO GPRegression: Generating error bars 16/03/03 20:45:42 INFO RegressionMetrics: Regression Model Performance: MEDV 16/03/03 20:45:42 INFO RegressionMetrics: ============================ 16/03/03 20:45:42 INFO RegressionMetrics: MAE: 5.800070254265218 16/03/03 20:45:42 INFO RegressionMetrics: RMSE: 7.739266267762397 16/03/03 20:45:42 INFO RegressionMetrics: RMSLE: 0.4150438478412412 16/03/03 20:45:42 INFO RegressionMetrics: R^2: 0.3609909626630624 16/03/03 20:45:42 INFO RegressionMetrics: Corr. Coefficient: 0.7633838930006132 16/03/03 20:45:42 INFO RegressionMetrics: Model Yield: 0.7341944950376289 16/03/03 20:45:42 INFO RegressionMetrics: Std Dev of Residuals: 6.287519509352036 Source Code \u00b6 Below is the example program as a github gist, to view the original program in DynaML, click here .","title":"Boston Housing"},{"location":"repl-examples/p2_boston_housing/#attribute-information","text":"CRIM : per capita crime rate by town ZN : proportion of residential land zoned for lots over 25,000 sq.ft. INDUS : proportion of non-retail business acres per town CHAS : Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) NOX : nitric oxides concentration (parts per 10 million) RM : average number of rooms per dwelling AGE : proportion of owner-occupied units built prior to 1940 DIS : weighted distances to five Boston employment centres RAD : index of accessibility to radial highways TAX : full-value property-tax rate per $10,000 PTRATIO : pupil-teacher ratio by town B : 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town LSTAT : % lower status of the population MEDV : Median value of owner-occupied homes in $1000's","title":"Attribute Information:"},{"location":"repl-examples/p2_boston_housing/#model","text":"Below is a GP model for predicting the MEDV \\begin{align} & MEDV(\\mathbf{u}) = f(\\mathbf{u}) + \\epsilon(\\mathbf{u}) \\\\ & f \\sim \\mathcal{GP}(m(\\mathbf{u}), K(\\mathbf{u},\\mathbf{v})) \\\\ & \\mathbb{E}[\\epsilon(\\mathbf{u}).\\epsilon(\\mathbf{v})] = K_{noise}(\\mathbf{u}, \\mathbf{v})\\\\ \\end{align} \\begin{align} & MEDV(\\mathbf{u}) = f(\\mathbf{u}) + \\epsilon(\\mathbf{u}) \\\\ & f \\sim \\mathcal{GP}(m(\\mathbf{u}), K(\\mathbf{u},\\mathbf{v})) \\\\ & \\mathbb{E}[\\epsilon(\\mathbf{u}).\\epsilon(\\mathbf{v})] = K_{noise}(\\mathbf{u}, \\mathbf{v})\\\\ \\end{align}","title":"Model"},{"location":"repl-examples/p2_boston_housing/#syntax","text":"The TestGPHousing() program can be run in the REPL, below is a description of each of its arguments. Parameter | Type | Default value |Notes --------|-----------|-----------|------------| kernel | CovarianceFunction | - | The kernel function driving the GP model. noise | CovarianceFunction | - | The additive noise that corrupts the values of the latent function. trainFraction | Double | 0.75 | Fraction of the data to be used for model training and hyper-parameter selection. columns | List [ Int ] | 13, 0,.., 12 | The columns to be selected for analysis (indexed from 0), first one is the target column. grid| Int | 5 | The number of grid points for each hyper-parameter step | Double | 0.2| The space between grid points. globalOpt | String | ML | The model selection procedure \"GS\" , \"CSA\" , or \"ML\" stepSize | Double | 0.01 | Only relevant if globalOpt = \"ML\" , determines step size of steepest ascent. maxIt | Int | 300 | Maximum iterations for ML model selection procedure. TestGPHousing ( kernel = new FBMKernel ( 0.55 ) + new LaplacianKernel ( 2.5 ), noise = new RBFKernel ( 1.5 ), grid = 5 , step = 0.03 , globalOpt = \"GS\" , trainFraction = 0.45 ) 16/03/03 20:45:41 INFO GridSearch: Optimum value of energy is: 278.1603309851301 Configuration: Map(hurst -> 0.4, beta -> 2.35, bandwidth -> 1.35) 16/03/03 20:45:41 INFO SVMKernel$: Constructing kernel matrix. 16/03/03 20:45:42 INFO GPRegression: Generating error bars 16/03/03 20:45:42 INFO RegressionMetrics: Regression Model Performance: MEDV 16/03/03 20:45:42 INFO RegressionMetrics: ============================ 16/03/03 20:45:42 INFO RegressionMetrics: MAE: 5.800070254265218 16/03/03 20:45:42 INFO RegressionMetrics: RMSE: 7.739266267762397 16/03/03 20:45:42 INFO RegressionMetrics: RMSLE: 0.4150438478412412 16/03/03 20:45:42 INFO RegressionMetrics: R^2: 0.3609909626630624 16/03/03 20:45:42 INFO RegressionMetrics: Corr. Coefficient: 0.7633838930006132 16/03/03 20:45:42 INFO RegressionMetrics: Model Yield: 0.7341944950376289 16/03/03 20:45:42 INFO RegressionMetrics: Std Dev of Residuals: 6.287519509352036","title":"Syntax"},{"location":"repl-examples/p2_boston_housing/#source-code","text":"Below is the example program as a github gist, to view the original program in DynaML, click here .","title":"Source Code"},{"location":"repl-examples/p2_examples/","text":"A good way to learn using DynaML's many features is to practice on some data science case studies. The dynaml-examples module contains a number of model training and testing experiments which are intended to serve as instructive material for getting comfortable with the DynaML API. The example programs cover a number of categories. Regression Classification System Identification","title":"Examples Module"},{"location":"repl-examples/p2_lssvm_powerplant/","text":"System identification refers to the process of learning a predictive model for a given dynamic system i.e. a system whose dynamics evolve with time. The most important aspect of these models is their structure, specifically the following are the common dynamic system models for discretely sampled time dependent systems. DaISy: System Identification Database \u00b6 DaISy is a database of (artificial and real world) dynamic systems maintained by the STADIUS research group at KU Leuven. We will work with the power plant data set listed on the DaISy home page in this post. Using DynaML , which comes preloaded with the power plant data, we will train LSSVM models to predict the various output indicators of the power plant in question. System Identification Models \u00b6 Below is a quick and dirty description of non-linear auto-regressive (NARX) models which are popular in the system identification research community and among practitioners. Nonlinear AutoRegresive (NAR) \u00b6 Signal y(t) y(t) modeled as a function of its previous p p values \\begin{align} y(t) = f(y(t-1), y(t-2), \\cdots, y(t-p)) + \\epsilon(t) \\end{align} \\begin{align} y(t) = f(y(t-1), y(t-2), \\cdots, y(t-p)) + \\epsilon(t) \\end{align} Nonlinear AutoRegressive with eXogenous inputs (NARX) \u00b6 Signal y(t) y(t) modeled as a function of the previous p p values of itself and the m m exogenous inputs u_{1}, \\cdots u_{m} u_{1}, \\cdots u_{m} \\begin{align} \\begin{split} y(t) = & f(y(t-1), y(t-2), \\cdots, y(t-p), \\\\ & u_{1}(t-1), u_{1}(t-2), \\cdots, u_{1}(t-p),\\\\ & \\cdots, \\\\ & u_{m}(t-1), u_{m}(t-2), \\cdots, u_{m}(t-p)) \\\\ & + \\epsilon(t) \\end{split} \\end{align} \\begin{align} \\begin{split} y(t) = & f(y(t-1), y(t-2), \\cdots, y(t-p), \\\\ & u_{1}(t-1), u_{1}(t-2), \\cdots, u_{1}(t-p),\\\\ & \\cdots, \\\\ & u_{m}(t-1), u_{m}(t-2), \\cdots, u_{m}(t-p)) \\\\ & + \\epsilon(t) \\end{split} \\end{align} Pont-sur-Sambre Power Plant Data \u00b6 You can obtain the metadata from this link , it is also summarized below. Data Attributes \u00b6 Instances : 200 Inputs : Gas flow Turbine valves opening Super heater spray flow Gas dampers Air flow Outputs : 6. Steam pressure 7. Main stem temperature 8. Reheat steam temperature System Model \u00b6 An LS-SVM NARX of autoregressive order p = 2 p = 2 is chosen to model the plant output data. An LS-SVM model builds a predictor of the following form. \\begin{align*} y(x) = \\sum_{k = 1}^{N}\\alpha_k K(\\mathbf{x}, \\mathbf{x_k}) + b \\end{align*} \\begin{align*} y(x) = \\sum_{k = 1}^{N}\\alpha_k K(\\mathbf{x}, \\mathbf{x_k}) + b \\end{align*} Which is the result of solving the following linear system. \\left[\\begin{array}{c|c} 0 & 1^\\intercal_v \\\\ \\hline 1_v & K + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline y \\end{array}\\right] \\left[\\begin{array}{c|c} 0 & 1^\\intercal_v \\\\ \\hline 1_v & K + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline y \\end{array}\\right] Here the matrix K K is constructed from the training data using a kernel function K(\\mathbf{x}, \\mathbf{y}) K(\\mathbf{x}, \\mathbf{y}) . Choice of Kernel Function \u00b6 For this problem we choose a polynomial kernel. \\begin{align*} K(\\mathbf{x},\\mathbf{y}) = K_{poly}(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^{T}.\\mathbf{y} + \\alpha)^{d} \\end{align*} \\begin{align*} K(\\mathbf{x},\\mathbf{y}) = K_{poly}(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^{T}.\\mathbf{y} + \\alpha)^{d} \\end{align*} Syntax \u00b6 The DaisyPowerPlant program can be used to train and test LS-SVM models on the Pont Sur-Sambre power plant data. Parameter | Type | Default value |Notes --------|-----------|-----------|------------| kernel | CovarianceFunction | - | The kernel function driving the LS-SVM model. deltaT | Int | 2 | Order of auto-regressive model i.e. number of steps in the past to look for input features. timelag | Int | 0 | The number of steps in the past to start using inputs. num_training | Int | 150 | Number of training data instances. column| Int | 7 | The column number of the output variable (indexed from 0). opt | Map [ String , Double ] | - | Extra options for model selection routine. Steam Pressure \u00b6 DynaML > DaisyPowerPlant ( new PolynomialKernel ( 2 , 0.5 ), opt = Map ( \"regularization\" -> \"2.5\" , \"globalOpt\" -> \"GS\" , \"grid\" -> \"4\" , \"step\" -> \"0.1\" ), num_training = 100 , deltaT = 2 , column = 6 ) 16 /03/04 17 :13:43 INFO RegressionMetrics: Regression Model Performance: steam pressure 16 /03/04 17 :13:43 INFO RegressionMetrics: ============================ 16 /03/04 17 :13:43 INFO RegressionMetrics: MAE: 82 .12740530161123 16 /03/04 17 :13:43 INFO RegressionMetrics: RMSE: 104 .39251587470388 16 /03/04 17 :13:43 INFO RegressionMetrics: RMSLE: 0 .9660077848586197 16 /03/04 17 :13:43 INFO RegressionMetrics: R^2: 0 .8395534877128238 16 /03/04 17 :13:43 INFO RegressionMetrics: Corr. Coefficient: 0 .9311734118932473 16 /03/04 17 :13:43 INFO RegressionMetrics: Model Yield: 0 .6288000962818303 16 /03/04 17 :13:43 INFO RegressionMetrics: Std Dev of Residuals: 87 .82754320038951 Reheat Steam Temperature \u00b6 DaisyPowerPlant ( new PolynomialKernel ( 2 , 1.5 ), opt = Map ( \"regularization\" -> \"2.5\" , \"globalOpt\" -> \"GS\" , \"grid\" -> \"4\" , \"step\" -> \"0.1\" ), num_training = 150 , deltaT = 1 , column = 8 ) 16 /03/04 16 :50:42 INFO RegressionMetrics: Regression Model Performance: reheat steam temperature 16 /03/04 16 :50:42 INFO RegressionMetrics: ============================ 16 /03/04 16 :50:42 INFO RegressionMetrics: MAE: 124 .60921194767073 16 /03/04 16 :50:42 INFO RegressionMetrics: RMSE: 137 .33314302068544 16 /03/04 16 :50:42 INFO RegressionMetrics: RMSLE: 0 .5275727128626408 16 /03/04 16 :50:42 INFO RegressionMetrics: R^2: 0 .8247581957573777 16 /03/04 16 :50:42 INFO RegressionMetrics: Corr. Coefficient: 0 .9744133881055823 16 /03/04 16 :50:42 INFO RegressionMetrics: Model Yield: 0 .7871288689840381 16 /03/04 16 :50:42 INFO RegressionMetrics: Std Dev of Residuals: 111 .86852905896446 Source Code \u00b6 Below is the example program as a github gist, to view the original program in DynaML, click here .","title":"Pont-sur-Sambre Power Plant"},{"location":"repl-examples/p2_lssvm_powerplant/#daisy-system-identification-database","text":"DaISy is a database of (artificial and real world) dynamic systems maintained by the STADIUS research group at KU Leuven. We will work with the power plant data set listed on the DaISy home page in this post. Using DynaML , which comes preloaded with the power plant data, we will train LSSVM models to predict the various output indicators of the power plant in question.","title":"DaISy: System Identification Database"},{"location":"repl-examples/p2_lssvm_powerplant/#system-identification-models","text":"Below is a quick and dirty description of non-linear auto-regressive (NARX) models which are popular in the system identification research community and among practitioners.","title":"System Identification Models"},{"location":"repl-examples/p2_lssvm_powerplant/#nonlinear-autoregresive-nar","text":"Signal y(t) y(t) modeled as a function of its previous p p values \\begin{align} y(t) = f(y(t-1), y(t-2), \\cdots, y(t-p)) + \\epsilon(t) \\end{align} \\begin{align} y(t) = f(y(t-1), y(t-2), \\cdots, y(t-p)) + \\epsilon(t) \\end{align}","title":"Nonlinear AutoRegresive (NAR)"},{"location":"repl-examples/p2_lssvm_powerplant/#nonlinear-autoregressive-with-exogenous-inputs-narx","text":"Signal y(t) y(t) modeled as a function of the previous p p values of itself and the m m exogenous inputs u_{1}, \\cdots u_{m} u_{1}, \\cdots u_{m} \\begin{align} \\begin{split} y(t) = & f(y(t-1), y(t-2), \\cdots, y(t-p), \\\\ & u_{1}(t-1), u_{1}(t-2), \\cdots, u_{1}(t-p),\\\\ & \\cdots, \\\\ & u_{m}(t-1), u_{m}(t-2), \\cdots, u_{m}(t-p)) \\\\ & + \\epsilon(t) \\end{split} \\end{align} \\begin{align} \\begin{split} y(t) = & f(y(t-1), y(t-2), \\cdots, y(t-p), \\\\ & u_{1}(t-1), u_{1}(t-2), \\cdots, u_{1}(t-p),\\\\ & \\cdots, \\\\ & u_{m}(t-1), u_{m}(t-2), \\cdots, u_{m}(t-p)) \\\\ & + \\epsilon(t) \\end{split} \\end{align}","title":"Nonlinear AutoRegressive with eXogenous inputs (NARX)"},{"location":"repl-examples/p2_lssvm_powerplant/#pont-sur-sambre-power-plant-data","text":"You can obtain the metadata from this link , it is also summarized below.","title":"Pont-sur-Sambre Power Plant Data"},{"location":"repl-examples/p2_lssvm_powerplant/#data-attributes","text":"Instances : 200 Inputs : Gas flow Turbine valves opening Super heater spray flow Gas dampers Air flow Outputs : 6. Steam pressure 7. Main stem temperature 8. Reheat steam temperature","title":"Data Attributes"},{"location":"repl-examples/p2_lssvm_powerplant/#system-model","text":"An LS-SVM NARX of autoregressive order p = 2 p = 2 is chosen to model the plant output data. An LS-SVM model builds a predictor of the following form. \\begin{align*} y(x) = \\sum_{k = 1}^{N}\\alpha_k K(\\mathbf{x}, \\mathbf{x_k}) + b \\end{align*} \\begin{align*} y(x) = \\sum_{k = 1}^{N}\\alpha_k K(\\mathbf{x}, \\mathbf{x_k}) + b \\end{align*} Which is the result of solving the following linear system. \\left[\\begin{array}{c|c} 0 & 1^\\intercal_v \\\\ \\hline 1_v & K + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline y \\end{array}\\right] \\left[\\begin{array}{c|c} 0 & 1^\\intercal_v \\\\ \\hline 1_v & K + \\gamma^{-1} \\mathit{I} \\end{array}\\right] \\left[\\begin{array}{c} b \\\\ \\hline \\alpha \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ \\hline y \\end{array}\\right] Here the matrix K K is constructed from the training data using a kernel function K(\\mathbf{x}, \\mathbf{y}) K(\\mathbf{x}, \\mathbf{y}) .","title":"System Model"},{"location":"repl-examples/p2_lssvm_powerplant/#choice-of-kernel-function","text":"For this problem we choose a polynomial kernel. \\begin{align*} K(\\mathbf{x},\\mathbf{y}) = K_{poly}(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^{T}.\\mathbf{y} + \\alpha)^{d} \\end{align*} \\begin{align*} K(\\mathbf{x},\\mathbf{y}) = K_{poly}(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^{T}.\\mathbf{y} + \\alpha)^{d} \\end{align*}","title":"Choice of Kernel Function"},{"location":"repl-examples/p2_lssvm_powerplant/#syntax","text":"The DaisyPowerPlant program can be used to train and test LS-SVM models on the Pont Sur-Sambre power plant data. Parameter | Type | Default value |Notes --------|-----------|-----------|------------| kernel | CovarianceFunction | - | The kernel function driving the LS-SVM model. deltaT | Int | 2 | Order of auto-regressive model i.e. number of steps in the past to look for input features. timelag | Int | 0 | The number of steps in the past to start using inputs. num_training | Int | 150 | Number of training data instances. column| Int | 7 | The column number of the output variable (indexed from 0). opt | Map [ String , Double ] | - | Extra options for model selection routine.","title":"Syntax"},{"location":"repl-examples/p2_lssvm_powerplant/#steam-pressure","text":"DynaML > DaisyPowerPlant ( new PolynomialKernel ( 2 , 0.5 ), opt = Map ( \"regularization\" -> \"2.5\" , \"globalOpt\" -> \"GS\" , \"grid\" -> \"4\" , \"step\" -> \"0.1\" ), num_training = 100 , deltaT = 2 , column = 6 ) 16 /03/04 17 :13:43 INFO RegressionMetrics: Regression Model Performance: steam pressure 16 /03/04 17 :13:43 INFO RegressionMetrics: ============================ 16 /03/04 17 :13:43 INFO RegressionMetrics: MAE: 82 .12740530161123 16 /03/04 17 :13:43 INFO RegressionMetrics: RMSE: 104 .39251587470388 16 /03/04 17 :13:43 INFO RegressionMetrics: RMSLE: 0 .9660077848586197 16 /03/04 17 :13:43 INFO RegressionMetrics: R^2: 0 .8395534877128238 16 /03/04 17 :13:43 INFO RegressionMetrics: Corr. Coefficient: 0 .9311734118932473 16 /03/04 17 :13:43 INFO RegressionMetrics: Model Yield: 0 .6288000962818303 16 /03/04 17 :13:43 INFO RegressionMetrics: Std Dev of Residuals: 87 .82754320038951","title":"Steam Pressure"},{"location":"repl-examples/p2_lssvm_powerplant/#reheat-steam-temperature","text":"DaisyPowerPlant ( new PolynomialKernel ( 2 , 1.5 ), opt = Map ( \"regularization\" -> \"2.5\" , \"globalOpt\" -> \"GS\" , \"grid\" -> \"4\" , \"step\" -> \"0.1\" ), num_training = 150 , deltaT = 1 , column = 8 ) 16 /03/04 16 :50:42 INFO RegressionMetrics: Regression Model Performance: reheat steam temperature 16 /03/04 16 :50:42 INFO RegressionMetrics: ============================ 16 /03/04 16 :50:42 INFO RegressionMetrics: MAE: 124 .60921194767073 16 /03/04 16 :50:42 INFO RegressionMetrics: RMSE: 137 .33314302068544 16 /03/04 16 :50:42 INFO RegressionMetrics: RMSLE: 0 .5275727128626408 16 /03/04 16 :50:42 INFO RegressionMetrics: R^2: 0 .8247581957573777 16 /03/04 16 :50:42 INFO RegressionMetrics: Corr. Coefficient: 0 .9744133881055823 16 /03/04 16 :50:42 INFO RegressionMetrics: Model Yield: 0 .7871288689840381 16 /03/04 16 :50:42 INFO RegressionMetrics: Std Dev of Residuals: 111 .86852905896446","title":"Reheat Steam Temperature"},{"location":"repl-examples/p2_lssvm_powerplant/#source-code","text":"Below is the example program as a github gist, to view the original program in DynaML, click here .","title":"Source Code"},{"location":"repl-examples/p2_winequality_logistic/","text":"The wine quality data set is a common example used to benchmark classification models. Here we use the DynaML scala machine learning environment to train classifiers to detect 'good' wine from 'bad' wine. A short listing of the data attributes/columns is given below. The UCI archive has two files in the wine quality data set namely winequality-red.csv and winequality-white.csv . We train two separate classification models, one for red wine and one for white. Attribute Information: \u00b6 Inputs: \u00b6 fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol Output (based on sensory data): \u00b6 quality (score between 0 and 10) Data Output Preprocessing \u00b6 The wine quality target variable can take integer values from 0 to 10 , first we convert this into a binary class variable by setting the quality to be 'good'(encoded by the value 1 ) if the numerical value is greater than 6 and 'bad' (encoded by value 0 ) otherwise. Model \u00b6 Below is a classification model for predicting the quality label y y . Logit \u00b6 \\begin{align} P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\sigma(w^T \\varphi(\\mathbf{x}) + b) \\\\ \\sigma(z) &= \\frac{1}{1 + exp(-z)} \\end{align} \\begin{align} P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\sigma(w^T \\varphi(\\mathbf{x}) + b) \\\\ \\sigma(z) &= \\frac{1}{1 + exp(-z)} \\end{align} Probit \u00b6 The probit regression model is an alternative to the logit model it is represented as. \\begin{align} P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\Phi(w^T \\varphi(\\mathbf{x}) + b) \\\\ \\Phi(z) &= \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2 \\pi}} exp(-\\frac{z^{2}}{2}) dz \\end{align} \\begin{align} P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\Phi(w^T \\varphi(\\mathbf{x}) + b) \\\\ \\Phi(z) &= \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2 \\pi}} exp(-\\frac{z^{2}}{2}) dz \\end{align} Syntax \u00b6 The TestLogisticWineQuality program in the examples package trains and tests logit and probit models on the wine quality data. Parameter | Type | Default value |Notes --------|-----------|-----------|------------| training | Int | 100 | Number of training samples test | Int | 1000 | Number of test samples columns | List [ Int ] | 11, 0, ... , 10 | The columns to be selected for analysis (indexed from 0), first one is the target column. stepSize | Double | 0.01 | Step size chosen for GradientDescent maxIt | Int | 30 | Maximum number of iterations for gradient descent update. mini | Double | 1.0 | Fraction of training samples to sample for each batch update. regularization | Double | 0.5 | Regularization parameter. wineType | String | red | The type of wine: red or white modelType | String | logistic | The type of model: logistic or probit Red Wine \u00b6 TestLogisticWineQuality ( stepSize = 0.2 , maxIt = 120 , mini = 1.0 , training = 800 , test = 800 , regularization = 0.2 , wineType = \"red\" ) 16/04/01 15:21:57 INFO BinaryClassificationMetrics: Classification Model Performance 16/04/01 15:21:57 INFO BinaryClassificationMetrics: ============================ 16/04/01 15:21:57 INFO BinaryClassificationMetrics: Accuracy: 0.8475 16/04/01 15:21:57 INFO BinaryClassificationMetrics: Area under ROC: 0.7968417788802267 16/04/01 15:21:57 INFO BinaryClassificationMetrics: Maximum F Measure: 0.7493563745371187 White Wine \u00b6 TestLogisticWineQuality ( stepSize = 0.26 , maxIt = 300 , mini = 1.0 , training = 3800 , test = 1000 , regularization = 0.0 , wineType = \"white\" ) 16/04/01 15:27:17 INFO BinaryClassificationMetrics: Classification Model Performance 16/04/01 15:27:17 INFO BinaryClassificationMetrics: ============================ 16/04/01 15:27:17 INFO BinaryClassificationMetrics: Accuracy: 0.829 16/04/01 15:27:17 INFO BinaryClassificationMetrics: Area under ROC: 0.7184782682020251 16/04/01 15:27:17 INFO BinaryClassificationMetrics: Maximum F Measure: 0.7182203962483446 Source Code \u00b6","title":"Wine Quality"},{"location":"repl-examples/p2_winequality_logistic/#attribute-information","text":"","title":"Attribute Information:"},{"location":"repl-examples/p2_winequality_logistic/#inputs","text":"fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol","title":"Inputs:"},{"location":"repl-examples/p2_winequality_logistic/#output-based-on-sensory-data","text":"quality (score between 0 and 10)","title":"Output (based on sensory data):"},{"location":"repl-examples/p2_winequality_logistic/#data-output-preprocessing","text":"The wine quality target variable can take integer values from 0 to 10 , first we convert this into a binary class variable by setting the quality to be 'good'(encoded by the value 1 ) if the numerical value is greater than 6 and 'bad' (encoded by value 0 ) otherwise.","title":"Data Output Preprocessing"},{"location":"repl-examples/p2_winequality_logistic/#model","text":"Below is a classification model for predicting the quality label y y .","title":"Model"},{"location":"repl-examples/p2_winequality_logistic/#logit","text":"\\begin{align} P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\sigma(w^T \\varphi(\\mathbf{x}) + b) \\\\ \\sigma(z) &= \\frac{1}{1 + exp(-z)} \\end{align} \\begin{align} P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\sigma(w^T \\varphi(\\mathbf{x}) + b) \\\\ \\sigma(z) &= \\frac{1}{1 + exp(-z)} \\end{align}","title":"Logit"},{"location":"repl-examples/p2_winequality_logistic/#probit","text":"The probit regression model is an alternative to the logit model it is represented as. \\begin{align} P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\Phi(w^T \\varphi(\\mathbf{x}) + b) \\\\ \\Phi(z) &= \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2 \\pi}} exp(-\\frac{z^{2}}{2}) dz \\end{align} \\begin{align} P(y \\ = 1 \\ | \\ \\mathbf{x}) &= \\Phi(w^T \\varphi(\\mathbf{x}) + b) \\\\ \\Phi(z) &= \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2 \\pi}} exp(-\\frac{z^{2}}{2}) dz \\end{align}","title":"Probit"},{"location":"repl-examples/p2_winequality_logistic/#syntax","text":"The TestLogisticWineQuality program in the examples package trains and tests logit and probit models on the wine quality data. Parameter | Type | Default value |Notes --------|-----------|-----------|------------| training | Int | 100 | Number of training samples test | Int | 1000 | Number of test samples columns | List [ Int ] | 11, 0, ... , 10 | The columns to be selected for analysis (indexed from 0), first one is the target column. stepSize | Double | 0.01 | Step size chosen for GradientDescent maxIt | Int | 30 | Maximum number of iterations for gradient descent update. mini | Double | 1.0 | Fraction of training samples to sample for each batch update. regularization | Double | 0.5 | Regularization parameter. wineType | String | red | The type of wine: red or white modelType | String | logistic | The type of model: logistic or probit","title":"Syntax"},{"location":"repl-examples/p2_winequality_logistic/#red-wine","text":"TestLogisticWineQuality ( stepSize = 0.2 , maxIt = 120 , mini = 1.0 , training = 800 , test = 800 , regularization = 0.2 , wineType = \"red\" ) 16/04/01 15:21:57 INFO BinaryClassificationMetrics: Classification Model Performance 16/04/01 15:21:57 INFO BinaryClassificationMetrics: ============================ 16/04/01 15:21:57 INFO BinaryClassificationMetrics: Accuracy: 0.8475 16/04/01 15:21:57 INFO BinaryClassificationMetrics: Area under ROC: 0.7968417788802267 16/04/01 15:21:57 INFO BinaryClassificationMetrics: Maximum F Measure: 0.7493563745371187","title":"Red Wine"},{"location":"repl-examples/p2_winequality_logistic/#white-wine","text":"TestLogisticWineQuality ( stepSize = 0.26 , maxIt = 300 , mini = 1.0 , training = 3800 , test = 1000 , regularization = 0.0 , wineType = \"white\" ) 16/04/01 15:27:17 INFO BinaryClassificationMetrics: Classification Model Performance 16/04/01 15:27:17 INFO BinaryClassificationMetrics: ============================ 16/04/01 15:27:17 INFO BinaryClassificationMetrics: Accuracy: 0.829 16/04/01 15:27:17 INFO BinaryClassificationMetrics: Area under ROC: 0.7184782682020251 16/04/01 15:27:17 INFO BinaryClassificationMetrics: Maximum F Measure: 0.7182203962483446","title":"White Wine"},{"location":"repl-examples/p2_winequality_logistic/#source-code","text":"","title":"Source Code"},{"location":"scaladoc/v1.4/core/","text":"","title":"dynaml-core"},{"location":"scaladoc/v1.4/examples/","text":"","title":"dynaml-examples"},{"location":"scaladoc/v1.4/pipes/","text":"","title":"dynaml-pipes"},{"location":"scaladoc/v1.4.1/core/","text":"","title":"dynaml-core"},{"location":"scaladoc/v1.4.1/examples/","text":"","title":"dynaml-examples"},{"location":"scaladoc/v1.4.1/pipes/","text":"","title":"dynaml-pipes"},{"location":"scaladoc/v1.4.2/core/","text":"","title":"dynaml-core"},{"location":"scaladoc/v1.4.2/examples/","text":"","title":"dynaml-examples"},{"location":"scaladoc/v1.4.2/pipes/","text":"","title":"dynaml-pipes"},{"location":"scaladoc/v1.4.3/core/","text":"","title":"dynaml-core"},{"location":"scaladoc/v1.4.3/examples/","text":"","title":"dynaml-examples"},{"location":"scaladoc/v1.4.3/pipes/","text":"","title":"dynaml-pipes"},{"location":"scaladoc/v1.5/core/","text":"","title":"dynaml-core"},{"location":"scaladoc/v1.5/examples/","text":"","title":"dynaml-examples"},{"location":"scaladoc/v1.5/pipes/","text":"","title":"dynaml-pipes"},{"location":"scaladoc/v1.5.1/core/","text":"","title":"dynaml-core"},{"location":"scaladoc/v1.5.1/examples/","text":"","title":"dynaml-examples"},{"location":"scaladoc/v1.5.1/pipes/","text":"","title":"dynaml-pipes"},{"location":"scaladoc/v1.5.2/core/","text":"","title":"dynaml-core"},{"location":"scaladoc/v1.5.2/examples/","text":"","title":"dynaml-examples"},{"location":"scaladoc/v1.5.2/pipes/","text":"","title":"dynaml-pipes"},{"location":"scaladoc/v1.5.2/repl/","text":"","title":"dynaml-repl"},{"location":"scaladoc/v1.5.3/core/","text":"","title":"dynaml-core"},{"location":"scaladoc/v1.5.3/examples/","text":"","title":"dynaml-examples"},{"location":"scaladoc/v1.5.3/pipes/","text":"","title":"dynaml-pipes"},{"location":"scaladoc/v1.5.3/repl/","text":"","title":"dynaml-repl"},{"location":"utils/package/","text":"Summary The utils object contains some useful helper functions which are used by a number of API components of DynaML. String/File Processing \u00b6 Load File into a Stream \u00b6 val content = utils . textFileToStream ( \"data.csv\" ) String Replace \u00b6 Replace all occurrences of a string (or regular expression) in a target string val new_str = utils . replace ( find = \",\" )( replace = \"|\" )( input = \"1,2,3,4\" ) URL download \u00b6 Download the content of a url to a specified location on disk. utils . downloadURL ( \"www.google.com\" , \"google_home_page.html\" ) Write to File \u00b6 val content : Stream [ String ] = _ utils . writeToFile ( \"foo.csv\" )( content ) Numerics \u00b6 log1p \u00b6 Calculates log_{e}(1+x) log_{e}(1+x) . val l = utils . log1pExp ( 0.02 ) Haar DWT Matrix \u00b6 Constructs the Haar discrete wavelet transform matrix for orders which are powers of two. val dwt_mat = utils . haarMatrix ( math . pow ( 2 , 3 ). toInt ) Hermite Polynomials \u00b6 The Hermite polynomials are an important class of orthogonal polynomials used in numerical analysis. There are two definitions of the Hermite polynomials i.e. the probabilist and physicist definitions, which are equivalent up-to a scale factor. The the utils object, the probabilist polynomials are calculated. //Calculate the 3rd order Hermite polynomial val h3 = ( x : Double ) => utils . hermite ( 3 , x ) h3 ( 2.5 ) Chebyshev Polynomials \u00b6 Chebyshev polynomials are another important class of orthogonal polynomials used in numerical analysis. There are two types, the first kind and second kind . //Calculate the Chebyshev polynomial of second kind order 3 val c23 = ( x : Double ) => utils . chebyshev ( 3 , x , kind = 2 ) c23 ( 2.5 ) Quick Select \u00b6 The quick select aims to find the k^{th} k^{th} smallest element of a list of numbers. val second = utils . quickselect ( List ( 3 , 2 , 4 , 5 , 1 , 6 ), 2 ) Median \u00b6 val second = utils . median ( List ( 3 , 2 , 4 , 5 , 1 , 6 )) Sample Statistics \u00b6 Calculate the mean and variance (or covariance), minimum, maximum of a list of DenseVector [ Double ] instances. val data : List [ DenseVector [ Double ]] = _ val ( mu , vard ) : ( DenseVector [ Double ], DenseVector [ Double ]) = utils . getStats ( data ) val ( mean , cov ) : ( DenseVector [ Double ], DenseMatrix [ Double ]) = utils . getStatsMult ( data ) val ( min , max ) = utils . getMinMax ( data )","title":"`utils` object"},{"location":"utils/package/#stringfile-processing","text":"","title":"String/File Processing"},{"location":"utils/package/#load-file-into-a-stream","text":"val content = utils . textFileToStream ( \"data.csv\" )","title":"Load File into a Stream"},{"location":"utils/package/#string-replace","text":"Replace all occurrences of a string (or regular expression) in a target string val new_str = utils . replace ( find = \",\" )( replace = \"|\" )( input = \"1,2,3,4\" )","title":"String Replace"},{"location":"utils/package/#url-download","text":"Download the content of a url to a specified location on disk. utils . downloadURL ( \"www.google.com\" , \"google_home_page.html\" )","title":"URL download"},{"location":"utils/package/#write-to-file","text":"val content : Stream [ String ] = _ utils . writeToFile ( \"foo.csv\" )( content )","title":"Write to File"},{"location":"utils/package/#numerics","text":"","title":"Numerics"},{"location":"utils/package/#log1p","text":"Calculates log_{e}(1+x) log_{e}(1+x) . val l = utils . log1pExp ( 0.02 )","title":"log1p"},{"location":"utils/package/#haar-dwt-matrix","text":"Constructs the Haar discrete wavelet transform matrix for orders which are powers of two. val dwt_mat = utils . haarMatrix ( math . pow ( 2 , 3 ). toInt )","title":"Haar DWT Matrix"},{"location":"utils/package/#hermite-polynomials","text":"The Hermite polynomials are an important class of orthogonal polynomials used in numerical analysis. There are two definitions of the Hermite polynomials i.e. the probabilist and physicist definitions, which are equivalent up-to a scale factor. The the utils object, the probabilist polynomials are calculated. //Calculate the 3rd order Hermite polynomial val h3 = ( x : Double ) => utils . hermite ( 3 , x ) h3 ( 2.5 )","title":"Hermite Polynomials"},{"location":"utils/package/#chebyshev-polynomials","text":"Chebyshev polynomials are another important class of orthogonal polynomials used in numerical analysis. There are two types, the first kind and second kind . //Calculate the Chebyshev polynomial of second kind order 3 val c23 = ( x : Double ) => utils . chebyshev ( 3 , x , kind = 2 ) c23 ( 2.5 )","title":"Chebyshev Polynomials"},{"location":"utils/package/#quick-select","text":"The quick select aims to find the k^{th} k^{th} smallest element of a list of numbers. val second = utils . quickselect ( List ( 3 , 2 , 4 , 5 , 1 , 6 ), 2 )","title":"Quick Select"},{"location":"utils/package/#median","text":"val second = utils . median ( List ( 3 , 2 , 4 , 5 , 1 , 6 ))","title":"Median"},{"location":"utils/package/#sample-statistics","text":"Calculate the mean and variance (or covariance), minimum, maximum of a list of DenseVector [ Double ] instances. val data : List [ DenseVector [ Double ]] = _ val ( mu , vard ) : ( DenseVector [ Double ], DenseVector [ Double ]) = utils . getStats ( data ) val ( mean , cov ) : ( DenseVector [ Double ], DenseMatrix [ Double ]) = utils . getStatsMult ( data ) val ( min , max ) = utils . getMinMax ( data )","title":"Sample Statistics"},{"location":"utils/utils_data_transforms/","text":"Summary Some attribute transformations are included in the DynaML distribution, here we show how to use them. All of them inherit ReversibleScaler [ I ] trait. They are contained in the dynml.utils package. Gaussian Centering \u00b6 Gaussian scaling/centering involves calculating the sample mean and variance of data and applying a gaussian standardization operations using the calculated statistics. It has different implementations in slightly varying contexts. Univariate \u00b6 Univariate gaussian scaling involves \\begin{align} x &\\in \\mathbb{R} \\\\ \\mu &\\in \\mathbb{R} \\\\ \\sigma &\\in \\mathbb{R} \\\\ \\bar{x} &= \\frac{x-\\mu}{\\sigma} \\end{align} \\begin{align} x &\\in \\mathbb{R} \\\\ \\mu &\\in \\mathbb{R} \\\\ \\sigma &\\in \\mathbb{R} \\\\ \\bar{x} &= \\frac{x-\\mu}{\\sigma} \\end{align} val mean = - 1.5 val sigma = 2.5 val ugs = UnivariateGaussianScaler ( mean , sigma ) val x = 3.0 val xs = ugs ( x ) val xhat = ugs . i ( xs ) Multivariate \u00b6 The data attributes form components of a vector, in this case we can assume each component is independent and calculate the diagonal variance or compute all the component covariances in the form of a symmetric matrix. \\begin{align} x &\\in \\mathbb{R}^n \\\\ \\mu &\\in \\mathbb{R}^n \\\\ \\Sigma &\\in \\mathbb{R}^{n \\times n}\\\\ L L^\\intercal &= \\Sigma \\\\ \\bar{x} &= L^{-1} (x - \\mu) \\end{align} \\begin{align} x &\\in \\mathbb{R}^n \\\\ \\mu &\\in \\mathbb{R}^n \\\\ \\Sigma &\\in \\mathbb{R}^{n \\times n}\\\\ L L^\\intercal &= \\Sigma \\\\ \\bar{x} &= L^{-1} (x - \\mu) \\end{align} Diagonal \u00b6 In this case the sample covariance matrix calculated from the data is diagonal and neglecting the correlations between the attributes. \\Sigma = \\begin{pmatrix} \\sigma^{2}_1 & \\cdots & 0\\\\ \\vdots & \\ddots & \\vdots\\\\ 0 & \\cdots & \\sigma^{2}_n \\end{pmatrix} \\Sigma = \\begin{pmatrix} \\sigma^{2}_1 & \\cdots & 0\\\\ \\vdots & \\ddots & \\vdots\\\\ 0 & \\cdots & \\sigma^{2}_n \\end{pmatrix} val mean : DenseVector [ Double ] = DenseVector (- 1.5 , 1.5 , 0.25 ) val sigma : DenseVector [ Double ] = DenseVector ( 0.5 , 2.5 , 1.0 ) val gs = GaussianScaler ( mean , sigma ) val x : DenseVector [ Double ] = DenseVector ( 0.2 , - 3.5 , - 1.5 ) val xs = gs ( x ) val xhat = gs . i ( xs ) Full Matrix \u00b6 When the sample covariance matrix is calculated taking into account correlations between data attributes. val mean : DenseVector [ Double ] = DenseVector (- 1.5 , 1.5 , 0.25 ) val sigma : DenseMatrix [ Double ] = DenseMatrix ( ( 2.5 , 0.5 , 0.25 ), ( 0.5 , 3.5 , 1.2 ), ( 0.25 , 1.2 , 2.25 ) ) val mv_gs = MVGaussianScaler ( mean , sigma ) val x : DenseVector [ Double ] = DenseVector ( 0.2 , - 3.5 , - 1.5 ) val xs = mv_gs ( x ) val xhat = mv_gs . i ( xs ) Mean Centering \u00b6 Univariate \u00b6 \\begin{align} x &\\in \\mathbb{R} \\\\ \\mu &\\in \\mathbb{R} \\\\ \\bar{x} &= x-\\mu \\end{align} \\begin{align} x &\\in \\mathbb{R} \\\\ \\mu &\\in \\mathbb{R} \\\\ \\bar{x} &= x-\\mu \\end{align} val c = - 1.5 val ums = UnivariateMeanScaler ( c ) val x = 3.0 val xs = ums ( x ) val xhat = ums . i ( xs ) Multivariate \u00b6 \\begin{align} x &\\in \\mathbb{R}^n \\\\ \\mu &\\in \\mathbb{R}^n \\\\ \\bar{x} &= x - \\mu \\end{align} \\begin{align} x &\\in \\mathbb{R}^n \\\\ \\mu &\\in \\mathbb{R}^n \\\\ \\bar{x} &= x - \\mu \\end{align} val mean : DenseVector [ Double ] = DenseVector (- 1.5 , 1.5 , 0.25 ) val mms = MeanScaler ( mean ) val x : DenseVector [ Double ] = DenseVector ( 0.2 , - 3.5 , - 1.5 ) val xs = mms ( x ) val xhat = mms . i ( xs ) Min-Max Scaling \u00b6 Min-max scaling is also known as 0,1 0,1 scaling because attributes are scaled down to the domain [0, 1] [0, 1] . This is done by calculating the minimum and maximum of attribute values. val min : DenseVector [ Double ] = DenseVector (- 1.5 , 1.5 , 0.25 ) val max : DenseVector [ Double ] = DenseVector ( 0.5 , 2.5 , 1.0 ) val min_max_scaler = MinMaxScaler ( min , max ) val x : DenseVector [ Double ] = DenseVector ( 0.2 , - 3.5 , - 1.5 ) val xs = min_max_scaler ( x ) val xhat = min_max_scaler . i ( xs ) Principal Component Analysis \u00b6 Principal component analysis consists of projecting data onto the eigenvectors of its sample covariance matrix. val mean : DenseVector [ Double ] = DenseVector (- 1.5 , 1.5 , 0.25 ) val sigma : DenseMatrix [ Double ] = DenseMatrix ( ( 2.5 , 0.5 , 0.25 ), ( 0.5 , 3.5 , 1.2 ), ( 0.25 , 1.2 , 2.25 ) ) val pca = PCAScaler ( mean , sigma ) val x : DenseVector [ Double ] = DenseVector ( 0.2 , - 3.5 , - 1.5 ) val xs = pca ( x ) val xhat = pca . i ( xs ) Slicing scalers It is possible to slice the scalers shown above if they act on vectors. For example. //Slice on subset of columns val gs_sub : GaussianScaler = gs ( 0 to 1 ) //Slice on a single column val gs_last : UnivariateGaussianScaler = gs ( 2 )","title":"Data Transforms"},{"location":"utils/utils_data_transforms/#gaussian-centering","text":"Gaussian scaling/centering involves calculating the sample mean and variance of data and applying a gaussian standardization operations using the calculated statistics. It has different implementations in slightly varying contexts.","title":"Gaussian Centering"},{"location":"utils/utils_data_transforms/#univariate","text":"Univariate gaussian scaling involves \\begin{align} x &\\in \\mathbb{R} \\\\ \\mu &\\in \\mathbb{R} \\\\ \\sigma &\\in \\mathbb{R} \\\\ \\bar{x} &= \\frac{x-\\mu}{\\sigma} \\end{align} \\begin{align} x &\\in \\mathbb{R} \\\\ \\mu &\\in \\mathbb{R} \\\\ \\sigma &\\in \\mathbb{R} \\\\ \\bar{x} &= \\frac{x-\\mu}{\\sigma} \\end{align} val mean = - 1.5 val sigma = 2.5 val ugs = UnivariateGaussianScaler ( mean , sigma ) val x = 3.0 val xs = ugs ( x ) val xhat = ugs . i ( xs )","title":"Univariate"},{"location":"utils/utils_data_transforms/#multivariate","text":"The data attributes form components of a vector, in this case we can assume each component is independent and calculate the diagonal variance or compute all the component covariances in the form of a symmetric matrix. \\begin{align} x &\\in \\mathbb{R}^n \\\\ \\mu &\\in \\mathbb{R}^n \\\\ \\Sigma &\\in \\mathbb{R}^{n \\times n}\\\\ L L^\\intercal &= \\Sigma \\\\ \\bar{x} &= L^{-1} (x - \\mu) \\end{align} \\begin{align} x &\\in \\mathbb{R}^n \\\\ \\mu &\\in \\mathbb{R}^n \\\\ \\Sigma &\\in \\mathbb{R}^{n \\times n}\\\\ L L^\\intercal &= \\Sigma \\\\ \\bar{x} &= L^{-1} (x - \\mu) \\end{align}","title":"Multivariate"},{"location":"utils/utils_data_transforms/#diagonal","text":"In this case the sample covariance matrix calculated from the data is diagonal and neglecting the correlations between the attributes. \\Sigma = \\begin{pmatrix} \\sigma^{2}_1 & \\cdots & 0\\\\ \\vdots & \\ddots & \\vdots\\\\ 0 & \\cdots & \\sigma^{2}_n \\end{pmatrix} \\Sigma = \\begin{pmatrix} \\sigma^{2}_1 & \\cdots & 0\\\\ \\vdots & \\ddots & \\vdots\\\\ 0 & \\cdots & \\sigma^{2}_n \\end{pmatrix} val mean : DenseVector [ Double ] = DenseVector (- 1.5 , 1.5 , 0.25 ) val sigma : DenseVector [ Double ] = DenseVector ( 0.5 , 2.5 , 1.0 ) val gs = GaussianScaler ( mean , sigma ) val x : DenseVector [ Double ] = DenseVector ( 0.2 , - 3.5 , - 1.5 ) val xs = gs ( x ) val xhat = gs . i ( xs )","title":"Diagonal"},{"location":"utils/utils_data_transforms/#full-matrix","text":"When the sample covariance matrix is calculated taking into account correlations between data attributes. val mean : DenseVector [ Double ] = DenseVector (- 1.5 , 1.5 , 0.25 ) val sigma : DenseMatrix [ Double ] = DenseMatrix ( ( 2.5 , 0.5 , 0.25 ), ( 0.5 , 3.5 , 1.2 ), ( 0.25 , 1.2 , 2.25 ) ) val mv_gs = MVGaussianScaler ( mean , sigma ) val x : DenseVector [ Double ] = DenseVector ( 0.2 , - 3.5 , - 1.5 ) val xs = mv_gs ( x ) val xhat = mv_gs . i ( xs )","title":"Full Matrix"},{"location":"utils/utils_data_transforms/#mean-centering","text":"","title":"Mean Centering"},{"location":"utils/utils_data_transforms/#univariate_1","text":"\\begin{align} x &\\in \\mathbb{R} \\\\ \\mu &\\in \\mathbb{R} \\\\ \\bar{x} &= x-\\mu \\end{align} \\begin{align} x &\\in \\mathbb{R} \\\\ \\mu &\\in \\mathbb{R} \\\\ \\bar{x} &= x-\\mu \\end{align} val c = - 1.5 val ums = UnivariateMeanScaler ( c ) val x = 3.0 val xs = ums ( x ) val xhat = ums . i ( xs )","title":"Univariate"},{"location":"utils/utils_data_transforms/#multivariate_1","text":"\\begin{align} x &\\in \\mathbb{R}^n \\\\ \\mu &\\in \\mathbb{R}^n \\\\ \\bar{x} &= x - \\mu \\end{align} \\begin{align} x &\\in \\mathbb{R}^n \\\\ \\mu &\\in \\mathbb{R}^n \\\\ \\bar{x} &= x - \\mu \\end{align} val mean : DenseVector [ Double ] = DenseVector (- 1.5 , 1.5 , 0.25 ) val mms = MeanScaler ( mean ) val x : DenseVector [ Double ] = DenseVector ( 0.2 , - 3.5 , - 1.5 ) val xs = mms ( x ) val xhat = mms . i ( xs )","title":"Multivariate"},{"location":"utils/utils_data_transforms/#min-max-scaling","text":"Min-max scaling is also known as 0,1 0,1 scaling because attributes are scaled down to the domain [0, 1] [0, 1] . This is done by calculating the minimum and maximum of attribute values. val min : DenseVector [ Double ] = DenseVector (- 1.5 , 1.5 , 0.25 ) val max : DenseVector [ Double ] = DenseVector ( 0.5 , 2.5 , 1.0 ) val min_max_scaler = MinMaxScaler ( min , max ) val x : DenseVector [ Double ] = DenseVector ( 0.2 , - 3.5 , - 1.5 ) val xs = min_max_scaler ( x ) val xhat = min_max_scaler . i ( xs )","title":"Min-Max Scaling"},{"location":"utils/utils_data_transforms/#principal-component-analysis","text":"Principal component analysis consists of projecting data onto the eigenvectors of its sample covariance matrix. val mean : DenseVector [ Double ] = DenseVector (- 1.5 , 1.5 , 0.25 ) val sigma : DenseMatrix [ Double ] = DenseMatrix ( ( 2.5 , 0.5 , 0.25 ), ( 0.5 , 3.5 , 1.2 ), ( 0.25 , 1.2 , 2.25 ) ) val pca = PCAScaler ( mean , sigma ) val x : DenseVector [ Double ] = DenseVector ( 0.2 , - 3.5 , - 1.5 ) val xs = pca ( x ) val xhat = pca . i ( xs ) Slicing scalers It is possible to slice the scalers shown above if they act on vectors. For example. //Slice on subset of columns val gs_sub : GaussianScaler = gs ( 0 to 1 ) //Slice on a single column val gs_last : UnivariateGaussianScaler = gs ( 2 )","title":"Principal Component Analysis"}]}